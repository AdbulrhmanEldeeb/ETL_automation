subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[R] Sources: Reasons why KG outperformes RD in Retrievers?,"Are there any sources discussing WHY Retriever work better with KG in contrast to RD? I find it super intuitive to say its better because in knowledge graphs we have more semantic structure and relations are discovered effeciently. In my mind its ""of course the graph is richer/more dense"" but when collaborated on a paper, it struck me that I wasnt able to justify that claim. I found no source whatsoever that actually explained why that might be the case.

The only source i got was this one:  
[https://arxiv.org/abs/2311.07509](https://arxiv.org/abs/2311.07509)

also here in [](https://www.reddit.com/r/LocalLLaMA/) sub last year:  [https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a\_benchmark\_to\_understand\_the\_role\_of\_knowledge/](https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a_benchmark_to_understand_the_role_of_knowledge/)

So all we we're able to say was ""We justify our decision because KG works better then RD \[source to benchmark paper\]""

I would have loved to discuss why exactly KG are better suited and give arguments about information density, semantic strutuce or the better selection of related entities. But everything I found were only articles that threw around wild claims or pointed out easier/native implementation, which technically could also be achieved with RD.

Can anyone point me to sources? would love to read an in-depth discussion on the reasons of better performance.",PopPsychological4106,1h447eu,https://reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,https://www.reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,2024-12-01 14:17:17,6,0.64,6,0,9,0,0,False,False,True,False,False,Research,self,t3_1h447eu
MachineLearning,Augmentation for Images with ROI [D],"I have an Image with roi (x\_min,y\_min,x\_max,y\_max). I want do Random flip,rotate,skew, translate ,etc.. with torchvison. what are the different ways in which you can transform the roi respectively in order to match with the augmented image ?",Brief_Papaya121,1h41z2x,https://reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,https://www.reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,2024-12-01 12:11:25,3,0.71,3,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h41z2x
MachineLearning,[Discussion] Advice Needed: Rejected from COLING 2025 – Which Conference Should I Target Next?,"
I got a rejection from COLING 2025 with review scores of 4, 3, 3. I’m revising the manuscript and looking for advice on the next best NLP conference to target. Any suggestions for similar top-tier venues?

Thanks!",Cold-Traffic-7586,1h3igva,https://reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,https://www.reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,2024-11-30 18:08:01,4,0.75,4,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h3igva
MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

&gt;Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

&gt;Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&amp;#x200B;

Please remember that this community is geared towards those with experience.",AutoModerator,1h3u444,https://reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,2024-12-01 03:30:15,23,0.83,23,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h3u444
MachineLearning,What's the best Open Source Image-Upscaling Model? [Discussion],"I'm using [Playground-v2.5-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) to make some images for YouTube thumbnails. I'm really happy with the results:

[1024x1024 base image of mars base.](https://preview.redd.it/uuo4sdgwp44e1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=84d61bf4d7fbf2457df1037e95603166390efa12)

But I would like the image to be 1920x1080 pixels, and my only options are 1024x1024, or 1280x720 pixels. At the moment, I can get to 1920x1080 with Photoshop's outpainting:

[1920x1080 outpainted image of mars base.](https://preview.redd.it/07tt5ix4q44e1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=cdfb7cefa8d2bb4f187d2ab6b86aaba17596506a)

This is okay, but photoshops outpainting is manual and has a fairly significant quality drop. Ideally, I would generate an image in 1280x720 then upscale to 1920x1080 programmatically.

I've heard of the following models:

* Real-ERSGAN
* Waifu2
* SRGAN

But before I jump into any of them, what open-source model is generally considered best to achieve this? I have an RTX 3060 12GB of VRAM.",FPGA_Superstar,1h3qcon,https://reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,https://www.reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,2024-12-01 00:13:57,27,0.88,27,0,8,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ROcJ4oUu0I-1wtjwdAycnGfQ3s8tbIsJdyufqyhsVyI.jpg,t3_1h3qcon
MachineLearning,"[R] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",,moschles,1h40wms,https://reddit.com/r/MachineLearning/comments/1h40wms/r_qwenvl_a_versatile_visionlanguage_model_for/,https://storage.prod.researchhub.com/uploads/papers/2023/12/25/2308.12966.pdf,2024-12-01 10:59:28,7,1.0,7,0,0,0,0,False,False,False,False,False,Research,default,t3_1h40wms
MachineLearning,[P] TIME-MOE: Billion-Scale Time Series Forecasting with Mixture-of-Experts,"**Time-MOE** is a 2.4B parameter open-source time-series foundation model using **Mixture-of-Experts (MOE)** for zero-shot forecasting.

You can find an analysis of the model [here](https://aihorizonforecast.substack.com/p/time-moe-billion-scale-time-series)",nkafr,1h3j1cm,https://reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,https://www.reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,2024-11-30 18:33:59,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1h3j1cm
MachineLearning,[P] A complete transformer model built in Excel,,Revolutionary-Way290,1h3hj6j,https://reddit.com/r/MachineLearning/comments/1h3hj6j/p_a_complete_transformer_model_built_in_excel/,https://x.com/ProfTomYeh/status/1859282491955130452,2024-11-30 17:25:45,1,1.0,1,0,2,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/JFO-VdcjE87RX0w6rQ5kwyN7HblXjJoN1QbFc-1p18M.jpg,t3_1h3hj6j
