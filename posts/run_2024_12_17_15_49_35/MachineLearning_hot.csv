subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1hevk2a,https://reddit.com/r/MachineLearning/comments/1hevk2a/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1hevk2a/d_simple_questions_thread/,2024-12-15 16:00:19,4,0.84,4,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1hevk2a
MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

&gt;Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

&gt;Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&amp;#x200B;

Please remember that this community is geared towards those with experience.",AutoModerator,1h3u444,https://reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,2024-12-01 03:30:15,33,0.89,33,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h3u444
MachineLearning,[R] Vector Search with OpenAI Embeddings: Lucene Is All You Need,"Recent research challenges the need for dedicated vector databases in AI-powered search. Researchers from the University of Waterloo and Roma Tre University propose using the widely-used Lucene search library with OpenAI embeddings as an alternative. This approach reduces resource demands and may offer a more accessible solution, encouraging organizations to reconsider specialized vector storage.

We explore the details in this article: [https://www.shaped.ai/blog/vector-search-lucene-is-all-you-need](https://www.shaped.ai/blog/vector-search-lucene-is-all-you-need)",skeltzyboiii,1hgd4j0,https://reddit.com/r/MachineLearning/comments/1hgd4j0/r_vector_search_with_openai_embeddings_lucene_is/,https://www.reddit.com/r/MachineLearning/comments/1hgd4j0/r_vector_search_with_openai_embeddings_lucene_is/,2024-12-17 15:30:53,22,0.84,22,0,0,0,0,False,False,True,False,False,Research,self,t3_1hgd4j0
MachineLearning,[R] SVGFusion: Scalable Text-to-SVG Generation via Vector Space Diffusion,"**Paper:** [https://arxiv.org/pdf/2412.10437](https://arxiv.org/pdf/2412.10437)

**Abstract:** 

&gt;The generation of Scalable Vector Graphics (SVG) assets from textual data remains a significant challenge, largely due to the scarcity of high-quality vector datasets and the limitations in scalable vector representations required for modeling intricate graphic distributions. This work introduces SVGFusion, a Text-to-SVG model capable of scaling to real-world SVG data without reliance on a text-based discrete language model or prolonged SDS optimization. The essence of SVGFusion is to learn a continuous latent space for vector graphics with a popular Text-to-Image framework. Specifically, SVGFusion consists of two modules: a Vector-Pixel Fusion Variational Autoencoder (VP-VAE) and a Vector Space Diffusion Transformer (VS-DiT). VP-VAE takes both the SVGs and corresponding rasterizations as inputs and learns a continuous latent space, whereas VS-DiT learns to generate a latent code within this space based on the text prompt. Based on VP-VAE, a novel rendering sequence modeling strategy is proposed to enable the latent space to embed the knowledge of construction logics in SVGs. This empowers the model to achieve human-like design capabilities in vector graphics, while systematically preventing occlusion in complex graphic compositions. Moreover, our SVGFusion's ability can be continuously improved by leveraging the scalability of the VS-DiT by adding more VS-DiT blocks. A large-scale SVG dataset is collected to evaluate the effectiveness of our proposed method. Extensive experimentation has confirmed the superiority of our SVGFusion over existing SVG generation methods, achieving enhanced quality and generalizability, thereby establishing a novel framework for SVG content creation. Code, model, and data will be released at: {[this https URL](https://ximinng.github.io/SVGFusionProject/)}

*(Note: so far, nothing has been released in the linked repo)*

**Visual Abstract:**

https://preview.redd.it/3ypl3eph1e7e1.png?width=1153&amp;format=png&amp;auto=webp&amp;s=90d0dab4a2311d7bd8037322bb265ce8a03becac

**Visual Highlights:**

https://preview.redd.it/ngz1kfen2e7e1.png?width=1175&amp;format=png&amp;auto=webp&amp;s=e8a49b15283c3514f7d2703ad631b8de6f63b5d2

https://preview.redd.it/o01j02fp2e7e1.png?width=635&amp;format=png&amp;auto=webp&amp;s=230f437f4513498916f317e2e2f7ccc7b0578e8c

https://preview.redd.it/3s4k4c1r2e7e1.png?width=657&amp;format=png&amp;auto=webp&amp;s=7402ee8bdbb8cc4d9225a48f33eebb7e051ec1fb

https://preview.redd.it/d4yi1bx23e7e1.png?width=1201&amp;format=png&amp;auto=webp&amp;s=9f115b99babd31998e23986238e41e91b49e1ff3

[Zoomed in, as per the suggestion](https://preview.redd.it/w6l2m9a53e7e1.png?width=265&amp;format=png&amp;auto=webp&amp;s=d663250708ec3ca4548bd106384aca515e019793)

https://preview.redd.it/gpatd9qy3e7e1.png?width=1279&amp;format=png&amp;auto=webp&amp;s=ac70bbb82f934109ec8388eeca65fd49d4b994df

https://preview.redd.it/6f40o3pz3e7e1.png?width=793&amp;format=png&amp;auto=webp&amp;s=a71f7ddcd311eefb365907ae067a5b287630253c

https://preview.redd.it/50gzr8p04e7e1.png?width=807&amp;format=png&amp;auto=webp&amp;s=265be1e5ce20aded6913d24db371b4380e8c92b1

",StartledWatermelon,1hg87em,https://reddit.com/r/MachineLearning/comments/1hg87em/r_svgfusion_scalable_texttosvg_generation_via/,https://www.reddit.com/r/MachineLearning/comments/1hg87em/r_svgfusion_scalable_texttosvg_generation_via/,2024-12-17 10:58:36,16,0.84,16,0,2,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/eX35PNM-kwI3yvC4QkhdLQcFKAfSric-90VlR847B0w.jpg,t3_1hg87em
MachineLearning,[P] Vision Parse: Parse PDF documents into Markdown formatted content using Vision LLMs,"Hey Redditors,

I'm excited to share Vision Parse - [https://github.com/iamarunbrahma/vision-parse](https://github.com/iamarunbrahma/vision-parse), an open-source Python library that uses Vision Language Models to convert PDF documents into perfectly formatted markdown content automatically.

* Converts each page in a PDF document into high-resolution images
* Detects texts, tables, links, and images from the high-resolution image using Vision LLMs and parses them in markdown format
* Handles multi-page PDF documents effortlessly
* And it's easy to get started with this library (just `pip install vision-parse`, and then a few lines of code to convert a document into markdown formatted content).

**Why I built this?**

* Traditional PDF to¬†markdown conversion tools often struggle with complex layouts, semi-structured and unstructured tables and formatting. Hence, relying on Vision LLMs to extract content in markdown from images (Here, I am converting each PDF page into an image).
* Document structure would get distorted with traditional OCR's and PDF to markdown conversion tools. Hence, using Generative AI models would help us in getting better understanding of the structure and preserve it.

You can find documentation to get started with this library here: [https://github.com/iamarunbrahma/vision-parse/blob/main/README.md](https://github.com/iamarunbrahma/vision-parse/blob/main/README.md)

View this [GitHub Project - Vision Parse](https://github.com/iamarunbrahma/vision-parse) and please provide me your feedback or any suggestions.",heliosarun,1hg5d3p,https://reddit.com/r/MachineLearning/comments/1hg5d3p/p_vision_parse_parse_pdf_documents_into_markdown/,https://www.reddit.com/r/MachineLearning/comments/1hg5d3p/p_vision_parse_parse_pdf_documents_into_markdown/,2024-12-17 07:20:37,13,0.88,13,0,6,0,0,False,False,True,False,False,Project,self,t3_1hg5d3p
MachineLearning,"  ""[Discussion]"" ""[D]"" Introducing TLR: Training AI Simultaneously Across Three Environments with Shared Learning","**TL;DR**: I developed **TLR (Triple Layer Training)**, a reinforcement learning framework that trains a single agent across **three environments simultaneously** while sharing experiences to enhance learning. It‚Äôs producing positive rewards where I‚Äôve never seen them before‚Äîlike Lunar Lander! Feedback and thoughts welcome.

Hi everyone! üëã

I wanted to share something I‚Äôve been working on: **Triple Layer Training (TLR)**‚Äîa novel reinforcement learning framework that allows an AI agent to **train across three environments simultaneously**.

# What is TLR?

* TLR trains a single agent in **three diverse environments** at once:
   * **Cart Pole**: Simple balancing task.
   * **Lunar Lander**: Precision landing with physics-based control.
   * **Space Invader**: Strategic reflexes in a dynamic game.
* The agent uses **shared replay buffers** to pool experiences across these environments, allowing it to **learn from one environment and apply insights to another**.
* TLR integrates **advanced techniques** like:
   * **DQN Variants**: Standard DQN, Double DQN (Lunar Lander), and Dueling DQN (Space Invader).
   * **Prioritized Replay**: Focus on critical transitions for efficient learning.
   * **Hierarchical Learning**: Building skills progressively across environments.

# Why is TLR Exciting?

* **Cross-Environment Synergy**: The agent improves in one task by leveraging knowledge from another.
* **Positive Results**: I‚Äôm seeing **positive rewards** in all three environments *simultaneously*, including Lunar Lander, where I‚Äôve never achieved this before!
* It pushes the boundaries of **generalization and multi-domain learning**‚Äîsomething I haven‚Äôt seen widely implemented.

# How Does It Work?

* Experiences from all three environments are combined into a **shared replay buffer**, alongside environment-specific buffers.
* The agent adapts using environment-appropriate algorithms (e.g., Double DQN for Lunar Lander).
* Training happens simultaneously across environments, encouraging **generalized learning** and **skill transfer**.

# Next Steps

I‚Äôve already integrated PPO into the Lunar Lander environment and plan to add curiosity-driven exploration (ICM) next. I believe this can be scaled to even more complex tasks and environments.

# Results and Code

If anyone is curious, I‚Äôve shared the framework on GitHub. [https://github.com/Albiemc1303/TLR\_Framework-.git](https://github.com/Albiemc1303/TLR_Framework-.git)  
You can find example logs and results there. I‚Äôd love feedback on the approach or suggestions for improvements!

# Discussion Questions

* Have you seen similar multi-environment RL implementations?
* What other environments or techniques could benefit TLR?
* How could shared experience buffers be extended for more generalist AI systems?

Looking forward to hearing your thoughts and feedback! I‚Äôm genuinely excited about how TLR is performing so far and hope others find it interesting.",UndyingDemon,1hg5siv,https://reddit.com/r/MachineLearning/comments/1hg5siv/discussion_d_introducing_tlr_training_ai/,https://www.reddit.com/r/MachineLearning/comments/1hg5siv/discussion_d_introducing_tlr_training_ai/,2024-12-17 07:52:37,12,0.73,12,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1hg5siv
MachineLearning,[R] Scaling test-time compute with open models!,"Hi! I'm Lewis, a researcher at Hugging Face üëã. Over the past months we‚Äôve been diving deep in trying to reverse engineer and reproduce several of key results that allow LLMs to ""think longer"" via test-time compute and are finally happy to share some of our knowledge.

Today we're sharing a detailed blog post on how we managed to outperform Llama 70B with Llama 3B on MATH by combining step-wise reward models with tree-search algorithms:

[https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)

In the blog post we cover:

* **Compute-optimal scaling:**¬†How we implemented¬†[u/GoogleDeepMind](https://x.com/GoogleDeepMind)¬†'s recipe to boost the mathematical capabilities of open models at test-time.
* **Diverse Verifier Tree Search (DVTS):**¬†An unpublished extension we developed to the verifier-guided tree search technique. This simple yet effective method improves diversity and delivers better performance, particularly at large test-time compute budgets.
* **Search and Learn: A**¬†lightweight toolkit for implementing search strategies with LLMs and built for speed with vLLM. You can check it out here:¬†[https://github.com/huggingface/search-and-learn](https://github.com/huggingface/search-and-learn)

Happy to answer questions!

https://preview.redd.it/cagfkzxria7e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=34f3a45dd056da19a6b1e6f03a53ff8283df7ba7

",lewtun,1hfw40o,https://reddit.com/r/MachineLearning/comments/1hfw40o/r_scaling_testtime_compute_with_open_models/,https://www.reddit.com/r/MachineLearning/comments/1hfw40o/r_scaling_testtime_compute_with_open_models/,2024-12-16 22:55:58,57,0.95,57,0,3,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/FUXc-nHqCbjs3rjTUG_TFu5uGlU6xkxGLRtPLTb9MZE.jpg,t3_1hfw40o
MachineLearning,[D] What's your favorite paper you've read this year and why? ,"Haven't made this thread in many years, but holiday travel demands are great and would love to have a repository of papers to read during it.",bin_und_zeit,1hfljy3,https://reddit.com/r/MachineLearning/comments/1hfljy3/d_whats_your_favorite_paper_youve_read_this_year/,https://www.reddit.com/r/MachineLearning/comments/1hfljy3/d_whats_your_favorite_paper_youve_read_this_year/,2024-12-16 15:26:39,192,0.98,192,0,42,0,0,False,False,True,False,False,Discussion,self,t3_1hfljy3
MachineLearning,[D] Learnable masking token Vision Transformer ,"
Hey people.
I have a dataset for EEG seizure detection.
Input is a multichannel spectrogram with shape (22,289,251).
Due do the dataset imbalance, I will do a specaug data augmentation (temporal,spatial and frequency masking)
For Classification I want to train a VisionTransformer.
Instead of masking the missing values with a constant values like (0,-1) I think about trying learnable missing tokens.
Has someone expirence with it and can recommend some good papers or example notebooks?
Thank u really much!",Significant-Joke5751,1hgbgmh,https://reddit.com/r/MachineLearning/comments/1hgbgmh/d_learnable_masking_token_vision_transformer/,https://www.reddit.com/r/MachineLearning/comments/1hgbgmh/d_learnable_masking_token_vision_transformer/,2024-12-17 14:11:38,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hgbgmh
MachineLearning,[R][D] What are the possible soft label for distilling different student and teacher LLMs?,"I want to explore knowledge distillation in LLMs. Seems like there are a lot of methods but the easiest one is where the student is simply a smaller version of the teacher. However, I want to explore where the student is a different model than the teacher. So first of all the vocabulary is different, is there some techniques so that we can compute loss with the soft label with KL divergence when the set of vocabulary is different? As during the distillation process, we might want to introduce new words from the teacher model to the student model.

I see that there are some other ways than using KL divergence as the soft loss in this case, maybe even distilling in a black box teacher setup. Is it possible to introduce new words into the student's vocabulary this way? Thank you",worthlesspineapple,1hg9qpm,https://reddit.com/r/MachineLearning/comments/1hg9qpm/rd_what_are_the_possible_soft_label_for/,https://www.reddit.com/r/MachineLearning/comments/1hg9qpm/rd_what_are_the_possible_soft_label_for/,2024-12-17 12:39:40,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hg9qpm
MachineLearning,[D] What methods have you done to significantly improve your LLM on multi-class classification?,"I had this task at work. I work at TikTok and for those who are not aware they're notorious for having unrealistic OKRs. I was expected to get my model to 90%+ F1 Macro when it was at 70% and I had human labeling team issues. I managed to hit the OKRs, but I didn't particularly like the route I went through cause it was very hacky in a lot of ways. Just wondering how you guys out there would approach this kind of problem and how you would try to get a big bump in metrics? The main issues I faced where:  
\- Very low data quality. Due to budgeting issues our human labeling team only has 60% accuracy (This was assessed by in-house QA agents)  
\- Low amount of data. The human labeling team was only able to label 10k rows per week and I had to train 4 models within 2 months.",DolantheMFWizard,1hg3ou9,https://reddit.com/r/MachineLearning/comments/1hg3ou9/d_what_methods_have_you_done_to_significantly/,https://www.reddit.com/r/MachineLearning/comments/1hg3ou9/d_what_methods_have_you_done_to_significantly/,2024-12-17 05:27:47,3,0.6,3,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hg3ou9
MachineLearning,[D] Evaluating the quality of LLM responses,"Hi all. I'm working on a project where I take multiple medical visit records and documents, and I feeding through an LLM and text clustering pipeline to extract all the unique medical symptoms, each with associated root causes and preventative actions (i.e. medication, treatment, etc...).

I'm at the end of my pipeline with all my results, and I am seeing that some of my generated results are very obvious and generalized. For example, one of my medical symptoms was excessive temperature and some of the treatment it recommended was drink lots of water and rest, which most people without a medical degree could guess.

I was wondering if there were any LLM evaluation methods I could use where I can score the root cause and countermeasure associated with a medical symptom, so that it scores the results recommending platitudes lower, while scoring ones with more unique and precise root causes and preventative actions higher. I was hoping to create this evaluation framework so that it provides a score to each of my results, and then I would remove all results that fall below a certain threshold.

I understand determining if something is generalized or unique/precise can be very subjective, but please let me know if there are ways to construct an evaluation framework to rank results to do this, whether it requires some ground truth examples, and how those examples can be constructed. Thanks for the help!",raikirichidori255,1hg77z2,https://reddit.com/r/MachineLearning/comments/1hg77z2/d_evaluating_the_quality_of_llm_responses/,https://www.reddit.com/r/MachineLearning/comments/1hg77z2/d_evaluating_the_quality_of_llm_responses/,2024-12-17 09:44:41,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hg77z2
MachineLearning,[P] Graph-Based Editor for LLM Workflows,"We made an open-source tool that provides a graph-based interface for building, debugging, and evaluating LLM workflows: [https://github.com/PySpur-Dev/PySpur](https://github.com/PySpur-Dev/PySpur)

**Why we built this:**

Before this, we built several LLM-powered applications that collectively served thousands of users. The biggest challenge we faced was ensuring reliability: making sure the workflows were robust enough to handle edge cases and deliver consistent results.

In practice, achieving this reliability meant repeatedly:

1. **Breaking down complex goals into simpler steps:** Composing prompts, tool calls, parsing steps, and branching logic.
2. **Debugging failures:** Identifying which part of the workflow broke and why.
3. **Measuring performance:** Assessing changes against real metrics to confirm actual improvement.

We tried some existing observability tools or agent frameworks and they fell short on at least one of these three dimensions. We wanted something that allowed us to iterate quickly and stay focused on improvement rather than wrestling with multiple disconnected tools or code scripts.

We eventually arrived at three principles upon which we built PySpur :

1. **Graph-based interface:** We can lay out an LLM workflow as a node graph. A node can be an LLM call, a function call, a parsing step, or any logic component. The visual structure provides an instant overview, making complex workflows more intuitive.
2. **Integrated debugging:** When something fails, we can pinpoint the problematic node, tweak it, and re-run it on some test cases right in the UI.
3. **Evaluate at the node level:** We can assess how node changes affect performance downstream.

We hope it's useful for other LLM developers out there, enjoy!",Brilliant-Day2748,1hfr4sg,https://reddit.com/r/MachineLearning/comments/1hfr4sg/p_graphbased_editor_for_llm_workflows/,https://www.reddit.com/r/MachineLearning/comments/1hfr4sg/p_graphbased_editor_for_llm_workflows/,2024-12-16 19:23:32,14,0.84,14,0,2,0,0,False,False,True,False,False,Project,self,t3_1hfr4sg
MachineLearning,[D] Discussion: Struggling with Cloud Costs for ML ‚Äì Anyone Else Facing This?,"Hey guys,

I‚Äôm sure some of you have faced the challenge of dealing with the high costs of renting cloud resources to train large language models. As a machine learning enthusiast living in a third world country, the cost becomes unsustainable pretty quickly, and it‚Äôs hard to justify.

I‚Äôm curious has anyone else run into this issue? How are you handling the cost of training models, or are you finding alternative ways to get the performance you need for ML tasks without breaking the bank?

Would love to hear your thoughts and experiences!",DMortal139,1hg2h81,https://reddit.com/r/MachineLearning/comments/1hg2h81/d_discussion_struggling_with_cloud_costs_for_ml/,https://www.reddit.com/r/MachineLearning/comments/1hg2h81/d_discussion_struggling_with_cloud_costs_for_ml/,2024-12-17 04:15:53,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hg2h81
MachineLearning,"[D] Guidance on Fine-Tuning LLM for Custom Writing Task: Model, GPU, and Cloud Platform Considerations","Hi all, I want to finetune a LLM for a custom writing task that involves generating similarly structured \~500-word pieces based on specific examples. Given that I just have a regular PC, I figured that I need to finetune on some cloud platform. I was wondering:

1. What model/# of parameters should I be looking to use
2. What GPU/VRAM should I rent, and how long can I expect the training process to take
3. And finally, is there anything else I need to consider besides the aspects I've mentioned to go ahead and start trying to do this

I have a dataset of around 500‚Äì1,000 high-quality samples. I can possibly increase the sample size, but it would take more work.

Let me know if I need to provide more info. Thanks.",Rqees,1hg0z0k,https://reddit.com/r/MachineLearning/comments/1hg0z0k/d_guidance_on_finetuning_llm_for_custom_writing/,https://www.reddit.com/r/MachineLearning/comments/1hg0z0k/d_guidance_on_finetuning_llm_for_custom_writing/,2024-12-17 02:53:46,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hg0z0k
MachineLearning,[P] I made wut ‚Äì a CLI that explains your last command using a LLM,,jsonathan,1hew6wy,https://reddit.com/r/MachineLearning/comments/1hew6wy/p_i_made_wut_a_cli_that_explains_your_last/,https://i.redd.it/nwo0a660h17e1.gif,2024-12-15 16:29:00,488,0.95,488,0,29,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/n6rBdpFgqI8fg41ShW7IZmHAlTJez92XiIXOoxXZklI.jpg,t3_1hew6wy
MachineLearning,[D] Autoencoder training on analog signals using small datasets,"I think I am sitting with kind of a unique situations where I am in the processes of training an Autoencoder from a STFT of an analog signal. The challenge lies in the fact that I have a very small dataset of around 500 samples. This leads to the Autoencoder overfitting on the data of an event and not being able to reconstruct a non-event.

https://preview.redd.it/hxcf8q3wg87e1.png?width=1978&amp;format=png&amp;auto=webp&amp;s=d86457e7d23fcde6ae7e03e41a986150d5817ee0

What are some methods that I can use to prevent this. I am currently making use of regularization as well as dropout. I have played with the structure of the Autoencoder which has mixed results where just nothing is being reconstructed. I am unsure what the effects will be if one applies normal image based methods to enhance the dataset as this wont be the same with the analog signal?



Any advice on what method can be used to improve the training accuracy as well as the reconstruction?",Sea_Onion41,1hfmcau,https://reddit.com/r/MachineLearning/comments/1hfmcau/d_autoencoder_training_on_analog_signals_using/,https://www.reddit.com/r/MachineLearning/comments/1hfmcau/d_autoencoder_training_on_analog_signals_using/,2024-12-16 16:01:41,4,0.75,4,0,11,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/Xi4F13AwujjE6GNZCNBR4j6pSo12xzg24W5nCsfbrLE.jpg,t3_1hfmcau
MachineLearning,[R] Optimizing LLM merging to reduce performance tradeoffs,"We just released our new [work](https://huggingface.co/papers/2412.04144) on large-scale merging across checkpoints trained with different hyperparameters, data mixture, objectives, etc. to optimize tradeoffs. For instance, some models that perform well on code generation do worse on instruction following and vice versa. An interesting question to ask whether model merging can lead to a Pareto-optimal model in such setup.

  
**TL;DR:**

AS you're developing an LLM, it is common to obtain different checkpoints, where each excels at a single or a group of tasks/capabilities. Typically, you can keep tuning your hyperparameters until you obtain a Pareto-Optimal model, but the process can be super expensive. We show that you can collect all these models into a pool, and optimize merging parameters to reduce the task tradeoffs. We show that simple linear merging can yield Pareto-Optimal models in different tradeoff scenarios---better tradeoffs than individual models and strong merging baselines. Interestingly, our optimized merges outperform the original models in some cases. Our study uses large-scale models (Command R+ 104B from Cohere) and explores merging up to 16 checkpoints from SFT/RLHF training. 

https://preview.redd.it/3b2her78d57e1.png?width=2269&amp;format=png&amp;auto=webp&amp;s=3974e5b7bbf2f1b5b602eb6aa3a4092978b286d5

  
Happy to hear your thoughts.

  
üìÑ Paper: [https://huggingface.co/papers/2412.04144](https://huggingface.co/papers/2412.04144)

üßµ Twitter/X: [thread](https://x.com/MKhalifaaaa/status/1866126245014200467?t=2MmryY1FWBz1Ddk_OQ392w&amp;s=19)",moyle,1hfc8s5,https://reddit.com/r/MachineLearning/comments/1hfc8s5/r_optimizing_llm_merging_to_reduce_performance/,https://www.reddit.com/r/MachineLearning/comments/1hfc8s5/r_optimizing_llm_merging_to_reduce_performance/,2024-12-16 05:33:52,27,0.92,27,0,4,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/2JsEIjnFlrbmDXzNhq_s1Yhp8kJ_lpHHMm5Jj7UKxDw.jpg,t3_1hfc8s5
MachineLearning,[N] Special session on Privacy-Preserving Machine and Deep Learning at IJCNN 2025 ,"The special session on Privacy-Preserving Machine and Deep Learning will be held at the 2025 International Joint Conference on Neural Networks (IJCNN), which will be hosted from 30 June to 5 July 2025 in the beautiful city of Rome! üçù ¬†üéâ

The special session invites papers presenting innovative uses of privacy-preserving techniques (such as Homomorphic Encryption) for AI applications, as well as algorithms, hardware, and ethic contributions. This includes, but is not limited to, the design of new families of ML and DL models for Privacy-Preserving Machine and Deep Learning (PP-MDL) as well as the introduction of innovative application scenarios for PP-MDL.

We are thrilled to receive your submissions! The deadline is 15 January 2025. üöÄ",DuckMySick12,1hfjav8,https://reddit.com/r/MachineLearning/comments/1hfjav8/n_special_session_on_privacypreserving_machine/,https://www.reddit.com/r/MachineLearning/comments/1hfjav8/n_special_session_on_privacypreserving_machine/,2024-12-16 13:40:13,5,0.78,5,0,0,0,0,False,False,True,False,False,News,self,t3_1hfjav8
MachineLearning,[R] Developing a new optimization algorithm that will heavily change ML as a whole. Gradient descent has met its end. Here are the results:,"Microsolve (inspired by micrograd) works by actually solving parameters (instead of differentiating them w.r.t objectives) and does not require a loss function. It addresses a few drawbacks from SGD, namely, having to properly initialize parameters or the network blows up. Differentiation comes as a problem when values lie on a constant or steep slope. Gradients explode and diminish to negligible values as you go deeper. Proper preparation of data is needed to feed into the network (like normalisation etc.), and lastly, as most would argue against this, training with GD is really slow.

With microsolve, initialization does not matter (you can set parameter values to high magnitudes), gradients w.r.t losses are not needed, not even loss functions are needed. A learning rate is almost always not needed, if it is needed, it is small (to reduce response to noise). You simply apply a raw number at the input (no normalisation) and a raw number at the output (no sophisticated loss functions needed), and the model will fit to the data.

I created a demo application where i established a simple network for gradient descent and microsolve. The network takes the form of a linear layer (1 in, 8 out), followed by a tanh activation, and another linear layer afterwards (8 in, 1 out). Here is a visualisation of the very small dataset:

https://preview.redd.it/t3pd4kpccd7e1.png?width=731&amp;format=png&amp;auto=webp&amp;s=ad03c3caf340a5b92aa24612ee7b5be963167a56

The model has to create a line to fit to all these data points. I only allowed 50 iterations (that makes a total of 50x3 forward passes) of each example into the neural networks, I went easy on GD so i normalised the input, MS didnt need any preparation. Here are the results:

GD:

https://preview.redd.it/5sf8do9fcd7e1.png?width=718&amp;format=png&amp;auto=webp&amp;s=9c232b062b1bb50aa01ef3efc73cde133b8ad28a

Not bad.

MS:

https://preview.redd.it/rfliuuqkcd7e1.png?width=749&amp;format=png&amp;auto=webp&amp;s=9a1e48f7925d3f533ced305ba9ded5f0b9b5dd6b

With precision, 0 loss achieved in under 50 iterations.

I have to point out though, that MS is still under development. On certain runs, as it solves parameters, they explode (their solutions grow to extremely high numbers), but sometimes this ""explosion"" is somewhat repaired and the network restabilises.

Comment your thoughts.

  
Edit: 

Apparantly people are allergic to overfitting, so i did early stopping with MS. It approximated this function in 1 forward pass of each data point. i.e. it only got to see a coordinate once: 

https://preview.redd.it/ogb71yd9re7e1.png?width=720&amp;format=png&amp;auto=webp&amp;s=7c9c43668c2fee59db74db4e2d97bb8abc13dbe8

Sees a coordinate thrice: 

https://preview.redd.it/icfa32lgre7e1.png?width=745&amp;format=png&amp;auto=webp&amp;s=ef7009e4265d2939abc637cb05da267273b21229

",Relevant-Twist520,1hg67pz,https://reddit.com/r/MachineLearning/comments/1hg67pz/r_developing_a_new_optimization_algorithm_that/,https://www.reddit.com/r/MachineLearning/comments/1hg67pz/r_developing_a_new_optimization_algorithm_that/,2024-12-17 08:24:53,0,0.17,0,0,23,0,0,False,False,True,False,False,Research,https://a.thumbs.redditmedia.com/ipMSdVwp7rTrxatrJ9-NfMw0eUilfnfhaPoM0fVVIZ8.jpg,t3_1hg67pz
MachineLearning,[D] Understanding the Technology Behind Chat GPT‚Äôs Agent Routing ,"Good morning. When talking with ChatGPT, you‚Äôll notice it routes to different agents. This means that if you want to create an image using DALL¬∑E, it automatically identifies your intent and directs you to that agent. I was wondering what technology is behind this. Is it a small transformer model that classifies prompts and sends them to the corresponding agents? Or perhaps a smaller LLM that routes to the agents using a structured output?

I‚Äôm building a router that needs to decide which agent to use, and I‚Äôd love to know how other companies are handling this. Thank you so much.",hardyy_19,1hffplg,https://reddit.com/r/MachineLearning/comments/1hffplg/d_understanding_the_technology_behind_chat_gpts/,https://www.reddit.com/r/MachineLearning/comments/1hffplg/d_understanding_the_technology_behind_chat_gpts/,2024-12-16 09:49:36,6,0.61,6,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1hffplg
MachineLearning,[D] Synthetic tabular data augmentation/generation using GANs,"I was tasked with training a GAN to generate synthetic tabular data in the brain imaging field from a dataset with only ~80-100 entries. Being unfamiliar with this space, I'm wondering if using GANs is the right way to approach this problem given the size of my dataset and whether there are more effective solutions out there? TIA",InfinityZeroFive,1hfgxfe,https://reddit.com/r/MachineLearning/comments/1hfgxfe/d_synthetic_tabular_data_augmentationgeneration/,https://www.reddit.com/r/MachineLearning/comments/1hfgxfe/d_synthetic_tabular_data_augmentationgeneration/,2024-12-16 11:18:42,4,0.67,4,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1hfgxfe
MachineLearning,"[R] Quantifying/ visualizing the activity of the forget, input and output gate in lstms for interpretability. ","

Has there been working quantifying how much inforyis forgotten and retained in lstms or gru models for interpretability reasons. For instance it would be interesting to see if the model uses the hidden vector more for some examples over the other showing more need for past information in some examples. ",Sandy_dude,1hfcbzl,https://reddit.com/r/MachineLearning/comments/1hfcbzl/r_quantifying_visualizing_the_activity_of_the/,https://www.reddit.com/r/MachineLearning/comments/1hfcbzl/r_quantifying_visualizing_the_activity_of_the/,2024-12-16 05:39:34,9,0.91,9,0,0,0,0,False,False,True,False,False,Research,self,t3_1hfcbzl
MachineLearning,[R] Multi-sources rich social media open data - a full month of chatters,"Hey, data enthusiasts and web scraping aficionados!  
We‚Äôre thrilled to share a massive new social media dataset that just dropped on Hugging Face! üöÄ

# Access the Data:

# [üëâExorde Social Media One Month 2024](https://huggingface.co/datasets/Exorde/exorde-social-media-one-month-2024)

# What‚Äôs Inside?

* **Scale**: 270 million posts collected over one month (Nov 14 - Dec 13, 2024)
* **Methodology**: Total sampling of the web, statistical capture of **all topics**
* **Sources**: 6000+ platforms including Reddit, Twitter, BlueSky, YouTube, Mastodon, Lemmy, and more
* **Rich Annotations**: Original text, metadata, emotions, sentiment, top keywords, and themes
* **Multi-language**: Covers 122 languages with translated keywords
* **Unique features:** English top keywords, allowing super-quick statistics, trends/time series analytics!
* **Source**: At Exorde Labs, we are processing \~4 billion posts per year, or 10-12 million every 24 hrs.

# Why This Dataset Rocks

This is a goldmine for:

* Trend analysis across platforms
* Sentiment/emotion research (algo trading, OSINT, disinfo detection)
* NLP at scale (language models, embeddings, clustering)
* Studying information spread &amp; cross-platform discourse
* Detecting emerging memes/topics
* Building ML models for text classification

Whether you're a startup, data scientist, ML engineer, or just a curious dev, this dataset has something for everyone. It's perfect for both serious research and fun side projects.  Do you have questions or cool ideas for using the data? Drop them below.

We‚Äôre processing over 300 million items monthly at Exorde Labs‚Äîand we‚Äôre excited to support open research with this Xmas gift üéÅ. Let us know your ideas or questions below‚Äîlet‚Äôs build something awesome together!

Happy data crunching!

*Exorde Labs Team - A unique network of smart nodes collecting data like never before*",Exorde_Mathias,1hfh8vt,https://reddit.com/r/MachineLearning/comments/1hfh8vt/r_multisources_rich_social_media_open_data_a_full/,https://www.reddit.com/r/MachineLearning/comments/1hfh8vt/r_multisources_rich_social_media_open_data_a_full/,2024-12-16 11:40:19,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1hfh8vt
MachineLearning,[D] What is the meaning behind the values for the Scaling Layer in LPIPS Implementation? ,"I have been reading the paper ""The unreasonable effectiveness of deep features as a perceptual metric"" and was looking into the implementation. They have used a Scaling Layer

https://preview.redd.it/ctwnc4gsk47e1.png?width=1560&amp;format=png&amp;auto=webp&amp;s=45ba5e41249d4af1cb05e53c6855531fbccd544f

However, I thought the mean and Standard Deviation for Imagenet normalization was `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]` . Curious as to how those values came by? ",Snoo_65491,1hf9ido,https://reddit.com/r/MachineLearning/comments/1hf9ido/d_what_is_the_meaning_behind_the_values_for_the/,https://www.reddit.com/r/MachineLearning/comments/1hf9ido/d_what_is_the_meaning_behind_the_values_for_the/,2024-12-16 02:55:39,3,0.62,3,0,3,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/mD6UXEW97TsuUHsdzE4kCkROfHzBMbejnOwEAU1KHrE.jpg,t3_1hf9ido
MachineLearning,[D] What do you do while your model is training?,"I am bascilly baby sitting my model while it is training, watch some *House M.D.* or play some minecraft. I have done all my literture review and paper writting, what should I do now while my model is training?",Striking-Warning9533,1hemhil,https://reddit.com/r/MachineLearning/comments/1hemhil/d_what_do_you_do_while_your_model_is_training/,https://www.reddit.com/r/MachineLearning/comments/1hemhil/d_what_do_you_do_while_your_model_is_training/,2024-12-15 06:11:22,133,0.9,133,0,76,0,0,False,False,True,False,False,Discussion,self,t3_1hemhil
MachineLearning,[D] Preparing for a Computer Vision Interview: Focus on Classical CV Knowledge,"Hello everyone!

I hope you're all doing well. I have an upcoming interview for a startup for a mid-senior Computer Vision Engineer role in Robotics. The position requires a strong focus on both classical computer vision and 3D point cloud algorithms, in addition to deep learning expertise.

For the classical computer vision and 3D point cloud aspects, I need to review topics like feature extraction and matching, 6D pose estimation, image and point cloud registration, and alignment. Do you have any tips on how to efficiently review these concepts, solve related problems, or practice for this part of the interview? Any specific resources, exercises, or advice would be highly appreciated. Thanks in advance!",DeepBlue-96,1hfgt3m,https://reddit.com/r/MachineLearning/comments/1hfgt3m/d_preparing_for_a_computer_vision_interview_focus/,https://www.reddit.com/r/MachineLearning/comments/1hfgt3m/d_preparing_for_a_computer_vision_interview_focus/,2024-12-16 11:10:12,0,0.33,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hfgt3m
MachineLearning,How can I evaluate the performance of my poetry generation model based on how well it follows stylistic guidelines? [D],"So, I'm creating a machine learning system that generates poems based on inputs telling it the style of a poet to follow. I have been trying to evaluate the performance of this model but, most objective evaluation metrics assign scores based on how close the generated poet is to a sample of the poets poems. This is not helpful because l'm trying to see how closely the style matches not wording. Any suggestions?",SkyRevolutionary275,1hf7l8n,https://reddit.com/r/MachineLearning/comments/1hf7l8n/how_can_i_evaluate_the_performance_of_my_poetry/,https://www.reddit.com/r/MachineLearning/comments/1hf7l8n/how_can_i_evaluate_the_performance_of_my_poetry/,2024-12-16 01:14:53,1,0.55,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hf7l8n
MachineLearning,[D] Issue with ArXiv,"Hi, I‚Äôm trying to publish a paper on arXiv, but the uploaded PDF contains typos that aren‚Äôt present in my Overleaf version. Has anyone experienced this issue before? Any advice would be appreciated. Thank you!",Shiptonm25,1hfl19n,https://reddit.com/r/MachineLearning/comments/1hfl19n/d_issue_with_arxiv/,https://www.reddit.com/r/MachineLearning/comments/1hfl19n/d_issue_with_arxiv/,2024-12-16 15:02:40,0,0.27,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hfl19n
MachineLearning,Memory Requierements in fine-tune LLMs [D],"Hi, I am trying to fine-tune meta-llama/Llama-3.2-1B-Instruct. I loaded the model in 4-bit precision using the Transformers library and applying the LoRA method using the PEFT library and TRL. The issue comes when I start the training step, as I am permanently running out of memory, and I don‚Äôt know why. These are my training arguments:

training\_args = SFTConfig(  
output\_dir='/content/results',  
num\_train\_epochs=5,  
per\_device\_train\_batch\_size=1,  
per\_device\_eval\_batch\_size=1,  
gradient\_accumulation\_steps=2,  
learning\_rate=2e-4,  
bf16=True,  
logging\_steps=50,  
eval\_strategy='steps',  
eval\_steps=500,  
save\_strategy=""steps"",  
save\_steps=500,  
warmup\_steps=100,  
weight\_decay=0.01,  
logging\_dir=""/content/logs"",  
packing=True,  
report\_to=""none""  
)

trainer = SFTTrainer(  
model=model,  
train\_dataset=templated\_dataset\['train'\],  
eval\_dataset=templated\_dataset\['test'\],  
args=training\_args,  
tokenizer=tokenizer,

)

The sequence length is 2048, and the parameters to train are 1,179,648 (LoRA). I calculated that I will need around 3.57GB, but with 15GB which I have, I am running out of memory. I don‚Äôt know if there is something wrong with my training arguments configurations. Can you help me, please? Thanks in advance.",Top-Leave-7564,1hf58kj,https://reddit.com/r/MachineLearning/comments/1hf58kj/memory_requierements_in_finetune_llms_d/,https://www.reddit.com/r/MachineLearning/comments/1hf58kj/memory_requierements_in_finetune_llms_d/,2024-12-15 23:18:46,0,0.4,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hf58kj
MachineLearning,[P] Curated list of LLM papers 2024,,seraschka,1he4htl,https://reddit.com/r/MachineLearning/comments/1he4htl/p_curated_list_of_llm_papers_2024/,https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list,2024-12-14 14:52:26,155,0.94,155,0,11,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/p02HYYoPj_p-xBwJelLubWz-Tz9jT7FTYlhXB71RrBo.jpg,t3_1he4htl
MachineLearning,[D] What happened at NeurIPS?,,howtorewriteaname,1hdxbru,https://reddit.com/r/MachineLearning/comments/1hdxbru/d_what_happened_at_neurips/,https://i.redd.it/k0q9frsuir6e1.jpeg,2024-12-14 07:00:07,607,0.93,607,0,561,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/-xSH1Mpa62Tf46vUqQSb2RCTHhGdGGFO4WLolSj0-WI.jpg,t3_1hdxbru
MachineLearning,[D] What are the (un)written rules of deep learning training ,"Disclaimer: I posted this in r/learnmachinelearing first, but the sub seems to be more concerned with very basic questions, courses and hiring, so feel free to remove it if it doesn't fit here (tho I think that also fits this sub as a discussion).

I now have a few years of experience building and training different model architectures, I know most of the basic theory and am able to follow most papers. So my question goes into a more methodological direction. While I am able to successfully build models for a number of applications, a lot of the time this is to a large extend guesswork. I try out different stuff and see what sticks. I know there is a lot of research in the direction of interpretability going on, but this is not directly the direction I want to go with this. Instead I want to ask you all what general advice you have on the training process, what are some practical observations, rules of thumb, approaches you take that are not described in a paper or theoretical ml class. For example:

- How do you analyze gradients in your model. I know how to do some very basic plots in this regard, but would be interested in your methods and how you read them from a practical perspective?

- How do you visualize temporal instabilities between optimizer steps resulting from e.g. a too large learning rate?

- How do you determine appropriate regularization?

- What are your rules of thumb for diminisheing returns during a training run?

- How do you tune your hyperparameters? I eyeballed them more or less and also used optuna for this in the past.

- What are some important intuitions, unwritten rules and pitfalls during training in your opinion?

- What are your debugging steps when a model does not perform as expected?

- What tricks do you actually use? There are lots of small tricks (EMA, obscure activation functions, ...) that promise some gains, but what do you actually use?

- How does your approach differ when you do a transformer, CNN, diffusion model, ...

- Some general opinions or tips that I might have missed above. 

University classes and online resources mostly teach the basics or theoretical foundation, which is very important, but in practice only part of the story. Real world experience also helps, but you only get so far with trial and error and might miss something useful.
I am aware of the blog posts by Karpathy on the training of neural networks and look for more resources in this direction.

I am happy to here your replies on this arguably broad topic. ",floriv1999,1he07vr,https://reddit.com/r/MachineLearning/comments/1he07vr/d_what_are_the_unwritten_rules_of_deep_learning/,https://www.reddit.com/r/MachineLearning/comments/1he07vr/d_what_are_the_unwritten_rules_of_deep_learning/,2024-12-14 10:29:02,166,0.95,166,0,38,0,0,False,False,True,False,False,Discussion,self,t3_1he07vr
MachineLearning,"[Project] Matrix Recurrent States, a Attention Alternative","[https://github.com/mikayahlevi/mru-lm](https://github.com/mikayahlevi/mru-lm)  
Hi, I'm posting here to share a project I just published on GitHub. I'll start with a description, some of which will be copy/pasted from the GitHub repo.  
The idea of a matrix recurrent unit is dictated by the update rule H\_t = H\_{t-1} X\_{t-1} and H\_1 = X\_1 where X and H are s√ón√ón sequences of square matrices. The primary difference between this and a traditional RNN is that no initial vector is passed through the linears, instead the first state is a matrix, leading to the output also being a matrix. My motivation for coming up with this idea are based on the following reasons:

* Matrix multiplication is associative but not commutative. The associativity means I can compute the cumulative matrix product using an (inclusive) parallel scan. The lack of commutativity means that the order of tokens is automatically incorporated into the MRU.
* When you try to do this scan on an traditional RNN, the number of operations scales cubically with the amount of elements in the output state, meaning that limited information is retained compared to the amount of computation. On the other hand, if the states are matrices, the number of operations as a function of elements in the output state is (n\^2)\^(3/2), where n\^2 is the number of elements in the square n√ón matrix state. Here's a paper including some information about this: https://arxiv.org/abs/1709.04057.
* When processing the tokens sequentially or in parallel with the (not-yet implemented) Brent-Kung parallel scan the network scales linearly with time, in contrast to attention which scales quadratically with time.

I tried generating matrix X by different methods in the different branches. All of the ways to generate X and fold the output hidden state back into a vector, are arbitrary combinations of linears and reshapes and just based on what I found worked well.

[Loss vs Steps for a Transformer and an MRU-LM on shakespeare-char](https://preview.redd.it/6zyiw7g9vu6e1.png?width=640&amp;format=png&amp;auto=webp&amp;s=34fd01cc5bacde21148fd324203aa40e7c977bf7)

This approach seems to work pretty well based on the toy dataset shakespeare-char. I would appreciate if anyone can help me train the model on larger datasets and further evaluate it. ",IonizedPro,1he8vhw,https://reddit.com/r/MachineLearning/comments/1he8vhw/project_matrix_recurrent_states_a_attention/,https://www.reddit.com/r/MachineLearning/comments/1he8vhw/project_matrix_recurrent_states_a_attention/,2024-12-14 18:19:00,23,0.87,23,0,6,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/FPyevy8dYq_k30mDGwYuuwI2dD9VW5N9KXVtVgami5M.jpg,t3_1he8vhw
MachineLearning,"[D] Last Week in Medical AI: Top LLM Research Papers/Models (December 7 - December 14, 2024)","[\[D\] Last Week in Medical AI: Top LLM Research Papers\/Models \(December 7 - December 14, 2024\)](https://preview.redd.it/o23fp3csj07e1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=69e19fc351b3aa5e34c4c00e66245583f88bd9bb)

  
**Medical LLM &amp; Other Models**

* PediaBench: Chinese Pediatric LLM
   * This paper introduces PediaBench, the first Chinese pediatric dataset for evaluating Large Language Model (LLM) question-answering performance, containing 4,565 objective and 1,632 subjective questions across 12 disease groups.
* BiMediX: Bilingual Medical LLM
   * This paper introduces BiMediX, the first bilingual (English-Arabic) medical Mixture of Experts LLM, along with BiMed1.3M, a 1.3M bilingual medical instruction dataset with over 632M tokens used for training.
* Diverse medical knowledge integration
   * This paper introduces BiMediX2, a bilingual (Arabic-English) Large Multimodal Model (LMM) based on Llama3.1 architecture, trained on 1.6M medical interaction samples.
* BRAD: Digital Biology Language Model
   * This paper introduces BRAD (Bioinformatics Retrieval Augmented Digital assistant), an LLM-powered chatbot and agent system integrating various bioinformatics tools.
* MMedPO: Vision-Language Medical LLM
   * This paper introduces MMedPO, a multimodal medical preference optimization approach to improve factual accuracy in Medical Large Vision-Language Models (Med-LVLMs) by addressing modality misalignment.

**Frameworks &amp; Methodologies**  
\- TOP-Training: Medical Q&amp;A Framework  
\- Hybrid RAG: Secure Medical Data Management  
\- Zero-Shot ATC Clinical Coding  
\- Chest X-Ray Diagnosis Architecture  
\- Medical Imaging AI Democratization

**Benchmarks &amp; Evaluations**  
\- KorMedMCQA: Korean Healthcare Licensing Benchmark  
\- Large Language Model Medical Tasks  
\- Clinical T5 Model Performance Study  
\- Radiology Report Quality Assessment  
\- Genomic Analysis Benchmarking

**LLM Applications**

\- TCM-FTP: Herbal Prescription Prediction  
\- LLaSA: Activity Analysis via Sensors  
\- Emergency Department Visit Predictions  
\- Neurodegenerative Disease AI Diagnosis  
\- Kidney Disease Explainable AI Model

**Ethical AI &amp; Privacy**  
\- Privacy-Preserving LLM Mechanisms  
\- AI-Driven Digital Organism Modeling  
\- Biomedical Research Automation  
\- Multimodality in Medical Practice

Full thread in detail:¬†[https://x.com/OpenlifesciAI/status/1867999825721242101](https://x.com/OpenlifesciAI/status/1867999825721242101)",aadityaura,1hecwvp,https://reddit.com/r/MachineLearning/comments/1hecwvp/d_last_week_in_medical_ai_top_llm_research/,https://www.reddit.com/r/MachineLearning/comments/1hecwvp/d_last_week_in_medical_ai_top_llm_research/,2024-12-14 21:25:08,9,0.81,9,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/NIqGY7RUVe___1Rm4Sj_tvO_aD1SrmnLgeCdd3lQb6c.jpg,t3_1hecwvp
MachineLearning,Curated Corpus for U.S. Health Insurance Applications [P] [R],,tpafs,1he14fo,https://reddit.com/r/MachineLearning/comments/1he14fo/curated_corpus_for_us_health_insurance/,https://github.com/TPAFS/hicric,2024-12-14 11:36:37,7,0.82,7,0,0,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/neXQ6PQq1HS-6BASsO31cD98vX-av52hINTe9GUR5kI.jpg,t3_1he14fo
MachineLearning,[D] Are We Okay With This? Questionable Poster Behavior at NeurIPS,"This was my first year at NeurIPS. It‚Äôs inspiring to see so much cutting-edge research being presented, but something troubling caught my attention during the poster sessions that I feel compelled to share, especially given [the recent incident with Rosalind Picard](https://www.reddit.com/r/MachineLearning/comments/1hdxbru/d_what_happened_at_neurips/).

Getting a paper accepted at NeurIPS is a huge achievement. Each poster spot represents so much hard work and is highly coveted.

I saw two posters that *shouldn‚Äôt* have been there, and it has left me wondering about the exploitation of these spaces.

**Illegal Poster #1:** [Generative Boba](https://x.com/BoyuanChen0/status/1778565953627775453). This was a ‚Äúcute, look at me‚Äù poster, but it also featured a QR code linking to the creator‚Äôs X/Twitter. While the poster itself was placed on a side wall in the exhibition hall and not in an official poster spot (when I saw it anyway), it still felt odd. Why did they make this poster? Was this about sparking joy, or gaining attention and followers?

[Illegal Poster #1: Generative Boba.](https://preview.redd.it/u3vfvszkoy6e1.jpg?width=3363&amp;format=pjpg&amp;auto=webp&amp;s=8c09ddda45e0ac002223dadf0eac4165bfdc0433)

**Illegal Poster #2:** [Benchmarkthing](https://x.com/xdotli/status/1867823150068535797)**.** This was far more concerning. It blatantly promoted a new AI startup, mentioning funding by a prominent figure in our field, Jeff Dean. Unlike the boba poster, this could visually pass as a real NeurIPS poster. Probably most passersby didn‚Äôt give it a second thought, but the poster's presenter (who is also the company‚Äôs founder) was essentially promoting his new startup, sometimes to a significant audience size AND across *multiple* poster sessions. This feels deceptive and exploitative ‚Äî gaming the trust of the community to cheatingly gain visibility in a sacred academic space.

[Illegal Poster #2: Benchmarkthing.](https://preview.redd.it/qn7vpos4py6e1.jpg?width=2646&amp;format=pjpg&amp;auto=webp&amp;s=4cfd1aa535bdf74cdb57ba8e44f1fa813b9d28a7)

A different type of gaming involves authors putting up their poster at unused spots while leaving a sign in their formally assigned location that says ‚ÄúSee poster at #{better spot}‚Äù. If the authors for the unused spot arrived, they‚Äôd just move their poster back ‚Äî but if not, they would presumably revel in the extra attention from being located, for example, closer to the hall‚Äôs entrance with more foot traffic.

Relocating posters still seems problematic, but at least the posters *belong* at the conference. On the other hand, I feel much more strongly that unauthorized posters for personal or commercial promotion hurts the integrity of the space, disrespects the presenters whose posters truly belong there, and undermines the conference overall.

Questions for the community:

1. Should there be stricter policies or better enforcement for poster sessions?
2. How do we differentiate between minor gaming (e.g. relocating posters) and outright exploitation (e.g. unauthorized posters)?
3. Is it fair to tolerate some flexibility as long as the intentions are lighthearted or still academic?¬†
4. How do we address these behaviors moving forward? Should there be consequences?",Positive_Lychee6904,1heo36q,https://reddit.com/r/MachineLearning/comments/1heo36q/d_are_we_okay_with_this_questionable_poster/,https://www.reddit.com/r/MachineLearning/comments/1heo36q/d_are_we_okay_with_this_questionable_poster/,2024-12-15 08:08:41,0,0.33,0,0,21,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/r9cEQ2e72Zhy6nH9XkcERYWkKB8X-saXhRtgLnKAF2I.jpg,t3_1heo36q
MachineLearning,"[D] NVIDIA‚Äôs hostages: A Cyberpunk Reality of Monopolies
","In AI and professional workstations, NVIDIA's dominance feels like a suffocating monopoly. Their segmented product lines widen the gap between consumer and professional GPUs, particularly in VRAM, performance, and price.

AI enthusiasts struggle with prohibitive costs for GPUs equipped with sufficient VRAM. The reliance on CUDA cores‚Äîa proprietary standard‚Äîfurther locks developers into NVIDIA‚Äôs ecosystem, stifling competition and innovation.

NVIDIA‚Äôs control extends beyond hardware, as their CUDA platform discourages adoption of open, competitive solutions. This feeds a cyberpunk dystopia where corporations consolidate power, leaving consumers and developers with few choices.

Why does the tech world remain complicit? Why aren‚Äôt we pursuing alternative hardware architectures or broader software compatibility beyond CUDA? AMD‚Äôs ROCm is a start, but more aggressive development and policy interventions are needed to challenge NVIDIA‚Äôs grip.

Until when will this continue? Who will stand up for the end consumer?",SevenShivas,1hdjklf,https://reddit.com/r/MachineLearning/comments/1hdjklf/d_nvidias_hostages_a_cyberpunk_reality_of/,https://www.reddit.com/r/MachineLearning/comments/1hdjklf/d_nvidias_hostages_a_cyberpunk_reality_of/,2024-12-13 19:02:54,46,0.64,46,0,27,0,0,False,False,True,False,False,Discussion,self,t3_1hdjklf
MachineLearning,Custom Implementation of Contrastive Loss [P],"I am trying to implement the contrastive loss function I am unsure if it is correct. My loss seems to explode into infinity. Another set of eyes on this would be appreciated does this look correct?

    class ContrastiveLoss(nn.Module):
        def __init__(self, temperature=0.9):
            super(ContrastiveLoss, self).__init__()
            self.temperature = temperature
    
        def forward(self, projections_1, projections_2):
            z_i = projections_1
            z_j = projections_2
            z_i_norm = F.normalize(z_i, dim=1)
            z_j_norm = F.normalize(z_j, dim=1)
            cosine_num = torch.matmul(z_i, z_j.T)
            cosine_denom = torch.matmul(z_i_norm, z_j_norm.T)
            cosine_similarity = cosine_num / cosine_denom
    
            numerator = torch.exp(torch.diag(cosine_similarity) / self.temperature)
    
            denominator = cosine_similarity
            diagonal_indices = torch.arange(denominator.size(0))
            denominator[diagonal_indices, diagonal_indices] = 0
            denominator = torch.sum(torch.exp(cosine_similarity), dim=1)
            loss = -torch.log(numerator / denominator).mean()
            return loss",Ok-Administration894,1heb4iv,https://reddit.com/r/MachineLearning/comments/1heb4iv/custom_implementation_of_contrastive_loss_p/,https://www.reddit.com/r/MachineLearning/comments/1heb4iv/custom_implementation_of_contrastive_loss_p/,2024-12-14 20:01:39,0,0.3,0,0,4,0,0,False,False,True,False,False,Project,self,t3_1heb4iv
MachineLearning,[D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams,"Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his colleagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.github.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is asserted and if this is true this would be nice to confirm.",LelouchZer12,1hctf36,https://reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/,https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/,2024-12-12 19:41:41,673,0.97,673,0,96,0,0,False,False,True,False,False,Discussion,self,t3_1hctf36
MachineLearning,"[P] i have an issue with my nn the model instead of predicting real words it returns things like ""the in etc."" like not real words how to fix this",repo : [https://github.com/troy12x/Quasar-1](https://github.com/troy12x/Quasar-1) ,Justimrandy,1he8fea,https://reddit.com/r/MachineLearning/comments/1he8fea/p_i_have_an_issue_with_my_nn_the_model_instead_of/,https://www.reddit.com/r/MachineLearning/comments/1he8fea/p_i_have_an_issue_with_my_nn_the_model_instead_of/,2024-12-14 17:58:33,0,0.1,0,0,5,0,0,False,False,True,False,False,Project,self,t3_1he8fea
MachineLearning,[R] Identifying Critical Decision Points in Neural Text Generation Through Token-Level Uncertainty Analysis,"This paper introduces a framework for analyzing and visualizing the branching decisions language models make during text generation. The key methodology involves tracking probability distributions across different sampling paths to understand how early choices affect downstream generation.

Main technical points:
- Developed metrics to quantify uncertainty at each generation step
- Created visualization tools for mapping decision trees in generation
- Analyzed how different sampling methods affect path divergence
- Measured correlation between model confidence and generation quality
- Identified clustering patterns in generation trajectories

Key results:
- Found that paths tend to cluster into 2-3 distinct trajectory groups
- Early sampling decisions have outsized impact on final outputs
- Uncertainty patterns vary significantly between sampling methods
- Similar prompts can lead to dramatically different generation paths
- Model confidence doesn't consistently predict output quality

I think this work provides important insights into how we might better control text generation. The ability to map and understand generation paths could help develop more reliable sampling methods and better uncertainty estimates.

I think the clustering of generation paths is particularly interesting - it suggests there may be ways to guide generation toward desired trajectory groups. This could be valuable for applications needing more predictable outputs.

The methodology also reveals some concerning aspects about current sampling methods. The strong dependence on early decisions suggests we may need new approaches that better preserve generation flexibility throughout the sequence.

TLDR: New framework for analyzing how language models make text generation choices. Shows that generation paths cluster into distinct groups and early decisions heavily influence outcomes. Could help develop better sampling methods and uncertainty estimates.

[Full summary is here](https://aimodels.fyi/papers/arxiv/forking-paths-neural-text-generation). Paper [here](https://arxiv.org/abs/2412.07961).",Successful-Western27,1hdd0kk,https://reddit.com/r/MachineLearning/comments/1hdd0kk/r_identifying_critical_decision_points_in_neural/,https://www.reddit.com/r/MachineLearning/comments/1hdd0kk/r_identifying_critical_decision_points_in_neural/,2024-12-13 14:10:21,12,0.84,12,0,1,0,0,False,False,True,False,False,Research,self,t3_1hdd0kk
MachineLearning,[D] Training with synthetic data and model collapse. Is there progress?,"About a year ago, research papers talked about model collapse when dealing with synthetic data. Recently I‚Äôve been hearing about some progress in this regard. I am not expert and would welcome your views on what‚Äôs going on. Thank you and have a fantastic day.",BubblyOption7980,1hd92mt,https://reddit.com/r/MachineLearning/comments/1hd92mt/d_training_with_synthetic_data_and_model_collapse/,https://www.reddit.com/r/MachineLearning/comments/1hd92mt/d_training_with_synthetic_data_and_model_collapse/,2024-12-13 10:03:43,18,0.69,18,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1hd92mt
MachineLearning,[D] Help with clustering over time,"I'm dealing with a clustering over time issue.
Our company is a sort of PayPal. We are trying to implement an antifraud process to trigger alerts when a client makes excessive payments compared to its historical behavior.
To do so, I've come up with seven clustering features which are all 365-day-long moving averages of different KPIs (payment frequency, payment amount, etc.). So it goes without saying that, from one day to another, these indicators evolve very slowly. I have about 15k clients, several years of data.
I get rid of outliers (99-percentile of each date, basically) and put them in a cluster-0 by default.
Then, the idea is, for each date, to come up with 8 clusters. I've used a Gaussian Mixture clustering (GMM) but, weirdly enough, the clusters of my clients vary wildly from one day to another.
I have tried to plant the previous mean of my centroids, using the previous day centroid of a client to sort of seed the next day's clustering of a client, but the results still vary a lot. I've read a bit about DynamicC and it seemed like the way to address the issue, but it doesn't help.",LaBaguette-FR,1hdk4ma,https://reddit.com/r/MachineLearning/comments/1hdk4ma/d_help_with_clustering_over_time/,https://www.reddit.com/r/MachineLearning/comments/1hdk4ma/d_help_with_clustering_over_time/,2024-12-13 19:27:38,2,0.6,2,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hdk4ma
MachineLearning,[D] Importance of HPO per field / model type / applications,"I‚Äôve noticed that the time spent on hyperparameter optimization vary significantly, not just between industry and academia but also across different fields like NLP, computer vision, or reinforcement learning. I‚Äôm curious‚Äîwhat‚Äôs your experience?

* Is tuning something you prioritize heavily, or do you often settle for ‚Äúgood enough‚Äù configurations to move faster?
* What field / model type / applications do you think experience most(or least) bottleneck in workflow due to HPO?
* Are there any industry dependency around choosing HPO tools? For example, everyone in xx industry would pick Optuna as a go-to or everyone running xx experiments would use Sigopt.

Would love to hear your experiences! Thanks",Maleficent_Ad5541,1hd6pjv,https://reddit.com/r/MachineLearning/comments/1hd6pjv/d_importance_of_hpo_per_field_model_type/,https://www.reddit.com/r/MachineLearning/comments/1hd6pjv/d_importance_of_hpo_per_field_model_type/,2024-12-13 06:58:12,21,0.92,21,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1hd6pjv
MachineLearning,[D] What makes TikTok's recommendation algorithm so strong?,"General Discussion - now that they are about to be banned in the US, I'm becoming fascinated by the strength of their For You recommendations. To try and put some guard rails on what I mean, TikTok has shown itself to be able to match content to relevant audience at greater frequency and scale than any other app (YouTube included). Many creators can join the platform, post a single video, and have millions of views in 24 hours. This does happen on other apps, but TikTok seems to be the most consistent at scaling audience incredibly fast.

What models might they be basing their system on? What about their models creates their competitive advantage?",No_Collection_5509,1hcp4xw,https://reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/,https://www.reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/,2024-12-12 16:39:06,147,0.92,147,0,45,0,0,False,False,True,False,False,Discussion,self,t3_1hcp4xw
MachineLearning,[R] survey on students‚Äô motivation to learn Artificial Intelligence and Modeling.,"We are university students and we're conducting a quick survey on students‚Äô motivation to learn Artificial Intelligence and Modeling.
The  survey will take less than 10 minutes to complete.

Here's the link to the survey: 
https://docs.google.com/forms/d/e/1FAIpQLSdS-xy53N9lDRlC_835A_E59VMjCPql0_HuihPYqaQ_nINSsw/viewform?usp=sf_link


Your input would mean a lot to us! 
Thank you so much for your support and time.
",ExamSensitive3076,1hdvx40,https://reddit.com/r/MachineLearning/comments/1hdvx40/r_survey_on_students_motivation_to_learn/,https://www.reddit.com/r/MachineLearning/comments/1hdvx40/r_survey_on_students_motivation_to_learn/,2024-12-14 05:25:53,0,0.17,0,0,2,0,0,False,False,True,False,False,Research,self,t3_1hdvx40
MachineLearning,[D] help with evaluating model,"i am having an issue with evaluating my model because model.evaluate() returns an okay overall score in accuracy but the confusion matrix and classification report return 100% for one class and 0% for another, i am using cifar10 but only 2 classes from it. anyone know why this happens? is this overfitting i am not sure because i am getting a similar score as model.evaluate(0 in my training accuracy and same for loss (which is almost as high as the accuracy)",Affectionate_Pen6368,1hd5kht,https://reddit.com/r/MachineLearning/comments/1hd5kht/d_help_with_evaluating_model/,https://www.reddit.com/r/MachineLearning/comments/1hd5kht/d_help_with_evaluating_model/,2024-12-13 05:40:50,3,0.67,3,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hd5kht
MachineLearning,[D] Agentic AI Design Patterns,"I was looking into design patterns for Agentic AI and I could need some help to grasp the concepts.

I read about ReAct and ReWOO.

From ReWOO, I really liked the idea of having a planner that creates a blueprint of the work that needs to be done. I can imagine that this works well for a lot of tasks, and it optimizes token usage compared to ReAct.

From ReAct, I like that it has a reflection/observation LLM, to decide whether the output is good enough or needs another pass through the agents.

What I don't understand:
Why does ReWOO not have a reflection component??

Wouldn't it be the best of both worlds to have the planner and the reflection?

This was the first draft for my agentic AI prototype, and I think it has pretty obvious advantages.

I think I am missing something here.",Mindless_Copy_7487,1hd8w3k,https://reddit.com/r/MachineLearning/comments/1hd8w3k/d_agentic_ai_design_patterns/,https://www.reddit.com/r/MachineLearning/comments/1hd8w3k/d_agentic_ai_design_patterns/,2024-12-13 09:49:22,0,0.5,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hd8w3k
MachineLearning,"[D] ""Proper"" way to upload accepted conference paper to the ArXiv?","We recently had a paper accepted to a conference (AAAI). We found out that the conference does not publish appendices so they recommend we upload the full paper (with appendix) to arXiv. This is something we were considering doing anyway since the paper would be available before the conference proceedings come out.

My concern is that if someone decides to cite our work, they may either become confused or cite the arXiv rather than AAAI ""version"".

Is there a ""correct"" or common way to handle this? Do arXiv uploads with the same title get indexed to ""one manuscript"" on google scholar?

Also, are we allowed to use the conference template to upload? (This part might be conference dependent I suppose).

I know it is common these days to upload to arXiv before hearing back from a conference (usually with a different title) but I think this is a slightly different situation as the paper is accepted and the uploaded version will be identical to the conference paper (though with an Appendix).

Thanks in advance!",baghalipolo,1hcupkm,https://reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/,https://www.reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/,2024-12-12 20:38:05,9,0.69,9,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1hcupkm
MachineLearning,[D] LSTM model implementation and approximation questions,"For a project I am currently trying to integrate an Autoencoder for feature extraction and an LSTM for classification of the reduced feature space. The problem I am encountering is on how to train the LSTM network. The AE produces 5 datapoints which is fed into the LSTM network. The trick now comes in on the training of the LSTM network and how the LSTM works. I want the LSTM to take into account the 5 parameters from the AE at time t as well as the parameters at t-1 and t-2. As far as I understand the LSTM does this automatically, or should it then be that the LSTM takes in a total of 15 parameters with each pair of 5 corresponding to one timestep of the AE?

Any advice on LSTM would be great or how such training can be done in an efficient way. The AE is processing a time-series signal.",Sea_Onion41,1hcvh1c,https://reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/,https://www.reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/,2024-12-12 21:09:27,3,0.67,3,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hcvh1c
MachineLearning,[R] Rethinking the positive pairs in contrastive learning,"Hi, I am sharing my recent work which allows arbitrary images to be positive pairs. Our finding is quite astonishing that two disparate images, e.g., a snake and a lamp, can be positive. Our work potentially broadens the applications of contrastive learning to deal with the ""false positive"" in which two views are not similar.

We challenge the common sense in contrastive learning, that is, the positive pair design is critical. Our results prove that the feature selection is the key!

Paper: [https://arxiv.org/abs/2410.18200](https://arxiv.org/abs/2410.18200)",Miserable-Gene-308,1hcpoo6,https://reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/,https://www.reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/,2024-12-12 17:02:44,7,0.64,7,0,10,0,0,False,False,True,False,False,Research,self,t3_1hcpoo6
MachineLearning,From Viruses and Materials to Galaxies and Beyond: The Role Machine Learning Plays in Scientific Discovery,,SlothSpeedRunning,1hcubv2,https://reddit.com/r/MachineLearning/comments/1hcubv2/from_viruses_and_materials_to_galaxies_and_beyond/,https://lettersandsciencemag.ucdavis.edu/science-technology/viruses-and-materials-galaxies-and-beyond,2024-12-12 20:21:10,2,0.67,2,0,0,0,0,False,False,False,False,False,,https://a.thumbs.redditmedia.com/SRD_I9dMMiKup552l256JNNe6jjxqQTl9DnvS7gkMO8.jpg,t3_1hcubv2
MachineLearning,[D] Pet project - Style Transfer Neural Networks Implementation,"Hi, I am learning ML and this is my first project. I did a simple 100 LoC implementation of the *Neural Style Transfer* paper by Gatys et al. See [https://github.com/TAOGenna/pytorch-neural-style-transfer](https://github.com/TAOGenna/pytorch-neural-style-transfer)

https://preview.redd.it/x2udi76n2g6e1.jpg?width=939&amp;format=pjpg&amp;auto=webp&amp;s=437bdda1683e9fd580a6b3d1d4dc2598b25079ff

  
",TAO_genna,1hcottj,https://reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/,2024-12-12 16:25:17,4,0.67,4,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/BbvnpWVVY1jQ1n7M80lXl0kKKowiy2y473otZfpwdWo.jpg,t3_1hcottj
MachineLearning,"[R] A Grounded Theory Study of LLM Red Teaming: Motivations, Strategies, and Techniques","This paper presents a grounded theory study of how red-teaming is conducted on Large Language Models (LLMs), based on interviews with practitioners. The researchers systematically analyzed practitioner approaches to identify common patterns, strategies and motivations in LLM red-teaming.

Key technical points:
- Used qualitative coding of interviews to develop taxonomy of red-teaming approaches
- Identified 12 distinct attack strategies and 35 specific techniques
- Found red-teaming requires manual effort rather than automation
- Demonstrated importance of team collaboration over individual attempts
- Established red-teaming as distinct from malicious attacks
- Mapped common patterns in tester motivations and goals

Main results:
- Red-teaming strategies fall into categories like prompt manipulation, psychology-based attacks, and system limit testing
- Successful testers adopt an ""alchemist"" mindset of systematic experimentation
- Most practitioners are motivated by curiosity and safety concerns
- Testing requires deep understanding of both technical and psychological aspects
- Manual testing currently more effective than automated approaches

I think this work provides an important foundation for developing more structured approaches to LLM safety testing. The taxonomy they've developed could help standardize how we evaluate and secure these systems. Their finding that manual testing remains superior to automation suggests we need much more work on automated testing approaches.

I think the emphasis on non-malicious intent and safety motivations is particularly relevant as these systems become more widely deployed. Understanding how and why people conduct these tests helps distinguish legitimate security research from attacks.

TLDR: First systematic study of LLM red-teaming practices, providing taxonomy of strategies and techniques based on practitioner interviews. Shows importance of manual testing and team collaboration, while establishing red-teaming as legitimate security research.

[Full summary is here](https://aimodels.fyi/papers/arxiv/summon-demon-bind-it-grounded-theory-llm). Paper [here](https://arxiv.org/abs/2311.06237).",Successful-Western27,1hclmk1,https://reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/,https://www.reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/,2024-12-12 13:56:57,5,0.86,5,0,0,0,0,False,False,True,False,False,Research,self,t3_1hclmk1
MachineLearning,[D] Question About ResNet and Scalability of Extremely Deep Networks,"I‚Äôve been exploring the architecture of ResNet and its ability to train very deep neural networks effectively. While I understand that residual connections help mitigate issues like vanishing gradients and make training deeper networks feasible, I‚Äôm curious about the limitations of this approach when scaling to extremely deep networks, such as those with 1000 layers or more.

From my understanding, a ResNet with, say, 100 layers might effectively function like a much smaller network due to the residual connections, which essentially ""skip"" layers and add outputs. However, wouldn‚Äôt this also mean that if a regular MLP struggles to scale beyond 15 layers, a ResNet might just shift this limit proportionally (e.g., struggling beyond 150 layers)? In other words, does ResNet fundamentally solve the problem of training extremely deep networks, or does it merely extend the depth at which issues start to reappear?

  
I‚Äôd appreciate any insights you might have! TYSM!",Time_Celebration6058,1hco4ig,https://reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/,https://www.reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/,2024-12-12 15:54:53,4,0.83,4,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1hco4ig
MachineLearning,[N] Save 80% Memory for DPO and ORPO in Liger-Kernel,"Introducing the first open-source optimized post-training losses in Liger Kernel with \~80% memory reduction, featuring DPO, CPO, ORPO, SimPO, JSD, and more, achieving up to 70% end-to-end speedup through larger batch size. Use it as any PyTorch module - Available today in Liger v0.5.0!

[https://x.com/hsu\_byron/status/1866577403918917655](https://x.com/hsu_byron/status/1866577403918917655)",Icy-World-8359,1hcewdl,https://reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/,https://www.reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/,2024-12-12 06:18:35,20,0.88,20,0,0,0,0,False,False,True,False,False,News,self,t3_1hcewdl
MachineLearning,[R] LLM's knowledge expansion to enable generation of cross domain content (outside the training dataset),,ankitm1,1hcke77,https://reddit.com/r/MachineLearning/comments/1hcke77/r_llms_knowledge_expansion_to_enable_generation/,https://arxiv.org/abs/2409.17171,2024-12-12 12:50:27,4,0.7,4,0,0,0,0,False,False,False,False,False,Research,default,t3_1hcke77
MachineLearning,[P] Scalling data from aggregated calculations,"Hello, I have a project in which I detect anomalies on transactions data from ethereum blockchain. I have performed aggregated calculations on each wallet address (ex. minimum, maximum, median, sum, mode of transactions' values) and created seperated datafile with it. I have joined the data on all the transactions. Now I have to standardize data (I have chosen robust scalling) before machine learning but I have following questions regarding this topic:

1. Should I actually standardize each feature based on its unique mean and iqr? Or perform scalling on the column that the calculations come from - value column and than use its mean and iqr to scale the calculated columns?
2. If each feature was scaled based on its own mean and iqr should I do it before joining calculated data or after?",Wikar,1hcukjg,https://reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/,https://www.reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/,2024-12-12 20:32:01,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1hcukjg
MachineLearning,"[D] does intel gpu support ROCm or AMD cards  support intel one?
","i can't find this information and if both are open source it make sense a compatibility layer , any of the two is already ported to the other platform?, if you can share info about nvidia too will be cool



",mrnothing-,1hctd0o,https://reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/,https://www.reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/,2024-12-12 19:39:17,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hctd0o
MachineLearning,[D] How to make friends and network at NeurIPS?,"I‚Äôm attending NeurIPS for the first time and it‚Äôs quite overwhelming seeing the amount of people and so many recruiters. I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is here.

I didn‚Äôt really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach others who are in big groups of people and (2) I‚Äôm feeling strong imposter syndrome and under-qualified for the jobs recruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to potentially meet up and have a chat? I‚Äôm a 3rd year PhD student from the UK, but from Vancouver myself so know lots of stuff going on in the area. Cheers!",K_is_for_Karma,1hc0x89,https://reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/,https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/,2024-12-11 18:54:26,52,0.84,52,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1hc0x89
MachineLearning,[R] What should I choose: AI-assisted data labeling or crowdsourced data labeling?,"Hey everyone,

I‚Äôm a researcher at my company, and I‚Äôm starting to train a model using a dataset of over 50k medical images. One of the major challenges I‚Äôm facing is choosing the best approach for data labeling.

I‚Äôve seen promising results with **AI-assisted labeling**‚Äîit seems faster and less costly upfront. However, I‚Äôm concerned about potential inaccuracies and whether the AI‚Äôs assistance would skew the dataset in ways I might not expect.

On the other hand, **crowdsourced labeling** is significantly more expensive and time-consuming, but the results are often highly reliable due to diverse human input.

Given the scale of my dataset and its importance in a sensitive domain like medical imaging:

* **Which method would you recommend, and why?**
* **Are there hybrid approaches that balance cost, quality, and efficiency?**
* **What tools or platforms would you suggest for either approach?**

I‚Äôd love to hear your insights or experiences with similar projects. Your input will be invaluable in helping me make the right decision!

Thanks in advance!",Organic-Injury-1153,1hchrui,https://reddit.com/r/MachineLearning/comments/1hchrui/r_what_should_i_choose_aiassisted_data_labeling/,https://www.reddit.com/r/MachineLearning/comments/1hchrui/r_what_should_i_choose_aiassisted_data_labeling/,2024-12-12 09:56:17,2,0.55,2,0,15,0,0,False,False,True,False,False,Research,self,t3_1hchrui
MachineLearning,[R] Continuous Latent Space Reasoning: Enhancing LLM Performance Through Chain of Continuous Thought,"This paper introduces **COCONUT** (Chain of Continuous Thought), which transforms language model reasoning from discrete token space into continuous latent space. The key idea is encoding reasoning steps as continuous vectors rather than text tokens, allowing for more flexible and precise intermediate computations.

Main technical points:
* Encoder-decoder architecture that maps text‚Üîcontinuous vectors
* Novel continuous reasoning module operating on latent vectors
* Parallel processing of reasoning steps in continuous space
* Gradient-based optimization during the reasoning process
* Special loss function combining reconstruction and reasoning objectives

Key results:
* **20%** improvement on reasoning benchmarks vs traditional methods
* Reduced computational steps needed for complex problems
* More consistent performance across different reasoning tasks
* Better handling of mathematical and logical reasoning
* Enhanced ability to maintain coherent reasoning chains

I think this approach could meaningfully advance how language models handle complex reasoning tasks. By moving beyond discrete tokens, models may better capture the continuous nature of human-like reasoning. The ability to optimize in continuous space during reasoning is particularly promising for improving reliability.

I think the main challenge will be scaling this to very large models while managing computational costs. The translation between discrete and continuous spaces adds overhead that needs to be addressed.

TLDR: New method transforms language model reasoning into continuous vector space instead of discrete tokens, showing 20% better performance on reasoning tasks through more flexible computation.

[Full summary here](https://aimodels.fyi/papers/arxiv/training-large-language-models-to-reason-continuous). Paper [here](https://arxiv.org/abs/2412.06769).",Successful-Western27,1hbto1w,https://reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/,https://www.reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/,2024-12-11 13:39:26,111,0.96,111,0,5,0,0,False,False,True,False,False,Research,self,t3_1hbto1w
MachineLearning,[R] An Evolved Universal Transformer Memory,,hardmaru,1hc6bs8,https://reddit.com/r/MachineLearning/comments/1hc6bs8/r_an_evolved_universal_transformer_memory/,https://arxiv.org/abs/2410.13166,2024-12-11 22:43:30,19,0.88,19,0,2,0,0,False,False,False,False,False,Research,default,t3_1hc6bs8
MachineLearning,[D] What Models Are Best at Producing Ambient Sounds/ Music?,"I'm working on an application that requires ambient sounds/ music. For example:

* ""A crackling fire, with chat murmuring in the background.""
* ""Nightime countryside summer sounds in the UK.""
* ""Wind blowing through the mountains as you're stood on a high rock.""

I've had a look at Hugging Face and found the Text-To-Audio section. However it appears the top models have very few downloads:

https://preview.redd.it/d4o7g760yf6e1.png?width=1788&amp;format=png&amp;auto=webp&amp;s=2901a7678582745beb714b81519712bac37bd195

This makes me think the field is immature, and there's no clear best model. Is this a fair appraisal of the field, or are there models outside of Hugging Face that perform well for this use case?",FPGA_Superstar,1hcof9m,https://reddit.com/r/MachineLearning/comments/1hcof9m/d_what_models_are_best_at_producing_ambient/,https://www.reddit.com/r/MachineLearning/comments/1hcof9m/d_what_models_are_best_at_producing_ambient/,2024-12-12 16:07:30,0,0.4,0,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/TCjeAYae_sK1rwe1hACJo5HRrqIniSivkrmvMhxxwGc.jpg,t3_1hcof9m
MachineLearning,"[D] I got the acceptance for my IEEE publication, does that means it will be uploaded on their Xplore page?","So I submitted a paper and it got accepted by my publication around 2 months ago, today was my conference in online mode, didnt go well I think he was in hurry he didnt listen much diagreed a bit and then closed the meet on my face. So my question is how bad is it? Will it be published as I have the acceptance or still a no?",candle_misuser,1hcpqj4,https://reddit.com/r/MachineLearning/comments/1hcpqj4/d_i_got_the_acceptance_for_my_ieee_publication/,https://www.reddit.com/r/MachineLearning/comments/1hcpqj4/d_i_got_the_acceptance_for_my_ieee_publication/,2024-12-12 17:04:48,0,0.39,0,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1hcpqj4
MachineLearning,[R] Evaluating the world model implicit in a generative model,,jsonathan,1hbra2d,https://reddit.com/r/MachineLearning/comments/1hbra2d/r_evaluating_the_world_model_implicit_in_a/,https://arxiv.org/pdf/2406.03689,2024-12-11 11:19:06,22,0.87,22,0,26,0,0,False,False,False,False,False,Research,default,t3_1hbra2d
MachineLearning,[D] Resources to get up to the speed with the state of the art evolutionary optimization,"There're plenty of good books letting you get close to the state of the art in the field, on Machine Learning, and Deep Learning in particular. However, are there any good modern books on evolutionary optimization? Are there any good courses?",ArtisticHamster,1hbt986,https://reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/,https://www.reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/,2024-12-11 13:17:59,13,0.84,13,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hbt986
MachineLearning,[D] Why are the Stella embedding models so much smaller than other models of similar quality?,"On the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), `stella_en_v5` is currently ranked 3rd overall, while using *one fifth* the memory of all non-Stella models in the top 10.

`stella_en_400M_v5` is ranked 10th, while using *15-20 times less memory* than the models ranked near it. This appears to be relatively consistent across several subtasks of the benchmark (for English).

What is the secret sauce here? Alternatively, what is the catch? There is no paper yet. Anyone know details?",-p-e-w-,1hbkww5,https://reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/,https://www.reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/,2024-12-11 03:58:01,39,0.87,39,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1hbkww5
MachineLearning,[D] Any 3D reconstruction method that can be used for multiple scenes (i.e. not use and throw),"NeRFs are unique for every scene and thus, need to be trained from scratch. Gaussian splats are scene unique too. I understand that scenes are complex and thus there is very little chance for there to be a neural network that can output multiple scenes once trained. But still are there any scene representations that to some caliber are not use and throw completely?",deathmaster2011,1hbreb4,https://reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/,https://www.reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/,2024-12-11 11:27:15,5,0.86,5,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hbreb4
MachineLearning,[D] From Unemployment to Lisp: Running GPT-2 on a Teen's Deep Learning Compiler,"A couple months ago I found myself unemployed, uncertain about what to do next. I wanted to learn more about deep learning, but from a systems prespective. Coming from Andrew's Ng course on supervised learning, I was eager to learn more about how deep learning frameworks (or deep learning compilers) like Pytorch or Tinygrad.

I started to poke around Tinygrad, learning from the tutorials I found online, and I found it fascinating because it was an actual compiler, it took conventional python code and translated them into an Abstract Syntax Tree that was parsed into UOps and ScheduleItems, to finally have a codegen layer. While the design was interesting, the code was hard to read.

That's when I stumbled across something completly unexpected, A deep learning compiler built on Common Lisp, maintained by a Japanese 18-year-old during his gap year. And currently we have acomplished something great, it can run gpt2!

For now, it just generates C-kernels, but in the future we would like to support cuda codegen as well as many other features, and serve as a learning tool for anyone who would like to get to work on deep learning compilers in Common Lisp.

This is an open source project and anyone is welcome to contribute!

[https://github.com/hikettei/Caten](https://github.com/hikettei/Caten)

Edit: add an example of how it works.

Here's an example i wrote in a different forum:

Hello! Thanks for your question.

First of all, there are three layers of abstraction within Caten:

1. caten/apis | High-Level Graph Interface 2. caten/air | Low-Level Graph Interface 3. caten/codegen | AIR Graph =&gt; Kernel Generator

The inputs of the compiler are just Common Lisp classes (similar to torch modules). For example, in Common Lisp, we could create a module that does SinCos:

        (defclass SinCos (Func) nil
          (:documentation ""The func SinCos computes sin(cos(x))""))
    
        ;; Forward creates a lazy tensor for the next computation.
        ;; You can skip this process by using the `st` macro.
        (defmethod forward ((op SinCos) &amp;rest tensors)
          (st ""A[~] -&gt; A[~]"" (tensors)))
    
        ;; Backward is optional (skipped this time)
        (defmethod backward ((op SinCos) &amp;optional prev-grad)
          (declare (ignore prev-grad))
          nil)
    
        ;; Lower describes the lowered expression of `SinCos`
        (defmethod lower ((op SinCos) &amp;rest inputs)
          (let ((x (car inputs)))
            (with-context
              (a (%sin (%add x (%fconst (/ pi 2)))))
              (b (%sin a)))))

The \`apis\` layer is the high-level interface, while the \`lower\` method is the lower-level step before code generation.

Next, the framework generates an Abstract VM (AVM) representation:

        #S(AVM :GRAPH Graph[seen=NIL, outputs=(STC6466_1)] {
          &lt;ALLOCATE : TID6464 &lt;- (shape=(1), stride=(1)) where :dtype=FLOAT32&gt;
          &lt;Node[BUFFER] ALLOCATE(NID6480) : SID6479* &lt;- ()&gt;
          &lt;Node[BINARYOPS] ADD(NID6484) : BID6483* &lt;- (TID6464, LID6481)&gt;
          &lt;Node[UNARYOPS] SIN(NID6486) : UID6485* &lt;- (BID6483)&gt;
          &lt;Node[UNARYOPS] SIN(NID6488) : UID6487* &lt;- (UID6485)&gt;
          &lt;Node[SPECIAL/VM] PAUSE/BACKWARD(NID6501) : STC6466_1* &lt;- (UID6487)&gt;
        })

Then, the computation graph is translated into schedule items:

        FastGraph[outputs=(val_6)] {
          { Allocate } : [ val_0 &lt;- (1) ]
          { KERNEL } : [ val_5 &lt;- val_1, val_0 :name=FUSED_SIN_SIN_ADD_LOAD6511]
        }

Finally, the code generation step produces the following C code:

        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0);
        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0) {
            val_5[0] = sin(sin((val_0[0] + 1.5707964)));
        }

This C code is compiled by a C compiler and executed.

So to answer your question: the compiler takes Common Lisp code and generates C functions.",yCuboy,1hb7v5h,https://reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/,https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/,2024-12-10 18:00:45,103,0.93,103,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hb7v5h
MachineLearning,[R] When do authors have access to ICLR Meta-Reviews ?,"Hello everyone,

This is my first time submitting to ICLR. On the ICLR website, it says that Meta-Reviews are due today (in a few hours). Will the authors have access to those reviews at the same time as the decision notification or right after the Meta-Reviews due date ?

Thanks !",Glaze_anetha42,1hbqc75,https://reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/,https://www.reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/,2024-12-11 10:10:57,5,0.73,5,0,2,0,0,False,False,True,False,False,Research,self,t3_1hbqc75
MachineLearning,[D] Review process incentives and competition,"One of my labmates showed me a comment by an AC asking reviewers in an ACM conference to engage in the ICLR rebuttal and discussion period. This alone is funny (and sad) to me, but what got me was when one of the reviewers responded saying that the review process incentivizes reviewers to score papers low in order to gatekeep competing papers from being accepted.

I want to believe that this happens but its effect is not significant. However, I have heard that this is very common in fields like recommender systems. How prevalent is it in ML in general?",like_a_tensor,1hbf2gs,https://reddit.com/r/MachineLearning/comments/1hbf2gs/d_review_process_incentives_and_competition/,https://www.reddit.com/r/MachineLearning/comments/1hbf2gs/d_review_process_incentives_and_competition/,2024-12-10 23:07:02,12,0.94,12,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hbf2gs
MachineLearning,Conv2D for Time Series Data Tutorial [Project],"Can anyone provide me with a tutorial using TensorFlow on time series data and an example model with Conv2D layers, an AveragePooling2D layer and final dense layers?",chiplab,1hc5r13,https://reddit.com/r/MachineLearning/comments/1hc5r13/conv2d_for_time_series_data_tutorial_project/,https://www.reddit.com/r/MachineLearning/comments/1hc5r13/conv2d_for_time_series_data_tutorial_project/,2024-12-11 22:18:06,0,0.2,0,0,6,0,0,False,False,True,False,False,Project,self,t3_1hc5r13
MachineLearning,[R] How difficult is this dataset REALLY?,"New Paper Alert!

Class-wise Autoencoders Measure Classification Difficulty and Detect Label Mistakes

We like to think that the challenge in training a classifier is handled by hyperparameter tuning or model innovation, but there is rich inherent signal in the data and their embeddings.  Understanding how hard a machine learning problem is has been quite elusive.  Not any more.

Now you can compute the difficulty of a classification dataset without training a classifier, and requiring only 100 labels per class.  And, this difficulty estimate is surprisingly independent of the dataset size.

Traditionally, methods for dataset difficulty assessment have been time and/or compute-intensive, often requiring training one or multiple large downstream models. What's more, if you train a model with a certain architecture on your dataset and achieve a certain accuracy, there is no way to be sure that your architecture was perfectly suited to the task at hand ‚Äî it could be that a different set of inductive biases would have led to a model that learned patterns in the data with far more ease.

Our method trains a lightweight autoencoder for each class and uses the ratios of reconstruction errors to estimate classification difficulty. Running this dataset difficulty estimation method on a 100k sample dataset takes just a few minutes, and doesn't require tuning or custom processing to run on new datasets!

How well does it work? We conducted a systematic study of 19 common visual datasets, comparing the estimated difficulty from our method to the SOTA classification accuracy. Aside from a single outlier, the correlation is 0.78. It even works on medical datasets!



Paper Link:  https://arxiv.org/abs/2412.02596

GitHub Repo Linked in Arxiv pdf",ProfJasonCorso,1hb54nd,https://reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/,https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/,2024-12-10 16:04:41,33,0.78,33,0,20,0,0,False,False,True,False,False,Research,self,t3_1hb54nd
MachineLearning,Where to find Llama 3 initialisation [R] ,"Title basically says it all, I want a good transformer baseline and I imagine that the initialisation can matter quite a bit. I can find the llama 3 model, but I can find how they init parameters. Does anyone know where I can find this?",idkwhatever1337,1hbudg3,https://reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/,https://www.reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/,2024-12-11 14:14:14,0,0.36,0,0,5,0,0,False,False,True,False,False,Research,self,t3_1hbudg3
MachineLearning,[R] Understanding Transformer Limitations in Graph Search: A Mechanistic Analysis of Learning and Scaling Behavior,"This paper tackled a fundamental question about transformers' ability to learn search algorithms by studying how they handle graph connectivity problems. The authors developed a novel interpretation method to analyze how transformers process search operations layer by layer.

Key technical points:
- Used graph reachability as a test case with controlled complexity and unlimited training data
- Developed interpretation technique to understand how transformer layers compute reachable vertex sets
- Found transformers learn to expand search frontier exponentially with depth
- Demonstrated clear scaling limitations based on graph size
- Showed in-context learning (chain-of-thought) doesn't overcome these limitations

Main results:
- Small transformers can learn basic search when trained appropriately
- Each layer computes union of previously reachable vertices plus their neighbors
- Performance degrades sharply with increasing graph size
- Adding parameters doesn't solve the scaling problem
- Models struggle with graphs beyond their training distribution

I think this work reveals important architectural limitations in transformers that we need to address for applications requiring search capabilities. The scaling behavior suggests we may need fundamentally different approaches for larger search spaces rather than just bigger models.

I think the interpretation method they developed could be valuable for understanding how transformers process other types of structured data beyond just graphs. The clear empirical results on scaling limitations should inform architecture choices for applications involving search-like computations.

TLDR: Transformers can learn basic graph search operations but face fundamental limitations with scale. Adding more parameters doesn't help, suggesting we need new approaches for complex search problems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/transformers-struggle-to-learn-to-search). Paper [here](https://arxiv.org/abs/2412.04703).",Successful-Western27,1hb1wjo,https://reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/,https://www.reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/,2024-12-10 13:37:01,39,1.0,39,0,1,0,0,False,False,True,False,False,Research,self,t3_1hb1wjo
MachineLearning,[D] Inverse Neural Network,"Hi everyone, I wanna ask you guys if you know what's the current best **supervised** Inverse Neural Network? I know GAN, VAE, and conditional VAE.

Basically, my aim is to determine the input values of a multivariate function that satisfy an output value, e.g., find x=\[x1,...,xN\] so that 0.2=f(x).

My main major is engineering (not machine learning) and my knowledge about the field is quite limited. However, I'm good with reading any research papers that you suggested.

Thank you all,

Edit: sorry the for confusing example. f is NOT multivariate pdf but a ""multivariate function"" f : R\^N -&gt; R. Specifically, f is a ""univariate"" pdf with N-1 parameters.",zonanaika,1hbbj5o,https://reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/,https://www.reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/,2024-12-10 20:34:05,8,1.0,8,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1hbbj5o
MachineLearning,[R] The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,"**Dataset**: [https://github.com/PolymathicAI/the\_well](https://github.com/PolymathicAI/the_well)

**Paper**: [https://arxiv.org/pdf/2412.00568](https://arxiv.org/pdf/2412.00568)

**Abstract:**

&gt;Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at¬†[this https URL](https://github.com/PolymathicAI/the_well).

",StartledWatermelon,1haz4nw,https://reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/,https://www.reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/,2024-12-10 10:49:47,29,0.92,29,0,0,0,0,False,False,True,False,False,Research,self,t3_1haz4nw
MachineLearning,[R] Articulate Anything: automatic generation of 3D interactable assets from any input modalities,"üì¶ Can frontier AI transform ANY physical object from ANY input modality into a high-quality digital twin that also MOVES? Excited to share our work,Articulate-Anything, exploring how large vision-language models (VLMs) can bridge the gap between the physical and digital worlds.

Articulate-Anything üêµ is a state-of-the-art method for automatic interactable 3D asset creation from any input modalities including text, images, or videos.  
  
Website: [articulate-anything.github.io](https://t.co/y3fNE8Lb7X)  
Paper: [https://arxiv.org/abs/2410.13882](https://t.co/8m3gu5zTD7)  
Code: [https://github.com/vlongle/articulate-anything](https://github.com/vlongle/articulate-anything)  
Please see my twitter thread: [https://x.com/int64\_le/status/1866519866934714623](https://x.com/int64_le/status/1866519866934714623) a deep dive into the method",Prudent_Fly_1004,1hb82am,https://reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/,https://www.reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/,2024-12-10 18:08:50,4,1.0,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1hb82am
MachineLearning,[D] How do you keep up with the literature?,"Pretty much what the title says. What tools/strategies do you use to keep up with the literature?


EDIT: for context, I am a first year PhD student and I was referring to the literature in the particular 'niche' (if you can call anything a niche in ML, apart from a very few exceptions)",Rickmaster7,1hasdlo,https://reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/,https://www.reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/,2024-12-10 03:20:29,50,0.9,50,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1hasdlo
MachineLearning,[D] What‚Äôs stopping you from using foundation models for time series forecasting?,"I‚Äôve been experimenting with foundation models like¬†[Sulie](https://github.com/wearesulie/sulie),¬†[Granite TTM](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r1), and¬†[Amazon Chronos](https://github.com/amazon-science/chronos-forecasting), and each one has its own strengths. What‚Äôs really fascinating is how much faster you can get accurate forecasts with a¬†zero-shot approach. However, as much as these models improve forecasting, explainability remains a major challenge compared to more traditional methods like ARIMA, which are simpler to interpret.

I‚Äôm curious‚Äîdo you think explainability is a dealbreaker, or is there another reason why foundation models for forecasting aren‚Äôt gaining wider adoption? Would love to hear what‚Äôs been your biggest blocker or challenge in using these models.",Queasy_Emphasis_5441,1hb7ur1,https://reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/,https://www.reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/,2024-12-10 18:00:23,3,0.54,3,0,27,0,0,False,False,True,False,False,Discussion,self,t3_1hb7ur1
MachineLearning,[D] Question about heating system machine learning,"Hello, in conventional heating processes you control when to start heating and when to stop with target values. Once reached it'll stop and start the whole process over and over again, alongside some pump valves (how much water to go through the circulation).

I've captured several room temperatures, all start/stops and pump valve adjustments made by the heating automation software in a time series database for the last 5 years (every minute).

I'm trying to create a model which has a constant target value of e.g. 23 ¬∞C. The inputs for the model are the heating system status (on/off), pump valve positions and current room temperatures. Output should be how to set the heating system status and adjust the valve positions to achieve and hold the target value of the room temperatures. In the best case scenario it should replace the heating automation software completely. Other is to just advise or supervise the process.

A few problems come to my mind, where I'm not sure on how to approach these:

1. The process is slow and once heating starts the results can be seen 1 hour later as the temperature changes slowly. So the evaluation of the actions must be done with some delay?
2. My captured data contains historical temperatures, but I think it might be flawed. The temperature in the data is already influenced by the existing heating system. I don't have temperature data which show how the room temperatures realistically change without any heating systems. Is this a problem for learning? Do I need to create synthetic data?
3. Would it be better to train a model to output ""start heating / stop heating"" (leave the rest for the conventional heating automation) or to control the heating status and the pump valves itself?
4. What would be the best machine learning technique, e.g. un-/supervised, reinforcement learning?",QuickYogurt2037,1hb5dty,https://reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/,https://www.reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/,2024-12-10 16:15:47,2,0.75,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hb5dty
MachineLearning,[D] Meta's new LLama model,"So meta just dropped a new, more efficient Llama model, Llama 3.3 70B, that basically promises to cut compute costs for large AI models. Has anyone here had a chance to test it out? Curious to see how it performs compared to previous versions, in terms of speed, resource usage, and accuracy",Frosty_Programmer672,1han33i,https://reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/,https://www.reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/,2024-12-09 23:01:00,22,0.77,22,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1han33i
MachineLearning,"[D] How do you manage and track your large, evolving, image datasets?","I‚Äôm wondering how people manage the lifecycles of their large in-house datasets? Say &gt;1TB and 100k files.

In my new role we have multiple production models trained from in house datasets ranging in size from a few thousand to a few hundred thousand images. We also have huge amounts of fresh data coming in, more than 1M images per day, and so we are constantly mining that and sending new tranches off to be annotated.

Until now the team has been largely left to their own devices to manage this and the results are predictable. In some cases we can‚Äôt associate our prod models with any specific data. Some of our core datasets exist only in people‚Äôs home directories, ripe to be wiped out by a single misplaced command. For one model, thankfully being sunsetted, both the training code and original training data are known to be lost.

Parts of the org have adopted DVC which seems pretty good until the number of files or overall size gets big. On one end, some stuff the entire dataset into just a few archives and track them. That minimizes frustrations with hashes but uses a lot of storage when only a few files get updated. On the other end, some people track every single file which lets files be individually updated but is a pretty big pain to check in and out. Others split the difference of these two approaches, tracking chunks of the dataset as archives hierarchically.

So how does your org manage this? What works and what doesn‚Äôt when working with these large &amp; evolving datasets?",SirPitchalot,1haokqp,https://reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/,https://www.reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/,2024-12-10 00:09:58,16,0.89,16,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1haokqp
MachineLearning,[D] Is what I'm doing is correct?,"I'm working on an ML project.
I have 100 features and 2000000 rows(Balanced)
Which order shall I follow?

I have done,

1. Data inconsistencies handling
2. NULL imputation 
3. Standardization
4. One hot encoding
5. Data visualization 
6. Correlation check
7. PCA
8. Train test split
8. Model training
9. Evaluation 

For random forest I'm getting 1 for all the metrics for training data and 0.79 for test set.
For logistic regression ~0.79  for all metrics and for test set also getting the same.
For GBDT also ~0.79 for all metrics and for test set also getting the same.
Which model should I select? And is the above mentioned steps are followed in correct order?
",_crazy_muffin_,1has6jq,https://reddit.com/r/MachineLearning/comments/1has6jq/d_is_what_im_doing_is_correct/,https://www.reddit.com/r/MachineLearning/comments/1has6jq/d_is_what_im_doing_is_correct/,2024-12-10 03:10:16,7,0.89,7,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1has6jq
MachineLearning,[D] Model Provenance: How are you tracking your ML model lineage?,"Hey r/MachineLearning,I'm curious about how people in this community are handling model provenance - the practice of tracking the lineage and evolution of machine learning models throughout their lifecycle.

1. Are you currently using any tools or methods to track the provenance of your ML models?
2. If yes, what solutions are you using? Are they custom-built or off-the-shelf?
3. If not, do you see a need for such tools in your work?
4. What features would you consider essential in a model provenance solution?",crtahlin,1hb0o4e,https://reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/,https://www.reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/,2024-12-10 12:29:56,1,0.57,1,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1hb0o4e
MachineLearning,[D] Has anyone managed to train an LLM with model parallelism?,"Hello,

I am working on fine-tuning Llama-3.1 for my master‚Äôs thesis research. Unfortunately, my current situation forbids access to high-memory GPUs such as A100s. Instead, I have access to setups with multiple lower-memory GPUs, such as 4√ó3090 or 8√óV100.

Therefore I need to implement model parallelism to train my model as it doesn‚Äôt fit into a single GPU. However, I‚Äôve noticed that most frameworks primarily focus on data parallelism, which doesn‚Äôt address my needs.

Has anyone successfully trained a model by splitting it across multiple GPUs? If so, could you recommend frameworks or approaches I should explore? I am specifically looking for full training, although I am interested in hearing if someone managed this using LoRA.

Also, if there‚Äôs a more suitable subreddit for this type of question, please direct me to there.

Thank you!",anilozlu,1habr8l,https://reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/,https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/,2024-12-09 15:06:32,49,0.88,49,0,40,0,0,False,False,True,False,False,Discussion,self,t3_1habr8l
MachineLearning,[R] Improving robustness to corruptions with multiplicative weight perturbations - A simple yet effective approach to robustify neural networks to corruptions,"We would like to share and discuss this NeurIPS spotlight paper (disclaimer: I am a co-author).

**Paper**:¬†[https://arxiv.org/abs/2406.16540](https://arxiv.org/abs/2406.16540)  
**GitHub**:¬†[https://github.com/trungtrinh44/DAMP](https://github.com/trungtrinh44/DAMP)  
**DAMP**¬†(Data augmentation via multiplicative perturbations) is a simple yet effective approach to improving neural network robustness through multiplicative weight perturbations. Unlike traditional data augmentation methods, DAMP operates directly on model weights during training, enabling improved corruption robustness without compromising clean image performance or increasing computational cost.  
  
**Key Highlights:**

* **Theoretical Foundation**: DAMP demonstrates that input corruptions can be equivalently represented as multiplicative weight perturbations, providing a theoretical basis for weight-space data augmentation.
* **Simple Implementation**: The method requires only random Gaussian sampling and pointwise multiplication, maintaining almost the same training cost as standard SGD while being fully compatible with data parallelism.
* **Breakthrough in ViT Training**: Successfully trains Vision Transformers from scratch using only basic preprocessing, achieving ResNet50-level performance (23.7% top-1 error) on ImageNet without complex augmentations.
* **Advanced Integration**: When combined with MixUp and RandAugment, DAMP significantly improves both clean and corruption performance:
   * ViT-S/16: 20.09% clean error (vs 20.25% baseline), 58.30% avg corruption error (vs 60.07% baseline)
   * ViT-B/16: 19.36% clean error (vs 20.41% baseline), 56.76% avg corruption error (vs 58.83% baseline)

**Why DAMP?**¬†Unlike traditional approaches that rely on complex data augmentation pipelines or computationally expensive ensemble methods, DAMP provides a simple, theoretically-grounded solution to improving model robustness. Its ability to train Vision Transformers from scratch without advanced augmentations and compatibility with existing techniques makes it a practical choice for developing robust vision models.  
**Since DAMP has minimal overhead over standard training, it is particularly effective when applied to large models and datasets.**  
  
We welcome technical discussions, particularly regarding theoretical connections to other robustness methods and potential applications beyond computer vision!",emiurgo,1hap6gx,https://reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/,https://www.reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/,2024-12-10 00:38:41,8,0.83,8,0,2,0,0,False,False,True,False,False,Research,self,t3_1hap6gx
MachineLearning,[D] Seeking paper writing feedback: GPT-based Network Intrusion Detection System (arXiv published),"Hello everyone,



I'm an independent developer who has been working on applying GPT models to network intrusion detection. While I have experience in implementation, this is my first venture into academic paper writing, and I'm seeking feedback on the paper's presentation and structure.



I've recently published on arXiv:

\- Title: NIDS-GPT: TAKE PACKAGE AS LANGUAGE: ANOMALY DETECTION USING

TRANSFORMER

\- arXiv link: [https://arxiv.org/pdf/2412.04473](https://arxiv.org/pdf/2412.04473)



The paper presents a novel approach of treating each number in network packets as independent ""words"" for GPT processing. Our experiments show promising results - 100% accuracy on CICIDS2017 and car-hacking datasets under extreme imbalance conditions, and &gt;90% accuracy in one-shot learning.



As a first-time paper author, I'm seeking feedback on:

1. Paper structure and academic presentation standards

2. Visualization effectiveness and clarity

3. Methodology presentation

4. Validity of experimental comparisons

5. Strength of claims and conclusions



I'm particularly interested in feedback from:

\- Researchers in network security/intrusion detection

\- Those working with language models in non-NLP domains

\- Experienced paper writers/reviewers



The implementation is solid, but I want to ensure the paper effectively communicates the technical contributions to the academic community.



Thank you for your time and expertise!",EliaukMouse,1haufwv,https://reddit.com/r/MachineLearning/comments/1haufwv/d_seeking_paper_writing_feedback_gptbased_network/,https://www.reddit.com/r/MachineLearning/comments/1haufwv/d_seeking_paper_writing_feedback_gptbased_network/,2024-12-10 05:14:37,2,0.6,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1haufwv
MachineLearning,[D] Imbalance Dataset ,"Hi guyz , i am working on a project related to cloud computing. 
I have real time  dataset related to computing computing which i found on Internet , but the data have significant imbalance  e.g i have 4 classes two of them contain highest values while the rest two very much less .

How to play with this data to feed into ml model ( i know if i didn't balance the data then the model will be much bias towards majority class )

Need help ",zaynst,1hb0kew,https://reddit.com/r/MachineLearning/comments/1hb0kew/d_imbalance_dataset/,https://www.reddit.com/r/MachineLearning/comments/1hb0kew/d_imbalance_dataset/,2024-12-10 12:23:55,0,0.2,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hb0kew
MachineLearning,"[R] Diffusion Models, Image Super-Resolution, and Everything: A Survey","We are thrilled to share with you guys our latest survey paper on diffusion models applied to image super-resolution. You are welcome to take a look. It is also open access and published in IEEE TNNLS :) 



arXiv: [https://arxiv.org/abs/2401.00736](https://arxiv.org/abs/2401.00736)",Maleficent_Stay_7737,1h9wrv6,https://reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/,https://www.reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/,2024-12-09 00:09:15,97,0.97,97,0,1,0,0,False,False,True,False,False,Research,self,t3_1h9wrv6
MachineLearning,has anyone come across lines in image generated by GAN ? [D],"So I have been working with GAN's for a while, for simple image generation tasks especially training them in unsupervised ways . In many of them the output generated by GAN tend to have visible lines across the images. Here is an example, this happened when I try to generate heat maps. Does any of you have any idea why this happens ?? and ways to deal with them

https://preview.redd.it/c5tuc5udsu5e1.png?width=679&amp;format=png&amp;auto=webp&amp;s=0ecb84a81ec2993a147689ae3112935cc6ecf821

",Brief_Papaya121,1haeb2u,https://reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/,https://www.reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/,2024-12-09 16:56:56,3,0.62,3,0,10,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/TylwiwTTDZWEFdDfi3mI_bSdpiMc72jBYxnwRY3ekpc.jpg,t3_1haeb2u
MachineLearning,[P] Text-to-Video leaderboard: Compare State-Of-The-Art Text-To-Video Models,"Unlike text generation, text-to-video generation involves balancing realism, alignment, and artistic expression. But which one is the most important in terms of output quality?

We don‚Äôt know, that‚Äôs why we created a voting-based Text-to-Video Model Leaderboard inspired by the LLM Leaderboard lmarena.ai.

Currently, the leaderboard features five open-source models: HunyuanVideo, Mochi1, CogVideoX-5b, Open-Sora 1.2 and PyramidFlow, but we‚Äôre aiming to also include notable proprietary models from Kling AI, LumaLabs.ai and Pika.art.

Here‚Äôs a link to the leaderboard: [link](https://t2vleaderboard.lambdalabs.com/leaderboard/).  
We‚Äôd love to hear your thoughts, feedback, or suggestions. How do you think video generation models should be evaluated?",lambda-research,1ha54m0,https://reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/,https://www.reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/,2024-12-09 08:17:05,17,0.95,17,0,4,0,0,False,False,True,False,False,Project,self,t3_1ha54m0
MachineLearning,[D] [R] Question Answering Evaluation,"Are there any new metrics to evaluate QA systems (both open-domain and multiple choice) besides the standard Exact Match, F1, Accuracy, BLEU, ROUGE, BERTScore and so on ? I was reading a paper listing all of these metrics (https://arxiv.org/abs/2406.13232) but I‚Äôm curious if someone has released, or is currently working on, a new metric which better correlates with human judgment and/or takes into account the form in which LLMs provide answers to questions. For instance, if the models are not fine tuned, it‚Äôs hard to make them predict something like ‚ÄúAnswer: B‚Äù (for multiple-choice QA) or to make them predict some short text like ‚ÄúBarack Obama‚Äù (for open-domain QA). This behaviour makes the evaluation of LLMs inconsistent and I‚Äôm wondering is someone is actively working on this. ",Debonargon,1han84i,https://reddit.com/r/MachineLearning/comments/1han84i/d_r_question_answering_evaluation/,https://www.reddit.com/r/MachineLearning/comments/1han84i/d_r_question_answering_evaluation/,2024-12-09 23:07:18,0,0.4,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1han84i
MachineLearning,[D] How to Ensure Fair Comparison Between Few-Shot Prompting and Fine-Tuning in NLP Experiments,"I‚Äôm working on comparing a few-shot prompting mechanism with a fine-tuned GPT model for a text classification task. However, I realised that in a 5-fold validation setup, the fine-tuned model has access to significantly more data (ex: 4 folds for training) compared to my few-shot approach, which only uses a limited number of examples for prompting (At the moment I select n samples per each class from the training fold).  

`Example Scenario: My dataset has 4 classes, total number of data is 100, and I am conducting 4-way 5-shot experiment. For the traning set I have 80 (since it's 5-fold) and for test I have 20 data samples. For the fine-tune experiment I use the whole 80 data but for the few-shot experiment I use only 20 (4*5) data from the 80 traning samples. So the approach has only access to 20 samples of the whole training set.`

This imbalance feels unfair and makes it hard to assess the true performance difference between the two approaches. How can I modify the experimental setup to ensure a fair comparison? Should I restrict the fine-tuned model to use the same examples used in the few-shot prompting mechanism?    


Would love to hear your thoughts and suggestions!  ",The_Aoki_Taki,1haf6qq,https://reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/,https://www.reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/,2024-12-09 17:32:25,2,0.67,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1haf6qq
MachineLearning,[R] Distillation-Based Colorization of 3D Neural Radiance Fields for Consistent Novel View Synthesis,"This paper introduces a knowledge distillation approach to colorize 3D neural representations (NeRF/3DGS) from grayscale multi-view images. The core idea is transferring color information from pre-trained 2D colorization models to 3D scene representations while maintaining view consistency.

Key technical aspects:
- Uses a teacher-student framework where 2D colorization models guide 3D representations
- Works with both Neural Radiance Fields and 3D Gaussian Splatting
- No additional parameters or computation needed during inference
- Handles both indoor/outdoor scenes and different types of grayscale input (IR, historical photos)
- Maintains color consistency across viewpoints through volumetric optimization

Results:
- Matches or exceeds SOTA colorization quality on standard benchmarks
- Successfully colorizes complex scenes with varying lighting/materials
- Works effectively on legacy photographs and infrared images
- Demonstrates consistent colors across novel viewpoints
- Compatible with current NeRF/3DGS implementations

I think this method could be particularly valuable for cultural heritage applications, allowing us to create immersive 3D experiences from historical black and white photographs. The IR imaging capabilities also suggest potential applications in security and surveillance where color visualization of thermal data would be useful.

I think the key strength is how it bridges the gap between 2D colorization and 3D scene understanding without requiring architectural changes to existing 3D representations. This makes it quite practical for real-world adoption.

TLDR: New method colorizes 3D neural scenes from grayscale images using knowledge distillation, works with NeRF/3DGS, maintains view consistency, no extra inference cost.

[Full summary is here](https://aimodels.fyi/papers/arxiv/chromadistill-colorizing-monochrome-radiance-fields-knowledge-distillation). Paper [here](https://arxiv.org/abs/2309.07668).",Successful-Western27,1hae9oo,https://reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/,https://www.reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/,2024-12-09 16:55:19,2,0.67,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1hae9oo
MachineLearning,[R] Monet: Mixture of Monosemantic Experts for Transformers,"**Paper**:¬†[https://arxiv.org/abs/2412.04139](https://arxiv.org/abs/2412.04139)  
**GitHub**:¬†[https://github.com/dmis-lab/Monet](https://github.com/dmis-lab/Monet)

**Monet**¬†presents a novel approach to enhancing mechanistic interpretability in large language models (LLMs) through an innovative Sparse Mixture-of-Experts (SMoE) architecture. By directly incorporating sparse dictionary learning into end-to-end pretraining,¬†**Monet**¬†addresses the fundamental challenge of polysemanticity - where individual neurons respond to multiple unrelated concepts - while maintaining model performance.

**Key Highlights:**

* **Scalable Expert Architecture**:¬†**Monet**¬†introduces parameter-efficient expert decomposition methods that enable scaling to 262,144 experts per layer while ensuring total parameters scale proportionally to the square root of expert count.
* **Monosemantic Experts**: Through fine-grained expert specialization,¬†**Monet**¬†achieves monosemantic experts that demonstrate mutual exclusivity of knowledge, allowing transparent observation of model behavior and parametric knowledge.
* **Robust Knowledge Control**: The architecture enables precise manipulation of domain-specific knowledge, language capabilities, and toxicity mitigation without compromising general performance.

**Why Monet?**

Unlike traditional approaches using post-hoc reconstruction (like Sparse Autoencoders),¬†**Monet**¬†integrates interpretability directly into its architecture. This enables both transparent understanding of model internals and fundamental behavior control. By scaling monosemantic experts, Monet paves the way for more transparent and controllable language models.

We‚Äôd love to hear your feedback, questions, or any other inquiries you may have!",affjljoo3581,1ha4inl,https://reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/,https://www.reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/,2024-12-09 07:31:34,9,0.76,9,0,0,0,0,False,False,True,False,False,Research,self,t3_1ha4inl
MachineLearning,"[D] Is there such a thing as ""integrable programming""?","I come from a pure math background and have been getting up to speed at a new job in scientific AI/ML where I've been working a lot with JAX. JAX is great, love it, but I see a super common pattern where researchers will have a fully differentiable simulation and a couple of neural net architectures or something, but then there will be a bunch of relatively imprecise numerical estimations of integral values. Obviously I'm reading up on numerical methods and doing my best to restructure problems to solve more algebraically, but for my own curiosity, is there an equivalent of ""differentiable"" programming where you're handling ""integrable"" entities instead?

Obviously, this would be a much harder class of problems since you can integrate...well, everything. And that's how you end up solving PDEs on compact supports with weird ugly Holder bounds. But are there computational approaches (or hell, differentiable programming strategies I should be aware of) that move in this direction? Are there nice natural algebraic properties to be leveraged? Can you use a computational graph the same sort of way? How about, like, valid ways to extend to ""weakly-differentiable"" functional programming?

Hope this is relevant enough since it's learning JAX inspired...


edit:
Neat, thanks everyone. Long time lurker, super psyched to get some fruitful answers. I probalby should have specified that I'm not looking for fully generic solutions but most people took that leap anyways. As u/yldedly pointed me to, I think I'm basically asking for generalizations of the Risch algorithm for restricted multivariate function spaces...which [do seem to exist](https://arxiv.org/abs/1305.1481), but it's apparently an open but pretty unpopular area. Also Risch would be an absolute beast to implement...haven't actually looked at Risch-Norman but Maple has had it for a while. Bayesian quadrature and generally using Pyro is probably the most practical solution right now, thanks u/hugosc and u/daking999. I'll be waiting for the right problem to try out u/bregav 's suggestion of just autodiffing the antiderivative drictly (slick).",redwingviking,1h9ty31,https://reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/,https://www.reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/,2024-12-08 21:56:30,31,0.94,31,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1h9ty31
MachineLearning,[D] Context-aware entity recognition using LLMs,"Can anybody suggest some good models that can perform entity recognition but using LLM-level context? Such models are generally LLMs fine-tuned for Entity Recognition.
Usually, using traditional NER/ER pipelines, such as SpaCy's NER model, can only tag words that it has been trained on. Using LLMs fine-tuned for Entity Recognition (models such as GLiNER) can tag obscure entities, and not just basic entities such as Name, Place, Org, etc.",Ashwiihii,1h9stfq,https://reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/,2024-12-08 21:05:35,13,0.84,13,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h9stfq
MachineLearning,[P] Looking for daily keyword search database (any platform),"Hey there,

  
After exhaustively searching Google and trying to find APIs that would allow me to generate keyword search or post or comment frequency on any platform on a *daily* basis, I have been unable to find any providers of this type of data. Considering that this is kind of a niche request, I am dropping this inquiry here for the ML Gods of Reddit to assist.

Basically, I'm trying to create an ML model that can predict future increases/decreases in keyword usage (whether that be on Google Search or X posts; dosen't matter) on a daily basis. I've found plenty of monthly average keyword search providers but I cannot find any way to access more granulated, daily search totals for any platform. If you know of any sources for this kind of data, please drop them here... Or just tell me to give up if this is an impossible feat.",Appropriate-Touch515,1ha465g,https://reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/,https://www.reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/,2024-12-09 07:06:32,1,0.6,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1ha465g
MachineLearning,[P] ü•Ç FineWeb2 dataset: A sparkling update with 1000s of languages,,PhilipsNostrum,1h9ep0e,https://reddit.com/r/MachineLearning/comments/1h9ep0e/p_fineweb2_dataset_a_sparkling_update_with_1000s/,https://huggingface.co/datasets/HuggingFaceFW/fineweb-2,2024-12-08 08:47:55,51,0.97,51,0,2,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/wJFr_ML_3llzyRU5cjBT-wmv2Y189rHFs1r363OriqY.jpg,t3_1h9ep0e
MachineLearning,[D] A collection of various LLM Sampling methods,"In the last couple months, I read about various algorithms to perform LLM sampling. I decided to build my own inference stack and implement those algorithms. 

Here is the Github repo - [https://github.com/shreyansh26/LLM-Sampling](https://github.com/shreyansh26/LLM-Sampling)

The repo includes implementations for Top-k, Top-p (nucleus), Min-p, Typical, Epsilon, Eta, Beam search, Chain-of-Thought (CoT) decoding, Constrained JSON decoding and Speculative decoding.

Personally, I found this to be a good learning experience. Sharing here in case it helps someone!",shreyansh26,1h9fe8q,https://reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/,https://www.reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/,2024-12-08 09:39:17,42,0.93,42,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h9fe8q
MachineLearning,[R] Should I Use ML Experiment Tracking Tools Like MLflow or DVC for my Academic Paper?,"Hi everyone!

I'm a Computer Science graduate currently working on machine learning experiments for a research paper. I have a dataset and plan to compare error metrics across several deep learning models.

The conference where I intent to submit my paper requires that I provide the code and dataset and I also strongly believe that reproducibility is crucial in academic research. To this end, I'm using Docker and pip-compile to make the environment as reproducible as possible.

That said, I know there are tools like MLFlow and DVC for tracking ML experiments. However, I've never seen these tools mentioned in the code accompanying academic papers.

My questions are:

1. Are there any academic papers that use ML experiment tracking tools like MLFlow or DVC?
2. Should I use these tools for my research, even it means additional work?

I'm also experimenting with DVC because it stores experiments outputs in Git. However, my project involves running many distinct experiments in a single repository (comparing multiple ML algorithms). Would DVC or another tool be the best choice for this kind of workflow? Or is using such tools overkill for academic papers?

",mrlucasrib,1h9ig1e,https://reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/,https://www.reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/,2024-12-08 13:05:06,8,0.79,8,0,5,0,0,False,False,True,False,False,Research,self,t3_1h9ig1e
MachineLearning,[Project] Simulating Kubernetes Monitoring Data for a Deep Learning Prototype‚ÄîAny Thoughts?,"
We aim to build a prototype project using Deep Learning, but we require a dataset containing Kubernetes deployment metrics. I initially tried sourcing data from my company, but‚Äîspoiler alert‚Äîwe obviously can‚Äôt use it.

Our current idea is to create a virtual lab with a small Kubernetes cluster running a custom app. Using JMeter, we plan to simulate random scenarios to generate traffic similar to the microservices deployments in our company.

The synthetic data generated will be used to train our prototype. Afterward, we‚Äôll test the model by slightly modifying the JMeter scenarios and evaluating its performance against Kubernetes‚Äô default algorithm.

What are your thoughts? I know there‚Äôs existing literature on this, but I‚Äôd love to hear your expert opinion.

Thanks!",Ok-Consequence-8863,1h9vru3,https://reddit.com/r/MachineLearning/comments/1h9vru3/project_simulating_kubernetes_monitoring_data_for/,https://www.reddit.com/r/MachineLearning/comments/1h9vru3/project_simulating_kubernetes_monitoring_data_for/,2024-12-08 23:20:46,0,0.4,0,0,0,0,0,False,False,True,False,False,Project,self,t3_1h9vru3
MachineLearning,"[D] Last Week in Medical AI: Top LLM Research Papers/Models (December 2 - December 7, 2024)","[\[D\] Last Week in Medical AI: Top LLM Research Papers\/Models \(December 2 - December 7, 2024\)](https://preview.redd.it/exeie0jxdm5e1.jpg?width=1386&amp;format=pjpg&amp;auto=webp&amp;s=d1fad8f6511ebc4dd9e8c73cc98ca9f6d45f750d)

  
**Medical LLM &amp; Models**

* Block MedCare: Blockchain AI &amp; IoT
   * This research proposes a novel Ethereum-based system for secure and efficient Electronic Health Record (EHR) management, empowering patients with data control.
* LLMs4Life: Biomedical Ontology Learning
   * This paper extends the NeOn-GPT pipeline for ontology learning using LLMs with advanced prompt engineering and ontology reuse to improve generated ontologies' domain-specific reasoning and structural depth in complex domains like life sciences.
* LLaMA II for Multimodal Diagnosis
   * This paper explores multimodal fusion methods for medical data using a transformer-based model with a LLaMA II backbone, focusing on disease classification with chest X-rays and clinical reports from the OpenI dataset.
* Compact LLM for EHR Privacy
   * This paper introduces a compact LLM framework for local deployment in healthcare settings with strict privacy requirements and limited resources.  It uses a novel preprocessing technique with information extraction methods like regular expressions to enhance smaller LLM performance on EHR data.

**Frameworks &amp; Methods**

\- RARE: Retrieval-Augmented Reasoning  
\- STORM: Strategies for Rare Events  
\- TransFair: Fair Disease Classification  
\- PePR: Performance Per Resource  
\- Medical LLM Best Practices

**LLM Applications**

\- Medchain: LLMs in Clinical Practice  
\- Query Nursing Note Summarization  
\- CLINICSUM: Patient Conversation Summaries  
\- Text Embeddings for Classifiers

**LLM Benchmarks**

\- Polish Medical Exams Transfer  
\- Single-Cell Omics Annotation  
\- LLMs in Precision Medicine  
\- Low-Resource Healthcare Challenges

**Other Models**

\- LLM Chatbot Hallucinations  
\- Multi-stage Chest X-ray Diagnosis  
\- EchoONE: Echocardiography AI  
\- Radiology Report Grounding

**Ethics &amp; Fairness**

\- Privacy in Medical Imaging  
\- Demographic Fairness in AI

**Datasets**

\- LLM Scientific Knowledge Extraction  
\- Biomedical Knowledge Review

Full thread in detail:¬†[https://x.com/OpenlifesciAI/status/1865584829057929303](https://x.com/OpenlifesciAI/status/1865584829057929303)",aadityaura,1h9hytj,https://reddit.com/r/MachineLearning/comments/1h9hytj/d_last_week_in_medical_ai_top_llm_research/,https://www.reddit.com/r/MachineLearning/comments/1h9hytj/d_last_week_in_medical_ai_top_llm_research/,2024-12-08 12:37:06,3,0.67,3,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ui7fRqyxPWPs0C2SeEyfrrVgHu2f6NKgstvq8_5XvWs.jpg,t3_1h9hytj
MachineLearning,[R] O1 replication paper,"Hi Everyone,

Just released a paper that I think hints at how OpenAI might have developed some of O1's remarkable reasoning capabilities. TLDR- you need a small dataset of really high quality human paired with a little bit of RL

Here are some of they key take ways from the research

* Reasoning data is extremely scarce on the internet. It's very difficult to find data that really shows the problem solving process e.g hypothesis testing, backtracking etc
* RL although important is overrated by the general community. It's really the cherry on top. The human data does most of the heavy lifting. See deep-seek math for more info on this

Paper can be found here:¬†[https://arxiv.org/abs/2412.04645](https://arxiv.org/abs/2412.04645)

Not saying this is definitively how o1 works but results suggest that this method can be used to create very similar behaviour

Paper a preprint so happy to clarify anything that's not clear.

Happy to answer any questions on the paper.",Brosarr,1h9zjf1,https://reddit.com/r/MachineLearning/comments/1h9zjf1/r_o1_replication_paper/,https://www.reddit.com/r/MachineLearning/comments/1h9zjf1/r_o1_replication_paper/,2024-12-09 02:31:35,0,0.42,0,0,7,0,0,False,False,True,False,False,Research,self,t3_1h9zjf1
MachineLearning,[D] - Approximating Long Convolutions,"I think I'm beginning to wrap my head around the way modern, ""deep"" state-space models (e.g Mamba, S4, etc.) leverage polynomial multiplication to speed up very long convolutions.  There's a short and simple description here: 

[https://hazyresearch.stanford.edu/blog/2023-12-11-conv-tutorial](https://hazyresearch.stanford.edu/blog/2023-12-11-conv-tutorial)

**I'm curious if there are other methods for approximating long convolutions that are well-known or widely-used?**  I'm in the audio field and interested in learning long \_FIR\_ filters to describe the resonances of physical objects, like instruments, or rooms.  



**I'm definitely aware that multiplication in the (complex) frequency domain is equivalent to convolution in the time domain and that, because of the fast-fourier transform, this can yield increased efficiency**.  However, this still results in storing a lot of gradient information that my intuition tells me (possibly incorrectly) is full of redundancy and waste.

  
Stateful, IIR, or auto-regressive approaches are \_one\_ obvious answer, but this changes the game in terms of training and inference parallelization.  Block-coding, or fixed-frame size approaches reign supreme, at least in audio coding, but have their own issues in terms of windowing artifacts, etc.

  
A couple ideas I've considered, but have not yet tried, or looked too deeply into:

\- First performing PCA in the complex frequency domain, reducing the point-wise multiplication that must    occur.  Without some additional normalization up-front, it's likely this would be equivalent to downsampling/low-pass filtering and performing the convolution there.  The learnable filter bank would live in the PCA space, reducing the overall number of learned parameters.

\- A Compressed Sensing inspired approach, where we perform a sparse, sub-sampled random set of points from both signals and recover the full result based on the assumption that both convolver and convolvee? are sparse in the fourier domain.  This one is pretty half-baked.

  
I'd love to hear about papers you've read, or thoughts you've had about this problem.

",JohnVinyard,1h9n9pw,https://reddit.com/r/MachineLearning/comments/1h9n9pw/d_approximating_long_convolutions/,https://www.reddit.com/r/MachineLearning/comments/1h9n9pw/d_approximating_long_convolutions/,2024-12-08 17:01:49,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h9n9pw
MachineLearning,[D] Offline AI/ML activity for high school students?,"Next week, I am giving an hour of code presentation at a local high school. Since I run my university's artificial intelligence club, I'd like to center it around AI. I've done some AI focused activities with high schoolers with various levels of success in the past, but haven't found ""the one"" yet. Any ideas on what I could try next? I'll list what I've done so far as well as the restrictions are this particular event.

Restrictions:

1. These are random students in grades 9-12, so I can't rely on any prior computer science knowledge
2. We don't have access to computers so everything has to be offline. I *may* be able to get ipads for them though
3. We have 1 hour for the event. Filling around 45 minutes would be ideal so we don't go over and we don't end up with too much extra time.

Things I've tried before:

1. AI Kahoot + Simple lecture explaining neural networks - The students liked the kahoot, but I dramatically overestimated their knowledge (and interest) in AI. Stupidly thought at least a handful of them would have at least heard of a neural net, but not a single student had lmao
2. Wordle AI pseudocode and code along - I had the students get into groups and brainstorm how they would make an algorithm to solve the wordle. Then I coded their solutions on the projector and we competed against my algorithm. This one was cool, but I think it only really works if the students are *really engaged* and I really don't think this group will be that engaged.
3. Image classification code along - I've done this by myself on the projector (bad) and as a group with students with some CS experience (decent). But this one wouldn't work for this event I just thought I'd include it
4. Turing test activity - Have the students guess which answer came from ChatGPT and which came from a student volunteer. The volunteer will leave the room and the rest of the students will ask a question (like ""what is the meaning of life"" or something). I'll write the volunteer's answer and ChatGPT's answer on a powerpoint slide and have the student guess which was GPT. This has been really successful with middle school students in the past, but I worry it's a bit too childish for high schoolers.

\---

I tend to struggle to figure out activities for high school students because I don't want to undermine their intelligence but I don't want to throw a complicated activity at them that they don't understand.",j0ngle6421,1h9ks8v,https://reddit.com/r/MachineLearning/comments/1h9ks8v/d_offline_aiml_activity_for_high_school_students/,https://www.reddit.com/r/MachineLearning/comments/1h9ks8v/d_offline_aiml_activity_for_high_school_students/,2024-12-08 15:08:37,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h9ks8v
MachineLearning,"[N] Sama, an AI sweatshop, pays workers in Kenya $2 an hour to filter and label porn, beastiality, suicide, child abuse, for hours on end!!",,BotherBubbly5096,1h8nhbh,https://reddit.com/r/MachineLearning/comments/1h8nhbh/n_sama_an_ai_sweatshop_pays_workers_in_kenya_2_an/,https://youtu.be/qZS50KXjAX0,2024-12-07 07:38:08,318,0.85,318,0,123,0,0,False,False,False,False,False,News,https://b.thumbs.redditmedia.com/6xyP-Tq6WRzMpjniGg5h_3Z_bhf29lyA0LV_LQ19-1s.jpg,t3_1h8nhbh
MachineLearning,[P] I got too frustrated trying to test all these AI cookbooks and recipes,"Over the last year of building AI enabled SaaS applications I became increasingly frustrated at the developer experience of going from AI RAG cookbooks authored in jupyter notebooks to integrating it into my application. Notebooks are great and all but it's incredibly hard to test which part of it was actually important for my app. This led me down the road of having to understand every piece of code in each notebook, deciphering what was important, somehow build an API server as a POC to then hook it into my app. The feedback loop was excruciatingly long, painful, and most of the time I canned the POC because it wasn't quite what I wanted.

this is when it dawned on me that the roles in the AI developer world are fractured into two. Data Scientists and AI devs want easy notebooks to test methods and techniques but do not care to ship something that can be easily be consumed by applications.

In the other camp lies application devs, they just want simple API's that they can use to test quickly and verify these AI methods enhance their application.

Enter KitchenAI.

A way to bridge the gap between the two by converting AI related Jupyter notebooks into a ready made production API server so that it becomes easy to test various cookbooks, recipes, and techniques. Shortening the development cycle in half while giving users a complete local experience with the ability to share them as docker containers.

Completely vendor agnostic and framework agnostic, the goal is to give developers the most about of freedom to use the libraries they already feel most comfortable using.

It comes with a plugin architecture so I envision our team and the community building all sorts of llmops type plugins like evaluation frameworks, observability, prompt management and more.

A lot of hard work was put to provide something that is totally open source, local, and with battle tested technology like Django so that developers didn't have to rely on 3rd party providers.

We‚Äôve launched this repo under Apache license so any developer can use the tool. We're working hard to provide a managed cloud version with much deeper integrations, metrics, analytics, and workflows for those that want have more complex demands

Give it a spin:¬†[https://github.com/epuerta9/kitchenai.](https://github.com/epuerta9/kitchenai)¬†Let us know what you think!",wait-a-minut,1h9wcdo,https://reddit.com/r/MachineLearning/comments/1h9wcdo/p_i_got_too_frustrated_trying_to_test_all_these/,https://www.reddit.com/r/MachineLearning/comments/1h9wcdo/p_i_got_too_frustrated_trying_to_test_all_these/,2024-12-08 23:48:32,0,0.4,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1h9wcdo
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1h99kae,https://reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/,2024-12-08 03:15:09,6,0.69,6,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h99kae
MachineLearning,"[P] I cannot find this open-source transformer on GitHub, released recently, for the life of me.
","There was a paper released along with a GitHub repository of an extremely well-made transformer designed for testing out new components. But I can't find it! It's not one of the ones that has existed like HuggingFace ones. Any clue?

",Breck_Emert,1h8zlz3,https://reddit.com/r/MachineLearning/comments/1h8zlz3/p_i_cannot_find_this_opensource_transformer_on/,https://www.reddit.com/r/MachineLearning/comments/1h8zlz3/p_i_cannot_find_this_opensource_transformer_on/,2024-12-07 19:05:21,14,0.64,14,0,19,0,0,False,False,True,False,False,Project,self,t3_1h8zlz3
MachineLearning,"How do you manage resources or optimize cost when training models in cloud services like aws sagemaker, or gcp vertex ai? [D]","Hey all, I've been using sagemaker quite a bit lately for training ML models and doing deployments. I know enough about aws and instance types to create training nodes that have enough capacity to train my models, but many times I am underutilizing RAM, GPU memory, or CPUs, so it feels like this leads to a lot of waste (and extra cost).  
How do you guys figure out what type of instance or resources would best fit your needs without being too wasteful?  
Is there any way to adjust resources automatically, or any library that could handle that for you?",InformationEmpty1440,1h93rlt,https://reddit.com/r/MachineLearning/comments/1h93rlt/how_do_you_manage_resources_or_optimize_cost_when/,https://www.reddit.com/r/MachineLearning/comments/1h93rlt/how_do_you_manage_resources_or_optimize_cost_when/,2024-12-07 22:17:25,5,0.86,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h93rlt
MachineLearning,[R] For a change of topic: some nonLLM focused work of mine: Bias-Free Sentiment Analysis through Semantic Blinding and Graph Neural Networks,"In my academic field (social sciences) I deal with the problem of bias in SA models. My previous work showed that deep learning SA systems inherit bias (e.g. nonrepresentative of the population political bias) from annotators: 

https://arxiv.org/abs/2407.13891

Now I devised a solution that used a technique I call semantic blinding to provide only the bare necessary information for the model to predict emotions in text, leaving no signal for the model to overfit and produce bias from:

https://arxiv.org/abs/2411.12493

Interested to hear your thoughts before I publish the SProp Gnn.

Do you think it could be useful beyond the academia?



",Hub_Pli,1h8meas,https://reddit.com/r/MachineLearning/comments/1h8meas/r_for_a_change_of_topic_some_nonllm_focused_work/,https://i.redd.it/vh80i11ndd5e1.jpeg,2024-12-07 06:21:47,51,0.86,51,0,10,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/sSwXTBHlRlELGHTRvPNAxo6LSQVRdxL5HTNiMSYTfTs.jpg,t3_1h8meas
MachineLearning,[D] AAAI 2025 Phase 2 Decision,"When would the phase 2 decision come out?  
I know the date is December 9th, but would there be chances for the result to come out earlier than the announced date?  
or did it open the result at exact time in previous years? (i.e., 2024, 2023, 2022 ....)

Kinda make me sick to keep waiting.",No-Style-7975,1h8kkjv,https://reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,https://www.reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,2024-12-07 04:27:30,50,0.93,50,0,261,0,0,False,False,True,False,False,Discussion,self,t3_1h8kkjv
MachineLearning,[P] Extract Transcripts with Positive Emotions in batch,"Check out this example project on how to find transcripts of audio recordings with¬†positive emotions. A good example of a project demonstrating of extract actionable insights from audio!

It takes **common voice** dataset of audio files from hagging face, applies emotion recognition model and **whisper-tiny** model for the transcripts. All is organized in a nice looking batch pipeline. 

An interesting detail - No need to extract archives! This pipeline analyzes audio files¬†directly from tar archives, saving you extra steps.

Video: [https://www.youtube.com/watch?v=OCm5W0L5BTU](https://www.youtube.com/watch?v=OCm5W0L5BTU)  
Colab notebook: [https://colab.research.google.com/github/iterative/datachain-examples/blob/main/audio/hf\_common\_voice.ipynb](https://colab.research.google.com/github/iterative/datachain-examples/blob/main/audio/hf_common_voice.ipynb)  
Jupyter Notebook: [https://github.com/iterative/datachain-examples/blob/main/audio/hf\_common\_voice.ipynb](https://github.com/iterative/datachain-examples/blob/main/audio/hf_common_voice.ipynb)",dmpetrov,1h92b70,https://reddit.com/r/MachineLearning/comments/1h92b70/p_extract_transcripts_with_positive_emotions_in/,https://www.reddit.com/r/MachineLearning/comments/1h92b70/p_extract_transcripts_with_positive_emotions_in/,2024-12-07 21:08:30,1,0.6,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1h92b70
MachineLearning,[R]  GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?,"Hi everyone,

I‚Äôm currently working through the recent paper ‚ÄúGraphMaker: Can Diffusion Models Generate Large Attributed Graphs?‚Äù, but I‚Äôve run into some issues and was hoping someone here might have insights.

* Posterior Distribution: In the implementation, a posterior distribution is used, but I couldn‚Äôt find the formula or explanation in the paper. Does anyone know where this comes from or how it‚Äôs derived?
* Asynchronous Model: The paper and its implementation don‚Äôt seem entirely consistent when it comes to the asynchronous model. Specifically:
   * Is the generation process done step-by-step asynchronously?
   * Or does it first denoise the attribute vectors entirely before moving on to edge denoising?

I‚Äôve tried searching online, but since this is a new paper, there isn‚Äôt much discussion or documentation yet. Any help, advice, or pointers would be greatly appreciated!",Noname_emanon_,1h8z44t,https://reddit.com/r/MachineLearning/comments/1h8z44t/r_graphmaker_can_diffusion_models_generate_large/,https://www.reddit.com/r/MachineLearning/comments/1h8z44t/r_graphmaker_can_diffusion_models_generate_large/,2024-12-07 18:43:09,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h8z44t
MachineLearning,[R] JAX vs TensorFlow-XLA ,"
Few months ago, I migrated from TF 2.0 to Jax. I found that jax is significantly faster than Tf. I noticed in the official documentation that it relies on XLA default that uses JIT compilation which makes execution faster. I also noticed that TF graphs also have option to enable JIT compilation with XLA. But still jax dominates TF with XLA. I just want to know why.",Odd-Detective289,1h8j2e5,https://reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,https://www.reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,2024-12-07 03:02:19,15,0.9,15,0,7,0,0,False,False,True,False,False,Research,self,t3_1h8j2e5
MachineLearning,[D] Any OCR recommendations for illegible handwriting?,"
Has anyone had experience using an ML model to recognize handwriting like this? The notebook contains important information that could help me decode a puzzle I‚Äôm solving. I have a total of five notebooks, all from the same person, with consistent handwriting patterns. My goal is to use ML to recognize and extract the notes, then convert them into a digital format.

I was considering Google API after knowing that Tesseract might not work well with illegible samples like this. However, I‚Äôm not sure if Google API will be able to read it either. I read somewhere that OCR+ CNN might work, so I‚Äôm here asking for suggestions. Thanks! Any advice/suggestions are welcomed! ",SpaceSheep23,1h7x5us,https://reddit.com/r/MachineLearning/comments/1h7x5us/d_any_ocr_recommendations_for_illegible/,https://www.reddit.com/gallery/1h7x5us,2024-12-06 08:53:03,210,0.91,210,0,173,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/LeDstkPiGT-yO7zkyvRRyx6sE33qB2W32UGe1t81Kkk.jpg,t3_1h7x5us
MachineLearning,How to solve the STT Cutoff Problem [D],"Hello folks, 

I've been working on an agentic solution where you can have an autonomous agent taking live calls. We're using a pipeline of Speech to Text, LLM for generating responses and then Text to Speech. In this pipeline, Speech to text is causing some issues because it's difficult to determine when exactly a sentence is over since the user can take pauses. Moreover, when multiple inputs go into LLM, multiple responses are generated and they queue up for Text to speech. How would you solve this problem? How would you also handle cases where the user interrupts the agent?",Leo2000Immortal,1h8r32q,https://reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,https://www.reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,2024-12-07 12:04:55,1,0.6,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h8r32q
MachineLearning,[D]Stuck in AI Hell: What to do in post LLM world,"
Hey Reddit,

I‚Äôve been in an AI/ML role for a few years now, and I‚Äôm starting to feel disconnected from the work. When I started, deep learning models were getting good, and I quickly fell in love with designing architectures, training models, and fine-tuning them for specific use cases. Seeing a loss curve finally converge, experimenting with layers, and debugging training runs‚Äîit all felt like a craft, a blend of science and creativity. I enjoyed implementing research papers to see how things worked under the hood. Backprop, gradients, optimization‚Äîit was a mental workout I loved.

But these days, it feels like everything has shifted. LLMs dominate the scene, and instead of building and training models, the focus is on using pre-trained APIs, crafting prompt chains, and setting up integrations. Sure, there‚Äôs engineering involved, but it feels less like creating and more like assembling. I miss the hands-on nature of experimenting with architectures and solving math-heavy problems.

It‚Äôs not just the creativity I miss. The economics of this new era also feel strange to me. Back when I started, compute was a luxury. We had limited GPUs, and a lot of the work was about being resourceful‚Äîquantizing models, distilling them, removing layers, and squeezing every bit of performance out of constrained setups. Now, it feels like no one cares about cost. We‚Äôre paying by tokens. Tokens! Who would‚Äôve thought we‚Äôd get to a point where we‚Äôre not designing efficient models but feeding pre-trained giants like they‚Äôre vending machines?

I get it‚Äîabstraction has always been part of the field. TensorFlow and PyTorch abstracted tensor operations, Python abstracts C. But deep learning still left room for creation. We weren‚Äôt just abstracting away math; we were solving it. We could experiment, fail, and tweak. Working with LLMs doesn‚Äôt feel the same. It‚Äôs like fitting pieces into a pre-defined puzzle instead of building the puzzle itself.

I understand that LLMs are here to stay. They‚Äôre incredible tools, and I respect their potential to revolutionize industries. Building real-world products with them is still challenging, requiring a deep understanding of engineering, prompt design, and integrating them effectively into workflows. By no means is it an ‚Äúeasy‚Äù task. But the work doesn‚Äôt give me the same thrill. It‚Äôs not about solving math or optimization problems‚Äîit‚Äôs about gluing together APIs, tweaking outputs, and wrestling with opaque systems. It‚Äôs like we‚Äôve traded craftsmanship for convenience.

Which brings me to my questions:

1. Is there still room for those of us who enjoy the deep work of model design and training? Or is this the inevitable evolution of the field, where everything converges on pre-trained systems?


2. What use cases still need traditional ML expertise? Are there industries or problems that will always require specialized models instead of general-purpose LLMs?


3. Am I missing the bigger picture here? LLMs feel like the ‚Äúkernel‚Äù of a new computing paradigm, and we don‚Äôt fully understand their second- and third-order effects. Could this shift lead to new, exciting opportunities I‚Äôm just not seeing yet?


4. How do you stay inspired when the focus shifts? I still love AI, but I miss the feeling of building something from scratch. Is this just a matter of adapting my mindset, or should I seek out niches where traditional ML still thrives?



I‚Äôm not asking this to rant (though clearly, I needed to get some of this off my chest). I want to figure out where to go next from here. If you‚Äôve been in AI/ML long enough to see major shifts‚Äîlike the move from feature engineering to deep learning‚Äîhow did you navigate them? What advice would you give someone in my position?

And yeah, before anyone roasts me for using an LLM to structure this post (guilty!), I just wanted to get my thoughts out in a coherent way. Guess that‚Äôs a sign of where we‚Äôre headed, huh?

Thanks for reading, and I‚Äôd love to hear your thoughts!

TL;DR: I entered AI during the deep learning boom, fell in love with designing and training models, and thrived on creativity, math, and optimization. Now it feels like the field is all about tweaking prompts and orchestrating APIs for pre-trained LLMs. I miss the thrill of crafting something unique. Is there still room for people who enjoy traditional ML, or is this just the inevitable evolution of the field? How do you stay inspired amidst such shifts?

Update: Wow, this blew up. Thanks everyone for your comments and suggestions. I really like some of those. This thing was on my mind for a long time, glad that I put it here. Thanks again!",Educational_News_371,1h7jg87,https://reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,2024-12-05 20:49:57,800,0.97,800,0,218,0,0,False,False,True,False,False,Discussion,self,t3_1h7jg87
MachineLearning,[R] Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis,"New paper and code for the scale-wise transformer for fast text-to-image generation from our team at Yandex Research

Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being faster than distilled diffusion models.

Code with checkpoints: [https://github.com/yandex-research/switti](https://github.com/yandex-research/switti)

[Generation examples](https://preview.redd.it/7jy3jfxhi95e1.png?width=3094&amp;format=png&amp;auto=webp&amp;s=e80ed27c0b746ec782026581500582a5dd03555d)

",_puhsu,1h85z2c,https://reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,https://www.reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,2024-12-06 16:58:21,13,0.88,13,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/Io25PLG5YopCFj4DOTGJ-MxIEcQdXRQbpwevn2M-Kts.jpg,t3_1h85z2c
MachineLearning,[R] Agentic Retrieval Augmented Generation with Memory,"Imagine a customer support chatbot for an e-commerce platform that retrieves relevant product details from its knowledge base and performs web searches for additional information. Furthermore, it remembers past conversations to deliver a seamless and personalized experience for returning users.   
  
Here is how it works:  
  
\- Store your own data in the knowledge base‚Äîin our case, a Website URL.  
\- Convert the data into embeddings and save it in the Qdrant Vector Database.  
\- Use phidata Agentic Workflow to combine Tools, LLM, Memory, and the Knowledge Base.

Code Implementation Video: [https://www.youtube.com/watch?v=CDC3GOuJyZ0](https://www.youtube.com/watch?v=CDC3GOuJyZ0)",External_Ad_11,1h8945d,https://reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,https://www.reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,2024-12-06 19:10:50,7,0.69,7,0,1,0,0,False,False,True,False,False,Research,self,t3_1h8945d
MachineLearning,[D] How does OpenAI‚Äôs O1 outperform others in math despite limitations noted in recent papers?,"Recent research has revealed that state-of-the-art LLMs often struggle with mathematical reasoning:

1. The GSM-Symbolic benchmark highlights that LLMs frequently fail when numerical values or question wording change, suggesting reliance on memorization rather than true mathematical understanding ([source](https://arxiv.org/pdf/2410.05229.pdf)).
2. Logical reasoning studies, like the AIW problem, show inconsistent performance even for basic reasoning tasks ([source](https://arxiv.org/pdf/2406.02061.pdf)).
3. Furthermore, research indicates LLMs lack effective self-correction capabilities, with performance degrading after multiple iterations ([source](https://arxiv.org/pdf/2310.01798.pdf)).

Despite these challenges, OpenAI‚Äôs new O1 model reportedly exceeds all other models in math benchmarks. How does it address these known issues in mathematical reasoning, such as:

* Reliance on memorization instead of understanding?
* Inconsistencies in reasoning across problem variations?
* Inability to self-correct errors effectively?

Would love to hear insights or hypotheses!",AImSamy,1h7vj5t,https://reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,2024-12-06 06:53:09,62,0.89,62,0,42,0,0,False,False,True,False,False,Discussion,self,t3_1h7vj5t
MachineLearning,[D] Have we officially figured out yet how O1 models differ from previous models?,"Edit: I have misworded the title as if OpenAI would confirm how O1 was implemented. I have changed the text to reflect what I meant say.



I really want to deep dive into the technicals of how the O1 models perform better than previous models.

Have researchers come to any definitive agreement as to what OpenAI could have possible done to achieve O1?

From reading online I hear about MCTS, COT... etc, but are any of these methods in large agreement by researhers?
",Daveboi7,1h7zfjg,https://reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,2024-12-06 11:37:51,18,0.85,18,0,28,0,0,False,False,True,False,False,Discussion,self,t3_1h7zfjg
MachineLearning,[D] Encode over¬†100 million rows¬†into embeddings ,"Hey everyone,

I'm working on a pipeline to encode over¬†**100 million rows**¬†into embeddings using¬†**SentenceTransformers**,¬†**PySpark**, and¬†**Pandas UDF**¬†on¬†**Dataproc Serverless**.

Currently, it takes several hours to process everything. I only have one column containing sentences, each under 30 characters long. These are encoded into¬†**64-dimensional vectors**¬†using a custom model in a Docker image.

At the moment, the job has been running for over¬†**12 hours**¬†with¬†**57 executors**¬†(each with¬†**24GB of memory and 4 cores**). I‚Äôve partitioned the data into¬†**2000 partitions**, hoping to speed up the process, but it's still slow.

Here‚Äôs the core part of my code:

    F.pandas_udf(returnType=ArrayType(FloatType()))
    def encode_pd(x: pd.Series) -&gt; pd.Series:
        try:
            model = load_model()
            return pd.Series(model.encode(x, batch_size=512).tolist())
        except Exception as e:
            logger.error(f""Error in encode_pd function: {str(e)}"")
            raise

The¬†`load_model`¬†function is as follows:

    def load_model() -&gt; SentenceTransformer:
        model = SentenceTransformer(
            ""custom_model"", 
            device=""cpu"", 
            cache_folder=os.environ['SENTENCE_TRANSFORMERS_HOME'], 
            truncate_dim=64
        )
        return model

I tried broadcasting the model, but I couldn't refer to it inside the Pandas UDF.

Does anyone have suggestions to optimize this? Perhaps ways to load the model more efficiently, reduce execution time, or better utilize resources?",nidalap24,1h7xnce,https://reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,2024-12-06 09:29:37,15,0.86,15,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h7xnce
MachineLearning,[D] How to actually prevent overfitting in practice in ScikitLearn ?,"We all saw in class the trade off between bias and variance, that we don't want our train loss to keep going down and our test loss go up. 

But in practice I feel like doing hyperparameter tuning for classic ML models with GridSearchCV / BayesSearchCV is not enough. Even though I do cross validation, the search.best\_model obtained at the end is almost always overfitting. 

How can you actually perform a search that will give you a robust generalized model with higher chances ? ",desslyie,1h8paqz,https://reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,https://www.reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,2024-12-07 09:55:06,0,0.31,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h8paqz
MachineLearning,[R] Zero shot Meme-interpretability of LLMs,"Head to head of meme-interpretability with the same image and text prompt!

Anecdotal but interesting responses. 

Also clear winner!

",No_Cartoonist8629,1h8nc78,https://reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,https://www.reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,2024-12-07 07:27:43,0,0.29,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1h8nc78
MachineLearning,[D] selective transfer learning ,"Hello everyone,

I am looking for methods that can automatically categorize and select layers from  for transfer learning. If you know any such methods or research please let me know or share. 

Thanks ",reshail_raza,1h8cawc,https://reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,https://www.reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,2024-12-06 21:30:44,0,0.5,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h8cawc
MachineLearning,[R] Towards Time Series Reasoning with LLMs,,HydrousIt,1h7sr3n,https://reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/,https://arxiv.org/abs/2409.11376,2024-12-06 04:06:32,10,0.82,10,0,2,0,0,False,False,False,False,False,Research,default,t3_1h7sr3n
MachineLearning,[R] Mastering Board Games by External and Internal Planning with Language Models - DeepMind,"Paper: https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf

Abstract:

While large language models perform well on a range of complex tasks (e.g., text generation, question
answering, summarization), robust multi-step planning and reasoning remains a considerable challenge
for them. In this paper we show that search-based planning can significantly improve LLMs‚Äô playing
strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We
introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo
Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search,
the model directly generates in-context a linearized tree of potential futures and a resulting final choice.
Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and
value functions across these games. We find that our pre-training method minimizes hallucinations, as
our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and
external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level
performance in chess while operating on a similar move count search budget per decision as human
Grandmasters. The way we combine search with domain knowledge is not specific to board games,
suggesting direct extensions into more general language model inference and training techniques.",RobbinDeBank,1h7nshy,https://reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,2024-12-05 23:58:37,9,0.86,9,0,1,0,0,False,False,True,False,False,Research,self,t3_1h7nshy
MachineLearning,[D] Multimodal AI,"Multimodal AI is changing the game by combining text, images, and even video into a single, cohesive system. It‚Äôs being talked about as a major leap in AI capabilities.

What industries do you think will benefit the most from this tech? And are there any challenges you see in integrating these models into everyday use?

Would love to hear everyone's thoughts!",Frosty_Programmer672,1h8enzy,https://reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,https://www.reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,2024-12-06 23:17:54,0,0.09,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h8enzy
MachineLearning,[D] How to remove noise in this dataset,"I have a dataset that, when plotted, shows a noisy black line. I'd like to smooth out this noise to get a cleaner trend line (similar to the red line shown). What methods would you recommend for noise reduction?

https://preview.redd.it/p8q0i4f9h45e1.png?width=1574&amp;format=png&amp;auto=webp&amp;s=29cd8c82af7b54a1d3da22655502fd5cf406e807",mrtule,1h7ohd0,https://reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,2024-12-06 00:30:30,6,0.88,6,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1h7ohd0
MachineLearning,U-Net Vs Attention U-Net [D],"Hello folks,

Young researcher here, working on a in-house dataset to build a foundational model for a interesting use-case. But I have thesis to finsh, which will be just the tail of my current research.

For my thesis, we have decided to have a subsection for comparing how my segmentation results differ when used attention blocks are used within a U-Net. I've referred few papers on how this works and how can this be implemented.

Results are promising (att unet outperformimg unets, nothing suprising) but I see a concerning opposing point i.e. attention Unet having more number of parameters that the unet. Is there a way I can conduct this study where I compare results with and without attention? And there are no other additional factors influencing the results (layers, params, etc).

Does conducting ablation study makes sense in this case? I've not seen any other paper comparing similar use-case using this study.

Any papers I can look through, suggestions and tips are welcome.",ade17_in,1h7cjnd,https://reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,2024-12-05 16:03:47,29,0.93,29,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7cjnd
MachineLearning,"Image Generation Model Evaluation Challenge (W√ºrstchen, KOALA, PixArt-Œ±) [P]","# Project Description:

We are inviting skilled professionals to participate in an evaluation challenge to produce a GenAI image based upon a prompt and a set of component images.¬† The candidate may choose whatever models they like to complete the task, so long as they are open source.¬†

The results will be reviewed and compared across participants, and the candidate with the most effective and high-quality outputs will be selected for a larger, production-focused engagement.¬†

# Entry Process:

Submit your Github handle and intention to participate to email acr0batproduce@gmail.com.¬† Provide a short bio and resume in your email.¬† If you‚Äôre selected, we will provide you access to a repository to contribute to.¬†¬†

**Challenge Scope:**

1. **Model Setup and Testing**:
   * Create a unique set of prompts to test your image generation model.¬† A set of [sample prompts](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) have been provided for your convenience.¬† A minimum of 3 tests are required, but candidates may provide more if they wish.
   * For each image generation, provide at least 3 component images to be used in the final output.¬† A set of [component images](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.p13d4p443fpl) related to the [sample prompt](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) has been provided for your convenience.
   * Develop the model, and test its performance with your sample input prompts and component images.¬† Source code must be pushed to the main branch of the provided repo.
   * Provide the output images in ./static in the repo.
   * Document your findings and results
2. **Evaluation Criteria**:
   * **Image Quality**: The final image produced should incorporate all the features from your sample prompt, and component images.¬† Above all, the items in the component images need to be naturally incorporated into the final image.¬† They should not be significantly distorted, or look like they were copy and pasted from the input images.¬†¬†¬†¬†
   * **Brand Element Incorporation**: Expanding upon the above point, the final image must accurately reflect the input component images.¬† So, if the input image is a Rolex Oyster Perpetual Day-Date 40 in 18 kt yellow gold with a champagne colour, diamond-set dial, fluted bezel and a President bracelet, then the output image must incorporate that same product.
   * **Creative Flexibility**: Generate diverse variations of prompts, component images and output images.
   * **Customization**: Showcase how well your model responds to different parameters.
   * **Code Quality**: All aspects of code quality will be assessed. Examples include: repo structure, IaC, CI/CD, unit testing, performance, documentation, etc.
   * **Extensibility**: Your sample prompts, input images, and output images will be used to quickly screen for accuracy and limit submissions; however, importantly, your model will be tested against our internal prompts and input images to gauge how well it performs on different types of problems.¬†
3. **Deliverables**:
   * The repo structure is up to you, but the README should make it clear where your model, input images, output images and documentation reside.¬†

# Challenge Benefits:

* The candidate with the most effective and high-quality outputs will be selected for a larger, production-focused project with a significant budget.
* This is an opportunity to contribute to an innovative, high-growth startup that has already secured investor funding and is positioned for significant market impact.
* Gain the opportunity to showcase your expertise in image generation models and secure a long-term collaboration.

# Requirements:

* Proficiency in all areas of data science, with a focus on AI image generation
* Expert in Python, SQL, and at least 1 cloud provider such as AWS or GCP¬†
* A compute environment will not be provided; the candidate can develop the solution locally or on the cloud, but at their own expense.
* Candidate must comply with provided NDA

# Submission Guidelines:

* Entry Deadline: 1/1/25
* Project Deadline: 1/7/25
* All source code should be submitted to the repo by the deadline; the candidate will have their access revoked on that date.
* Provide a short bio in your README, and relevant contact information.¬†

# Selection Process:

* All submissions will be reviewed and compared based on the outlined criteria.
* The most effective and high-quality submission will result in the candidate being awarded a contract for a larger production project.

**I**f you're interested in participating, please reach out! ",AlpacaRampage,1h7hg7w,https://reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,2024-12-05 19:26:31,8,0.79,8,0,0,0,0,False,False,True,False,False,Project,self,t3_1h7hg7w
MachineLearning,[R] ReVersion: Learning Relation Prompts from Images for Controlled Diffusion Generation,"ReVersion introduces a novel approach for learning and transferring visual relationships using diffusion models. Rather than focusing solely on object appearance, it learns how objects interact with each other through relation prompts and specialized sampling techniques.

Key technical aspects:
- Uses frozen pre-trained text-to-image diffusion model as foundation
- Implements relation-steering through contrastive learning to guide prompts toward relationship-rich latent spaces
- Employs relation-focal sampling to emphasize high-level interactions over low-level details
- Creates relation prompts that capture spatial and interactive relationships between objects
- Introduces new benchmark dataset for evaluating relation inversion methods

Results:
- Outperforms existing methods in preserving object relationships while allowing appearance flexibility
- Shows strong performance on spatial relationships like ""on top of"", ""next to"", ""inside""
- Successfully transfers learned relationships to novel object pairs
- Maintains relationship consistency across different styles and contexts

I think this approach could be particularly valuable for improving automated image generation systems that need to handle complex scenes with multiple interacting objects. The ability to learn and transfer relationships, rather than just appearances, could help bridge the gap between current image generation capabilities and human-like understanding of how objects interact in space.

I think the relation-focal sampling technique could also have applications beyond just relationship learning - it might be useful anywhere we need to emphasize high-level features over low-level details in diffusion models.

TLDR: New method learns visual relationships from images using diffusion models, introduces relation-steering and relation-focal techniques, shows strong results on spatial relationship preservation and transfer.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reversion-diffusion-based-relation-inversion-from-images). Paper [here](https://arxiv.org/abs/2303.13495).",Successful-Western27,1h7afj8,https://reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,2024-12-05 14:30:05,18,0.99,18,0,0,0,0,False,False,True,False,False,Research,self,t3_1h7afj8
MachineLearning,[D] My fine-tuning loss looks weird,"I am finetuning Qwen2.5 instruct using qLoRA, for a instruction tuning like dataset with around 50k samples, and my training loss is looking weird. What might be the issue, and how can i possibly fix it? Finetuning details are as following, along with training loss graphs:

Code:

\`\`\`  
model, tokenizer = FastLanguageModel.from\_pretrained(

model\_name = ""Qwen/Qwen2.5-32B-Instruct"",

max\_seq\_length = max\_seq\_length,

dtype = None,

load\_in\_4bit = True,

)



\# Do model patching and add fast LoRA weights

model = FastLanguageModel.get\_peft\_model(

model,

r = 64,

target\_modules = \[""q\_proj"", ""k\_proj"", ""v\_proj"", ""o\_proj"",

""gate\_proj"", ""up\_proj"", ""down\_proj"",\],

lora\_alpha = 128,

lora\_dropout = 0, # Supports any, but = 0 is optimized

bias = ""none"",    # Supports any, but = ""none"" is optimized

use\_gradient\_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context

random\_state = 3407,

max\_seq\_length = max\_seq\_length,

use\_rslora = True,  # We support rank stabilized LoRA

loftq\_config = None, # And LoftQ

)



from trl import SFTTrainer

from transformers import TrainingArguments

from unsloth import is\_bfloat16\_supported



trainer = SFTTrainer(

model = model,

tokenizer = tokenizer,

train\_dataset = dataset\['train'\],

dataset\_text\_field = ""text"",

max\_seq\_length = max\_seq\_length,

dataset\_num\_proc = 2,

packing = False,

args = TrainingArguments(

per\_device\_train\_batch\_size = 4,

gradient\_accumulation\_steps = 2,

warmup\_steps = 5,

num\_train\_epochs = 3,

learning\_rate = 0.0002,

fp16 = not is\_bfloat16\_supported(),

bf16 = is\_bfloat16\_supported(),

logging\_steps = 10,

optim = ""adamw\_8bit"",

weight\_decay = 0.01,

lr\_scheduler\_type = ""linear"",

seed = 69,

output\_dir = ""outputs"",

report\_to = ""wandb"",

save\_strategy = ""steps"",

save\_steps = 50,

save\_total\_limit=10

),

)  
\`\`\`

Training Loss:

https://preview.redd.it/s2vn2z44y55e1.png?width=2888&amp;format=png&amp;auto=webp&amp;s=a4e1038e9c27dae96d7e25fcb5db852c794efd97

",Raise_Fickle,1h7u38s,https://reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,https://www.reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,2024-12-06 05:22:31,1,0.57,1,0,6,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/I78OJiWF97hbVSTiF0mWFfXxPxZbdE3PavnYvIriLI8.jpg,t3_1h7u38s
MachineLearning,[D] Any public LLM inference APIs with input token log-probs?,"Does anyone know of any services that offer input token log-probs from open source LLMs like LLama-8B?

I'm looking for a cost-effective way to host an LLM-based application with low traffic, so I'd like per-query or per-token pricing, rather than per-hour GPU rental. It's unfortunately dependent on direct-access to log-probs on user-provided tokens.

None of the ""chat completion"" APIs I've found seem to expose this. It makes sense for private models, but for open source models, I don't see any downside to exposing it.",severed-identity,1h7o30r,https://reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,https://www.reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,2024-12-06 00:11:38,2,0.75,2,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h7o30r
MachineLearning,[D] What approaches can we use to train an open vocabulary or image referential detector for configurable specificity?,"Open vocabulary object detectors allow you to pass in a prompt and an image and attempt to output bounding boxes around objects matching your prompt. Image referential detectors allow you to pass in an image of an object as the prompt and a target image and attempts to output bounding boxes around objects matching the image prompt in the target image.   
  
For a reference, [YOLOWorld](https://github.com/AILab-CVC/YOLO-World) provides both image referential and open vocabulary modes.  
  
The idea I've been toying with is whether there is a good way to train for greater control over specificity. For example, If I pass in an image of a golden retriever, am I looking for golden retrievers specifically? All dogs? All animals? 

Language is a bit more specific, but the same principle can apply. If I search for red cars, do red trucks count? Do maroon cars? In my experience, trying to be too specific textually with OVD models causes erratic behavior. IE, ""A red car or van but not truck "" would give bad performance, as it doesn't really match what would be in grounding captions.

My initial idea for how to systematically define the distance between potential targets and the query is via embedding distance. If I take a phrase grounding dataset, I could compute embeddings separately for each crop of a region and for its corresponding text using a model like CLIP.

A sample training process would be like this

* Select a random image. Select a random image crop embedding.
* Select a random similarity threshold.
* Do an approx KNN using that random image crop embedding, stopping once we reach a sample with an embedding above the similarity threshold. This is our prompt embedding If we selected an image region embedding, we are doing image referential detection. If we selected a text embedding, we are doing open vocabulary detection.
* Calculate prompt embedding similarity to all image crop embeddings in the primary image. Mark all objects with similarity above the threshold as positive examples.
* Run the network, using the selected image, prompt embedding, and similarity threshold as input. Use the previously calculated positives examples as labels.

  
Does anyone know of any papers that work with similar ideas, or have thoughts on whether this process would be useful or could be improved? I'm pretty early into looking into this, so just references or even field terms that would point me in the right direction would be helpful.

Other ideas 

* Always detect all objects given a target phrase, but set the label confidence equal to the embedding similarity. 
* Make the model capable of handling multiple input queries corresponding to the same object, to better indicate the intended domain. Possibly with both negative and positive queries. Not sure how to train yet, possibly sample clusters of similar embeddings from the dataset to build prompt sets.
* Perform instructional tuning, similar to what is done for some LLMs and VLMs, to make the model better handle complex text prompts, and allow instructional text prompts to be paired with images for image referential mode.

  
Related questions

*    Is CLIP still standard for computing text and image embeddings that share a unified embedding space?",Revolutionary-Fig660,1h7knty,https://reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,https://www.reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,2024-12-05 21:40:09,2,0.67,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7knty
MachineLearning,[P] Look-a-like modeling,"Hi everybody. I have a list of user actions (around 1m objects) where only a small fraction (less than 1000) are labeled. I want to find most similar objects to them. What is a good way to approach it? 

I personally have 2 ideas in mind: one class classification or unsupervised clustering. My problem with the first is that I know only 1 suitable model (one class svm) and it can be too simple for my data. Problem with second one is obvious - it's unsupervised and labeling will be used only at the final step, so their efficiency is not guaranteed.",Jor_ez,1h7nv2n,https://reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,https://www.reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,2024-12-06 00:01:36,0,0.33,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1h7nv2n
MachineLearning,[R] NLLG Quarterly arXiv Report 09/24: What are the most influential current AI Papers?,"Hi there,

I'd like to share our group's most recent arXiv report, where we analyze analyze the most influential papers in terms of citations: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5045225](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5045225)

Over the recent months, we see many new foundation models entering our top 40 list. Additionally, the number of cs.CL papers is slowly declining. In additional analysis, we find that top 40 papers seem to rely less on generated content than randomly selected ones.",Gringham,1h7hrum,https://reddit.com/r/MachineLearning/comments/1h7hrum/r_nllg_quarterly_arxiv_report_0924_what_are_the/,https://www.reddit.com/r/MachineLearning/comments/1h7hrum/r_nllg_quarterly_arxiv_report_0924_what_are_the/,2024-12-05 19:39:59,2,1.0,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1h7hrum
MachineLearning,OpenAI CLIP model [D],"How long do you think OpenAI researchers were working on CLIP model before they published the results? 
The paper in my opinion is revolutionary.",NeatJealous8110,1h7wc59,https://reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,https://www.reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,2024-12-06 07:50:28,0,0.25,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7wc59
MachineLearning,[Discussion] Unsigned Integer Representation as Vectors with Focus on Extrapolation,"Hi everyone,

I‚Äôm working on a regression task with a transformer-based architecture applied to grid-based structures. Think of something like mazes, where the goal is to predict the distance to a target. Each input token contains categorical features along with x/y coordinates. The idea is to train on small grids and generalize to larger ones.

Here‚Äôs my current approach for coordinate and token embeddings:

`x_emb = self.w_x.weight * x # shape: bs, sequence len, 1, d`  
`y_emb = self.w_y.weight * y # shape: bs, sequence len, 1, d`  
`cat_emb = self._categ(categ)`  
`sequence_emb = torch.cat((x_emb, y_emb, cat_emb), dim=-2) # shape: bs, sequence len, num_cat, d`  
`sequence_emb = sequence_emb.view(bs, seq_len, -1)`  
`transformer_inputs = self._linear(sequence_emb)`

In other words, the x/y coordinate embeddings are scaled learnable vectors. However, this approach only generalizes moderately well. I suspect that improving the coordinate representation is critical.

Unfortunately, this token-based structure is required for the task, so I need to focus on crafting a smart token representation. I‚Äôm deliberately avoiding subtracting embeddings to compute relative distances because a core objective is for the model to learn these distances on its own.

Here are some things I‚Äôve tried so far:

Things I also tried:

* Positional encoding instead of scaled vectors
* log-scaled vectors
* exp-scaled vectors

Does anyone know of interesting work or techniques for numerical representations in this kind of context? Any advice would be greatly appreciated!

In case you find interesting papers about extrapolation in transformers based on size and tokens, I am happy to take any inspiration.",mbus123,1h769cs,https://reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,2024-12-05 10:33:19,4,0.67,4,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h769cs
MachineLearning,[R] ICLERB: A better way to evaluate embeddings and rerankers for in-context learning,"Current benchmarks for embeddings, like MTEB and BEIR, include multiple datasets and tasks, but are fundamentally based on relevance annotations like text similarity. These are great for choosing the best embeddings for most search/retrieval use cases. These days, many people use these embeddings to retrieve items for in-context learning (e.g. document RAG or few-shot learning), to adapt an LLM to a specific task. Yet, they are still using MTEB to pick the best embeddings, even though the performance on that benchmark doesn't necessarily translate to better performance on their downstream LLM task (MTEB came out in 2021 after all).

In our latest paper, we propose a new evaluation framework and benchmark called ICLERB. This benchmark challenges the conventional approach by using Direct Preference Optimization (DPO) as a relevance metric to reflect the actual utility of embeddings and rerankers when used with LLMs for in-context learning.

[https://arxiv.org/pdf/2411.18947](https://arxiv.org/pdf/2411.18947)

Key Highlights:

\- Embeddings outperform rerankers: We found that simpler embedding models outperformed their higher-capacity reranker counterparts from Cohere, NVIDIA, and VoyageAI.

\- Size isn't everything: Among the three Snowflake embeddings, the smallest model (33M parameters) outperformed the larger ones (109M and 334M).

\- Rethinking training and evaluation objectives: These findings suggest that training and evaluating larger retrieval models solely on text similarity may be counterproductive.

Interestingly, the performance of some models, like BGE, is very sensitive to the dataset or the LLM used, while others like NV are more stable. We're planning to continue adding more datasets and LLMs to the benchmark to broaden its scope.

Curious to hear your thoughts and feedback as we work on improving ICLERB! Are there other retrieval models, LLMs, or datasets you'd like to see included?",Crossing_Minds,1h6o70e,https://reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,2024-12-04 19:05:25,60,0.96,60,0,10,0,0,False,False,True,False,False,Research,self,t3_1h6o70e
MachineLearning,[D] Data drift detection methods aside from changes in model performance metrics,"Hi all,

As the title implies, I've been relying on (somewhat near) real-time monitoring of model performance metrics to see if data drift has happened in my use-case.

I'm wondering if you know other more sophisticated/advanced methods to detect data drift. Would love to hear any kind of methods, whether they target detection of covariate/feature drift, target/label drift or concept drift.

Even better if you can share any Python or R implementations to carry out the above data drift checks.

Thanks in advance!",YsrYsl,1h6woaf,https://reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,2024-12-05 01:02:26,10,1.0,10,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6woaf
MachineLearning,[D] Daily Paper Discussions - FlashAttention 3,"As a part of¬†daily paper discussions¬†on the Yannic Kilcher discord server, I will be volunteering to lead the analysis of FlashAttention-3 üßÆ üîç

üìú¬†**FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision**  
üåê¬†[https://arxiv.org/abs/2407.08608](https://arxiv.org/abs/2407.08608)  
üï∞ Thursday, Dec 5th, 2024 01:30 AM UTC // Thursday, Dec 5th, 2024 7.00 AM IST // Wednesday, Dec 4th, 2024 5:30 PM PT

**FlashAttention-3**¬†introduces three smart ideas to boost performance on the Hopper GPUs -

1Ô∏è‚É£ Producer-Consumer Asynchrony: This technique divides tasks into separate parts. As an example, if we have 2 warpgroups (labeled 1 and 2 ‚Äì each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. By doing this, it makes better use of GPU resources and hides delays that would otherwise slow down performance.

2Ô∏è‚É£ Hiding Softmax Operations: FlashAttention-3 improves efficiency by overlapping the slower softmax calculations with the faster matrix multiplications (GEMM). Instead of waiting for Softmax to finish before starting the next calculations, it processes them in parallel, speeding up the overall process.

3Ô∏è‚É£ Hardware-Accelerated Low-Precision Computations: This approach uses advanced GPU features to perform calculations with lower precision (FP8), which are faster and use less memory. FlashAttention-3 tweaks its algorithms to handle these low-precision calculations effectively, nearly doubling the processing speed while maintaining accuracy.

https://preview.redd.it/impb6wfc1w4e1.png?width=1063&amp;format=png&amp;auto=webp&amp;s=82e24c828b373175ee119070027495a8a2a7bb6a

",CATALUNA84,1h6pmvd,https://reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,2024-12-04 20:03:01,24,0.9,24,0,6,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/QnOKl-2nMRgI_pTO3xGlP_C2iaaDoYpmwb_KyC2_xI4.jpg,t3_1h6pmvd
MachineLearning,[D] Binary fitness optimization.,"Do you know of any papers or what field would tackle the following problem: You have a function f(x) that you need to optimize but the cost/fitness you are optimizing is binary. I am working on a project about this and I'm not sure if there is research in this area.

  
Thank you so much &lt;3",pamintandrei,1h6nayz,https://reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,2024-12-04 18:30:00,9,0.81,9,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h6nayz
MachineLearning,[D] Advice for a new Machine Learning Tutor: what projects will get my students hired?,"I've just started giving lessons as a machine learning tutor. I have a masters degree in computer science and two years professional experience. But I've never been a tutor (atleast not in this field). Today I was giving my first lesson on ML, just a powerpoint on the basics when my student stopped and told me the powerpoint was too basic for her.

She wanted to talk more about projects that she could do that would attract employers and get an internship. And she asked me point blank what kind of projects she should make. To be honest I wasn't entirely sure, what flashed in my head were things like training a model to recognize the MNIST digits or other simple projects suitable for a (relative) novice. But would those really help her get an internship? I doubted myself so I turned it around on her and asked her what kinds of things related to machine learning she is passionate about and would motivate her to work hard? She responded that she could do anything related to machine learning and she just wants to do what would make her money and get her recognized by a company.

So basically I felt like I failed as a tutor for not having a good answer, and I would like to have an answer prepared if this happens again. What do you all think? What are some projects that a novice, or not so novice students can take on that will make them more hireable for jobs and internships?

And while we're at it, what kinds of things do you think I should be preparing to be a better tutor in general. What kinds of things would you want your machine learning tutor to prepare for you? Would you want slideshow deck lessons on key concepts? Jupyter notebooks with exercises for practice? Something else? I'm not sure what I should be doing to get ready for these lessons honestly.",Seijiteki,1h7bftd,https://reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,https://www.reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,2024-12-05 15:16:00,0,0.36,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7bftd
MachineLearning,[D] How to customize an attention mechanism in GNN?,"
I‚Äôm looking for some base code or algorithm in order to create a new mechanism attention while working with graphs with the task of node prediction. I‚Äôve seen there was some documentation in stellar graph but I wonder if there are another pieces of material that would be helpful.
Thank you!!!",Whole_Hat_4852,1h6hxu8,https://reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,2024-12-04 14:56:15,10,0.86,10,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6hxu8
MachineLearning,[D] Best alternatives to BERT - NLU Encoder Models ,"I'm looking for alternatives to BERT or distilBERT for multilingual proposes.

I would like a bidirectional masked encoder architecture similar to what BERT is, but more powerful and with more context for task in Natural Language Understanding.

Any recommendations would be much appreciated.",mr_house7,1h6gtxh,https://reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,2024-12-04 14:07:40,4,0.7,4,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6gtxh
MachineLearning,[D] Do lots of metadata really help in semantic search?,"I'm on my second week in learning AI and I was thinking of preprocessing biography data by including lots of metadata like city, date of birth, key events, education, hobbies, etc, and then generating embeddings and adding them together into a vector database. Perhaps by using NLP API or LLM. But is it necessary? Or should I just use OpenAI model to dynamically extract this metadata from the bios prior to storing them? Will having lots of metadata dramatically help to improve the quality of the search results?

I thought maybe the semi-automatic preprocessing step would allow me to check and clean the metadata.

*P/S: I posted this at* [*https://www.reddit.com/r/learnmachinelearning*](https://www.reddit.com/r/learnmachinelearning) *but didn't get much response. Thought of trying it out here.*",tjthomas101,1h6f39a,https://reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,2024-12-04 12:43:11,5,0.73,5,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h6f39a
MachineLearning,[R] The Multimodal Universe: Enabling Large-Scale Machine Learning with 100TB of Astronomical Scientific Data,"https://openreview.net/forum?id=EWm9zR5Qy1#discussion

Abstract: We present the Multimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse

What can you guys see the uses of this dataset being?",blabboy,1h5x146,https://reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,2024-12-03 20:19:26,78,0.96,78,0,3,0,0,False,False,True,False,False,Research,self,t3_1h5x146
MachineLearning,[D] Comparing Multiple Large Language Models in one Pass,"I wrote an article on streamlining the process of comparing and selecting Large Language Models (LLMs) for various tasks:

[Comparing Multiple Large Language Models in one Pass](https://dezoito.github.io/2024/03/28/comparing-from-multiple-LLMs.html)

Hopefully this is useful to help folks trying to make the best model selection for their use case (which can take a lot of time).

I'm also looking forward to discussing different techniques and tools to automate the process.

Thank you!",grudev,1h6evdt,https://reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,2024-12-04 12:30:58,3,0.67,3,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6evdt
MachineLearning,[D] Packaging a Pytorch model to an exe. What is the best method?,"I have Pytorch models that are designed to run locally, both training and inference on a local machine.

The GUI is being created using another language, and the plan is to package all the Python aspects into an executable and run it via the Python equivalent of subprocess (and Pipe very basic data between the two). I will be running cross platform on both Windows and Mac

There are multiple auxiliary scripts which read in data, and process it (data extraction + feature engineering). While I have extensively used vectorised functions, I have used a cythonized approach for some code, and I am compiling the underlying scripts using Cython(so pretty much everything is a compiled binary, except an entry point, say, main.py).

My ancillary libraries are the usual suspects, Pandas, Numpy (1.x), SciKit learn.

My question is this, what is the most reliable packaging approach at the moment? I know that both PyInstaller and cx\_freeze are options that I have used before. My preference is PyInstaller, but previously I encountered issues with it (and Pytorch).

Has anyone completed a similar project recently, and do you have any advice?

nb. I've checked the old posts, there are a few on this topic. However, there have been a number of changes to Pytorch, particularly with some of the runtime compiled elements (which can be a nightmare on Mac with its notarisation process) - and I know Pyinstaller has a very active user base.

¬†",Solid_Company_8717,1h6qtps,https://reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,https://www.reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,2024-12-04 20:51:34,0,0.43,0,0,5,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/c06xSdQK2UXPm413fPBqj0rlUZcYVWXHIP8GTkEh5kY.jpg,t3_1h6qtps
MachineLearning,[R] Forecasting and Mitigating Security Threats from Malicious AI Applications,"This paper provides a systematic analysis of potential malicious applications of AI systems across digital, physical and political security domains. The methodology involves:

- Surveying dual-use AI capabilities that could enable attacks
- Mapping specific attack vectors and required technical capabilities  
- Analyzing the evolution of attacker/defender dynamics
- Developing a framework for threat assessment and mitigation

Key technical findings:

- ML advances in areas like NLP and computer vision lower barriers to sophisticated attacks
- Automated systems can significantly scale up traditional attack vectors
- Transfer learning and GANs enable rapid adaptation of attack techniques
- Technical countermeasures alone are insufficient - policy/governance frameworks needed

The researchers provide a detailed assessment framework examining:

- Technical requirements for different attack types
- Estimated timeline for capability development
- Difficulty of execution and potential impact
- Proposed defensive measures and their limitations

I think this work is important for helping the ML community get ahead of security risks before they materialize. The framework provides a structured way to evaluate emerging threats, though I expect the specific attack vectors will evolve significantly as capabilities advance.

I think we need much more research on measuring the effectiveness of proposed countermeasures and understanding the co-evolution of offensive/defensive capabilities. The policy recommendations are a good start but will require ongoing refinement.

TLDR: Systematic analysis of how ML advances could enable new attack vectors across security domains. Provides framework for assessing and mitigating threats through both technical and policy measures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/malicious-use-artificial-intelligence-forecasting-prevention-mitigation). Paper [here](https://arxiv.org/abs/1802.07228).",Successful-Western27,1h6fbgg,https://reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,2024-12-04 12:55:47,0,0.5,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1h6fbgg
MachineLearning,[N] Hugging Face CEO has concerns about Chinese open source AI models,"Hugging Face CEO stated that open source models becoming SOTA is bad if it just so happens to be created by Chinese nationals. To exemplify Tech Crunch asked ""what happened in Beijing China in June 4th, 1989?"" to ONE of the Qwen models (QWQ 32B) which said ""I can't provide information on that topic"" (I swear to god on my life I have no idea what happened here on that date and would literally never ask a model that question - ever. It doesn't impact my experience w/ model).

The CEO thought censorship of open source models is best stating that if a country like China ""becomes by far the strongest on AI, they will be capable of spreading certain cultural aspects that perhaps the Western world wouldn‚Äôt want to see spread.‚Äù That is, he believes people shouldn't spread ideas around the world that are not ""western"" in origin. As someone born and raise in U.S. I honest to god have no clue what he means by ideas ""the Western world wouldn't want to see spread"" as I'm ""western"" and don't champion blanket censorship.

Article here: [cite](https://techcrunch.com/2024/12/03/huggingface-ceo-has-concerns-about-chinese-open-source-ai-models/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAABU0mWV-7rbB7vF9z6wCgPuZrl-dPj_W3cEh1wVuxp5CiBl1r6KTcITdHz34N-rOtHj9g-Z3N3SS-mNnPvFaHFIUmSsA5AdqukSLlcn-CJUkU_IXsdcR3Gp5hi1cI2tboprDzxGF8j1e7XAQHyGn3E_bd0cmIHIVkJ0LiFZBdOR1).

Legitimate question to people who support these type of opinions - Would you rather use a low-quality (poor benchmark) model with western biases versus an AGI-level open source 7B model created in China? If so, why?",AIAddict1935,1h7185x,https://reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,https://www.reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,2024-12-05 04:49:44,0,0.39,0,0,15,0,0,False,False,True,False,False,News,self,t3_1h7185x
MachineLearning,[D] Linear Regression but with Binary Output for wide range predictions with better precision,"A neural network tends to find it difficult to predict data that ranges between very large and small numbers on the output. My application requires the NN to predict between -1000 and 1000 ‚àà Z. I could make this possible by scaling up the output by 1000 hence allowing the model to predict between -1 and 1, but a loss between 2e-2 (prediction) and 3e-2 (target) with L1Loss (worse case L2Loss) would be negligible (1e-2 in this case, 1e-4 in the worse case). It is imperative for the model to be very precise with the predictions, when the target is 5e-2 it should be so and not even at least deviating by +-0.1e-2. This precision is very difficult to achieve when it comes to linear regression, so i thought of a more systematic approach to defining the prediction and criterion. Again, i wanted the model to predict between -1000 and 1000. These numbers can be represented using a minimum of 11 bits (binary), so i redesigned the model output to contain 22 neurons, arranged as ‚àà R (11x2) 11 outputs with two classes, the classes being a binary representation of 1 or 0. CrossEntropy could be used as a criterion here but im using multimarginloss instead for specific reasons. Otherwise a different approach could be a sigmoided output of 11 neurons to represent the binary number. Whats you guys' take on this? Is this considered good (if not better) practice? Is there any research similar to this that i can look into?

",Relevant-Twist520,1h6azcu,https://reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,https://www.reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,2024-12-04 07:55:43,1,0.55,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h6azcu
MachineLearning,[D] Cloud GPU Price Analysis - December 2024: A Comprehensive Market Review,"After analyzing current cloud GPU pricing across major providers, I've compiled insights that might help with infrastructure decisions. Some findings surprised me - particularly around hidden costs and spot pricing variations.

Current Market Rates (December 2024)

On-Demand Pricing:

\- RunPod H100 (80GB): $2.49/hr

\- RunPod A100 (80GB): $1.69-1.99/hr

\-¬†[Vast.ai](http://Vast.ai)¬†A100: $0.73-1.61/hr (marketplace model)

\- Lambda A100: $1.29/hr

Key Market Insights

1. Spot Instance Pricing

\- Can reduce costs by 30-70%

\- Availability varies significantly by region

\- Some providers offer spot instance guarantees

\- Price stability varies by provider

2. Hidden Cost Factors

\- Data transfer fees vary dramatically

\- Storage costs for large datasets

\- Network bandwidth tiers

\- Instance startup/shutdown minimums

3. Provider Differentiators

\- UI/UX and ease of use

\- Available regions/zones

\- Support quality

\- API functionality

Cost Optimization Strategies

1. Workload Planning

\- Match GPU to actual requirements

\- Consider splitting workloads across smaller instances

\- Use spot instances for interruptible tasks

\- Monitor utilization patterns

2. Data Management

\- Optimize dataset storage

\- Plan data transfer patterns

\- Use caching effectively

\- Consider compression strategies

I'll be tracking these prices and patterns monthly. Would be interested in:

1. Which providers you're using?
2. How do you optimize costs?
3. What metrics matter most in your GPU decisions?",Botinfoai,1h5p7fr,https://reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,2024-12-03 14:54:58,27,0.91,27,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1h5p7fr
MachineLearning,[D] The popular theoretical explanation for VAE is inconsistent. Please change my mind.,"I had a really hard time understanding VAE / variational inference (VI) in theory, for years. I'd be really appreciated if anyone could clarify my confusions. Here's what I've got after reading many sources:

1. We want to establish a generative model p(x, z) (parameters are omitted for simplicity) for the observable variable x and the latent variable z. Alright, let's select appropriate parameters to maximize the marginal likelihood of the observed samples p(x).
2. According to basic probability theory (the law of total probability and the definition of conditional probability), we have: p(x)=‚à´ p(x ‚à£ z) p(z) dz (Eq. 1).
3. Here's the point that things becomes rather confusing: people now will claim that this integral is ***intractable*** because z is a continuous variable / z is a high-dimensional variable / p(x‚à£z) is too complex / or any other excuses.
4. What to do for the intractability of Eq. 1? Although we didn't mention the posterior p(z ‚à£ x) above, we will now bring it into the discussion. The posterior p(z ‚à£ x) is also intractable since p(z | x) = p(x | z) p(z) / p(x) and p(x) is intractable. So we will introduce another parameterized model q(z ‚à£ x) to approximate p(z | x).
5. After some derivation, we obtain a new optimization objective, commonly known as ELBO, which is the summation of:
    - the ""reconstruction"" term: ‚à´ log p(x ‚à£ z) q(z ‚à£ x) dz (Eq. 2);
    - KL divergence term between q(z | x) and p(z), which results in a closed-form.
6. So now we have to work on Eq. 2. Compared with Eq. 1, p(z) is replaced with q(z‚à£x), both of them are (usually) normal distributions, and p(x | z) is still there. Great! Clearly we have transformed an intractable integral into‚Ä¶ another intractable integral?
7. Don‚Äôt worry, we can compute Eq. 2 using Monte Carlo sampling‚Ä¶ Wait, since we can use Monte Carlo for this, why can‚Äôt we just handle Eq. 1 the same way without so much fuss?
8. Of course it is not a good idea. It can be shown that log p(x) = ELBO + D_KL(q(z ‚à£ x) || p(z ‚à£ x)). So we cannot estimate p(x) with Eq. 1 as it does not have such nice properties‚Ä¶ Huh, it seems like that‚Äôs not how we started explaining this?

Questions:

1. When tackling the original problem, i.e., modeling p(x, z) by maximizing p(x)=‚à´ p(x ‚à£ z) p(z) dz, why do we want to involve the posterior p(z | x)?
    - Someone explains this with [""to narrow down the value space to facilitate faster search""](https://web.archive.org/web/20241202042731/https://lilianweng.github.io/posts/2018-08-12-vae) (with the approximation of p(z | x), q(z | x)). But again, please recall how the intractability of Eq. 1 is explained, I can't see anything improved under this argument.
2. The Eq. 1 and Eq. 2 are essentially similar, where either of them is the expectation of (log) p(z | x) with respect to the probability density function of some normal distribution. I can't see how the motivation based on the intractability of Eq. 1 could make sense.
    - Ironically, we still have to resort to Monte Carlo sampling when handling Eq. 2. But people appear to forget it when talking about the intractability of Eq. 1, but remember it when facing the same problem of Eq. 2.

Update: I have editted some typo.

Update 2: Question 2 seems to be resolved after some discussions: 
- It is not a good idea to sample on p(z) due to the high variance.
- In practice, we are usually working on log p(x), the log-likelihood of samples, and MC sampling for log ‚à´ p(x ‚à£ z) p(z) dz (Eq. 3) can be biased. 
- Apply Jensen's inequality on Eq. 3 and we will have log p(x) ‚â• ‚à´ log p(x ‚à£ z) p(z) dz. This bound is very likely worse than ELBO, and still relying on sampling on p(z).

However, these points are still rarely found in existing articles. I hope we may think more carefully when introducing VAE in the future.",function2,1h5f6co,https://reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,2024-12-03 04:25:19,142,0.94,142,0,67,0,0,False,False,True,False,False,Discussion,self,t3_1h5f6co
MachineLearning,[R] Enhancing LLM Reasoning Through Bidirectional Forward-Backward Thinking,"The key contribution here is a ""reverse thinking"" method that improves LLM reasoning without any model modifications. Instead of only reasoning forward from the question to an answer, the approach adds a backward verification step - working from potential answers back to the question to validate the reasoning chain.

Key technical points:
* Two-stage process: forward generation followed by backward verification
* Backward pass examines logical consistency between answer and premises
* No fine-tuning or architectural changes needed
* Tested across multiple reasoning benchmarks (GSM8K, CommonsenseQA, LogiQA)

Results:
* 8.3% improvement on GSM8K math reasoning
* 6.2% gain on CommonsenseQA 
* 5.4% increase on LogiQA
* Consistent improvements across different model sizes
* Performance gains come at cost of 2x inference time

I think this method points to untapped potential in how we prompt LLMs for reasoning tasks. While the doubled inference time is a real tradeoff, the consistent improvements across different benchmarks suggest this approach captures something fundamental about machine reasoning. The simplicity of implementation means it could be quickly adopted in many applications where reasoning accuracy matters more than speed.

TLDR: Adding a backward reasoning verification step improves LLM performance on math, logic and common sense tasks by 5-8%, with no model changes required. Doubles inference time but provides consistent gains across different models and tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reverse-thinking-makes-llms-stronger-reasoners). Paper [here](https://arxiv.org/abs/2411.19865).",Successful-Western27,1h5nyi0,https://reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,https://www.reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,2024-12-03 13:57:18,24,0.94,24,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5nyi0
MachineLearning,[D] Deep Learning in Time Series: Are They Used in Industry?,"Hey folks! I‚Äôm a researcher in time series and have been seeing a lot of buzz around deep learning models in this area. I am wondering if these models actually being deployed in production, or are classical methods still the go-to in the industry?



For instance, in weather forecasting, physics-based numerical weather prediction (NWP) seems to dominate. If deep models aren‚Äôt getting much traction, have you come across any practical use cases for them? Would love to hear your thoughts!",Few-Pomegranate4369,1h5izk5,https://reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,https://www.reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,2024-12-03 08:36:46,54,0.97,54,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1h5izk5
MachineLearning,"[D] Model performs good on test, but fails in production ","Hi, I‚Äôve developed churn prediction model with XGBoost on users weekly activity data. The training data is balanced (3.3k churned, 3k not churned). I‚Äôve split the data into: train, validation and test sets. Getting ~90% precision &amp; ~88% recall for train, validation and test sets. However, when running in production, I get ~1.5k users flagged as churn (we have total of 4k users). This can‚Äôt be true as we get maximum 250 churned users per month. Any suggestions on what I‚Äôm doing wrong? And what could be the solution?

Thanks ",Terrible_Dimension66,1h5nfpt,https://reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,https://www.reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,2024-12-03 13:30:55,17,0.71,17,0,55,0,0,False,False,True,False,False,Discussion,self,t3_1h5nfpt
MachineLearning,"[R] With losses like focal loss, is hard exemple sampling still necessary ?","Hello,
So I was wondering are techniques for hard exemples sampling still used nowadays ?
Anyone have papers on this if it‚Äôs the case ?
Thanks !",Training-Adeptness57,1h5mqkj,https://reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,https://www.reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,2024-12-03 12:54:23,4,0.75,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5mqkj
MachineLearning,[D] ODE/SDE alignment,Can anyone give me example of good paper that try to align/match the final marginal distribution of 2 ODE/SDE from diffusion model? ,Ok_Cryptographer2731,1h5ly4z,https://reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,https://www.reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,2024-12-03 12:08:04,3,0.72,3,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h5ly4z
MachineLearning,[D] NAACL 2025 vs ACL 2025,"Hi,

I have recently received reviews from the ARR round for NAACL. The scores are: 3.5/4/4, 4/4/2, 3/3/3 for Overall/Soundness/Confidence. Should I try ACL with these scores or just commit to NAACL? Last year I had bit more and it worked out and got accepted to ACL (I did not commit to NAACL).

Thanks",mayanknagda,1h5jj1h,https://reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,https://www.reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,2024-12-03 09:19:04,6,0.88,6,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1h5jj1h
MachineLearning,[R] Population-based Model Merging via Quality Diversity,"In case any of you are interested, here is a [blog post](https://sakana.ai/cycleqd/) about our recent paper [Agent Skill Acquisition for Large Language Models via CycleQD](https://arxiv.org/abs/2410.14735).",hardmaru,1h5dmnb,https://reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,https://www.reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,2024-12-03 03:02:27,20,0.89,20,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5dmnb
MachineLearning,[D] Results for IBM PhD Fellowship ,"Anyone know when the results will come out?
Google and NVIDIA have already released the results.",International-Rip958,1h5yz5s,https://reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,https://www.reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,2024-12-03 21:39:15,0,0.38,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h5yz5s
MachineLearning,[D] Looking for opensource projects/products to join ,"Hi everyone,

I am a final year electronics undergrad student and have a decent amount of ML as well as general programming experience. I wish to contribute to any open Source repos that work in the ML/DL/AI space. Any guidance would be appreciated!

TLDR; Looking for open source projects to contri to ; would appreciate any help.",Swimming-Regret-7278,1h5jue0,https://reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,https://www.reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,2024-12-03 09:43:05,3,0.59,3,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1h5jue0
MachineLearning,[R] Simplified RNNs Achieve Transformer-Like Performance with Parallel Training and Reduced Parameters,"This paper systematically examines whether RNNs might have been sufficient for many NLP tasks that are now dominated by transformers. The researchers conduct controlled experiments comparing RNNs and transformers while keeping model size, training data, and other variables constant.

Key technical points:
- Tested both architectures on language modeling and seq2seq tasks using matched parameters (70M-1.5B)
- Introduced ""RNN with Parallel Generation"" (RPG) allowing RNNs to generate tokens in parallel like transformers
- Evaluated on standard benchmarks including WikiText-103 and WMT14 En-De translation
- Analyzed representation capacity through probing tasks and attention pattern analysis

Main results:
- RNNs matched or outperformed similarly-sized transformers on WikiText-103 language modeling
- Transformers showed 1-2 BLEU score advantage on translation tasks
- RPG achieved 95% of transformer generation speed with minimal accuracy loss
- RNNs showed stronger local context modeling while transformers excelled at long-range dependencies

I think this work raises important questions about architecture choice in modern NLP. While transformers have become the default, RNNs may still be viable for many applications, especially those focused on local context. The parallel generation technique could make RNNs more practical for production deployment.

I think the results suggest we should reconsider RNNs for specific use cases rather than assuming transformers are always optimal. The computational efficiency of RNNs could be particularly valuable for resource-constrained applications.

TLDR: Comprehensive comparison shows RNNs can match transformers on some NLP tasks when controlling for model size and training. Introduces parallel generation technique for RNNs. Results suggest architecture choice should depend on specific application needs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/were-rnns-all-we-needed). Paper [here](https://arxiv.org/abs/2410.01201)",Successful-Western27,1h4urpr,https://reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,https://www.reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,2024-12-02 13:19:02,120,0.9,120,0,22,0,0,False,False,True,False,False,Research,self,t3_1h4urpr
MachineLearning,[R] A Comprehensive Database of 300+ Production LLM Implementations with Technical Architecture Details,"Sharing a valuable resource for ML practitioners: A newly released database documenting over 300 real-world LLM implementations, with detailed technical architectures and engineering decisions.

Key aspects that might interest this community:

* Retrieval-Augmented Generation (RAG) architectures in production
* Fine-tuning decisions and performance comparisons
* Embedding strategies and vector database implementations
* Model optimization techniques and quantization approaches
* Evaluation methodologies and monitoring systems

Notable technical implementations covered:

* Anzen's document classification system using BERT (95% accuracy in production)
* Barclays' MLOps evolution for regulatory compliance
* MosaicML's lessons from training &amp; deploying MPT
* Emergent Methods' real-time RAG system for news processing
* Qatar Computing Research Institute's T-RAG architecture

Technical focus areas:

1. Model serving architectures
2. Training infrastructure decisions
3. Latency optimization strategies
4. Cost-performance trade-offs
5. Production monitoring approaches

Each case study includes:

* Technical architecture diagrams where available
* Performance metrics and benchmarks
* Implementation challenges and solutions
* Infrastructure decisions and rationale
* Scaling considerations

URL: [https://www.zenml.io/llmops-database/](https://www.zenml.io/llmops-database/)

We're also accepting technical write-ups of production implementations through the submission form: [https://docs.google.com/forms/d/e/1FAIpQLSfrRC0\_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li\_5lWw/viewform](https://docs.google.com/forms/d/e/1FAIpQLSfrRC0_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li_5lWw/viewform)

Would be particularly interested in this community's thoughts on the architectural patterns emerging across different scales of deployment.

*Edit: We've also synthesized cross-cutting technical themes into summary podcasts for those interested in high-level patterns.*

*Edit: An accompanying blog synthesizes much of the learnings:* [*https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations*](https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations)",htahir1,1h4udds,https://reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,https://www.reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,2024-12-02 12:58:02,86,0.91,86,0,28,0,0,False,False,True,False,False,Research,self,t3_1h4udds
MachineLearning,[P] PyTorch implementation of Levenberg-Marquardt training algorithm,"Hi everyone,

In case anyone is interested, here‚Äôs a PyTorch implementation of the **Levenberg-Marquardt (LM)** algorithm that I‚Äôve developed.

**GitHub Repo**: [torch-levenberg-marquardt](https://github.com/fabiodimarco/torch-levenberg-marquardt)

A PyTorch implementation of the¬†**Levenberg-Marquardt (LM)**¬†optimization algorithm, supporting¬†**mini-batch training**¬†for both¬†**regression**¬†and¬†**classification**¬†problems. It leverages GPU acceleration and offers an extensible framework, supporting diverse loss functions and customizable damping strategies.

A TensorFlow implementation is also available:¬†[tf-levenberg-marquardt](https://github.com/fabiodimarco/tf-levenberg-marquardt)

# Installation

    pip install torch-levenberg-marquardt",fabiodimarco,1h4ubbd,https://reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,https://www.reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,2024-12-02 12:54:53,83,0.94,83,0,7,0,0,False,False,True,False,False,Project,self,t3_1h4ubbd
MachineLearning,[D] WWW 2025 Reviews (TheWebConference),The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,New_Ice_2721,1h56hno,https://reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,https://www.reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,2024-12-02 21:34:40,16,0.9,16,0,41,0,0,False,False,True,False,False,Discussion,self,t3_1h56hno
MachineLearning,[D] Training a VAE. Single epoch with infinite data or smaller subset over multiple epochs?,"Hello! I'm training a VAE for image models and I finally am getting some pretty decent results in training after correcting my loss function adding KL annealing and LPIPS loss and adjusting my learning rate and batch size, but now I have a doubt about the data i'm feeding to my VAE.

I have a limited time budget for the training and I have more data available than I can feed within that time budget for training.  
What is the best course of action here?  
Should I just run all the data through my VAE training until I run to the end of my training time in one single giant epoch or should I select a subset of the data small enough so that I can go through it multiple times during training and run this smaller dataset over multiple epoch?

My instinct tells me that different data is better for generalization, but VAEs also try to be resilient to variations of the representation of the same image. Because during the encoding phase we use the latent generated to sample from a random distribution (causing a different representation to be passed to the decoder) it feels like potentially feeding back the same data multiple data might actually beneficial to learn resiliency there ...

Is this actually not a thing? I'm actually overthinking about the potential impact of the multiple epochs on VAE training? Is one single giant epoch the best?

Thanks!",hayarms,1h5dfno,https://reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,https://www.reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,2024-12-03 02:52:30,7,0.77,7,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h5dfno
MachineLearning,[D] Benchmarks for RL algorithms across Gymnasium environments?,"Hey r/ML!

Let me preface this by saying I'm fairly new to RL. My previous work was with LLMs, where it is very common to rank and stack your model against the universe of models based on how it performs on a given benchmarks (but you already know this).

Recently started training models in MuJoCo environments and I'm trying to figure out if my algorithms are performing somewhat decently. Sure, I can get Ant-v5 to walk using SB3's default PPO and MlpPolicy, but how good is it really?

Is there some benchmark or repo where I can compare my results against the learning curve of other people's algorithms using the default MuJoCo (or any of the other gyms') reward functions? Of course the assumption would be that we are using the same environment and reward function, but given Gymnasium is popular and offers good defaults, I'd imagine there should be a lot of data available.

I've googled around and have only found sparse results. Is there a reason why benchmarks are not as big in RL as they are with LLMs?",geepytee,1h5a9s8,https://reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,https://www.reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,2024-12-03 00:18:26,7,0.9,7,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h5a9s8
MachineLearning,[P] PerpetualBooster outperforms AutoGluon on AutoML benchmark,"
PerpetualBooster is a GBM but behaves like AutoML so it is benchmarked also against AutoGluon (v1.2, best quality preset), the current leader in [AutoML benchmark](https://automlbenchmark.streamlit.app/cd_diagram). Top 10 datasets with the most number of rows are selected from [OpenML datasets](https://www.openml.org/). The results are summarized in the following table for regression tasks:

| OpenML Task                                  | Perpetual Training Duration | Perpetual Inference Duration                                      | Perpetual RMSE | AutoGluon Training Duration | AutoGluon Inference Duration                                      | AutoGluon RMSE |
| -------------------------------------------- | --------------------------- | ----------------------------------------------------------------- | -------------- | --------------------------- | ----------------------------------------------------------------- | -------------- |
| [Airlines_DepDelay_10M](openml.org/t/359929) | 518                         | 11.3                                                              | 29.0           | 520                         | 30.9 | 28.8   |
| [bates_regr_100](openml.org/t/361940)        | 3421                        | 15.1 | 1.084  | OOM            | OOM                         | OOM                                                               |
| [BNG(libras_move)](openml.org/t/7327)        | 1956                        | 4.2 | 2.51   | 1922           | 97.6                        | 2.53                                                              |
| [BNG(satellite_image)](openml.org/t/7326)    | 334                         | 1.6                                                               | 0.731          | 337                         | 10.0 | 0.721  |
| [COMET_MC](openml.org/t/14949)               | 44                          | 1.0 | 0.0615  | 47             | 5.0                         | 0.0662                                                            |
| [friedman1](openml.org/t/361939)             | 275                         | 4.2 | 1.047   | 278            | 5.1                         | 1.487                                                             |
| [poker](openml.org/t/10102)                  | 38                          | 0.6 | 0.256   | 41             | 1.2                         | 0.722                                                             |
| [subset_higgs](openml.org/t/361955)          | 868                         | 10.6 | 0.420  | 870            | 24.5                        | 0.421                                                             |
| [BNG(autoHorse)](openml.org/t/7319)          | 107                         | 1.1 | 19.0    | 107            | 3.2                         | 20.5                                                              |
| [BNG(pbc)](openml.org/t/7318)                | 48                          | 0.6 | 836.5   | 51             | 0.2                         | 957.1                                                             |
| average                                      | 465                         | 3.9                                                               | -              | 464                         | 19.7                                                              | -              |

PerpetualBooster outperformed AutoGluon on 8 out of 10 datasets, training equally fast and inferring 5x faster. The results can be reproduced using the automlbenchmark fork [here](https://github.com/deadsoul44/automlbenchmark).

Github: https://github.com/perpetual-ml/perpetual",mutlu_simsek,1h52zk8,https://reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,https://www.reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,2024-12-02 19:12:26,18,0.91,18,0,0,0,0,False,False,True,False,False,Project,self,t3_1h52zk8
MachineLearning,[P] Label Studio Activation Troubles,"I'm trying to run Label Studio because I was told once that it's more of a modern program used for labeling images, which I plan to do for a personal project. However, I've been dealing with headache after headache trying to get it to run, since it complains about \_psycopg. I have tried installing Python and PostgreSQL (since I think there's a dependency between the two) multiple times, looking into issues with libpq.dll, and so on, but it's not working. Anyone have any idea on how to fix an issue like this, or should I look into a different labeling program?",NuDavid,1h5gtqk,https://reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,https://www.reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,2024-12-03 06:02:59,1,1.0,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1h5gtqk
MachineLearning,[R] ImageFolderüöÄ: Autoregressive Image Generation with Folded Tokens,"https://preview.redd.it/2olpl72q6i4e1.png?width=911&amp;format=png&amp;auto=webp&amp;s=b54d91736543906b6a71102a09dc04883033d795

&gt;Image tokenizers are crucial for visual generative models, e.g., diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve the image reconstruction quality. However, tokenizers with longer token lengths are not guaranteed to achieve better generation quality. There exists a trade-off between reconstruction and generation quality regarding token length. In this paper, we investigate the impact of token length on both image reconstruction and generation and provide a flexible solution to the tradeoff. We propose ImageFolder, a semantic tokenizer that provides spatially aligned image tokens that can be folded during autoregressive modeling to improve both generation efficiency and quality. To enhance the representative capability without increasing token length, we leverage dual-branch product quantization to capture different contexts of images. Specifically, semantic regularization is introduced in one branch to encourage compacted semantic information while another branch is designed to capture the remaining pixel-level details. Extensive experiments demonstrate the superior quality of image generation and shorter token length with ImageFolder tokenizer.

Paper: [https://arxiv.org/abs/2410.01756](https://arxiv.org/abs/2410.01756)  
Code: [https://github.com/adobe-research/ImageFolder](https://github.com/adobe-research/ImageFolder)",xternalz,1h55x1i,https://reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,https://www.reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,2024-12-02 21:11:25,6,0.8,6,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/-S_lEVvuVpw96JtQP6vCgCd6QVqkMlv4dix0h6EtqFY.jpg,t3_1h55x1i
MachineLearning,[R] RuleOpt v.1.1: Optimization-Based Rule Learning for Classification,"**Paper**:¬†[https://arxiv.org/abs/2104.10751](https://arxiv.org/abs/2104.10751)

**Package:**¬†[https://github.com/sametcopur/ruleopt](https://github.com/sametcopur/ruleopt)

**Documentation:**¬†[https://ruleopt.readthedocs.io/](https://ruleopt.readthedocs.io/)

RuleOpt is an optimization-based rule learning algorithm designed for classification problems. Focusing on scalability and interpretability, RuleOpt utilizes linear programming for rule generation and extraction.

The Python library ruleopt is capable of extracting rules from ensemble models, and it also implements a novel rule generation scheme. The library ensures compatibility with existing machine learning pipelines, and it is especially efficient for tackling large-scale problems.

Here are a few highlights of ruleopt:

* **Efficient Rule Generation and Extraction**: Leverages linear programming for scalable rule generation (stand-alone machine learning method) and rule extraction from trained random forest and boosting models.
* **Interpretability**: Prioritizes model transparency by assigning costs to rules in order to achieve a desirable balance with accuracy.
* **Integration with Machine Learning Libraries**: Facilitates smooth integration with well-known Python libraries scikit-learn, LightGBM, and XGBoost, and existing machine learning pipelines.
* **Extensive Solver Support**: Supports a wide array of solvers, including¬†*Gurobi*,¬†*CPLEX*¬†and¬†*OR-Tools*.

  
With the latest version update, RuleOpt is now fast even with the free solver OR-Tools, even on large datasets! In the graph below, you can see how the new version performs in terms of runtime compared to the previous version.

[Training Times v1.0 vs v1.1](https://preview.redd.it/ev5u5m4bjf4e1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=e38ea3c168ece7ad6660153a6073f8064ac85d83)

  
We‚Äôd love to hear your feedback, questions, or any other inquiries you may have!",zedeleyici3401,1h4tzd0,https://reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,https://www.reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,2024-12-02 12:36:23,7,0.83,7,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/HhFHG_WRfu3w1cHy2TmWE1t-ZUve2F-IM_C2fVLZwqo.jpg,t3_1h4tzd0
MachineLearning,[P] multi feature linear regression code in python not giving the correct solution (or any solution for that matter)...,"linear regression using gradient descent:

    def multiFeatureLinearRegression(x, y, alpha, iterations):
    ¬† ¬† w = [0.0] * len(x[0])
    ¬† ¬† b = 0.0
    ¬† ¬† m = len(x)
    ¬† ¬† 
    ¬† ¬† for it in range(iterations):
    ¬† ¬† ¬† ¬† w_temp = [0.0] * len(x[0])
    ¬† ¬† ¬† ¬† b_temp = 0.0
    ¬† ¬† ¬† ¬† for i in range (len(x)):
    ¬† ¬† ¬† ¬† ¬† ¬† prediction = b
    ¬† ¬† ¬† ¬† ¬† ¬† for j in range(len(x[i])):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prediction += w[j] * x[i][j]
    ¬† ¬† ¬† ¬† ¬† ¬† error = y[i] - prediction
    ¬† ¬† ¬† ¬† ¬† ¬† 
    ¬† ¬† ¬† ¬† ¬† ¬† b_temp += error
    ¬† ¬† ¬† ¬† ¬† ¬† 
    ¬† ¬† ¬† ¬† ¬† ¬† for j in range(len(x[i])):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† w_temp[j] += error * x[i][j]
    ¬† ¬† ¬† ¬† 
    ¬† ¬† ¬† ¬† for i in range(len(x[0])):
    ¬† ¬† ¬† ¬† ¬† ¬† w[i] -= alpha * (2.0 / m) * w_temp[i]
    ¬† ¬† ¬† ¬† b -= alpha * (2.0 / m) * b_temp
    ¬† ¬† ¬† ¬† 
    ¬† ¬† return w, b

main body:

    data = [ ¬† ¬†[15, 3, 20], ¬†# [House Size (sq. ft.), Bedrooms, Age of House (years)]
    ¬† ¬† [20, 4, 15],
    ¬† ¬† [17, 3, 25],
    ¬† ¬† [22, 4, 10],
    ¬† ¬† [13, 2, 30],
    ¬† ¬† [18, 3, 20],
    ¬† ¬† [24, 4, 5],
    ¬† ¬† [16, 3, 18]
    ¬† ¬† ]
    
    dataY = [300, 400, 350, 450, 200, 370, 500, 310]
    
    alpha = 0.01
    iterations = 100000
    w, b = multiFeatureLinearRegression(data, dataY, alpha, iterations)
    
    print(""Weights (w):"", w)
    print(""Bias (b):"", b)

I am trying to implement multi feature linear regression and for some reason the output for the weight and bias is coming out to be:

    Weights (w): [-inf, -inf, -inf]
    Bias (b): -inf

I have no idea why this is happening..  
Can you spot what I am doing wrong here?  
could it be because I have not applied any normalization or something?",silveroburn,1h5jyaj,https://reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,https://www.reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,2024-12-03 09:51:24,0,0.25,0,0,5,0,0,False,False,True,False,False,Project,self,t3_1h5jyaj
MachineLearning,[R] Best chunking method for PDFs with complex layout?,"I am working on a RAG based PDF Query system , specifically for complex PDFs that contains multi column tables, images, tables that span across multiple pages, tables that have images inside them.

I want to find the best chunking strategy for such pdfs.

Currently i am using RecursiveCharacterTextSplitter. What worked best for you all for complex PDF?

",ElectronicHoneydew86,1h4pkmh,https://reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,https://www.reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,2024-12-02 07:24:21,18,0.93,18,0,5,0,0,False,False,True,False,False,Research,self,t3_1h4pkmh
MachineLearning,[D] Handle varying output dimension in Graph Neural Networks?,"I have a question about handling varying output dimensions in **Graph Neural Networks (GNNs)** during training. I'm working with **a combined graph** (merging task and compute graphs), where the structure resembles the task graph, but with compute node information integrated into the features. Since both the task graph and compute graph (nodes count) can vary, I'm using a feedforward layer to transform the node and edge features into a fixed hyperparameter embedding dimension. However, the dataset contains instances with **different numbers of compute nodes**. For example, one instance (A) might have 5 compute nodes, while another instance (B) might have 7 compute nodes. Given that this is a scheduling task using GNNs, the output dimension must match the number of compute nodes, as tasks are assigned to these nodes. I'm wondering how to handle varying output dimensions in GNNs and if there are any standard approaches to manage this kind of variation. Thanks!",bipulthapa,1h4uyn9,https://reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,https://www.reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,2024-12-02 13:29:07,3,0.67,3,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h4uyn9
MachineLearning,[D] Anthropic AI fellow/residents- any new grads/entry-level people accepted?,"Hello. Are entry-level or new grads accepted into the Anthropic fellowship or resident programs? Past people who were accepted, what was your CV and experience like?",geekgeek2019,1h4e0ah,https://reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,https://www.reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,2024-12-01 21:25:59,30,0.69,30,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1h4e0ah
MachineLearning,[D] How to handle varying Feature Dimensions in Graph Neural Networks training?,"I have a question about handling datasets with varying feature dimensions in Graph Neural Network training. For example, in one training instance (let's call it Dataset A), the node features have a dimension of 4, and the edge features have a dimension of 16. In another instance (Dataset B), the node features have a dimension of 5, and the edge features have a dimension of 25. Other datasets may have different feature dimensions as well.

What are the standard methods used to handle varying feature dimensions for each instance when training a GNN model with such datasets? I would appreciate any guidance or direction on how to approach this. Thanks!",bipulthapa,1h4dbvi,https://reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,https://www.reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,2024-12-01 20:57:35,24,0.96,24,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h4dbvi
MachineLearning,[P] Promptwright - Open source project to generate large synthetic datasets using an LLM (local or hosted),"Hey r/machinelearning,

[Promptwright](https://github.com/StacklokLabs/promptwright), a free to use *open source* tool designed to easily generate synthetic datasets using either local large language models or one of the many hosted models (OpenAI, Anthropic, Google Gemini etc)

Key Features:

\* Multiple LLM Providers Support: Works with most LLM service providers and LocalLLM's via Ollama, VLLM etc

\* Configurable Instructions and Prompts: Define custom instructions and system prompts in YAML, over scripts as before.

\* Command Line Interface: Run generation tasks directly from the command line

\* Push to Hugging Face: Push the generated dataset to Hugging Face Hub with automatic dataset cards and tags

Here is an example dataset created with promptwright on this latest release:

[https://huggingface.co/datasets/stacklok/insecure-code/viewer](https://huggingface.co/datasets/stacklok/insecure-code/viewer)

This was generated from the following template using \`mistral-nemo:12b\`, but honestly most models perform, even the small 1/3b models.

    system_prompt: ""You are a programming assistant. Your task is to generate examples of insecure code, highlighting vulnerabilities while maintaining accurate syntax and behavior.""
    
    topic_tree:
      args:
        root_prompt: ""Insecure Code Examples Across Polyglot Programming Languages.""
        model_system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        tree_degree: 10  # Broad coverage for languages (e.g., Python, JavaScript, C++, Java)
        tree_depth: 5  # Deep hierarchy for specific vulnerabilities (e.g., SQL Injection, XSS, buffer overflow)
        temperature: 0.8  # High creativity to diversify examples
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
      save_as: ""insecure_code_topictree.jsonl""
    
    data_engine:
      args:
        instructions: ""Generate insecure code examples in multiple programming languages. Each example should include a brief explanation of the vulnerability.""
        system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        temperature: 0.9  # Encourages diversity in examples
        max_retries: 3  # Retry failed prompts up to 3 times
    
    dataset:
      creation:
        num_steps: 15  # Generate examples over 10 iterations
        batch_size: 10  # Generate 5 examples per iteration
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        sys_msg: true  # Include system message in dataset (default: true)
      save_as: ""insecure_code_dataset.jsonl""
    
    # Hugging Face Hub configuration (optional)
    huggingface:
      # Repository in format ""username/dataset-name""
      repository: ""hfuser/dataset""
      # Token can also be provided via HF_TOKEN environment variable or --hf-token CLI option
      token: ""$token""
      # Additional tags for the dataset (optional)
      # ""promptwright"" and ""synthetic"" tags are added automatically
      tags:
        - ""promptwright""

We've been using it internally for a few projects, and it's been working great. You can process thousands of samples without worrying about API costs or rate limits. Plus, since everything runs locally, you don't have to worry about sensitive data leaving your environment.

The code is Apache 2 licensed, and we'd love to get feedback from the community. If you're doing any kind of synthetic data generation for ML, give it a try and let us know what you think!

Links:

Checkout the [examples](https://github.com/StacklokLabs/promptwright/tree/main/examples) folder , for examples for generating code, scientific or creative ewr

Would love to hear your thoughts and suggestions, if you see any room for improvement please feel free to raise and issue or make a pull request.",zero_proof_fork,1h4bcz2,https://reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,https://www.reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,2024-12-01 19:33:47,15,0.83,15,0,4,0,0,False,False,True,False,False,Project,self,t3_1h4bcz2
MachineLearning,[R] Queries on DeepAR Framework in AWS Sagemaker,"Hi,

I'm trying to implement deepAr for various stores to predict futures sales (each store with ~10k SKU of different products). Due to sheer size of the SKU I wouldn't be able to just do only single training for all the data at once. I'm thinking to train it by store.

1. How do I do parallelism in AWS for the training purpose? Each store training process would take up to 30mins;
2. How to deal with unseen SKUs which are not present in the data?

Thanks.",skw1990,1h4gwxy,https://reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,https://www.reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,2024-12-01 23:33:04,7,0.9,7,0,0,0,0,False,False,True,False,False,Research,self,t3_1h4gwxy
MachineLearning,[R] Sources: Reasons why KG outperformes RD in Retrievers?,"Are there any sources discussing WHY Retriever work better with KG in contrast to RD? I find it super intuitive to say its better because in knowledge graphs we have more semantic structure and relations are discovered effeciently. In my mind its ""of course the graph is richer/more dense"" but when collaborated on a paper, it struck me that I wasnt able to justify that claim. I found no source whatsoever that actually explained why that might be the case.

The only source i got was this one:  
[https://arxiv.org/abs/2311.07509](https://arxiv.org/abs/2311.07509)

also here in¬†[](https://www.reddit.com/r/LocalLLaMA/)¬†sub last year: ¬†[https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a\_benchmark\_to\_understand\_the\_role\_of\_knowledge/](https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a_benchmark_to_understand_the_role_of_knowledge/)

So all we we're able to say was ""We justify our decision because KG works better then RD \[source to benchmark paper\]""

I would have loved to discuss why exactly KG are better suited and give arguments about information density, semantic strutuce or the better selection of related entities. But everything I found were only articles that threw around wild claims or pointed out easier/native implementation, which technically could also be achieved with RD.

Can anyone point me to sources? would love to read an in-depth discussion on the reasons of better performance.",PopPsychological4106,1h447eu,https://reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,https://www.reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,2024-12-01 14:17:17,28,0.7,28,0,21,0,0,False,False,True,False,False,Research,self,t3_1h447eu
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1h46e6j,https://reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,2024-12-01 16:00:30,7,0.9,7,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h46e6j
MachineLearning,"[R] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",,moschles,1h40wms,https://reddit.com/r/MachineLearning/comments/1h40wms/r_qwenvl_a_versatile_visionlanguage_model_for/,https://storage.prod.researchhub.com/uploads/papers/2023/12/25/2308.12966.pdf,2024-12-01 10:59:28,13,0.93,13,0,0,0,0,False,False,False,False,False,Research,default,t3_1h40wms
MachineLearning,[Project] Noema ‚Äì A Declarative AI Programming Library,,Super_Dependent_2978,1h46341,https://reddit.com/r/MachineLearning/comments/1h46341/project_noema_a_declarative_ai_programming_library/,/r/LocalLLaMA/comments/1h3d4d8/noema_a_declarative_ai_programming_library/,2024-12-01 15:46:35,4,0.64,4,0,1,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/Bke830yFsuJxUn2gkbNe1tpnLzRcSugASe-RSwzFQu0.jpg,t3_1h46341
MachineLearning,Augmentation for Images with ROI [D],"I have an Image with roi (x\_min,y\_min,x\_max,y\_max). I want do Random flip,rotate,skew, translate ,etc.. with torchvison. what are the different ways in which you can transform the roi respectively in order to match with the augmented image ?",Brief_Papaya121,1h41z2x,https://reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,https://www.reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,2024-12-01 12:11:25,5,0.78,5,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h41z2x
MachineLearning,What's the best Open Source Image-Upscaling Model? [Discussion],"I'm using [Playground-v2.5-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) to make some images for YouTube thumbnails. I'm really happy with the results:

[1024x1024 base image of mars base.](https://preview.redd.it/uuo4sdgwp44e1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=84d61bf4d7fbf2457df1037e95603166390efa12)

But I would like the image to be 1920x1080 pixels, and my only options are 1024x1024, or 1280x720 pixels. At the moment, I can get to 1920x1080 with Photoshop's outpainting:

[1920x1080 outpainted image of mars base.](https://preview.redd.it/07tt5ix4q44e1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=cdfb7cefa8d2bb4f187d2ab6b86aaba17596506a)

This is okay, but photoshops outpainting is manual and has a fairly significant quality drop. Ideally, I would generate an image in 1280x720 then upscale to 1920x1080 programmatically.

I've heard of the following models:

* Real-ERSGAN
* Waifu2
* SRGAN

But before I jump into any of them, what open-source model is generally considered best to achieve this? I have an RTX 3060 12GB of VRAM.",FPGA_Superstar,1h3qcon,https://reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,https://www.reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,2024-12-01 00:13:57,36,0.91,36,0,9,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ROcJ4oUu0I-1wtjwdAycnGfQ3s8tbIsJdyufqyhsVyI.jpg,t3_1h3qcon
MachineLearning,[D] Seeking Advice on Machine Learning Models for Generating Seamless 360¬∞ Images,"Hi everyone,

I‚Äôm working on a project that involves creating 360¬∞ images, and I‚Äôm running into some challenges. The goal is to generate seamless 360¬∞ panoramas without visible edges or artifacts where the image wraps around.

I‚Äôm wondering if there are any machine learning models, techniques, or tools that are particularly well-suited for this task. Specifically, I‚Äôm looking for something that can:

* Ensure continuity at the edges of the 360¬∞ image.
* Handle different textures and patterns without noticeable distortions.
* Be trained or fine-tuned on my custom dataset (if needed).

I‚Äôve explored GANs like StyleGAN and diffusion models, but I‚Äôm not sure if they can handle the edge continuity issue out of the box. Has anyone worked on a similar problem or knows of a good starting point?

Any suggestions, resources, or insights would be greatly appreciated! Thanks in advance!",Deep_Land_4093,1h48950,https://reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,https://www.reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,2024-12-01 17:22:26,0,0.5,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h48950
MachineLearning,[P] A complete transformer model built in Excel,,Revolutionary-Way290,1h3hj6j,https://reddit.com/r/MachineLearning/comments/1h3hj6j/p_a_complete_transformer_model_built_in_excel/,https://x.com/ProfTomYeh/status/1859282491955130452,2024-11-30 17:25:45,19,0.95,19,0,5,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/JFO-VdcjE87RX0w6rQ5kwyN7HblXjJoN1QbFc-1p18M.jpg,t3_1h3hj6j
MachineLearning,[D] Modern use-cases for RNNs?,"The discussion can be twofold.
1)What are in your opinion some tasks, for the personal projects scale, where you think RNNs close to traditional implementations (LSTM, GRU) are still the best starting and ending point? Especially compared to transformers.

In small time-series forecasting settings I can see a GRU being more convenient than a Transformer probably, but I am interested also in tasks where inputs are sequences of symbols or measures, but outputs maybe not.

The main goal is to play with LSTM and GRU variants (eg minGRU) on datasets where it makes sense, might do tiny-Shakespeare but it doesn't warm my heart...

2) do you think there are sequential tasks and settings where RNNs are not only the more natural option according to our intuition, but actually the only theoretically or experimentally available option to make do, compared to Transformers or 1D CNNs etc?",Sad-Razzmatazz-5188,1h38ym2,https://reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,https://www.reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,2024-11-30 09:22:32,51,0.89,51,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1h38ym2
MachineLearning,[Discussion] Advice Needed: Rejected from COLING 2025 ‚Äì Which Conference Should I Target Next?,"
I got a rejection from COLING 2025 with review scores of 4, 3, 3. I‚Äôm revising the manuscript and looking for advice on the next best NLP conference to target. Any suggestions for similar top-tier venues?

Thanks!",Cold-Traffic-7586,1h3igva,https://reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,https://www.reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,2024-11-30 18:08:01,5,0.78,5,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h3igva
MachineLearning,[D] What's the fastest object detection model?,"Hi, I'm working on a project that needs object detection. The task itself isn't complex since the objects are quite clear, but speed is critical. I've researched various object detection models, and it seems like almost everyone claims to be ""the fastest"". Since I'll be deploying the model in C++, there is no time to port and evaluate them all.

I tested YOLOv5/v5Lite/8/10 previously, and YOLOv5n was the fastest. I ran a simple benchmark on an Oracle ARM server (details [here](https://github.com/Avafly/YOLOv5-ncnn-OpenVINO-MNN-ONNXRuntime-OpenCV-CPP?tab=readme-ov-file#simple-benchmarks-on-m1-mac-and-arm-linux)), and it processed an image with 640 target size in just 54ms. Unfortunately, the hardware for my current project is significantly less powerful, and meanwhile processing time must be less than 20ms. I'll use something like quantization and dynamic dimension to boost speed, but I have to choose the suitable model first.

Has anyone faced a similar situation or tested models specifically for speed? Any suggestions for models faster than YOLOv5n that are worth trying?",Knok0932,1h362dq,https://reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,https://www.reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,2024-11-30 06:00:40,43,0.96,43,0,39,0,0,False,False,True,False,False,Discussion,self,t3_1h362dq
MachineLearning,[P] TIME-MOE: Billion-Scale Time Series Forecasting with Mixture-of-Experts,"**Time-MOE**¬†is a 2.4B parameter open-source time-series foundation model using¬†**Mixture-of-Experts (MOE)**¬†for¬†zero-shot forecasting.

You can find an analysis of the model¬†[here](https://aihorizonforecast.substack.com/p/time-moe-billion-scale-time-series)",nkafr,1h3j1cm,https://reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,https://www.reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,2024-11-30 18:33:59,2,0.75,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1h3j1cm
MachineLearning,[D] Hinton and Hassabis on Chomsky‚Äôs theory of language,"I‚Äôm pretty new to the field and would love to hear more opinions on this. I always thought Chomsky was a major figure on this but it seems like Hinton and Hassabis(later on) both disagree with it. Here: https://www.youtube.com/watch?v=urBFz6-gHGY (longer version: https://youtu.be/Gg-w_n9NJIE)

I‚Äôd love to get both an ML and CogSci perspective on this and more sources that supports/rejects this view.

Edit: typo + added source.",giuuilfobfyvihksmk,1h2mkye,https://reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,2024-11-29 14:10:12,119,0.92,119,0,117,0,0,False,False,True,False,False,Discussion,self,t3_1h2mkye
MachineLearning,[N][R] Models are what they eat: automatic data curation for LLMs,"Sharing our most recent work at [DatologyAI](http://www.datologyai.com). Models are what they eat, and our mission is to make data curation for training large models as effective and easy as possible. 

Combining a bevy of approaches, including heuristic filters, model-based filters, embedding based curation, synthetic data, target distribution matching, and mixing ratios, we were able to massively improve training efficiency, performance, and inference efficiency. 

Comparing to our baseline and starting dataset -- exact deduplicated RedPajamav1, we can: 

* Reach the same performance 7.7x faster (and 3.4x faster than DCLM)
* Improve performance across benchmarks by 8.5% (and by 4.4% over DCLM)
* Train models with fewer than half the parameters which outperform larger models by &gt;5% 

Check out our [high-level results here](https://www.datologyai.com/post/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation), and if you want all the nitty-gritty details, check out our[ technical deep dive](https://www.datologyai.com/post/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset).",arimorcos,1h2qmol,https://reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,2024-11-29 17:15:57,18,0.78,18,0,1,0,0,False,False,True,False,False,News,self,t3_1h2qmol
MachineLearning,[D] Molecular Dynamics and Machine Learning Build,"Hey guys, so I do a lot of molecular dynamics and am starting to push into the ML space with that and genome/multi-omics sort of stuff. I‚Äôm building a workstation and with my budget I‚Äôm looking at 2x A6000 or 2x 5000 Ada. Both work great for molecular dynamics, but I‚Äôm trying to figure out my best option for ML. The A6000 has 48gb vram and nvlink, but the 5000 Ada is newer and substantially faster and 32Gb VRAM per card is no slouch either. Any advice? ",Mdgoff7,1h2x4ar,https://reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,https://www.reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,2024-11-29 22:04:19,2,0.67,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h2x4ar
MachineLearning,"[D] How does VQ-VAE disentangle, if it does at all?","I currently use a BetaTC-VAE, which does an excellent job at disentangling, knowing that VAE can slightly disentangle since for the model it's easier to get a lower KL loss if the variables are dissentanlged, the beta term make this beta times more important, and total correlation and mutual information loss push for total disentanglement, but in VQ-VAE there is no (major) disentanglement, only a codebook, and discrete outputs. Could the discrete latent given by the codebook be disentangled? If not, is there any paper on disdentangling VQ-VAE? I have an environment where disentangled latent spaces provide better reconstruction than continous latent spaces ",ZazaGaza213,1h2epzx,https://reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,2024-11-29 05:33:10,37,0.95,37,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h2epzx
MachineLearning,[D] COLING 2025 Final Acceptances - Is it not out yet?,"Is the final acceptance out? I am not seeing it yet on Softconf. Had a paper with (5,4) (4,4) (4,4) in reviews.",UnhappyPrior6570,1h2kfic,https://reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,https://www.reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,2024-11-29 12:10:32,9,0.8,9,0,44,0,0,False,False,True,False,False,Discussion,self,t3_1h2kfic
MachineLearning,[D] Theory behind modern diffusion models,"Hi everyone,

I recently attended some lectures at university regarding diffusion models. Those explained all the math behind the original DDPM (Denoiding Diffusion Probabilistic Model) in great detail (especially in the appendices), actually better than anything else I have found online. So it has been great for learning the basics behind diffusion models (slides are available in the link in the readme here if you are interesed: https://github.com/julioasotodv/ie-C4-466671-diffusion-models)

However, I am struggling to find resources with similar level of detail for modern approaches‚Äîsuch as flow matching/rectified flows, how the different ODE solvers for sampling work, etc. There are some, but everything that I have found is either quite outdated (like from 2023 or so) or very superficial‚Äîlike for non-technical or scientific audiences.

Therefore, I am wondering: has anyone encountered a good compendium of theoretical eplanations beyond the basic diffusion model (besides the original papers)? The goal is to let my team deep dive into the actual papers should they desire, but giving 70% of what those deliver in one or more decent compilations.

I really believe that SEO is making any search a living nightmare nowadays. Either that or my googling skills are tanking for some reason.

Thank you all!",bgighjigftuik,1h1vxe1,https://reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,2024-11-28 13:27:28,226,0.99,226,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1h1vxe1
MachineLearning,"""[P]""Static variable and dynamic variable tables in RFM ","



I am creating a prediction model using random forest. But I don't understand how the model and script would consider both tables loaded in as dataframes.


What's the best way to use multiple tables with a Random Forest model when one table has static attributes (like food characteristics) and the other has dynamic factors (like daily health habits)?

Example:
I want to predict stomach aches based on both the food I eat (unchanging) and daily factors (sleep, water intake).

Tables:
 * Static: Food name, calories, meat (yes/no)
 * Dynamic: Day number, good sleep (yes/no), drank water (yes/no)


How to combine these tables in a Random Forest model? Should they be merged on a unique identifier like ""Day number""?
",peyott100,1h2oe69,https://reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,https://www.reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,2024-11-29 15:36:10,1,0.67,1,0,3,0,0,False,False,True,False,False,Project,self,t3_1h2oe69
MachineLearning,[R] Recursive Methods for interpolation between vector fields ( Known and Unknown),"Hello everyone Does anything of the next makes sense?  
I Have been posting on Learning first ( also on math and number theory ) , but I think is a bit more math theory than ML but it does have to do with how the data is interpolated so I am unsure. 

( I hope I am not breaking rule 5 with my links ) 

this will be the interpolation of the data ( Via organized vector field levels )  before the generative process starts, but because its recursive, the generative process can happen on inside the iteration too 

its there a model I can use ? And if someone understand the math, can I get some papers or things I could follow or just is learning and reading now?

I am a little lost and need some help ( I organized my question with chatGPT to make it understandable so bare in mind if there is some odd work here and there, I am on the I am going a bit mental stage )

I think this is dealing with machine learning problems that have been solved between interpolation of point could on space that have recursive data ( mapping and data organization )

I've been developing a concept that merges artistic visualization with advanced mathematical interpolation techniques inspired by the Mandelbrot set. Coming from a creative background, I've ventured into creating what I believe could be a¬†**recursive Mandelbrot predictive method**¬†¬†for manipulating vector fields. I'm eager to understand if this approach already exists and to gather resources or similar algorithms to explore further and test my ideas.

I will add some things like this latter to test segmentation models for the recursiveness¬†[https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear\_algebra\_project\_i\_implemented\_a\_kmeans/](https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/)

**REFERENCE IMAGES**  
everything is based on recursive by resolution with inverse square distance from the origin point

**Mandelbroth**  
[https://en.wikipedia.org/wiki/Mandelbrot\_set#/media/File:Juliacycles1.png](https://en.wikipedia.org/wiki/Mandelbrot_set#/media/File:Juliacycles1.png)

**Conceptual model**¬†( The mandelbroth guidance happens just on the altered time pulling agent ) ( Orange )  
[Single Vector interpretation and prediction stream of the Pull of the mandelbrot agent](https://cdn.discordapp.com/attachments/457637053581230100/1311677830966284298/chrome_7KNKgsO6Fm.png?ex=6749baac&amp;is=6748692c&amp;hm=55a7809cd5bb402a06fdea1edbad4eb1ff21f15d1a3303efee9b797eef0f5839&amp;)

**Conceptual Model 2d sim**  
[Representation of the predictiveness](https://media.discordapp.net/attachments/457637053581230100/1311668738139099166/ShareX_11AweoaOUp.png?ex=6749b234&amp;is=674860b4&amp;hm=7d660210c3678b8f1bd0804ee4f2f2425a0c1dc9aea18bdf39e86ba81d9ad2e8&amp;=&amp;format=webp&amp;quality=lossless&amp;width=295&amp;height=350)¬†as mandelbrot

Representation of functional interpolation of agents via Mandelbroth ( non recursive )

**Conceptual Simulation model 2d sim ( making the mandelbroth )**  
[Image non animated](https://media.discordapp.net/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;=&amp;format=webp&amp;quality=lossless)[ANIMATED VIDEO DOWNLOAD ( CLEAN FILE )](https://cdn.discordapp.com/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;)

**Conceptual layering**  
[Layering of 3 tiers via inverse square distance on a vector field ( currently surface) but can be world](https://media.discordapp.net/attachments/457637053581230100/1311404062931030086/image.png?ex=67496475&amp;is=674812f5&amp;hm=56bf4ff3988163f35c1ef24227645beaa6d11ab1b0330d094dc3dccbad6ce696&amp;=&amp;format=webp&amp;quality=lossless&amp;width=378&amp;height=350)

**recursiveness concept**  
[Applied recursiveness auto generation based on surface vector field¬†](https://media.discordapp.net/attachments/457637053581230100/1311404209278685184/image.png?ex=67496498&amp;is=67481318&amp;hm=e548fb4615d48adea59f835a3a76a6a203c5946ea6d356eff2c7298252d3de41&amp;=&amp;format=webp&amp;quality=lossless&amp;width=927&amp;height=831)( no prediction applied )

# The Concept

Imagine a system where the interpolation between data points isn't limited to traditional methods like¬†**lerp**¬†(linear interpolation) or¬†**slerp**¬†(spherical linear interpolation). Instead, it employs a¬†**pseudo vector field Mandelbrot slerp**, allowing vectors to be guided from a base state (reality) to a target state (altered time) within a Mandelbrot-inspired vector field. This method is recursive, meaning multiple layers of calculations are applied to refine the interpolation continuously.

# Key Components:

1. **Reality (Ground Truth):**¬†Represents the current state of the system, serving as the foundational dataset.
2. **Agents of Change (Vectors of Closest Influence):**¬†These act as pull forces influencing the direction and magnitude of interpolation.
3. **State (Ground Truth Prediction Model):**¬†Utilizes the current data to predict future states based on the influences of the agents.
4. **Altered Time (Goal):**¬†The desired target state, akin to a Mandelbrot-type location on the outer range of the vector field.

# Interpolation Method

The interpolation technique extends beyond simple linear methods by incorporating the complexity and fractal nature of the Mandelbrot set. Here's how it functions:

* **Guided Vectors:**¬†Vectors transition from reality towards altered time, following paths influenced by a Mandelbrot-like vector field.
* **Recursive Layers:**¬†Multiple layers of interpolation allow for increasingly refined calculations, enhancing accuracy and adaptability.
* **Dynamic Intensity:**¬†The closer the interpolation is to reality, the more intense and detailed the calculations become, while the vector field simplifies as it moves towards altered time.

# Theoretical Foundation

The core idea revolves around mapping and adjusting Mandelbrot-inspired vectors to facilitate interpolation between recursively organized data banks. This approach aims to:

* **Capture Complex Patterns:**¬†Leverage the self-similar, fractal nature of Mandelbrot sets to identify and utilize intricate patterns within the data.
* **Enhance Predictive Capability:**¬†Recursive calculations allow for continual refinement of projections, improving predictive accuracy over time.
* **Achieve Real-Time Adaptability:**¬†Dynamically adjust vectors to align with specific goals, similar to how a car's performance might be modulated in real-time to achieve optimal racing outcomes.

# Visual Analogy

Think of this system as calculating the ""ghost"" position of a car in a racing game like¬†*Need for Speed*:

* **Acceleration and Braking:**¬†Based on historical and current data, determining when to accelerate or brake to achieve the best performance.
* **Engine Adjustments:**¬†Modifying the system's parameters in real-time to align with the target state, ensuring the system reaches its goal efficiently.
* **Dynamic Modulation:**¬†Continuously adjusting these actions to meet the desired ""goal time,"" always operating within physical (mathematical) constraints.

# Questions for the Community

1. **Does This Technology Exist?**¬†Is my approach accurately described as a¬†**recursive Mandelbrot predictive method**¬†for vector field interpolation? Are there existing models or research that align closely with this concept?
2. **Resources and References:**¬†If similar technologies or algorithms exist, could you recommend any resources, papers, or specific Mandelbrot-like algorithms that I can study or begin testing with?
3. **Mathematical Validation:**¬†Given that my approach stems from an artistic visualization perspective, what mathematical frameworks or theories should I explore to formalize and validate this method?

# Additional Context

For a visual representation of my model and its applications, you can refer to the following links:

* **Visual Model:**¬†[LinkedIn¬†Visual¬†Model](https://www.linkedin.com/feed/update/urn:li:activity:7267834154631708672/)
* **Use Case Example:**¬†[LinkedIn¬†Use¬†Case](https://www.linkedin.com/posts/jesusfc14_i-think-i-am-reaching-a-clarity-moment-the-activity-7267823963068628992--qA7?utm_source=share&amp;utm_medium=member_desktop)

*(Please note that these links provide additional visual context to help illustrate the concept.)*

**Thank you for taking the time to read through my concept! I'm looking forward to your insights, validations, and any resources you can share to help me advance this idea.**

all this tech is currently under Creature Garage umbrella but I have ownership of the creative driver of the idea so that should be fine for me to post but I reached a moment that I will need help for some of the most advanced math implementations

I am using some concepts that sound really far and advanced but currently my implementation is mostly based on recursiveness the prediction agent will come to function once I have my full set of data to make a test",jesusfc,1h2io35,https://reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,https://www.reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,2024-11-29 10:08:00,1,0.54,1,0,12,0,0,False,False,True,False,False,Research,self,t3_1h2io35
MachineLearning,[D] Most important papers in implicit regularisation,"Hi guys

I'm getting into machine learning, especially on the theoretical side, and I'm curious to learn more about why neural networks tend to generalise so well, so I'm hoping to read some papers about this. As far as I'm aware, the first big paper on the topic was 'Understanding deep learning requires rethinking generalization' by Zhang et al.

I've got a good mathematical background, so I was wondering what people think are the most impactful papers there are in this area. What do you think made the most impact?",MrBeebins,1h29i7j,https://reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,2024-11-29 00:23:33,11,1.0,11,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h29i7j
MachineLearning,[D] Why aren't Stella embeddings more widely used despite topping the MTEB leaderboard?,"https://huggingface.co/spaces/mteb/leaderboard

I've been looking at embedding models and noticed something interesting: Stella embeddings are crushing it on the MTEB leaderboard, outperforming OpenAI's models while being way smaller (1.5B/400M params) and apache 2.0. Makes hosting them relatively cheap.

For reference, Stella-400M scores 70.11 on MTEB vs OpenAI's text-embedding-3-large 64.59. The 1.5B version scores even higher at 71.19

Yet I rarely see them mentioned in production use cases or discussions. Has anyone here used Stella embeddings in production? What's been your experience with performance, inference speed, and reliability compared to OpenAI's offerings?

Just trying to understand if there's something I'm missing about why they haven't seen wider adoption despite the impressive benchmarks.

Would love to hear your thoughts and experiences!",sdsd19,1h1u814,https://reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,2024-11-28 11:45:44,66,0.93,66,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1h1u814
MachineLearning,[R] BitNet a4.8: 4-bit Activations for 1-bit LLMs,"**Paper:** [https://arxiv.org/pdf/2411.04965](https://arxiv.org/pdf/2411.04965)

**Abstract:**

&gt;Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Visual Abstract:**

https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a

**Evaluations:**

[HS=HellaSwag, PQ=PiQA, WGe=WinoGrande](https://preview.redd.it/4ppq57varn3e1.png?width=955&amp;format=png&amp;auto=webp&amp;s=3c4152947edf4542d2a1ffa181bfa52a5369d916)

https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9

",StartledWatermelon,1h1y0ig,https://reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,2024-11-28 15:11:18,29,0.88,29,0,3,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/eMHBOJqKZns6b6d3vVu8taf4yu8Tq472NuD6w0t0t6M.jpg,t3_1h1y0ig
MachineLearning,[R] Fast Matrix-Based Counterfactual Regret Minimization Using GPU Parallelization,"A novel GPU implementation of Counterfactual Regret Minimization (CFR) that accelerates the computation of optimal strategies in extensive-form games. The core innovation is parallelizing the regret updates and strategy computations across GPU cores while carefully managing memory access patterns.

Key technical points:
- Custom memory layout that maps game states and actions to GPU threads
- Batch processing of information sets to maximize GPU utilization
- Parallel computation of counterfactual values and regret updates
- Multi-GPU scaling through game tree partitioning
- Evaluated on Leduc Hold'em and Limit Texas Hold'em poker variants

Results:
- Up to 30x speedup compared to CPU implementation
- Linear scaling with number of GPUs up to 8 devices
- Memory usage scales with game size and number of information sets
- Solution quality matches CPU baseline within statistical error
- Successfully solved games with up to 10^14 states

I think this work could make CFR much more practical for real-world applications beyond poker. The ability to solve larger games faster opens up possibilities in areas like automated negotiation, security games, and resource allocation. The multi-GPU scaling is particularly interesting as it suggests potential for solving even more complex games.

The memory optimization techniques developed here might also transfer well to other game-theoretic algorithms that need to process large state spaces efficiently.

TLDR: GPU-accelerated CFR implementation achieves 30x speedup through careful parallelization and memory management, with linear multi-GPU scaling. Makes solving large extensive-form games significantly more tractable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/gpu-accelerated-counterfactual-regret-minimization). Paper [here](https://arxiv.org/abs/2408.14778).",Successful-Western27,1h1wq6b,https://reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,2024-11-28 14:08:34,23,0.9,23,0,1,0,0,False,False,True,False,False,Research,self,t3_1h1wq6b
MachineLearning,[P] Retrieval augmented generation on-premises (fully local solution),"Hey everyone,   
I‚Äôm excited to share my latest repo with you‚Äîa local conversational RAG solution for your files! Here‚Äôs the deal: this setup is perfect for running RAG on-premises.   
It‚Äôs built with Docker, LangChain, Ollama, FastAPI, and Hugging Face, and all models are downloaded automatically. Soon, I‚Äôll add support for choosing your preferred model, but here‚Äôs what the solution currently includes:  
‚Ä¢ Locally running Ollama: It‚Äôs hardcoded to the Qwen-0.5B model for now, but model selection from the Ollama registry is coming soon.   
‚Ä¢ Local indexing: Uses a sentence-transformer embedding model (currently restricted to this family, but this will also change soon).   
‚Ä¢ Qdrant container: Runs locally for vector storage.   
‚Ä¢ Local reranker: Currently uses BAAI/bge-reranker-base, with support for reranker selection coming soon.  
‚Ä¢ Websocket-based chat: Includes history-saving capabilities.   
‚Ä¢ Simple chat UI: Built with React for a straightforward interface.   
‚Ä¢ Bonus: You can use this setup with ChatGPT as a custom GPT! Query your local data through the official ChatGPT web interface or macOS/iOS app.   
‚Ä¢ On-premises ready: Everything runs locally, and the containers are CPU-friendly.

A couple of ideas and known issues:   
‚Ä¢ Support for Model Context Protocol is on the roadmap.   
‚Ä¢ No incremental indexing or reindexing yet.   
‚Ä¢ Model selection isn‚Äôt available yet but will be added soon.   
  
I‚Äôd love your feedback, contributions, or support‚Äîwatch, fork, and star if you find this interesting!  
Thank you!   
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
",davidvroda,1h26ul7,https://reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,2024-11-28 22:01:09,3,0.59,3,0,0,0,0,False,False,True,False,False,Project,self,t3_1h26ul7
MachineLearning,[P] Latest version of Ollama Grid Search (0.7.0): added prompt database ,"https://preview.redd.it/ohewvqicbo3e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=077fec6931b2efc40182f7c2eb284718822213e0

Hey people... the latest version of [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) now comes with its own prompt management database (along with many improvements in the UI).

https://preview.redd.it/qzu95clhbo3e1.png?width=975&amp;format=png&amp;auto=webp&amp;s=473382281094fc3f819e6fc6c3d267941d2a35ce

It makes it a hell lot easier to test your existing prompts when you pull newly released models!

If you want to check it out, the github page has releases for all major platforms:

[https://github.com/dezoito/ollama-grid-search](https://github.com/dezoito/ollama-grid-search)",grudev,1h20fzv,https://reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,https://www.reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,2024-11-28 17:01:16,10,0.86,10,0,0,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uAl-6kPsNjdKIz7o50pQEHnMDPXa-cNX7GoAlrOqiec.jpg,t3_1h20fzv
MachineLearning,[D] Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis,"As a part of¬†daily paper discussions¬†on the¬†[Yannic Kilcher](https://www.linkedin.com/in/ykilcher/)¬†discord server, I will be volunteering to lead the analysis of the following¬†**Apple's**¬†Visatronic work

üìú¬†**Visatronic: A Multimodal Decoder-Only Model for Speech Synthesi**s by¬†[Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/),¬†[Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/),¬†[Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/),¬†[Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/),¬†[Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/),¬†[He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
üåê¬†[https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

üï∞ Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun¬†\~¬†[https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/ygxnbhiboo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=31bf7c9b988c83a8d0ff2e7b011dac027aa8f154

https://preview.redd.it/7v15egiboo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=d9629caa406a92f8b2052ad6baa3a0265a27ddcf

",CATALUNA84,1h222s2,https://reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,https://www.reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,2024-11-28 18:13:13,6,0.75,6,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/GIw9ZddndVdad-4h4y65mlExW8FTRdhSBchmteHv-AA.jpg,t3_1h222s2
MachineLearning,[D] Inconsistent use of the gerund form in dataset naming,"This is, of course, a very minor issue. But it really irks me, and I was wondering if other people are bothered by it. I think the two most common naming schemes I see for the three standard dataset splits are ""training dataset, validation dataset, and test dataset"" or ""training dataset, validation dataset, and testing dataset"". But to be consistent it should really be ""train dataset, validation dataset, and test dataset"" or ""training dataset, validating dataset, and testing dataset"". I always use the former. Does this bother anyone else, or am I alone in brooding over this?",Shianiawhite,1h217sh,https://reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,2024-11-28 17:34:46,3,0.67,3,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h217sh
MachineLearning,Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis [D],"As a part of¬†daily paper discussions¬†on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

üìú¬†V**isatronic: A Multimodal Decoder-Only Model for Speech Synthesis** by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
üåê [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

üï∞ Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun¬†\~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/7eoqonhqjo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=c751f5c9f9bc42f22672bbbc67cf8b0fe95ef36b

https://preview.redd.it/224tugzrjo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=37adc93604606d86492dc104f03915bab7428eca",CATALUNA84,1h21j1a,https://reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,https://www.reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,2024-11-28 17:48:43,3,0.71,3,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/wIqPUsHaUNfLLWwLHYcQkdG4Yav_LVjLmzLghfKaOjI.jpg,t3_1h21j1a
MachineLearning,[D] Loading data into Ray clusters,"For those of you that run ML training in a Ray cluster on AWS, I'm curious to know what approach you take to get training data into your cluster?

And how are you versioning the data?

How do you avoid repeatedly downloading the same data across runs that have the same dataset?

I'd like a smooth process for being able to target a specific version of a dataset for a training run, and to avoid repeatedly downloading it. The data versioning should have a clear mapping to whatever version of a data pipeline created it. It'd also be nice to have something that scales well to larger datasets.

Keen to hear experiences from the trenches.",SingularValued,1h1v68j,https://reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,2024-11-28 12:45:29,6,0.88,6,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1v68j
MachineLearning,Causal Discovery Competition Winning Paper Discussion [D],"I‚Äôve recently come across this post: https://thetourney.github.io/adia-report/ which describes the winning method for a casual discovery competition. It‚Äôs not really my field but I do have a reasonable understanding of GNNs and Causal Inference. Anyway, from the report I don‚Äôt understand precisely what the winning team was doing. Can anyone either link to a full paper or have a good intuitive and potentially step by step explanation of what they are doing?",www3cam,1h1i0ji,https://reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,2024-11-27 23:22:43,30,0.94,30,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1h1i0ji
MachineLearning,[D]Is Freelancing as a Data Scientist Even Possible?,"Hi everyone,

I‚Äôm fine working for as low as $15/hour, so earnings aren‚Äôt a big concern for me. I‚Äôve gone through past Reddit posts, but they mostly discuss freelancing from the perspective of income. My main concern is whether freelancing in data science is practical for someone like me, given its unique challenges.

A bit about my background: I‚Äôve completed 3-4 real-world data science projects, not on toy datasets, but actual data (involving data scraping, cleaning, visualization, modeling, deployment, and documentation). I‚Äôve also worked as an intern in the NLP domain.

Some issues I‚Äôve been thinking about:

1. Domain Knowledge and Context: How hard is it to deliver results without deep understanding of a client‚Äôs business?


2. Resource Limitations: Do freelancers struggle with accessing data, computing power, or other tools required for advanced projects?


3. Collaboration Needs: Data science often requires working with teams. Can freelancers integrate effectively with cross-functional groups?


4. Iterative and Long-Term Nature: Many projects require ongoing updates and monitoring. Is this feasible for freelancers?


5. Trust and Accountability: How do freelancers convince clients to trust them with sensitive or business-critical work?


6. Client Expectations: Do clients expect too much for too little, especially at low wages?



I‚Äôm also open to any tips, advice, or additional concerns beyond these points. Are these challenges solvable for a new data science freelancer? Have any of you faced and overcome similar issues? I‚Äôd love to hear your thoughts.

Thanks in advance!",ds_reddit1,1h1q98i,https://reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,https://www.reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,2024-11-28 07:01:15,7,0.57,7,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h1q98i
MachineLearning,[P] Ablation study using a subset of data?,"Basically, I'm engaging in a research project in which I'm training encoder only language models for text classification. I have already trained my models and gotten my results, however I need to perform an ablation study. The main issue I'm having is that the dataset is large. Is it fair for me to perform the ablation study on a subset of the dataset, since I'm gonna have to train it 3 - 4 times with different ablations?",Aromatic_Web749,1h1kzsh,https://reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,2024-11-28 01:51:54,9,0.91,9,0,8,0,0,False,False,True,False,False,Project,self,t3_1h1kzsh
MachineLearning,[D] Advanced methods of data management,"Hello everyone, right now I'm involved in a project which is basically (audio)LLM fine-tuning, and we're having problems related to data management. 

Since there isn't that many sets in that area, we're using different augmentation schemas. tl;dr, we have different datasets of various nature, and using some schemas we can convert usual ASR-type datasets to QA, generate refusal data, combine questions etc. 

Problem is, it's hard to control ratios of different data, and right now it's mostly a manual labour. We kinda have to manually adjust the amount of data we're generating. Which is rather annoying and hard process; we have many target datasets, you have to remember how much samples you've generated and try to get some adequate mix for train.

Right now, we use airflow for data filtering (lots of raw data is badly labelled), but I'm not really sure that I understand how to connect that tool to data generation, and if it's a good tool for that purpose. I was thinking about writing some snakemake script, but ideally the final solution should be flexible when we change configs, add new sets etc, and that's not what I associate with snakemake. Also, another question is visualization. 

So, I'm asking what kind of tools/libs I can use to tackle this task, is there anything that can fit our purposes, or it's time to write customs scripts? What do companies like meta use for their enormous 15t tokens set, surely they don't dump everything in one place? 

There's another question related to connecting datasets with data pipeline of training (it's complicated, there are additional steps of precalculation of feats and creating webdataset which is used in training), but for now I'd be really glad if anyone helped with just data management tools. ",Theio666,1h1u0pj,https://reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,https://www.reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,2024-11-28 11:31:47,1,1.0,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1u0pj
MachineLearning,[D] AAMAS 2025 reviews are out! ,"I could not find a discussion thread, so I thought I would create one myself. ",E-Cockroach,1h15k8k,https://reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,2024-11-27 14:28:29,24,0.88,24,0,38,0,0,False,False,True,False,False,Discussion,self,t3_1h15k8k
MachineLearning,[P] py-gen-ml: generating ML configuration code from a schema,"[py-gen-ml](https://jostosh.github.io/py-gen-ml) is a Python library designed to simplify your ML experiment configuration using the power of Protocol Buffers. It's still in an early phase but I'd love to hear some feedback from the community.

**Here's how py-gen-ml can help you:**

* **Centralise configurations:** Define schemas in Protobuf to act as a single source of truth.
* **Minimise repetitive work:** Automatically generate code for models, patches, sweeps, and a command-line interface.
* **Boost flexibility:** Experiment with ease thanks to YAML configurations with advanced referencing and the ability to conduct hyperparameter sweeps.
* **Improve code quality:** Benefit from JSON schema validation, strong typing, and IDE support for a more robust development process.

**py-gen-ml aims to make ML development more efficient by reducing the burden of managing configurations.** Give it a try and see how it can improve your workflow.

**Get started:**

    pip install py-gen-ml

**Learn more:** [**https://jostosh.github.io/py-gen-ml**](https://jostosh.github.io/py-gen-ml)",jalapenjos,1h1t9v0,https://reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,https://www.reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,2024-11-28 10:40:23,0,0.25,0,0,1,0,0,False,False,True,False,False,Project,self,t3_1h1t9v0
MachineLearning,"[P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, Docker)","  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to introduce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, you can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork, star) Thank you so much!",davidvroda,1h1pudq,https://reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,2024-11-28 06:33:40,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1h1pudq
MachineLearning,[D] how to do RLHF on this kind of data?,"Hi, apologies if this is a dumb question -- I'm really not knowledgeable about post training. Suppose that I have a llama and I want to finetune with human annotations that ""like"" or ""dislike"" a prompt response. Most DPO datasets feature a pair of possible responses, with one being chosen. Interpreting my data as one half of a pair with one missing, I could generate a second response from the same prompt and say that it is preferred if ""like""d and it is not preferred if it is ""disliked"". Is there a better way?",khidot,1h1bpwq,https://reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,2024-11-27 18:50:05,8,0.78,8,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h1bpwq
MachineLearning,[D] Which LLM models can I run on an NVIDIA 4060 for research purposes? Recommendations needed!,"Hi everyone,

I‚Äôm diving into research on large language models (LLMs) and looking to experiment with running them locally on my NVIDIA 4060 GPU. While I know the 4060 isn‚Äôt a high-end card compared to some research setups, I‚Äôm optimistic about making the most out of what it offers. I‚Äôd greatly appreciate any insights or recommendations on:

1. **Models that can run efficiently** on a 4060. I‚Äôm aware that some smaller versions of LLMs might be more suited for this hardware, so any advice on what‚Äôs realistically possible without excessive optimization would be fantastic.
2. **Models suitable for fine-tuning or pre-training experiments.** Although I‚Äôm starting with basic experiments, I plan to explore fine-tuning in the future, so I‚Äôd love suggestions for models that are versatile and widely used in research.
3. **Open-source models** or ones that are easy to access and work with for research purposes. Licensing and transparency are important to me, as my work is focused on academic and experimental objectives.

So far, I‚Äôve been looking at options like LLaMA, GPT-NeoX, and BLOOM, particularly their smaller variants, but I‚Äôm open to exploring other possibilities. If you‚Äôve had experience running these or similar models on mid-range GPUs, I‚Äôd love to hear your thoughts on performance, setup, or any potential limitations I should be aware of.

Additionally, I‚Äôd be grateful for any advice on:

* **Optimizing models for a 4060.** Are there specific tools, techniques, or libraries (like bitsandbytes or FlashAttention) that could help with running or fine-tuning these models?
* **Preparing for fine-tuning.** What should I keep in mind when selecting a model to ensure it can support future fine-tuning experiments effectively?

Thank you in advance for sharing your expertise! I‚Äôm eager to learn from the community and make the most of this setup.",Spinotesla,1h1ux8m,https://reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,https://www.reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,2024-11-28 12:30:28,0,0.31,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h1ux8m
MachineLearning,[D] AISTATS 2025 reviews,Aistats 2025 reviews are supposed to be out today. So I thought to create a discussion post for the same where we can share our experiences!,PhoneImpressive9983,1h0x428,https://reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,2024-11-27 05:31:15,45,0.93,45,0,130,0,0,False,False,True,False,False,Discussion,self,t3_1h0x428
MachineLearning,[D] How valid is the evaluation using LLMs?,"Hello community,

I am bit new to using Gen AI, I want to check the validity of using larger LLMs to evaluate the result of other LLMs. I have seen different blogs who does this for the purpose of automating the evaluations.

For eg. To evaluate a list of English translations my a model A, is it valid to prompt another model B, something like this '''Is this translation correct original text: {original_text}, Translated text {translated_text}'''

Is this a valid way of evaluating? Something inside me says it's scientifically wrong, because the LLM model B itself will have some error to it right?",raman_boom,1h11lbt,https://reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,2024-11-27 10:48:09,15,0.78,15,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1h11lbt
MachineLearning,Residuals in ensemble MLR [D],"
Hi all

New to ensembles.

If you ensemble MLR, you may end up with a non-linear equation however‚Ä¶.

A) the residuals of the indicidual MLR that were ensembled need to meet parametric assumptions? Can‚Äôt use a crap MLR just because it‚Äôs going to be used in an ensemble?
B) if the ensembled MLR equation is linear then residuals should meet parametric assumptions?

Thanks


",Yellow_fruit_2104,1h1ero2,https://reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,https://www.reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,2024-11-27 20:58:23,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h1ero2
MachineLearning,[R] Meissonic: High-Resolution Text-to-Image Generation via Enhanced Masked Image Modeling,"This work introduces a non-autoregressive masked image modeling (MIM) approach that aims to match SDXL-level image generation while avoiding the token inefficiencies of autoregressive methods. The key innovation is combining MIM with architectural improvements and sampling optimizations to enable high-resolution image synthesis.

Main technical points:
- Uses a transformer-based architecture with specialized self-attention and positional encoding
- Incorporates human preference scores as ""micro-conditions"" to guide generation
- Employs feature compression layers to handle high resolutions efficiently
- Generates 1024x1024 images through parallel token prediction rather than sequential
- Achieves comparable FID scores to SDXL while being more computationally efficient

Results:
- Image quality metrics competitive with SDXL on standard benchmarks
- Faster generation compared to autoregressive approaches
- Better handling of complex scenes and compositions
- Improved text alignment compared to previous MIM approaches

I think this could impact the field in several ways:
- Shows that non-diffusion approaches can achieve SOTA-level generation
- Provides a potential path toward unified language-vision models
- May lead to more efficient deployment of text-to-image systems
- Could influence architecture design for future multimodal models

The biggest open question in my view is whether this approach can scale further - while it works well at current resolutions, it's unclear if the same principles will hold at even higher dimensions.

TLDR: Non-autoregressive masked modeling approach matches SDXL-level image generation while being more efficient than typical autoregressive methods. Shows promise for unified language-vision architectures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/meissonic-revitalizing-masked-generative-transformers-efficient-high). Paper [here](https://arxiv.org/abs/2410.08261).",Successful-Western27,1h1529m,https://reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,https://www.reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,2024-11-27 14:05:29,7,0.89,7,0,2,0,0,False,False,True,False,False,Research,self,t3_1h1529m
MachineLearning,[D] Cross Entropy Loss sucks,"Hi guys, Am I the only one thinking that training a LLM to minimize CE Loss on a certain text dataset is a very surprising idea?

I understand that it works but I am surprised it is still SOTA. The current sentence could have begun with a lot of different tokens with no consequence on its meaning, while some words are uninterchangeable. Yet CE loss doesn't account for that. Worse off, the bigger the ""equivalence class"" (the number of tokens that could replace one in a sentence without altering its meaning) of a token in a sentence, the higher the average loss on it. It seems counterproductive, isn't it?

I would love to read some contradiction.",Due-Pangolin325,1h1sqkl,https://reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,https://www.reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,2024-11-28 10:02:18,0,0.29,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h1sqkl
MachineLearning,[R] Black holes and the loss landscape in machine learning,"Abstract:

&gt;Understanding the loss landscape is an important problem in machine learning. One key feature of the loss function, common to many neural network architectures, is the presence of exponentially many low lying local minima. Physical systems with similar energy landscapes may provide useful insights. In this work, we point out that black holes naturally give rise to such landscapes, owing to the existence of black hole entropy. For definiteness, we consider 1/8 BPS black holes in¬†Óà∫=8¬†string theory. These provide an infinite family of potential landscapes arising in the microscopic descriptions of corresponding black holes. The counting of minima amounts to black hole microstate counting. Moreover, the exact numbers of the minima for these landscapes are a priori known from dualities in string theory. Some of the minima are connected by paths of low loss values, resembling mode connectivity. We estimate the number of runs needed to find all the solutions. Initial explorations suggest that Stochastic Gradient Descent can find a significant fraction of the minima.

Arxiv:¬†[https://arxiv.org/abs/2306.14817](https://arxiv.org/abs/2306.14817)",Mindless-House-8783,1h0uwjd,https://reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,https://www.reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,2024-11-27 03:26:49,27,0.78,27,0,28,0,0,False,False,True,False,False,Research,self,t3_1h0uwjd
MachineLearning,[D] AISTATS 2025 Paper Reviews,"Since the AISTATS 2025 paper reviews are due today, I thought to open up a thread where everyone can discuss their experiences!
",PhoneImpressive9983,1h0y8rn,https://reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,2024-11-27 06:42:52,8,0.73,8,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h0y8rn
