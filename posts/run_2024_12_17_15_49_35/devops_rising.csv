subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
devops,I failed the last round of a Platform Engineer job interview because of a database migration question. How bad was my answer?,"For clarity, I am a Full-Stack developer, but these days my work is 80% front-end.

Anything I know about DevOps and Cloud-related stuff has been done in my own time by studying and applying it to my side projects. I'm trying to break into a cloud-focused role because it has interested me for a while now.

This round was a whiteboarding session where I was tasked with migrating a legacy system to a cloud provider. We went with GCP for service choice names, as we both have worked this provider.

One of the components of the legacy system was the database. The interviewer said let's assume it's an SQL-based database and it's running on a VM somewhere.

**By this point in the interview, I had asked questions and got the following points:**

\- Developers are manually having to SSH into VM's to make changes and they are struggling to manage.  
\- The company expects to grow within 12 months, up to 100k DAU. They currently have 10K DAU.

**Once we get into the database section, it kind of went like this:**

&gt;**Me:**  
For the database, I would recommend a managed SQL service, Cloud SQL.  
The advantage of this choice is that it takes a big amount of cognitive load from the developers. DB Admin tasks like backups can be done much easier. It is also more suitable to scale than self-managing on a VM.  
The disadvantage of this choice is the cost, it'll easily be one of the most expensive components of this architecture. But I feel like the trade-off is worth it, as managing a database for this scenario can be a full-time job in itself.

&gt;**Interviewer:**  
Ok great, that trade-off makes sense. Let's talk about the actual migration of the data from the current system to GCP. We want minimal downtime, we can maybe afford a maximum of an hour of downtime.

&gt;**Me:**  
Ok, what is the rough amount of data we need to transfer? I'm guessing it's not Petabytes of data?

&gt;**Interviewer:**  
Correct, it's not that level of data, but equally, it's not so little that we can just do a quick and simple export/import

&gt;**Me:**  
Ok, I know that cloud providers do have database migration services available. These aim to do a lot of the work for you. However, I am aware that they only fit very simple scenarios. You have to have a supported db version and a relatively simple schema, so let's assume these services do not fit the bill.

&gt;In that case, I would replicate the legacy database to the cloud-based one. Once replication is done, we can ensure new records are added by adding a dual write strategy at the application level, so both databases are fully up to date. Then, after communicating with users about the expected hour downtime, ideally during a low-traffic period, we can mark the old database as read-only and switch over to the new instance.

&gt;The upside of this approach is that we can get downtime down to &lt; hour.

&gt;The downside is that we need to write application-level code purely for this migration, which increases complexity and chances of mistakes, so it's something the developers will need to keep in mind.

And to be honest, the interviewer just OK'd that and we moved on to another question.

However, in the feedback, one of the main points was that I need to expand my knowledge on database migration strategies.

Was my answer straight-up wrong? Or did I fail to expand on the solution more?

**EDIT:**

For those wondering if this was the sole reason I was rejected, probably not. There was another question (network-related) that I flat-out got wrong. Just had a blank moment during the interview.

So the DB migration question I didn't do too great on + plus the network question + my competition being better than me on the day probably led to the rejection!",Squishyboots1996,1hfgw4b,https://reddit.com/r/devops/comments/1hfgw4b/i_failed_the_last_round_of_a_platform_engineer/,https://www.reddit.com/r/devops/comments/1hfgw4b/i_failed_the_last_round_of_a_platform_engineer/,2024-12-16 11:16:10,391,0.96,391,0,150,0,0,False,False,True,False,False,,self,t3_1hfgw4b
devops,Blameless culture?,"Does it really exist?
What does it look like in reality?",Skill-Additional,1hg65pv,https://reddit.com/r/devops/comments/1hg65pv/blameless_culture/,https://www.reddit.com/r/devops/comments/1hg65pv/blameless_culture/,2024-12-17 08:20:29,9,0.8,9,0,27,0,0,False,False,True,False,False,,self,t3_1hg65pv
devops,How would you manage/automate this mess?,"I believe we use almost every possible way to host SQL Server: on AWS EC2 (as an FCI cluster), on AWS EC2 (as an AG cluster), on Multi-AZ RDS SQL Server, on Single-AZ RDS SQL Server, and we‚Äôre currently in the middle of a POC for RDS Custom for SQL Server.

Our application follows a single-tenant database architecture, which means we host around 2000 databases‚Äîa number that is always changing:

* We onboard new customers, which means creating new databases.
* Customers terminate their subscriptions, which means we drop their database(s).
* We receive a backup file from the implementation/support team or the customer directly to create a new database (yes, each customer can have anywhere from **1 to 20+ DBs**) or override an existing database with their backup file.

At this point, even a simple restore operation is different across all our servers. For example, to override a database:

* **On an AG cluster**: Remove the database from AG first, put the database in single-user mode, restore it, switch back to multi-user mode, and then add it back to the AG.
* **On an FCI cluster**: Follow the same steps as AG, except for removing/adding it from/to AG.
* **On Single-AZ RDS**: Use a stored procedure. If you‚Äôre overriding an existing database, you can add logic to check if the database exists, drop it, and then restore using the stored procedure.
* **On Multi-AZ RDS**: Use the same stored procedure, but if you‚Äôre overriding an existing database, you need to drop it using different SQL commands (because it‚Äôs Multi-AZ) and then run the stored procedure for the restore operation.

I‚Äôm not even diving into other tasks, like managing users/logins (each customer has their own login, used by our applications), read-only users, or performing some other updates/changes across every single database.

So, how would you manage this mess?  
Does anyone know of any third-party automation tools that could help?",victor_yanukovich,1hg28ff,https://reddit.com/r/devops/comments/1hg28ff/how_would_you_manageautomate_this_mess/,https://www.reddit.com/r/devops/comments/1hg28ff/how_would_you_manageautomate_this_mess/,2024-12-17 04:02:11,14,0.89,14,0,16,0,0,False,False,True,False,False,,self,t3_1hg28ff
devops,CICD pipeline uses helm to deploy apps without packaging them in a OCI registry,"Am I the worst person alive? I have implemented helm deployment through our CICD pipeline, the build phase of the pipeline pulls the helm charts from the app's repository then makes an artifact from it. When its time to deploy the pipeline simply runs an helm upgrade command using the artifacted helm chart. I have seen a lot of oeople mentionning they instead package their helm charts and push them to an OCI registry which is fine but in our case I feel like its an additional step and aditional dependency (the OCI registry) which we don't really need.   Any thoughts?",Vonderchicken,1hfueg4,https://reddit.com/r/devops/comments/1hfueg4/cicd_pipeline_uses_helm_to_deploy_apps_without/,https://www.reddit.com/r/devops/comments/1hfueg4/cicd_pipeline_uses_helm_to_deploy_apps_without/,2024-12-16 21:41:58,30,0.86,30,0,24,0,0,False,False,True,False,False,,self,t3_1hfueg4
devops,Did 'vi' win the editor war?,I was just thinking that I haven't heard anyone talk about emacs for a few years. But all the new people I meet who need an editor that's present on 'any' system are using 'vi'. I think one of them had never even heard of emacs.,jacksbox,1hflvt6,https://reddit.com/r/devops/comments/1hflvt6/did_vi_win_the_editor_war/,https://www.reddit.com/r/devops/comments/1hflvt6/did_vi_win_the_editor_war/,2024-12-16 15:41:33,81,0.81,81,0,153,0,0,False,False,True,False,False,,self,t3_1hflvt6
devops,Incident Response Metrics,"When I started driving incident response processes, I spent a ton of time building custom reporting and massaging data to report out and show that our incidents were becoming less impactful, resolved faster, or trending in the right direction. Eventually, I built an internal self-service ops platform to streamline it all. The incidents of year 1 differed from those we saw 3 years later, increasing in complexity.

Given everyone's experience, I‚Äôm curious about the metrics you track on the operations side. I assume MTTR is one of them, but do you also monitor MTTD (detect)‚Äîthe time between when an impactful change was executed (the actual start of the incident) and when it was declared? Is anyone creating SLOs and properly rolling the SLIs up to them?

Do you look at DORA metrics, such as change failure rate, specifically changes that lead to incidents? Do you track anything related to team incident readiness?

I‚Äôd love to hear which metrics you find most valuable and if there are others you wish you had.

I'm always looking to learn from folks who‚Äôve been through it.",socalbigpapi,1hg369y,https://reddit.com/r/devops/comments/1hg369y/incident_response_metrics/,https://www.reddit.com/r/devops/comments/1hg369y/incident_response_metrics/,2024-12-17 04:57:14,5,0.86,5,0,4,0,0,False,False,True,False,False,,self,t3_1hg369y
devops,Migrating from Elasticsearch to Opensearch,"Hello all

I am working on a project to migrate some elasticsearch clusters to opensearch. I would like to know if anyone here has done this and can recommend strategies to do this in production for a customer facing application. So far I've evaluated two options, option 1 would require a ""read-only"" downtime where any writes to the cluster would error out. This would give time to migrate all the relevant indexes using the \`reindex\` operation and to the code to point to the OS cluster instead of the ES one. Option 2 is more involved, it would require updating the application code to dual-write for a period of time, then doing reindexing of all the data prior to when the dual-write was enabled, and when we have confidence, doing a full cut over. Option 2 has a lot of issues in that the code changes are more complex and I don't yet know how to deal with data divergence, e.g. if a write fails in OS but succeeds in ES. However, I am getting word that a 0 downtime approach is strongly preferred.

Any advice would be great!

edit: ES 7.10, OS 2.15

edit 2: for folks who have done or evaluated a no-downtime approach, I would love to hear more about this!",oneradsn,1hfmzdu,https://reddit.com/r/devops/comments/1hfmzdu/migrating_from_elasticsearch_to_opensearch/,https://www.reddit.com/r/devops/comments/1hfmzdu/migrating_from_elasticsearch_to_opensearch/,2024-12-16 16:29:33,15,0.9,15,0,17,0,0,False,False,True,False,False,,self,t3_1hfmzdu
devops,To version control or not to version control ,"I was assigned as a project manager to a team that has to migrate an mssql database to a Salesforce instance. 

The project used to be in the hands of one developer. I used to ask if everything was progressing fine. For a couple of months he assured me it did. When we actually had to see results it didn't work out.

Therefore higher management decided to add two developers to help him (and check on the work). I supported that.

In my company they usually don't expect project managers to have a lot of technical knowledge. But I do have some development experience and I wanted to really understand what they were doing, as this migration was by this point a huge mess and I could never challenge them when they said something.

The first thing I noticed was that they weren't using version control for their migration scripts. They use a combination of tsql scripts, ""SQL database procedures"" and ""Talend jobs"". 

So my first reaction was: we have to put things in version control because our scripts are all over the place and we can't track changes.

However they claimed that version control is not possible for the procedures and the Talend jobs.

Based on a couple of Google searches I realized that isn't entirely true. So when I told them that, they told me (quite annoyed) that it also wouldn't be practical as they have to put the scripts on a secured server and can't easily transfer them from their local computer to the secured server.

My reaction was that we then should find a way to fix that but the team unanimously concluded that version control really wasn't the biggest priority right now. We should urgently fix the bugs in the scripts and do this 'one time' migration. Although it isn't really one time as we have to do it for four offices.

I was really confident that I was right at the start, but I'm starting to hesitate as I feel that the whole team doesn't agree. What's your opinion on this? Or do you need extra info to answer the question?",uh_sorry_i_dont_know,1hfvflj,https://reddit.com/r/devops/comments/1hfvflj/to_version_control_or_not_to_version_control/,https://www.reddit.com/r/devops/comments/1hfvflj/to_version_control_or_not_to_version_control/,2024-12-16 22:25:49,4,0.63,4,0,31,0,0,False,False,True,False,False,,self,t3_1hfvflj
devops,üöÄ Data-Driven Feedback Loops: How DevOps and Data Science Inform Product Iterations,"In today‚Äôs fast-paced digital landscape, continuous product iteration is key to staying competitive. By combining DevOps and data science, businesses can create data-driven feedback loops that accelerate product development, improve decision-making, and ensure that every update is grounded in real-world insights.

Here‚Äôs how this powerful combination works:

‚úÖ Real-Time Data Collection &amp; Analysis: Automating data collection from user interactions, system performance, and customer feedback unlocks deeper insights and enables quicker action.

‚úÖ DevOps-Driven Speed: By automating deployment, monitoring, and incident response, DevOps ensures faster, more efficient iterations. Each release is optimized for performance and user experience when combined with data science.

‚úÖ Case Study ‚Äì Netflix: Netflix is a prime example of this approach, continuously using real-time user behaviour data to refine its recommendation algorithms. They also leverage A/B testing to assess and improve features before scaling them.



Looking ahead, AI and machine learning will supercharge these feedback loops, driving hyper-personalized experiences and real-time product iterations. As we embrace an increasingly automated and intelligent future, leveraging these technologies will be crucial to developing products that truly resonate with users.

Let‚Äôs harness the power of data and automation to keep innovating and improving!",iamaproudnomad,1hgboxz,https://reddit.com/r/devops/comments/1hgboxz/datadriven_feedback_loops_how_devops_and_data/,https://www.reddit.com/r/devops/comments/1hgboxz/datadriven_feedback_loops_how_devops_and_data/,2024-12-17 14:23:13,0,0.29,0,0,2,0,0,False,False,True,False,False,,self,t3_1hgboxz
devops,üöÄ Accelerating Business Growth Using AIOps and DevOps,"As organizations increasingly rely on the digital economy, efficiency and speed are essential for driving business growth. Two modern strategies that are pivotal in operational effectiveness are AIOps and DevOps.

Here‚Äôs how they complement each other:

üí° AIOps combines AI, machine learning, and big data to optimize IT operations, providing real-time insights, predictive alerts, and automated remediation to reduce downtime and enhance security.

‚öôÔ∏è DevOps bridges development and operations, streamlining product development cycles through automation, CI/CD pipelines, and continuous integration.

[https://www.linkedin.com/feed/update/urn:li:activity:7274474536102760449](https://www.linkedin.com/feed/update/urn:li:activity:7274474536102760449)

",iamaproudnomad,1hgbn14,https://reddit.com/r/devops/comments/1hgbn14/accelerating_business_growth_using_aiops_and/,https://www.reddit.com/r/devops/comments/1hgbn14/accelerating_business_growth_using_aiops_and/,2024-12-17 14:20:38,0,0.2,0,0,0,0,0,False,False,True,False,False,,self,t3_1hgbn14
devops,CICD pipeline uses helm to deploy apps without packaging them in a OCI registry,"Am I the worst person alive? I have implemented helm deployment through our CICD pipeline, the build phase of the pipeline pulls the helm charts from the app's repository then makes an artifact from it. When its time to deploy the pipeline simply runs an helm upgrade command using the artifacted helm chart. I have seen a lot of oeople mentionning they instead package their helm charts and push them to an OCI registry which is fine but in our case I feel like its an additional step and aditional dependency (the OCI registry) which we don't really need.   Any thoughts?",Vonderchicken,1hfueb7,https://reddit.com/r/devops/comments/1hfueb7/cicd_pipeline_uses_helm_to_deploy_apps_without/,https://www.reddit.com/r/devops/comments/1hfueb7/cicd_pipeline_uses_helm_to_deploy_apps_without/,2024-12-16 21:41:50,5,0.73,5,0,4,0,0,False,False,True,False,False,,self,t3_1hfueb7
devops,We Don't Need Perfect Code,[https://www.youtube.com/watch?v=al8KN5incV8](https://www.youtube.com/watch?v=al8KN5incV8),EyedApproximation,1hg82j2,https://reddit.com/r/devops/comments/1hg82j2/we_dont_need_perfect_code/,https://www.reddit.com/r/devops/comments/1hg82j2/we_dont_need_perfect_code/,2024-12-17 10:48:44,0,0.18,0,0,0,0,0,False,False,True,False,False,,self,t3_1hg82j2
devops,Where to store JWTs,"I was curious about where to store JWTs and found this video that explains it pretty clearly. It‚Äôs a quick watch and gave me a better understanding of the best practices for handling tokens securely. If you're wondering the same, it might be worth checking out! [https://youtu.be/c7vsTXuwm8A](https://youtu.be/c7vsTXuwm8A)",Frosty-Champion7811,1hga7gw,https://reddit.com/r/devops/comments/1hga7gw/where_to_store_jwts/,https://www.reddit.com/r/devops/comments/1hga7gw/where_to_store_jwts/,2024-12-17 13:06:37,0,0.21,0,0,5,0,0,False,False,True,False,False,,self,t3_1hga7gw
devops,"Software Engineer Jobs Report 12/11: 1200 new jobs. Every week I scrape the internet for recently posted software engineer jobs. I hand pick the best ones, put them in a list, and share them to help your job search. Here is last weeks spreadsheet. DevOps roles included.","Hey friends, every week I search the internet for software engineer jobs that have been recently posted on a company's career page. I collect the jobs, put them in a spreadsheet, and share them with anyone whose looking for their next role. All for free.

The data is sourced by my own web scraping bots, paid sources, free sources, VC sites, and the typical job board sites. I spend an ungodly amount on the web so you don't have too!

About me, I am a senior software engineer with a decade of work history, and ample job searching experience to know that its a long game and its a numbers game.

If there are other roles you'd like to see, let me know in the comments.

To get the nicely formatted spreadsheet, click¬†[here](https://airtable.com/appfZGu8o8kuDud9l/shryoIjUWvnc3Kr8T).

If you want to read my write up, click¬†[here](https://24hsoftwarejobs.beehiiv.com/p/jobs-report-12-11-24?utm_source=reddit&amp;utm_medium=r-devops-jobs-report-12-11-24).

if you want to get these in an email, click¬†[here](https://24hsoftwarejobs.beehiiv.com/subscribe?utm_source=reddit&amp;utm_medium=r-devops-jobs-report-12-11-24).

If you want to see all previous job reports, click¬†[here](https://24hsoftwarejobs.beehiiv.com/?utm_source=reddit&amp;utm_medium=r-devops-jobs-report-12-11-24).

Cheers!",innovatekit,1hey8g6,https://reddit.com/r/devops/comments/1hey8g6/software_engineer_jobs_report_1211_1200_new_jobs/,https://www.reddit.com/r/devops/comments/1hey8g6/software_engineer_jobs_report_1211_1200_new_jobs/,2024-12-15 18:00:07,84,0.91,84,0,4,0,0,False,False,True,False,False,,self,t3_1hey8g6
devops,The ways to make portfolio/practice different DevOps tools.,"Hi, I have completed couple of courses on different DevOps tools and now I want to practice working with those tools. The thing is I don't know how. 

It might sound stupid but when I was learning something like Python I just came up with some kind of program I want to make and started coding. With Terraform or Docker I find this approach not working as well and when it comes to practicing pipelines or Kubernetes I spend a lot of time on finding app I want to use to learn and not on the tools that matter. 

The same goes with me wanting to build a portfolio to have something to show during job searching. If I want to show what I know Python I just make an app and put it on GitHub but when it comes to DevOps I don't know what should I put there. 

So my question is how to practice the usage of those DevOps tools? Is making my own projects similar to programming ones the way or there is more efficient way? Should I just use some already make apps to learn and just focus on the DevOps stuff? ",Crezusik,1hf47wl,https://reddit.com/r/devops/comments/1hf47wl/the_ways_to_make_portfoliopractice_different/,https://www.reddit.com/r/devops/comments/1hf47wl/the_ways_to_make_portfoliopractice_different/,2024-12-15 22:30:19,30,0.92,30,0,11,0,0,False,False,True,False,False,,self,t3_1hf47wl
devops,In your opinion what's the best way to insert values for the app? ,"Most shop i know uses argo helm plugin.  With values.yaml file in the app repo where the argo will read the app and deploy it. The secrets will be managed either rblindfold/sops/secrets-injector sidecar. 

  
What's your preferred method? ",midlevelmybutt,1hfcaay,https://reddit.com/r/devops/comments/1hfcaay/in_your_opinion_whats_the_best_way_to_insert/,https://www.reddit.com/r/devops/comments/1hfcaay/in_your_opinion_whats_the_best_way_to_insert/,2024-12-16 05:36:32,6,1.0,6,0,4,0,0,False,False,True,False,False,,self,t3_1hfcaay
