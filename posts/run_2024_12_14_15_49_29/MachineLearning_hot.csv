subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1h99kae,https://reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/,2024-12-08 03:15:09,6,0.71,6,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h99kae
MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

&gt;Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

&gt;Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&amp;#x200B;

Please remember that this community is geared towards those with experience.",AutoModerator,1h3u444,https://reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,2024-12-01 03:30:15,35,0.91,35,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h3u444
MachineLearning,[D] What happened at NeurIPS?,,howtorewriteaname,1hdxbru,https://reddit.com/r/MachineLearning/comments/1hdxbru/d_what_happened_at_neurips/,https://i.redd.it/k0q9frsuir6e1.jpeg,2024-12-14 07:00:07,278,0.93,278,0,224,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/-xSH1Mpa62Tf46vUqQSb2RCTHhGdGGFO4WLolSj0-WI.jpg,t3_1hdxbru
MachineLearning,[D] What are the (un)written rules of deep learning training ,"Disclaimer: I posted this in r/learnmachinelearing first, but the sub seems to be more concerned with very basic questions, courses and hiring, so feel free to remove it if it doesn't fit here (tho I think that also fits this sub as a discussion).

I now have a few years of experience building and training different model architectures, I know most of the basic theory and am able to follow most papers. So my question goes into a more methodological direction. While I am able to successfully build models for a number of applications, a lot of the time this is to a large extend guesswork. I try out different stuff and see what sticks. I know there is a lot of research in the direction of interpretability going on, but this is not directly the direction I want to go with this. Instead I want to ask you all what general advice you have on the training process, what are some practical observations, rules of thumb, approaches you take that are not described in a paper or theoretical ml class. For example:

- How do you analyze gradients in your model. I know how to do some very basic plots in this regard, but would be interested in your methods and how you read them from a practical perspective?

- How do you visualize temporal instabilities between optimizer steps resulting from e.g. a too large learning rate?

- How do you determine appropriate regularization?

- What are your rules of thumb for diminisheing returns during a training run?

- How do you tune your hyperparameters? I eyeballed them more or less and also used optuna for this in the past.

- What are some important intuitions, unwritten rules and pitfalls during training in your opinion?

- What are your debugging steps when a model does not perform as expected?

- What tricks do you actually use? There are lots of small tricks (EMA, obscure activation functions, ...) that promise some gains, but what do you actually use?

- How does your approach differ when you do a transformer, CNN, diffusion model, ...

- Some general opinions or tips that I might have missed above. 

University classes and online resources mostly teach the basics or theoretical foundation, which is very important, but in practice only part of the story. Real world experience also helps, but you only get so far with trial and error and might miss something useful.
I am aware of the blog posts by Karpathy on the training of neural networks and look for more resources in this direction.

I am happy to here your replies on this arguably broad topic. ",floriv1999,1he07vr,https://reddit.com/r/MachineLearning/comments/1he07vr/d_what_are_the_unwritten_rules_of_deep_learning/,https://www.reddit.com/r/MachineLearning/comments/1he07vr/d_what_are_the_unwritten_rules_of_deep_learning/,2024-12-14 10:29:02,52,0.88,52,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1he07vr
MachineLearning,[P] Curated list of LLM papers 2024,,seraschka,1he4htl,https://reddit.com/r/MachineLearning/comments/1he4htl/p_curated_list_of_llm_papers_2024/,https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list,2024-12-14 14:52:26,10,0.81,10,0,5,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/p02HYYoPj_p-xBwJelLubWz-Tz9jT7FTYlhXB71RrBo.jpg,t3_1he4htl
MachineLearning,"[D] NVIDIA’s hostages: A Cyberpunk Reality of Monopolies
","In AI and professional workstations, NVIDIA's dominance feels like a suffocating monopoly. Their segmented product lines widen the gap between consumer and professional GPUs, particularly in VRAM, performance, and price.

AI enthusiasts struggle with prohibitive costs for GPUs equipped with sufficient VRAM. The reliance on CUDA cores—a proprietary standard—further locks developers into NVIDIA’s ecosystem, stifling competition and innovation.

NVIDIA’s control extends beyond hardware, as their CUDA platform discourages adoption of open, competitive solutions. This feeds a cyberpunk dystopia where corporations consolidate power, leaving consumers and developers with few choices.

Why does the tech world remain complicit? Why aren’t we pursuing alternative hardware architectures or broader software compatibility beyond CUDA? AMD’s ROCm is a start, but more aggressive development and policy interventions are needed to challenge NVIDIA’s grip.

Until when will this continue? Who will stand up for the end consumer?",SevenShivas,1hdjklf,https://reddit.com/r/MachineLearning/comments/1hdjklf/d_nvidias_hostages_a_cyberpunk_reality_of/,https://www.reddit.com/r/MachineLearning/comments/1hdjklf/d_nvidias_hostages_a_cyberpunk_reality_of/,2024-12-13 19:02:54,30,0.6,30,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1hdjklf
MachineLearning,Curated Corpus for U.S. Health Insurance Applications [P] [R],,tpafs,1he14fo,https://reddit.com/r/MachineLearning/comments/1he14fo/curated_corpus_for_us_health_insurance/,https://github.com/TPAFS/hicric,2024-12-14 11:36:37,0,0.33,0,0,0,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/neXQ6PQq1HS-6BASsO31cD98vX-av52hINTe9GUR5kI.jpg,t3_1he14fo
MachineLearning,[D]Dataset for my research paper,"Are therw any datasets which contains images both generated by models like stability,midjourney,runway and real images and need data of noise for both of them",eulasimp12,1he141v,https://reddit.com/r/MachineLearning/comments/1he141v/ddataset_for_my_research_paper/,https://www.reddit.com/r/MachineLearning/comments/1he141v/ddataset_for_my_research_paper/,2024-12-14 11:35:51,0,0.44,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1he141v
MachineLearning,[D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams,"Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his colleagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.github.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is asserted and if this is true this would be nice to confirm.",LelouchZer12,1hctf36,https://reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/,https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/,2024-12-12 19:41:41,619,0.97,619,0,79,0,0,False,False,True,False,False,Discussion,self,t3_1hctf36
MachineLearning,[R] Identifying Critical Decision Points in Neural Text Generation Through Token-Level Uncertainty Analysis,"This paper introduces a framework for analyzing and visualizing the branching decisions language models make during text generation. The key methodology involves tracking probability distributions across different sampling paths to understand how early choices affect downstream generation.

Main technical points:
- Developed metrics to quantify uncertainty at each generation step
- Created visualization tools for mapping decision trees in generation
- Analyzed how different sampling methods affect path divergence
- Measured correlation between model confidence and generation quality
- Identified clustering patterns in generation trajectories

Key results:
- Found that paths tend to cluster into 2-3 distinct trajectory groups
- Early sampling decisions have outsized impact on final outputs
- Uncertainty patterns vary significantly between sampling methods
- Similar prompts can lead to dramatically different generation paths
- Model confidence doesn't consistently predict output quality

I think this work provides important insights into how we might better control text generation. The ability to map and understand generation paths could help develop more reliable sampling methods and better uncertainty estimates.

I think the clustering of generation paths is particularly interesting - it suggests there may be ways to guide generation toward desired trajectory groups. This could be valuable for applications needing more predictable outputs.

The methodology also reveals some concerning aspects about current sampling methods. The strong dependence on early decisions suggests we may need new approaches that better preserve generation flexibility throughout the sequence.

TLDR: New framework for analyzing how language models make text generation choices. Shows that generation paths cluster into distinct groups and early decisions heavily influence outcomes. Could help develop better sampling methods and uncertainty estimates.

[Full summary is here](https://aimodels.fyi/papers/arxiv/forking-paths-neural-text-generation). Paper [here](https://arxiv.org/abs/2412.07961).",Successful-Western27,1hdd0kk,https://reddit.com/r/MachineLearning/comments/1hdd0kk/r_identifying_critical_decision_points_in_neural/,https://www.reddit.com/r/MachineLearning/comments/1hdd0kk/r_identifying_critical_decision_points_in_neural/,2024-12-13 14:10:21,8,0.76,8,0,1,0,0,False,False,True,False,False,Research,self,t3_1hdd0kk
MachineLearning,[D] Training with synthetic data and model collapse. Is there progress?,"About a year ago, research papers talked about model collapse when dealing with synthetic data. Recently I’ve been hearing about some progress in this regard. I am not expert and would welcome your views on what’s going on. Thank you and have a fantastic day.",BubblyOption7980,1hd92mt,https://reddit.com/r/MachineLearning/comments/1hd92mt/d_training_with_synthetic_data_and_model_collapse/,https://www.reddit.com/r/MachineLearning/comments/1hd92mt/d_training_with_synthetic_data_and_model_collapse/,2024-12-13 10:03:43,14,0.67,14,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1hd92mt
MachineLearning,[D] Importance of HPO per field / model type / applications,"I’ve noticed that the time spent on hyperparameter optimization vary significantly, not just between industry and academia but also across different fields like NLP, computer vision, or reinforcement learning. I’m curious—what’s your experience?

* Is tuning something you prioritize heavily, or do you often settle for “good enough” configurations to move faster?
* What field / model type / applications do you think experience most(or least) bottleneck in workflow due to HPO?
* Are there any industry dependency around choosing HPO tools? For example, everyone in xx industry would pick Optuna as a go-to or everyone running xx experiments would use Sigopt.

Would love to hear your experiences! Thanks",Maleficent_Ad5541,1hd6pjv,https://reddit.com/r/MachineLearning/comments/1hd6pjv/d_importance_of_hpo_per_field_model_type/,https://www.reddit.com/r/MachineLearning/comments/1hd6pjv/d_importance_of_hpo_per_field_model_type/,2024-12-13 06:58:12,16,0.9,16,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1hd6pjv
MachineLearning,[D] Help with clustering over time,"I'm dealing with a clustering over time issue.
Our company is a sort of PayPal. We are trying to implement an antifraud process to trigger alerts when a client makes excessive payments compared to its historical behavior.
To do so, I've come up with seven clustering features which are all 365-day-long moving averages of different KPIs (payment frequency, payment amount, etc.). So it goes without saying that, from one day to another, these indicators evolve very slowly. I have about 15k clients, several years of data.
I get rid of outliers (99-percentile of each date, basically) and put them in a cluster-0 by default.
Then, the idea is, for each date, to come up with 8 clusters. I've used a Gaussian Mixture clustering (GMM) but, weirdly enough, the clusters of my clients vary wildly from one day to another.
I have tried to plant the previous mean of my centroids, using the previous day centroid of a client to sort of seed the next day's clustering of a client, but the results still vary a lot. I've read a bit about DynamicC and it seemed like the way to address the issue, but it doesn't help.",LaBaguette-FR,1hdk4ma,https://reddit.com/r/MachineLearning/comments/1hdk4ma/d_help_with_clustering_over_time/,https://www.reddit.com/r/MachineLearning/comments/1hdk4ma/d_help_with_clustering_over_time/,2024-12-13 19:27:38,1,0.56,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hdk4ma
MachineLearning,[D] What makes TikTok's recommendation algorithm so strong?,"General Discussion - now that they are about to be banned in the US, I'm becoming fascinated by the strength of their For You recommendations. To try and put some guard rails on what I mean, TikTok has shown itself to be able to match content to relevant audience at greater frequency and scale than any other app (YouTube included). Many creators can join the platform, post a single video, and have millions of views in 24 hours. This does happen on other apps, but TikTok seems to be the most consistent at scaling audience incredibly fast.

What models might they be basing their system on? What about their models creates their competitive advantage?",No_Collection_5509,1hcp4xw,https://reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/,https://www.reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/,2024-12-12 16:39:06,127,0.9,127,0,34,0,0,False,False,True,False,False,Discussion,self,t3_1hcp4xw
MachineLearning,[R] survey on students’ motivation to learn Artificial Intelligence and Modeling.,"We are university students and we're conducting a quick survey on students’ motivation to learn Artificial Intelligence and Modeling.
The  survey will take less than 10 minutes to complete.

Here's the link to the survey: 
https://docs.google.com/forms/d/e/1FAIpQLSdS-xy53N9lDRlC_835A_E59VMjCPql0_HuihPYqaQ_nINSsw/viewform?usp=sf_link


Your input would mean a lot to us! 
Thank you so much for your support and time.
",ExamSensitive3076,1hdvx40,https://reddit.com/r/MachineLearning/comments/1hdvx40/r_survey_on_students_motivation_to_learn/,https://www.reddit.com/r/MachineLearning/comments/1hdvx40/r_survey_on_students_motivation_to_learn/,2024-12-14 05:25:53,0,0.17,0,0,1,0,0,False,False,True,False,False,Research,self,t3_1hdvx40
MachineLearning,[D] Build a 100M Business with Only AI ,"* Notion – A versatile platform for planning, organizing, and tracking everything from content ideas to project timelines.
* Komo AI – An AI search engine that simplifies research, speeds up fact-checking, and makes finding precise information 10x faster.
* X (formerly Twitter) – A key platform for building relationships, promoting products, and engaging with an active community.
* Braze – A robust tool for managing personalized marketing campaigns and building strong customer relationships.
* Shopify – A user-friendly platform for showcasing and selling digital products seamlessly.
* Chatfuel – A virtual assistant that automates customer interactions, from answering FAQs to guiding users through offerings.
* Canva – An intuitive design tool for creating visuals, social media posts, and product mockups without the need for graphic design expertise.
* BetaList – A platform for launching new products and gathering valuable feedback to refine offerings before scaling.",cheerysolemnity47,1hdxh1c,https://reddit.com/r/MachineLearning/comments/1hdxh1c/d_build_a_100m_business_with_only_ai/,https://www.reddit.com/r/MachineLearning/comments/1hdxh1c/d_build_a_100m_business_with_only_ai/,2024-12-14 07:10:15,0,0.05,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hdxh1c
MachineLearning,[D] help with evaluating model,"i am having an issue with evaluating my model because model.evaluate() returns an okay overall score in accuracy but the confusion matrix and classification report return 100% for one class and 0% for another, i am using cifar10 but only 2 classes from it. anyone know why this happens? is this overfitting i am not sure because i am getting a similar score as model.evaluate(0 in my training accuracy and same for loss (which is almost as high as the accuracy)",Affectionate_Pen6368,1hd5kht,https://reddit.com/r/MachineLearning/comments/1hd5kht/d_help_with_evaluating_model/,https://www.reddit.com/r/MachineLearning/comments/1hd5kht/d_help_with_evaluating_model/,2024-12-13 05:40:50,4,0.7,4,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hd5kht
MachineLearning,[D] Agentic AI Design Patterns,"I was looking into design patterns for Agentic AI and I could need some help to grasp the concepts.

I read about ReAct and ReWOO.

From ReWOO, I really liked the idea of having a planner that creates a blueprint of the work that needs to be done. I can imagine that this works well for a lot of tasks, and it optimizes token usage compared to ReAct.

From ReAct, I like that it has a reflection/observation LLM, to decide whether the output is good enough or needs another pass through the agents.

What I don't understand:
Why does ReWOO not have a reflection component??

Wouldn't it be the best of both worlds to have the planner and the reflection?

This was the first draft for my agentic AI prototype, and I think it has pretty obvious advantages.

I think I am missing something here.",Mindless_Copy_7487,1hd8w3k,https://reddit.com/r/MachineLearning/comments/1hd8w3k/d_agentic_ai_design_patterns/,https://www.reddit.com/r/MachineLearning/comments/1hd8w3k/d_agentic_ai_design_patterns/,2024-12-13 09:49:22,0,0.5,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hd8w3k
MachineLearning,"[D] ""Proper"" way to upload accepted conference paper to the ArXiv?","We recently had a paper accepted to a conference (AAAI). We found out that the conference does not publish appendices so they recommend we upload the full paper (with appendix) to arXiv. This is something we were considering doing anyway since the paper would be available before the conference proceedings come out.

My concern is that if someone decides to cite our work, they may either become confused or cite the arXiv rather than AAAI ""version"".

Is there a ""correct"" or common way to handle this? Do arXiv uploads with the same title get indexed to ""one manuscript"" on google scholar?

Also, are we allowed to use the conference template to upload? (This part might be conference dependent I suppose).

I know it is common these days to upload to arXiv before hearing back from a conference (usually with a different title) but I think this is a slightly different situation as the paper is accepted and the uploaded version will be identical to the conference paper (though with an Appendix).

Thanks in advance!",baghalipolo,1hcupkm,https://reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/,https://www.reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/,2024-12-12 20:38:05,6,0.65,6,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hcupkm
MachineLearning,[D] LSTM model implementation and approximation questions,"For a project I am currently trying to integrate an Autoencoder for feature extraction and an LSTM for classification of the reduced feature space. The problem I am encountering is on how to train the LSTM network. The AE produces 5 datapoints which is fed into the LSTM network. The trick now comes in on the training of the LSTM network and how the LSTM works. I want the LSTM to take into account the 5 parameters from the AE at time t as well as the parameters at t-1 and t-2. As far as I understand the LSTM does this automatically, or should it then be that the LSTM takes in a total of 15 parameters with each pair of 5 corresponding to one timestep of the AE?

Any advice on LSTM would be great or how such training can be done in an efficient way. The AE is processing a time-series signal.",Sea_Onion41,1hcvh1c,https://reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/,https://www.reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/,2024-12-12 21:09:27,5,0.78,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1hcvh1c
MachineLearning,[R] Rethinking the positive pairs in contrastive learning,"Hi, I am sharing my recent work which allows arbitrary images to be positive pairs. Our finding is quite astonishing that two disparate images, e.g., a snake and a lamp, can be positive. Our work potentially broadens the applications of contrastive learning to deal with the ""false positive"" in which two views are not similar.

We challenge the common sense in contrastive learning, that is, the positive pair design is critical. Our results prove that the feature selection is the key!

Paper: [https://arxiv.org/abs/2410.18200](https://arxiv.org/abs/2410.18200)",Miserable-Gene-308,1hcpoo6,https://reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/,https://www.reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/,2024-12-12 17:02:44,7,0.65,7,0,10,0,0,False,False,True,False,False,Research,self,t3_1hcpoo6
MachineLearning,From Viruses and Materials to Galaxies and Beyond: The Role Machine Learning Plays in Scientific Discovery,,SlothSpeedRunning,1hcubv2,https://reddit.com/r/MachineLearning/comments/1hcubv2/from_viruses_and_materials_to_galaxies_and_beyond/,https://lettersandsciencemag.ucdavis.edu/science-technology/viruses-and-materials-galaxies-and-beyond,2024-12-12 20:21:10,3,0.8,3,0,0,0,0,False,False,False,False,False,,https://a.thumbs.redditmedia.com/SRD_I9dMMiKup552l256JNNe6jjxqQTl9DnvS7gkMO8.jpg,t3_1hcubv2
MachineLearning,[D] Question About ResNet and Scalability of Extremely Deep Networks,"I’ve been exploring the architecture of ResNet and its ability to train very deep neural networks effectively. While I understand that residual connections help mitigate issues like vanishing gradients and make training deeper networks feasible, I’m curious about the limitations of this approach when scaling to extremely deep networks, such as those with 1000 layers or more.

From my understanding, a ResNet with, say, 100 layers might effectively function like a much smaller network due to the residual connections, which essentially ""skip"" layers and add outputs. However, wouldn’t this also mean that if a regular MLP struggles to scale beyond 15 layers, a ResNet might just shift this limit proportionally (e.g., struggling beyond 150 layers)? In other words, does ResNet fundamentally solve the problem of training extremely deep networks, or does it merely extend the depth at which issues start to reappear?

  
I’d appreciate any insights you might have! TYSM!",Time_Celebration6058,1hco4ig,https://reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/,https://www.reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/,2024-12-12 15:54:53,5,0.86,5,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hco4ig
MachineLearning,[N] Save 80% Memory for DPO and ORPO in Liger-Kernel,"Introducing the first open-source optimized post-training losses in Liger Kernel with \~80% memory reduction, featuring DPO, CPO, ORPO, SimPO, JSD, and more, achieving up to 70% end-to-end speedup through larger batch size. Use it as any PyTorch module - Available today in Liger v0.5.0!

[https://x.com/hsu\_byron/status/1866577403918917655](https://x.com/hsu_byron/status/1866577403918917655)",Icy-World-8359,1hcewdl,https://reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/,https://www.reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/,2024-12-12 06:18:35,20,0.88,20,0,0,0,0,False,False,True,False,False,News,self,t3_1hcewdl
MachineLearning,[D] Pet project - Style Transfer Neural Networks Implementation,"Hi, I am learning ML and this is my first project. I did a simple 100 LoC implementation of the *Neural Style Transfer* paper by Gatys et al. See [https://github.com/TAOGenna/pytorch-neural-style-transfer](https://github.com/TAOGenna/pytorch-neural-style-transfer)

https://preview.redd.it/x2udi76n2g6e1.jpg?width=939&amp;format=pjpg&amp;auto=webp&amp;s=437bdda1683e9fd580a6b3d1d4dc2598b25079ff

  
",TAO_genna,1hcottj,https://reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/,2024-12-12 16:25:17,3,0.64,3,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/BbvnpWVVY1jQ1n7M80lXl0kKKowiy2y473otZfpwdWo.jpg,t3_1hcottj
MachineLearning,"[R] A Grounded Theory Study of LLM Red Teaming: Motivations, Strategies, and Techniques","This paper presents a grounded theory study of how red-teaming is conducted on Large Language Models (LLMs), based on interviews with practitioners. The researchers systematically analyzed practitioner approaches to identify common patterns, strategies and motivations in LLM red-teaming.

Key technical points:
- Used qualitative coding of interviews to develop taxonomy of red-teaming approaches
- Identified 12 distinct attack strategies and 35 specific techniques
- Found red-teaming requires manual effort rather than automation
- Demonstrated importance of team collaboration over individual attempts
- Established red-teaming as distinct from malicious attacks
- Mapped common patterns in tester motivations and goals

Main results:
- Red-teaming strategies fall into categories like prompt manipulation, psychology-based attacks, and system limit testing
- Successful testers adopt an ""alchemist"" mindset of systematic experimentation
- Most practitioners are motivated by curiosity and safety concerns
- Testing requires deep understanding of both technical and psychological aspects
- Manual testing currently more effective than automated approaches

I think this work provides an important foundation for developing more structured approaches to LLM safety testing. The taxonomy they've developed could help standardize how we evaluate and secure these systems. Their finding that manual testing remains superior to automation suggests we need much more work on automated testing approaches.

I think the emphasis on non-malicious intent and safety motivations is particularly relevant as these systems become more widely deployed. Understanding how and why people conduct these tests helps distinguish legitimate security research from attacks.

TLDR: First systematic study of LLM red-teaming practices, providing taxonomy of strategies and techniques based on practitioner interviews. Shows importance of manual testing and team collaboration, while establishing red-teaming as legitimate security research.

[Full summary is here](https://aimodels.fyi/papers/arxiv/summon-demon-bind-it-grounded-theory-llm). Paper [here](https://arxiv.org/abs/2311.06237).",Successful-Western27,1hclmk1,https://reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/,https://www.reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/,2024-12-12 13:56:57,4,1.0,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1hclmk1
MachineLearning,[P] Scalling data from aggregated calculations,"Hello, I have a project in which I detect anomalies on transactions data from ethereum blockchain. I have performed aggregated calculations on each wallet address (ex. minimum, maximum, median, sum, mode of transactions' values) and created seperated datafile with it. I have joined the data on all the transactions. Now I have to standardize data (I have chosen robust scalling) before machine learning but I have following questions regarding this topic:

1. Should I actually standardize each feature based on its unique mean and iqr? Or perform scalling on the column that the calculations come from - value column and than use its mean and iqr to scale the calculated columns?
2. If each feature was scaled based on its own mean and iqr should I do it before joining calculated data or after?",Wikar,1hcukjg,https://reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/,https://www.reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/,2024-12-12 20:32:01,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1hcukjg
MachineLearning,[R] LLM's knowledge expansion to enable generation of cross domain content (outside the training dataset),,ankitm1,1hcke77,https://reddit.com/r/MachineLearning/comments/1hcke77/r_llms_knowledge_expansion_to_enable_generation/,https://arxiv.org/abs/2409.17171,2024-12-12 12:50:27,4,0.7,4,0,0,0,0,False,False,False,False,False,Research,default,t3_1hcke77
MachineLearning,"[D] does intel gpu support ROCm or AMD cards  support intel one?
","i can't find this information and if both are open source it make sense a compatibility layer , any of the two is already ported to the other platform?, if you can share info about nvidia too will be cool



",mrnothing-,1hctd0o,https://reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/,https://www.reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/,2024-12-12 19:39:17,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hctd0o
MachineLearning,[D] How to make friends and network at NeurIPS?,"I’m attending NeurIPS for the first time and it’s quite overwhelming seeing the amount of people and so many recruiters. I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is here.

I didn’t really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach others who are in big groups of people and (2) I’m feeling strong imposter syndrome and under-qualified for the jobs recruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to potentially meet up and have a chat? I’m a 3rd year PhD student from the UK, but from Vancouver myself so know lots of stuff going on in the area. Cheers!",K_is_for_Karma,1hc0x89,https://reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/,https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/,2024-12-11 18:54:26,49,0.83,49,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1hc0x89
MachineLearning,[R] Continuous Latent Space Reasoning: Enhancing LLM Performance Through Chain of Continuous Thought,"This paper introduces **COCONUT** (Chain of Continuous Thought), which transforms language model reasoning from discrete token space into continuous latent space. The key idea is encoding reasoning steps as continuous vectors rather than text tokens, allowing for more flexible and precise intermediate computations.

Main technical points:
* Encoder-decoder architecture that maps text↔continuous vectors
* Novel continuous reasoning module operating on latent vectors
* Parallel processing of reasoning steps in continuous space
* Gradient-based optimization during the reasoning process
* Special loss function combining reconstruction and reasoning objectives

Key results:
* **20%** improvement on reasoning benchmarks vs traditional methods
* Reduced computational steps needed for complex problems
* More consistent performance across different reasoning tasks
* Better handling of mathematical and logical reasoning
* Enhanced ability to maintain coherent reasoning chains

I think this approach could meaningfully advance how language models handle complex reasoning tasks. By moving beyond discrete tokens, models may better capture the continuous nature of human-like reasoning. The ability to optimize in continuous space during reasoning is particularly promising for improving reliability.

I think the main challenge will be scaling this to very large models while managing computational costs. The translation between discrete and continuous spaces adds overhead that needs to be addressed.

TLDR: New method transforms language model reasoning into continuous vector space instead of discrete tokens, showing 20% better performance on reasoning tasks through more flexible computation.

[Full summary here](https://aimodels.fyi/papers/arxiv/training-large-language-models-to-reason-continuous). Paper [here](https://arxiv.org/abs/2412.06769).",Successful-Western27,1hbto1w,https://reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/,https://www.reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/,2024-12-11 13:39:26,104,0.96,104,0,6,0,0,False,False,True,False,False,Research,self,t3_1hbto1w
MachineLearning,[R] What should I choose: AI-assisted data labeling or crowdsourced data labeling?,"Hey everyone,

I’m a researcher at my company, and I’m starting to train a model using a dataset of over 50k medical images. One of the major challenges I’m facing is choosing the best approach for data labeling.

I’ve seen promising results with **AI-assisted labeling**—it seems faster and less costly upfront. However, I’m concerned about potential inaccuracies and whether the AI’s assistance would skew the dataset in ways I might not expect.

On the other hand, **crowdsourced labeling** is significantly more expensive and time-consuming, but the results are often highly reliable due to diverse human input.

Given the scale of my dataset and its importance in a sensitive domain like medical imaging:

* **Which method would you recommend, and why?**
* **Are there hybrid approaches that balance cost, quality, and efficiency?**
* **What tools or platforms would you suggest for either approach?**

I’d love to hear your insights or experiences with similar projects. Your input will be invaluable in helping me make the right decision!

Thanks in advance!",Organic-Injury-1153,1hchrui,https://reddit.com/r/MachineLearning/comments/1hchrui/r_what_should_i_choose_aiassisted_data_labeling/,https://www.reddit.com/r/MachineLearning/comments/1hchrui/r_what_should_i_choose_aiassisted_data_labeling/,2024-12-12 09:56:17,1,0.53,1,0,15,0,0,False,False,True,False,False,Research,self,t3_1hchrui
MachineLearning,[R] An Evolved Universal Transformer Memory,,hardmaru,1hc6bs8,https://reddit.com/r/MachineLearning/comments/1hc6bs8/r_an_evolved_universal_transformer_memory/,https://arxiv.org/abs/2410.13166,2024-12-11 22:43:30,15,0.83,15,0,2,0,0,False,False,False,False,False,Research,default,t3_1hc6bs8
MachineLearning,[D] What Models Are Best at Producing Ambient Sounds/ Music?,"I'm working on an application that requires ambient sounds/ music. For example:

* ""A crackling fire, with chat murmuring in the background.""
* ""Nightime countryside summer sounds in the UK.""
* ""Wind blowing through the mountains as you're stood on a high rock.""

I've had a look at Hugging Face and found the Text-To-Audio section. However it appears the top models have very few downloads:

https://preview.redd.it/d4o7g760yf6e1.png?width=1788&amp;format=png&amp;auto=webp&amp;s=2901a7678582745beb714b81519712bac37bd195

This makes me think the field is immature, and there's no clear best model. Is this a fair appraisal of the field, or are there models outside of Hugging Face that perform well for this use case?",FPGA_Superstar,1hcof9m,https://reddit.com/r/MachineLearning/comments/1hcof9m/d_what_models_are_best_at_producing_ambient/,https://www.reddit.com/r/MachineLearning/comments/1hcof9m/d_what_models_are_best_at_producing_ambient/,2024-12-12 16:07:30,0,0.4,0,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/TCjeAYae_sK1rwe1hACJo5HRrqIniSivkrmvMhxxwGc.jpg,t3_1hcof9m
MachineLearning,"[D] I got the acceptance for my IEEE publication, does that means it will be uploaded on their Xplore page?","So I submitted a paper and it got accepted by my publication around 2 months ago, today was my conference in online mode, didnt go well I think he was in hurry he didnt listen much diagreed a bit and then closed the meet on my face. So my question is how bad is it? Will it be published as I have the acceptance or still a no?",candle_misuser,1hcpqj4,https://reddit.com/r/MachineLearning/comments/1hcpqj4/d_i_got_the_acceptance_for_my_ieee_publication/,https://www.reddit.com/r/MachineLearning/comments/1hcpqj4/d_i_got_the_acceptance_for_my_ieee_publication/,2024-12-12 17:04:48,0,0.35,0,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1hcpqj4
MachineLearning,[R] Evaluating the world model implicit in a generative model,,jsonathan,1hbra2d,https://reddit.com/r/MachineLearning/comments/1hbra2d/r_evaluating_the_world_model_implicit_in_a/,https://arxiv.org/pdf/2406.03689,2024-12-11 11:19:06,21,0.86,21,0,26,0,0,False,False,False,False,False,Research,default,t3_1hbra2d
MachineLearning,[D] Resources to get up to the speed with the state of the art evolutionary optimization,"There're plenty of good books letting you get close to the state of the art in the field, on Machine Learning, and Deep Learning in particular. However, are there any good modern books on evolutionary optimization? Are there any good courses?",ArtisticHamster,1hbt986,https://reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/,https://www.reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/,2024-12-11 13:17:59,9,0.77,9,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hbt986
MachineLearning,[D] Why are the Stella embedding models so much smaller than other models of similar quality?,"On the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), `stella_en_v5` is currently ranked 3rd overall, while using *one fifth* the memory of all non-Stella models in the top 10.

`stella_en_400M_v5` is ranked 10th, while using *15-20 times less memory* than the models ranked near it. This appears to be relatively consistent across several subtasks of the benchmark (for English).

What is the secret sauce here? Alternatively, what is the catch? There is no paper yet. Anyone know details?",-p-e-w-,1hbkww5,https://reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/,https://www.reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/,2024-12-11 03:58:01,37,0.86,37,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1hbkww5
MachineLearning,[D] Any 3D reconstruction method that can be used for multiple scenes (i.e. not use and throw),"NeRFs are unique for every scene and thus, need to be trained from scratch. Gaussian splats are scene unique too. I understand that scenes are complex and thus there is very little chance for there to be a neural network that can output multiple scenes once trained. But still are there any scene representations that to some caliber are not use and throw completely?",deathmaster2011,1hbreb4,https://reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/,https://www.reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/,2024-12-11 11:27:15,5,0.86,5,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hbreb4
MachineLearning,[R] When do authors have access to ICLR Meta-Reviews ?,"Hello everyone,

This is my first time submitting to ICLR. On the ICLR website, it says that Meta-Reviews are due today (in a few hours). Will the authors have access to those reviews at the same time as the decision notification or right after the Meta-Reviews due date ?

Thanks !",Glaze_anetha42,1hbqc75,https://reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/,https://www.reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/,2024-12-11 10:10:57,6,0.75,6,0,2,0,0,False,False,True,False,False,Research,self,t3_1hbqc75
MachineLearning,[D] From Unemployment to Lisp: Running GPT-2 on a Teen's Deep Learning Compiler,"A couple months ago I found myself unemployed, uncertain about what to do next. I wanted to learn more about deep learning, but from a systems prespective. Coming from Andrew's Ng course on supervised learning, I was eager to learn more about how deep learning frameworks (or deep learning compilers) like Pytorch or Tinygrad.

I started to poke around Tinygrad, learning from the tutorials I found online, and I found it fascinating because it was an actual compiler, it took conventional python code and translated them into an Abstract Syntax Tree that was parsed into UOps and ScheduleItems, to finally have a codegen layer. While the design was interesting, the code was hard to read.

That's when I stumbled across something completly unexpected, A deep learning compiler built on Common Lisp, maintained by a Japanese 18-year-old during his gap year. And currently we have acomplished something great, it can run gpt2!

For now, it just generates C-kernels, but in the future we would like to support cuda codegen as well as many other features, and serve as a learning tool for anyone who would like to get to work on deep learning compilers in Common Lisp.

This is an open source project and anyone is welcome to contribute!

[https://github.com/hikettei/Caten](https://github.com/hikettei/Caten)

Edit: add an example of how it works.

Here's an example i wrote in a different forum:

Hello! Thanks for your question.

First of all, there are three layers of abstraction within Caten:

1. caten/apis | High-Level Graph Interface 2. caten/air | Low-Level Graph Interface 3. caten/codegen | AIR Graph =&gt; Kernel Generator

The inputs of the compiler are just Common Lisp classes (similar to torch modules). For example, in Common Lisp, we could create a module that does SinCos:

        (defclass SinCos (Func) nil
          (:documentation ""The func SinCos computes sin(cos(x))""))
    
        ;; Forward creates a lazy tensor for the next computation.
        ;; You can skip this process by using the `st` macro.
        (defmethod forward ((op SinCos) &amp;rest tensors)
          (st ""A[~] -&gt; A[~]"" (tensors)))
    
        ;; Backward is optional (skipped this time)
        (defmethod backward ((op SinCos) &amp;optional prev-grad)
          (declare (ignore prev-grad))
          nil)
    
        ;; Lower describes the lowered expression of `SinCos`
        (defmethod lower ((op SinCos) &amp;rest inputs)
          (let ((x (car inputs)))
            (with-context
              (a (%sin (%add x (%fconst (/ pi 2)))))
              (b (%sin a)))))

The \`apis\` layer is the high-level interface, while the \`lower\` method is the lower-level step before code generation.

Next, the framework generates an Abstract VM (AVM) representation:

        #S(AVM :GRAPH Graph[seen=NIL, outputs=(STC6466_1)] {
          &lt;ALLOCATE : TID6464 &lt;- (shape=(1), stride=(1)) where :dtype=FLOAT32&gt;
          &lt;Node[BUFFER] ALLOCATE(NID6480) : SID6479* &lt;- ()&gt;
          &lt;Node[BINARYOPS] ADD(NID6484) : BID6483* &lt;- (TID6464, LID6481)&gt;
          &lt;Node[UNARYOPS] SIN(NID6486) : UID6485* &lt;- (BID6483)&gt;
          &lt;Node[UNARYOPS] SIN(NID6488) : UID6487* &lt;- (UID6485)&gt;
          &lt;Node[SPECIAL/VM] PAUSE/BACKWARD(NID6501) : STC6466_1* &lt;- (UID6487)&gt;
        })

Then, the computation graph is translated into schedule items:

        FastGraph[outputs=(val_6)] {
          { Allocate } : [ val_0 &lt;- (1) ]
          { KERNEL } : [ val_5 &lt;- val_1, val_0 :name=FUSED_SIN_SIN_ADD_LOAD6511]
        }

Finally, the code generation step produces the following C code:

        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0);
        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0) {
            val_5[0] = sin(sin((val_0[0] + 1.5707964)));
        }

This C code is compiled by a C compiler and executed.

So to answer your question: the compiler takes Common Lisp code and generates C functions.",yCuboy,1hb7v5h,https://reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/,https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/,2024-12-10 18:00:45,93,0.93,93,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hb7v5h
MachineLearning,Conv2D for Time Series Data Tutorial [Project],"Can anyone provide me with a tutorial using TensorFlow on time series data and an example model with Conv2D layers, an AveragePooling2D layer and final dense layers?",chiplab,1hc5r13,https://reddit.com/r/MachineLearning/comments/1hc5r13/conv2d_for_time_series_data_tutorial_project/,https://www.reddit.com/r/MachineLearning/comments/1hc5r13/conv2d_for_time_series_data_tutorial_project/,2024-12-11 22:18:06,0,0.13,0,0,6,0,0,False,False,True,False,False,Project,self,t3_1hc5r13
MachineLearning,[D] Review process incentives and competition,"One of my labmates showed me a comment by an AC asking reviewers in an ACM conference to engage in the ICLR rebuttal and discussion period. This alone is funny (and sad) to me, but what got me was when one of the reviewers responded saying that the review process incentivizes reviewers to score papers low in order to gatekeep competing papers from being accepted.

I want to believe that this happens but its effect is not significant. However, I have heard that this is very common in fields like recommender systems. How prevalent is it in ML in general?",like_a_tensor,1hbf2gs,https://reddit.com/r/MachineLearning/comments/1hbf2gs/d_review_process_incentives_and_competition/,https://www.reddit.com/r/MachineLearning/comments/1hbf2gs/d_review_process_incentives_and_competition/,2024-12-10 23:07:02,12,1.0,12,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hbf2gs
MachineLearning,[R] How difficult is this dataset REALLY?,"New Paper Alert!

Class-wise Autoencoders Measure Classification Difficulty and Detect Label Mistakes

We like to think that the challenge in training a classifier is handled by hyperparameter tuning or model innovation, but there is rich inherent signal in the data and their embeddings.  Understanding how hard a machine learning problem is has been quite elusive.  Not any more.

Now you can compute the difficulty of a classification dataset without training a classifier, and requiring only 100 labels per class.  And, this difficulty estimate is surprisingly independent of the dataset size.

Traditionally, methods for dataset difficulty assessment have been time and/or compute-intensive, often requiring training one or multiple large downstream models. What's more, if you train a model with a certain architecture on your dataset and achieve a certain accuracy, there is no way to be sure that your architecture was perfectly suited to the task at hand — it could be that a different set of inductive biases would have led to a model that learned patterns in the data with far more ease.

Our method trains a lightweight autoencoder for each class and uses the ratios of reconstruction errors to estimate classification difficulty. Running this dataset difficulty estimation method on a 100k sample dataset takes just a few minutes, and doesn't require tuning or custom processing to run on new datasets!

How well does it work? We conducted a systematic study of 19 common visual datasets, comparing the estimated difficulty from our method to the SOTA classification accuracy. Aside from a single outlier, the correlation is 0.78. It even works on medical datasets!



Paper Link:  https://arxiv.org/abs/2412.02596

GitHub Repo Linked in Arxiv pdf",ProfJasonCorso,1hb54nd,https://reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/,https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/,2024-12-10 16:04:41,32,0.78,32,0,20,0,0,False,False,True,False,False,Research,self,t3_1hb54nd
MachineLearning,Where to find Llama 3 initialisation [R] ,"Title basically says it all, I want a good transformer baseline and I imagine that the initialisation can matter quite a bit. I can find the llama 3 model, but I can find how they init parameters. Does anyone know where I can find this?",idkwhatever1337,1hbudg3,https://reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/,https://www.reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/,2024-12-11 14:14:14,0,0.4,0,0,4,0,0,False,False,True,False,False,Research,self,t3_1hbudg3
MachineLearning,[R] Understanding Transformer Limitations in Graph Search: A Mechanistic Analysis of Learning and Scaling Behavior,"This paper tackled a fundamental question about transformers' ability to learn search algorithms by studying how they handle graph connectivity problems. The authors developed a novel interpretation method to analyze how transformers process search operations layer by layer.

Key technical points:
- Used graph reachability as a test case with controlled complexity and unlimited training data
- Developed interpretation technique to understand how transformer layers compute reachable vertex sets
- Found transformers learn to expand search frontier exponentially with depth
- Demonstrated clear scaling limitations based on graph size
- Showed in-context learning (chain-of-thought) doesn't overcome these limitations

Main results:
- Small transformers can learn basic search when trained appropriately
- Each layer computes union of previously reachable vertices plus their neighbors
- Performance degrades sharply with increasing graph size
- Adding parameters doesn't solve the scaling problem
- Models struggle with graphs beyond their training distribution

I think this work reveals important architectural limitations in transformers that we need to address for applications requiring search capabilities. The scaling behavior suggests we may need fundamentally different approaches for larger search spaces rather than just bigger models.

I think the interpretation method they developed could be valuable for understanding how transformers process other types of structured data beyond just graphs. The clear empirical results on scaling limitations should inform architecture choices for applications involving search-like computations.

TLDR: Transformers can learn basic graph search operations but face fundamental limitations with scale. Adding more parameters doesn't help, suggesting we need new approaches for complex search problems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/transformers-struggle-to-learn-to-search). Paper [here](https://arxiv.org/abs/2412.04703).",Successful-Western27,1hb1wjo,https://reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/,https://www.reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/,2024-12-10 13:37:01,35,1.0,35,0,1,0,0,False,False,True,False,False,Research,self,t3_1hb1wjo
MachineLearning,[D] Inverse Neural Network,"Hi everyone, I wanna ask you guys if you know what's the current best **supervised** Inverse Neural Network? I know GAN, VAE, and conditional VAE.

Basically, my aim is to determine the input values of a multivariate function that satisfy an output value, e.g., find x=\[x1,...,xN\] so that 0.2=f(x).

My main major is engineering (not machine learning) and my knowledge about the field is quite limited. However, I'm good with reading any research papers that you suggested.

Thank you all,

Edit: sorry the for confusing example. f is NOT multivariate pdf but a ""multivariate function"" f : R\^N -&gt; R. Specifically, f is a ""univariate"" pdf with N-1 parameters.",zonanaika,1hbbj5o,https://reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/,https://www.reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/,2024-12-10 20:34:05,8,1.0,8,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1hbbj5o
MachineLearning,[R] The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,"**Dataset**: [https://github.com/PolymathicAI/the\_well](https://github.com/PolymathicAI/the_well)

**Paper**: [https://arxiv.org/pdf/2412.00568](https://arxiv.org/pdf/2412.00568)

**Abstract:**

&gt;Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at [this https URL](https://github.com/PolymathicAI/the_well).

",StartledWatermelon,1haz4nw,https://reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/,https://www.reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/,2024-12-10 10:49:47,22,0.88,22,0,0,0,0,False,False,True,False,False,Research,self,t3_1haz4nw
MachineLearning,[R] Articulate Anything: automatic generation of 3D interactable assets from any input modalities,"📦 Can frontier AI transform ANY physical object from ANY input modality into a high-quality digital twin that also MOVES? Excited to share our work,Articulate-Anything, exploring how large vision-language models (VLMs) can bridge the gap between the physical and digital worlds.

Articulate-Anything 🐵 is a state-of-the-art method for automatic interactable 3D asset creation from any input modalities including text, images, or videos.  
  
Website: [articulate-anything.github.io](https://t.co/y3fNE8Lb7X)  
Paper: [https://arxiv.org/abs/2410.13882](https://t.co/8m3gu5zTD7)  
Code: [https://github.com/vlongle/articulate-anything](https://github.com/vlongle/articulate-anything)  
Please see my twitter thread: [https://x.com/int64\_le/status/1866519866934714623](https://x.com/int64_le/status/1866519866934714623) a deep dive into the method",Prudent_Fly_1004,1hb82am,https://reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/,https://www.reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/,2024-12-10 18:08:50,4,1.0,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1hb82am
MachineLearning,[D] How do you keep up with the literature?,"Pretty much what the title says. What tools/strategies do you use to keep up with the literature?


EDIT: for context, I am a first year PhD student and I was referring to the literature in the particular 'niche' (if you can call anything a niche in ML, apart from a very few exceptions)",Rickmaster7,1hasdlo,https://reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/,https://www.reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/,2024-12-10 03:20:29,48,0.89,48,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1hasdlo
MachineLearning,[D] What’s stopping you from using foundation models for time series forecasting?,"I’ve been experimenting with foundation models like [Sulie](https://github.com/wearesulie/sulie), [Granite TTM](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r1), and [Amazon Chronos](https://github.com/amazon-science/chronos-forecasting), and each one has its own strengths. What’s really fascinating is how much faster you can get accurate forecasts with a zero-shot approach. However, as much as these models improve forecasting, explainability remains a major challenge compared to more traditional methods like ARIMA, which are simpler to interpret.

I’m curious—do you think explainability is a dealbreaker, or is there another reason why foundation models for forecasting aren’t gaining wider adoption? Would love to hear what’s been your biggest blocker or challenge in using these models.",Queasy_Emphasis_5441,1hb7ur1,https://reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/,https://www.reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/,2024-12-10 18:00:23,2,0.53,2,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1hb7ur1
MachineLearning,[D] Question about heating system machine learning,"Hello, in conventional heating processes you control when to start heating and when to stop with target values. Once reached it'll stop and start the whole process over and over again, alongside some pump valves (how much water to go through the circulation).

I've captured several room temperatures, all start/stops and pump valve adjustments made by the heating automation software in a time series database for the last 5 years (every minute).

I'm trying to create a model which has a constant target value of e.g. 23 °C. The inputs for the model are the heating system status (on/off), pump valve positions and current room temperatures. Output should be how to set the heating system status and adjust the valve positions to achieve and hold the target value of the room temperatures. In the best case scenario it should replace the heating automation software completely. Other is to just advise or supervise the process.

A few problems come to my mind, where I'm not sure on how to approach these:

1. The process is slow and once heating starts the results can be seen 1 hour later as the temperature changes slowly. So the evaluation of the actions must be done with some delay?
2. My captured data contains historical temperatures, but I think it might be flawed. The temperature in the data is already influenced by the existing heating system. I don't have temperature data which show how the room temperatures realistically change without any heating systems. Is this a problem for learning? Do I need to create synthetic data?
3. Would it be better to train a model to output ""start heating / stop heating"" (leave the rest for the conventional heating automation) or to control the heating status and the pump valves itself?
4. What would be the best machine learning technique, e.g. un-/supervised, reinforcement learning?",QuickYogurt2037,1hb5dty,https://reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/,https://www.reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/,2024-12-10 16:15:47,2,0.75,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hb5dty
MachineLearning,[D] Meta's new LLama model,"So meta just dropped a new, more efficient Llama model, Llama 3.3 70B, that basically promises to cut compute costs for large AI models. Has anyone here had a chance to test it out? Curious to see how it performs compared to previous versions, in terms of speed, resource usage, and accuracy",Frosty_Programmer672,1han33i,https://reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/,https://www.reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/,2024-12-09 23:01:00,21,0.76,21,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1han33i
MachineLearning,"[D] How do you manage and track your large, evolving, image datasets?","I’m wondering how people manage the lifecycles of their large in-house datasets? Say &gt;1TB and 100k files.

In my new role we have multiple production models trained from in house datasets ranging in size from a few thousand to a few hundred thousand images. We also have huge amounts of fresh data coming in, more than 1M images per day, and so we are constantly mining that and sending new tranches off to be annotated.

Until now the team has been largely left to their own devices to manage this and the results are predictable. In some cases we can’t associate our prod models with any specific data. Some of our core datasets exist only in people’s home directories, ripe to be wiped out by a single misplaced command. For one model, thankfully being sunsetted, both the training code and original training data are known to be lost.

Parts of the org have adopted DVC which seems pretty good until the number of files or overall size gets big. On one end, some stuff the entire dataset into just a few archives and track them. That minimizes frustrations with hashes but uses a lot of storage when only a few files get updated. On the other end, some people track every single file which lets files be individually updated but is a pretty big pain to check in and out. Others split the difference of these two approaches, tracking chunks of the dataset as archives hierarchically.

So how does your org manage this? What works and what doesn’t when working with these large &amp; evolving datasets?",SirPitchalot,1haokqp,https://reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/,https://www.reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/,2024-12-10 00:09:58,16,0.84,16,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1haokqp
MachineLearning,[D] Is what I'm doing is correct?,"I'm working on an ML project.
I have 100 features and 2000000 rows(Balanced)
Which order shall I follow?

I have done,

1. Data inconsistencies handling
2. NULL imputation 
3. Standardization
4. One hot encoding
5. Data visualization 
6. Correlation check
7. PCA
8. Train test split
8. Model training
9. Evaluation 

For random forest I'm getting 1 for all the metrics for training data and 0.79 for test set.
For logistic regression ~0.79  for all metrics and for test set also getting the same.
For GBDT also ~0.79 for all metrics and for test set also getting the same.
Which model should I select? And is the above mentioned steps are followed in correct order?
",_crazy_muffin_,1has6jq,https://reddit.com/r/MachineLearning/comments/1has6jq/d_is_what_im_doing_is_correct/,https://www.reddit.com/r/MachineLearning/comments/1has6jq/d_is_what_im_doing_is_correct/,2024-12-10 03:10:16,7,0.82,7,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1has6jq
MachineLearning,[D] Model Provenance: How are you tracking your ML model lineage?,"Hey r/MachineLearning,I'm curious about how people in this community are handling model provenance - the practice of tracking the lineage and evolution of machine learning models throughout their lifecycle.

1. Are you currently using any tools or methods to track the provenance of your ML models?
2. If yes, what solutions are you using? Are they custom-built or off-the-shelf?
3. If not, do you see a need for such tools in your work?
4. What features would you consider essential in a model provenance solution?",crtahlin,1hb0o4e,https://reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/,https://www.reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/,2024-12-10 12:29:56,0,0.5,0,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1hb0o4e
MachineLearning,[D] Has anyone managed to train an LLM with model parallelism?,"Hello,

I am working on fine-tuning Llama-3.1 for my master’s thesis research. Unfortunately, my current situation forbids access to high-memory GPUs such as A100s. Instead, I have access to setups with multiple lower-memory GPUs, such as 4×3090 or 8×V100.

Therefore I need to implement model parallelism to train my model as it doesn’t fit into a single GPU. However, I’ve noticed that most frameworks primarily focus on data parallelism, which doesn’t address my needs.

Has anyone successfully trained a model by splitting it across multiple GPUs? If so, could you recommend frameworks or approaches I should explore? I am specifically looking for full training, although I am interested in hearing if someone managed this using LoRA.

Also, if there’s a more suitable subreddit for this type of question, please direct me to there.

Thank you!",anilozlu,1habr8l,https://reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/,https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/,2024-12-09 15:06:32,50,0.88,50,0,40,0,0,False,False,True,False,False,Discussion,self,t3_1habr8l
MachineLearning,[R] Improving robustness to corruptions with multiplicative weight perturbations - A simple yet effective approach to robustify neural networks to corruptions,"We would like to share and discuss this NeurIPS spotlight paper (disclaimer: I am a co-author).

**Paper**: [https://arxiv.org/abs/2406.16540](https://arxiv.org/abs/2406.16540)  
**GitHub**: [https://github.com/trungtrinh44/DAMP](https://github.com/trungtrinh44/DAMP)  
**DAMP** (Data augmentation via multiplicative perturbations) is a simple yet effective approach to improving neural network robustness through multiplicative weight perturbations. Unlike traditional data augmentation methods, DAMP operates directly on model weights during training, enabling improved corruption robustness without compromising clean image performance or increasing computational cost.  
  
**Key Highlights:**

* **Theoretical Foundation**: DAMP demonstrates that input corruptions can be equivalently represented as multiplicative weight perturbations, providing a theoretical basis for weight-space data augmentation.
* **Simple Implementation**: The method requires only random Gaussian sampling and pointwise multiplication, maintaining almost the same training cost as standard SGD while being fully compatible with data parallelism.
* **Breakthrough in ViT Training**: Successfully trains Vision Transformers from scratch using only basic preprocessing, achieving ResNet50-level performance (23.7% top-1 error) on ImageNet without complex augmentations.
* **Advanced Integration**: When combined with MixUp and RandAugment, DAMP significantly improves both clean and corruption performance:
   * ViT-S/16: 20.09% clean error (vs 20.25% baseline), 58.30% avg corruption error (vs 60.07% baseline)
   * ViT-B/16: 19.36% clean error (vs 20.41% baseline), 56.76% avg corruption error (vs 58.83% baseline)

**Why DAMP?** Unlike traditional approaches that rely on complex data augmentation pipelines or computationally expensive ensemble methods, DAMP provides a simple, theoretically-grounded solution to improving model robustness. Its ability to train Vision Transformers from scratch without advanced augmentations and compatibility with existing techniques makes it a practical choice for developing robust vision models.  
**Since DAMP has minimal overhead over standard training, it is particularly effective when applied to large models and datasets.**  
  
We welcome technical discussions, particularly regarding theoretical connections to other robustness methods and potential applications beyond computer vision!",emiurgo,1hap6gx,https://reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/,https://www.reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/,2024-12-10 00:38:41,7,0.77,7,0,2,0,0,False,False,True,False,False,Research,self,t3_1hap6gx
MachineLearning,[D] Seeking paper writing feedback: GPT-based Network Intrusion Detection System (arXiv published),"Hello everyone,



I'm an independent developer who has been working on applying GPT models to network intrusion detection. While I have experience in implementation, this is my first venture into academic paper writing, and I'm seeking feedback on the paper's presentation and structure.



I've recently published on arXiv:

\- Title: NIDS-GPT: TAKE PACKAGE AS LANGUAGE: ANOMALY DETECTION USING

TRANSFORMER

\- arXiv link: [https://arxiv.org/pdf/2412.04473](https://arxiv.org/pdf/2412.04473)



The paper presents a novel approach of treating each number in network packets as independent ""words"" for GPT processing. Our experiments show promising results - 100% accuracy on CICIDS2017 and car-hacking datasets under extreme imbalance conditions, and &gt;90% accuracy in one-shot learning.



As a first-time paper author, I'm seeking feedback on:

1. Paper structure and academic presentation standards

2. Visualization effectiveness and clarity

3. Methodology presentation

4. Validity of experimental comparisons

5. Strength of claims and conclusions



I'm particularly interested in feedback from:

\- Researchers in network security/intrusion detection

\- Those working with language models in non-NLP domains

\- Experienced paper writers/reviewers



The implementation is solid, but I want to ensure the paper effectively communicates the technical contributions to the academic community.



Thank you for your time and expertise!",EliaukMouse,1haufwv,https://reddit.com/r/MachineLearning/comments/1haufwv/d_seeking_paper_writing_feedback_gptbased_network/,https://www.reddit.com/r/MachineLearning/comments/1haufwv/d_seeking_paper_writing_feedback_gptbased_network/,2024-12-10 05:14:37,2,0.63,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1haufwv
MachineLearning,[D] Imbalance Dataset ,"Hi guyz , i am working on a project related to cloud computing. 
I have real time  dataset related to computing computing which i found on Internet , but the data have significant imbalance  e.g i have 4 classes two of them contain highest values while the rest two very much less .

How to play with this data to feed into ml model ( i know if i didn't balance the data then the model will be much bias towards majority class )

Need help ",zaynst,1hb0kew,https://reddit.com/r/MachineLearning/comments/1hb0kew/d_imbalance_dataset/,https://www.reddit.com/r/MachineLearning/comments/1hb0kew/d_imbalance_dataset/,2024-12-10 12:23:55,0,0.3,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hb0kew
MachineLearning,"[R] Diffusion Models, Image Super-Resolution, and Everything: A Survey","We are thrilled to share with you guys our latest survey paper on diffusion models applied to image super-resolution. You are welcome to take a look. It is also open access and published in IEEE TNNLS :) 



arXiv: [https://arxiv.org/abs/2401.00736](https://arxiv.org/abs/2401.00736)",Maleficent_Stay_7737,1h9wrv6,https://reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/,https://www.reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/,2024-12-09 00:09:15,98,0.97,98,0,1,0,0,False,False,True,False,False,Research,self,t3_1h9wrv6
MachineLearning,[P] Text-to-Video leaderboard: Compare State-Of-The-Art Text-To-Video Models,"Unlike text generation, text-to-video generation involves balancing realism, alignment, and artistic expression. But which one is the most important in terms of output quality?

We don’t know, that’s why we created a voting-based Text-to-Video Model Leaderboard inspired by the LLM Leaderboard lmarena.ai.

Currently, the leaderboard features five open-source models: HunyuanVideo, Mochi1, CogVideoX-5b, Open-Sora 1.2 and PyramidFlow, but we’re aiming to also include notable proprietary models from Kling AI, LumaLabs.ai and Pika.art.

Here’s a link to the leaderboard: [link](https://t2vleaderboard.lambdalabs.com/leaderboard/).  
We’d love to hear your thoughts, feedback, or suggestions. How do you think video generation models should be evaluated?",lambda-research,1ha54m0,https://reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/,https://www.reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/,2024-12-09 08:17:05,16,0.94,16,0,2,0,0,False,False,True,False,False,Project,self,t3_1ha54m0
MachineLearning,[D] [R] Question Answering Evaluation,"Are there any new metrics to evaluate QA systems (both open-domain and multiple choice) besides the standard Exact Match, F1, Accuracy, BLEU, ROUGE, BERTScore and so on ? I was reading a paper listing all of these metrics (https://arxiv.org/abs/2406.13232) but I’m curious if someone has released, or is currently working on, a new metric which better correlates with human judgment and/or takes into account the form in which LLMs provide answers to questions. For instance, if the models are not fine tuned, it’s hard to make them predict something like “Answer: B” (for multiple-choice QA) or to make them predict some short text like “Barack Obama” (for open-domain QA). This behaviour makes the evaluation of LLMs inconsistent and I’m wondering is someone is actively working on this. ",Debonargon,1han84i,https://reddit.com/r/MachineLearning/comments/1han84i/d_r_question_answering_evaluation/,https://www.reddit.com/r/MachineLearning/comments/1han84i/d_r_question_answering_evaluation/,2024-12-09 23:07:18,0,0.4,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1han84i
MachineLearning,has anyone come across lines in image generated by GAN ? [D],"So I have been working with GAN's for a while, for simple image generation tasks especially training them in unsupervised ways . In many of them the output generated by GAN tend to have visible lines across the images. Here is an example, this happened when I try to generate heat maps. Does any of you have any idea why this happens ?? and ways to deal with them

https://preview.redd.it/c5tuc5udsu5e1.png?width=679&amp;format=png&amp;auto=webp&amp;s=0ecb84a81ec2993a147689ae3112935cc6ecf821

",Brief_Papaya121,1haeb2u,https://reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/,https://www.reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/,2024-12-09 16:56:56,3,0.64,3,0,10,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/TylwiwTTDZWEFdDfi3mI_bSdpiMc72jBYxnwRY3ekpc.jpg,t3_1haeb2u
MachineLearning,[D] How to Ensure Fair Comparison Between Few-Shot Prompting and Fine-Tuning in NLP Experiments,"I’m working on comparing a few-shot prompting mechanism with a fine-tuned GPT model for a text classification task. However, I realised that in a 5-fold validation setup, the fine-tuned model has access to significantly more data (ex: 4 folds for training) compared to my few-shot approach, which only uses a limited number of examples for prompting (At the moment I select n samples per each class from the training fold).  

`Example Scenario: My dataset has 4 classes, total number of data is 100, and I am conducting 4-way 5-shot experiment. For the traning set I have 80 (since it's 5-fold) and for test I have 20 data samples. For the fine-tune experiment I use the whole 80 data but for the few-shot experiment I use only 20 (4*5) data from the 80 traning samples. So the approach has only access to 20 samples of the whole training set.`

This imbalance feels unfair and makes it hard to assess the true performance difference between the two approaches. How can I modify the experimental setup to ensure a fair comparison? Should I restrict the fine-tuned model to use the same examples used in the few-shot prompting mechanism?    


Would love to hear your thoughts and suggestions!  ",The_Aoki_Taki,1haf6qq,https://reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/,https://www.reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/,2024-12-09 17:32:25,2,0.67,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1haf6qq
MachineLearning,[R] Distillation-Based Colorization of 3D Neural Radiance Fields for Consistent Novel View Synthesis,"This paper introduces a knowledge distillation approach to colorize 3D neural representations (NeRF/3DGS) from grayscale multi-view images. The core idea is transferring color information from pre-trained 2D colorization models to 3D scene representations while maintaining view consistency.

Key technical aspects:
- Uses a teacher-student framework where 2D colorization models guide 3D representations
- Works with both Neural Radiance Fields and 3D Gaussian Splatting
- No additional parameters or computation needed during inference
- Handles both indoor/outdoor scenes and different types of grayscale input (IR, historical photos)
- Maintains color consistency across viewpoints through volumetric optimization

Results:
- Matches or exceeds SOTA colorization quality on standard benchmarks
- Successfully colorizes complex scenes with varying lighting/materials
- Works effectively on legacy photographs and infrared images
- Demonstrates consistent colors across novel viewpoints
- Compatible with current NeRF/3DGS implementations

I think this method could be particularly valuable for cultural heritage applications, allowing us to create immersive 3D experiences from historical black and white photographs. The IR imaging capabilities also suggest potential applications in security and surveillance where color visualization of thermal data would be useful.

I think the key strength is how it bridges the gap between 2D colorization and 3D scene understanding without requiring architectural changes to existing 3D representations. This makes it quite practical for real-world adoption.

TLDR: New method colorizes 3D neural scenes from grayscale images using knowledge distillation, works with NeRF/3DGS, maintains view consistency, no extra inference cost.

[Full summary is here](https://aimodels.fyi/papers/arxiv/chromadistill-colorizing-monochrome-radiance-fields-knowledge-distillation). Paper [here](https://arxiv.org/abs/2309.07668).",Successful-Western27,1hae9oo,https://reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/,https://www.reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/,2024-12-09 16:55:19,2,0.67,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1hae9oo
MachineLearning,[R] Monet: Mixture of Monosemantic Experts for Transformers,"**Paper**: [https://arxiv.org/abs/2412.04139](https://arxiv.org/abs/2412.04139)  
**GitHub**: [https://github.com/dmis-lab/Monet](https://github.com/dmis-lab/Monet)

**Monet** presents a novel approach to enhancing mechanistic interpretability in large language models (LLMs) through an innovative Sparse Mixture-of-Experts (SMoE) architecture. By directly incorporating sparse dictionary learning into end-to-end pretraining, **Monet** addresses the fundamental challenge of polysemanticity - where individual neurons respond to multiple unrelated concepts - while maintaining model performance.

**Key Highlights:**

* **Scalable Expert Architecture**: **Monet** introduces parameter-efficient expert decomposition methods that enable scaling to 262,144 experts per layer while ensuring total parameters scale proportionally to the square root of expert count.
* **Monosemantic Experts**: Through fine-grained expert specialization, **Monet** achieves monosemantic experts that demonstrate mutual exclusivity of knowledge, allowing transparent observation of model behavior and parametric knowledge.
* **Robust Knowledge Control**: The architecture enables precise manipulation of domain-specific knowledge, language capabilities, and toxicity mitigation without compromising general performance.

**Why Monet?**

Unlike traditional approaches using post-hoc reconstruction (like Sparse Autoencoders), **Monet** integrates interpretability directly into its architecture. This enables both transparent understanding of model internals and fundamental behavior control. By scaling monosemantic experts, Monet paves the way for more transparent and controllable language models.

We’d love to hear your feedback, questions, or any other inquiries you may have!",affjljoo3581,1ha4inl,https://reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/,https://www.reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/,2024-12-09 07:31:34,9,0.76,9,0,0,0,0,False,False,True,False,False,Research,self,t3_1ha4inl
MachineLearning,"[D] Is there such a thing as ""integrable programming""?","I come from a pure math background and have been getting up to speed at a new job in scientific AI/ML where I've been working a lot with JAX. JAX is great, love it, but I see a super common pattern where researchers will have a fully differentiable simulation and a couple of neural net architectures or something, but then there will be a bunch of relatively imprecise numerical estimations of integral values. Obviously I'm reading up on numerical methods and doing my best to restructure problems to solve more algebraically, but for my own curiosity, is there an equivalent of ""differentiable"" programming where you're handling ""integrable"" entities instead?

Obviously, this would be a much harder class of problems since you can integrate...well, everything. And that's how you end up solving PDEs on compact supports with weird ugly Holder bounds. But are there computational approaches (or hell, differentiable programming strategies I should be aware of) that move in this direction? Are there nice natural algebraic properties to be leveraged? Can you use a computational graph the same sort of way? How about, like, valid ways to extend to ""weakly-differentiable"" functional programming?

Hope this is relevant enough since it's learning JAX inspired...


edit:
Neat, thanks everyone. Long time lurker, super psyched to get some fruitful answers. I probalby should have specified that I'm not looking for fully generic solutions but most people took that leap anyways. As u/yldedly pointed me to, I think I'm basically asking for generalizations of the Risch algorithm for restricted multivariate function spaces...which [do seem to exist](https://arxiv.org/abs/1305.1481), but it's apparently an open but pretty unpopular area. Also Risch would be an absolute beast to implement...haven't actually looked at Risch-Norman but Maple has had it for a while. Bayesian quadrature and generally using Pyro is probably the most practical solution right now, thanks u/hugosc and u/daking999. I'll be waiting for the right problem to try out u/bregav 's suggestion of just autodiffing the antiderivative drictly (slick).",redwingviking,1h9ty31,https://reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/,https://www.reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/,2024-12-08 21:56:30,30,0.94,30,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1h9ty31
MachineLearning,[D] Context-aware entity recognition using LLMs,"Can anybody suggest some good models that can perform entity recognition but using LLM-level context? Such models are generally LLMs fine-tuned for Entity Recognition.
Usually, using traditional NER/ER pipelines, such as SpaCy's NER model, can only tag words that it has been trained on. Using LLMs fine-tuned for Entity Recognition (models such as GLiNER) can tag obscure entities, and not just basic entities such as Name, Place, Org, etc.",Ashwiihii,1h9stfq,https://reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/,2024-12-08 21:05:35,10,0.76,10,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h9stfq
MachineLearning,[P] Looking for daily keyword search database (any platform),"Hey there,

  
After exhaustively searching Google and trying to find APIs that would allow me to generate keyword search or post or comment frequency on any platform on a *daily* basis, I have been unable to find any providers of this type of data. Considering that this is kind of a niche request, I am dropping this inquiry here for the ML Gods of Reddit to assist.

Basically, I'm trying to create an ML model that can predict future increases/decreases in keyword usage (whether that be on Google Search or X posts; dosen't matter) on a daily basis. I've found plenty of monthly average keyword search providers but I cannot find any way to access more granulated, daily search totals for any platform. If you know of any sources for this kind of data, please drop them here... Or just tell me to give up if this is an impossible feat.",Appropriate-Touch515,1ha465g,https://reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/,https://www.reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/,2024-12-09 07:06:32,1,0.6,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1ha465g
MachineLearning,[P] 🥂 FineWeb2 dataset: A sparkling update with 1000s of languages,,PhilipsNostrum,1h9ep0e,https://reddit.com/r/MachineLearning/comments/1h9ep0e/p_fineweb2_dataset_a_sparkling_update_with_1000s/,https://huggingface.co/datasets/HuggingFaceFW/fineweb-2,2024-12-08 08:47:55,51,0.98,51,0,2,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/wJFr_ML_3llzyRU5cjBT-wmv2Y189rHFs1r363OriqY.jpg,t3_1h9ep0e
MachineLearning,[D] A collection of various LLM Sampling methods,"In the last couple months, I read about various algorithms to perform LLM sampling. I decided to build my own inference stack and implement those algorithms. 

Here is the Github repo - [https://github.com/shreyansh26/LLM-Sampling](https://github.com/shreyansh26/LLM-Sampling)

The repo includes implementations for Top-k, Top-p (nucleus), Min-p, Typical, Epsilon, Eta, Beam search, Chain-of-Thought (CoT) decoding, Constrained JSON decoding and Speculative decoding.

Personally, I found this to be a good learning experience. Sharing here in case it helps someone!",shreyansh26,1h9fe8q,https://reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/,https://www.reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/,2024-12-08 09:39:17,41,0.93,41,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h9fe8q
MachineLearning,[R] Should I Use ML Experiment Tracking Tools Like MLflow or DVC for my Academic Paper?,"Hi everyone!

I'm a Computer Science graduate currently working on machine learning experiments for a research paper. I have a dataset and plan to compare error metrics across several deep learning models.

The conference where I intent to submit my paper requires that I provide the code and dataset and I also strongly believe that reproducibility is crucial in academic research. To this end, I'm using Docker and pip-compile to make the environment as reproducible as possible.

That said, I know there are tools like MLFlow and DVC for tracking ML experiments. However, I've never seen these tools mentioned in the code accompanying academic papers.

My questions are:

1. Are there any academic papers that use ML experiment tracking tools like MLFlow or DVC?
2. Should I use these tools for my research, even it means additional work?

I'm also experimenting with DVC because it stores experiments outputs in Git. However, my project involves running many distinct experiments in a single repository (comparing multiple ML algorithms). Would DVC or another tool be the best choice for this kind of workflow? Or is using such tools overkill for academic papers?

",mrlucasrib,1h9ig1e,https://reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/,https://www.reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/,2024-12-08 13:05:06,8,0.79,8,0,5,0,0,False,False,True,False,False,Research,self,t3_1h9ig1e
MachineLearning,[Project] Simulating Kubernetes Monitoring Data for a Deep Learning Prototype—Any Thoughts?,"
We aim to build a prototype project using Deep Learning, but we require a dataset containing Kubernetes deployment metrics. I initially tried sourcing data from my company, but—spoiler alert—we obviously can’t use it.

Our current idea is to create a virtual lab with a small Kubernetes cluster running a custom app. Using JMeter, we plan to simulate random scenarios to generate traffic similar to the microservices deployments in our company.

The synthetic data generated will be used to train our prototype. Afterward, we’ll test the model by slightly modifying the JMeter scenarios and evaluating its performance against Kubernetes’ default algorithm.

What are your thoughts? I know there’s existing literature on this, but I’d love to hear your expert opinion.

Thanks!",Ok-Consequence-8863,1h9vru3,https://reddit.com/r/MachineLearning/comments/1h9vru3/project_simulating_kubernetes_monitoring_data_for/,https://www.reddit.com/r/MachineLearning/comments/1h9vru3/project_simulating_kubernetes_monitoring_data_for/,2024-12-08 23:20:46,0,0.4,0,0,0,0,0,False,False,True,False,False,Project,self,t3_1h9vru3
MachineLearning,"[D] Last Week in Medical AI: Top LLM Research Papers/Models (December 2 - December 7, 2024)","[\[D\] Last Week in Medical AI: Top LLM Research Papers\/Models \(December 2 - December 7, 2024\)](https://preview.redd.it/exeie0jxdm5e1.jpg?width=1386&amp;format=pjpg&amp;auto=webp&amp;s=d1fad8f6511ebc4dd9e8c73cc98ca9f6d45f750d)

  
**Medical LLM &amp; Models**

* Block MedCare: Blockchain AI &amp; IoT
   * This research proposes a novel Ethereum-based system for secure and efficient Electronic Health Record (EHR) management, empowering patients with data control.
* LLMs4Life: Biomedical Ontology Learning
   * This paper extends the NeOn-GPT pipeline for ontology learning using LLMs with advanced prompt engineering and ontology reuse to improve generated ontologies' domain-specific reasoning and structural depth in complex domains like life sciences.
* LLaMA II for Multimodal Diagnosis
   * This paper explores multimodal fusion methods for medical data using a transformer-based model with a LLaMA II backbone, focusing on disease classification with chest X-rays and clinical reports from the OpenI dataset.
* Compact LLM for EHR Privacy
   * This paper introduces a compact LLM framework for local deployment in healthcare settings with strict privacy requirements and limited resources.  It uses a novel preprocessing technique with information extraction methods like regular expressions to enhance smaller LLM performance on EHR data.

**Frameworks &amp; Methods**

\- RARE: Retrieval-Augmented Reasoning  
\- STORM: Strategies for Rare Events  
\- TransFair: Fair Disease Classification  
\- PePR: Performance Per Resource  
\- Medical LLM Best Practices

**LLM Applications**

\- Medchain: LLMs in Clinical Practice  
\- Query Nursing Note Summarization  
\- CLINICSUM: Patient Conversation Summaries  
\- Text Embeddings for Classifiers

**LLM Benchmarks**

\- Polish Medical Exams Transfer  
\- Single-Cell Omics Annotation  
\- LLMs in Precision Medicine  
\- Low-Resource Healthcare Challenges

**Other Models**

\- LLM Chatbot Hallucinations  
\- Multi-stage Chest X-ray Diagnosis  
\- EchoONE: Echocardiography AI  
\- Radiology Report Grounding

**Ethics &amp; Fairness**

\- Privacy in Medical Imaging  
\- Demographic Fairness in AI

**Datasets**

\- LLM Scientific Knowledge Extraction  
\- Biomedical Knowledge Review

Full thread in detail: [https://x.com/OpenlifesciAI/status/1865584829057929303](https://x.com/OpenlifesciAI/status/1865584829057929303)",aadityaura,1h9hytj,https://reddit.com/r/MachineLearning/comments/1h9hytj/d_last_week_in_medical_ai_top_llm_research/,https://www.reddit.com/r/MachineLearning/comments/1h9hytj/d_last_week_in_medical_ai_top_llm_research/,2024-12-08 12:37:06,2,0.6,2,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ui7fRqyxPWPs0C2SeEyfrrVgHu2f6NKgstvq8_5XvWs.jpg,t3_1h9hytj
MachineLearning,[R] O1 replication paper,"Hi Everyone,

Just released a paper that I think hints at how OpenAI might have developed some of O1's remarkable reasoning capabilities. TLDR- you need a small dataset of really high quality human paired with a little bit of RL

Here are some of they key take ways from the research

* Reasoning data is extremely scarce on the internet. It's very difficult to find data that really shows the problem solving process e.g hypothesis testing, backtracking etc
* RL although important is overrated by the general community. It's really the cherry on top. The human data does most of the heavy lifting. See deep-seek math for more info on this

Paper can be found here: [https://arxiv.org/abs/2412.04645](https://arxiv.org/abs/2412.04645)

Not saying this is definitively how o1 works but results suggest that this method can be used to create very similar behaviour

Paper a preprint so happy to clarify anything that's not clear.

Happy to answer any questions on the paper.",Brosarr,1h9zjf1,https://reddit.com/r/MachineLearning/comments/1h9zjf1/r_o1_replication_paper/,https://www.reddit.com/r/MachineLearning/comments/1h9zjf1/r_o1_replication_paper/,2024-12-09 02:31:35,0,0.42,0,0,7,0,0,False,False,True,False,False,Research,self,t3_1h9zjf1
MachineLearning,[D] Offline AI/ML activity for high school students?,"Next week, I am giving an hour of code presentation at a local high school. Since I run my university's artificial intelligence club, I'd like to center it around AI. I've done some AI focused activities with high schoolers with various levels of success in the past, but haven't found ""the one"" yet. Any ideas on what I could try next? I'll list what I've done so far as well as the restrictions are this particular event.

Restrictions:

1. These are random students in grades 9-12, so I can't rely on any prior computer science knowledge
2. We don't have access to computers so everything has to be offline. I *may* be able to get ipads for them though
3. We have 1 hour for the event. Filling around 45 minutes would be ideal so we don't go over and we don't end up with too much extra time.

Things I've tried before:

1. AI Kahoot + Simple lecture explaining neural networks - The students liked the kahoot, but I dramatically overestimated their knowledge (and interest) in AI. Stupidly thought at least a handful of them would have at least heard of a neural net, but not a single student had lmao
2. Wordle AI pseudocode and code along - I had the students get into groups and brainstorm how they would make an algorithm to solve the wordle. Then I coded their solutions on the projector and we competed against my algorithm. This one was cool, but I think it only really works if the students are *really engaged* and I really don't think this group will be that engaged.
3. Image classification code along - I've done this by myself on the projector (bad) and as a group with students with some CS experience (decent). But this one wouldn't work for this event I just thought I'd include it
4. Turing test activity - Have the students guess which answer came from ChatGPT and which came from a student volunteer. The volunteer will leave the room and the rest of the students will ask a question (like ""what is the meaning of life"" or something). I'll write the volunteer's answer and ChatGPT's answer on a powerpoint slide and have the student guess which was GPT. This has been really successful with middle school students in the past, but I worry it's a bit too childish for high schoolers.

\---

I tend to struggle to figure out activities for high school students because I don't want to undermine their intelligence but I don't want to throw a complicated activity at them that they don't understand.",j0ngle6421,1h9ks8v,https://reddit.com/r/MachineLearning/comments/1h9ks8v/d_offline_aiml_activity_for_high_school_students/,https://www.reddit.com/r/MachineLearning/comments/1h9ks8v/d_offline_aiml_activity_for_high_school_students/,2024-12-08 15:08:37,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h9ks8v
MachineLearning,"[N] Sama, an AI sweatshop, pays workers in Kenya $2 an hour to filter and label porn, beastiality, suicide, child abuse, for hours on end!!",,BotherBubbly5096,1h8nhbh,https://reddit.com/r/MachineLearning/comments/1h8nhbh/n_sama_an_ai_sweatshop_pays_workers_in_kenya_2_an/,https://youtu.be/qZS50KXjAX0,2024-12-07 07:38:08,321,0.85,321,0,123,0,0,False,False,False,False,False,News,https://b.thumbs.redditmedia.com/6xyP-Tq6WRzMpjniGg5h_3Z_bhf29lyA0LV_LQ19-1s.jpg,t3_1h8nhbh
MachineLearning,[P] I got too frustrated trying to test all these AI cookbooks and recipes,"Over the last year of building AI enabled SaaS applications I became increasingly frustrated at the developer experience of going from AI RAG cookbooks authored in jupyter notebooks to integrating it into my application. Notebooks are great and all but it's incredibly hard to test which part of it was actually important for my app. This led me down the road of having to understand every piece of code in each notebook, deciphering what was important, somehow build an API server as a POC to then hook it into my app. The feedback loop was excruciatingly long, painful, and most of the time I canned the POC because it wasn't quite what I wanted.

this is when it dawned on me that the roles in the AI developer world are fractured into two. Data Scientists and AI devs want easy notebooks to test methods and techniques but do not care to ship something that can be easily be consumed by applications.

In the other camp lies application devs, they just want simple API's that they can use to test quickly and verify these AI methods enhance their application.

Enter KitchenAI.

A way to bridge the gap between the two by converting AI related Jupyter notebooks into a ready made production API server so that it becomes easy to test various cookbooks, recipes, and techniques. Shortening the development cycle in half while giving users a complete local experience with the ability to share them as docker containers.

Completely vendor agnostic and framework agnostic, the goal is to give developers the most about of freedom to use the libraries they already feel most comfortable using.

It comes with a plugin architecture so I envision our team and the community building all sorts of llmops type plugins like evaluation frameworks, observability, prompt management and more.

A lot of hard work was put to provide something that is totally open source, local, and with battle tested technology like Django so that developers didn't have to rely on 3rd party providers.

We’ve launched this repo under Apache license so any developer can use the tool. We're working hard to provide a managed cloud version with much deeper integrations, metrics, analytics, and workflows for those that want have more complex demands

Give it a spin: [https://github.com/epuerta9/kitchenai.](https://github.com/epuerta9/kitchenai) Let us know what you think!",wait-a-minut,1h9wcdo,https://reddit.com/r/MachineLearning/comments/1h9wcdo/p_i_got_too_frustrated_trying_to_test_all_these/,https://www.reddit.com/r/MachineLearning/comments/1h9wcdo/p_i_got_too_frustrated_trying_to_test_all_these/,2024-12-08 23:48:32,0,0.42,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1h9wcdo
MachineLearning,"[P] I cannot find this open-source transformer on GitHub, released recently, for the life of me.
","There was a paper released along with a GitHub repository of an extremely well-made transformer designed for testing out new components. But I can't find it! It's not one of the ones that has existed like HuggingFace ones. Any clue?

",Breck_Emert,1h8zlz3,https://reddit.com/r/MachineLearning/comments/1h8zlz3/p_i_cannot_find_this_opensource_transformer_on/,https://www.reddit.com/r/MachineLearning/comments/1h8zlz3/p_i_cannot_find_this_opensource_transformer_on/,2024-12-07 19:05:21,14,0.64,14,0,21,0,0,False,False,True,False,False,Project,self,t3_1h8zlz3
MachineLearning,"How do you manage resources or optimize cost when training models in cloud services like aws sagemaker, or gcp vertex ai? [D]","Hey all, I've been using sagemaker quite a bit lately for training ML models and doing deployments. I know enough about aws and instance types to create training nodes that have enough capacity to train my models, but many times I am underutilizing RAM, GPU memory, or CPUs, so it feels like this leads to a lot of waste (and extra cost).  
How do you guys figure out what type of instance or resources would best fit your needs without being too wasteful?  
Is there any way to adjust resources automatically, or any library that could handle that for you?",InformationEmpty1440,1h93rlt,https://reddit.com/r/MachineLearning/comments/1h93rlt/how_do_you_manage_resources_or_optimize_cost_when/,https://www.reddit.com/r/MachineLearning/comments/1h93rlt/how_do_you_manage_resources_or_optimize_cost_when/,2024-12-07 22:17:25,5,0.86,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h93rlt
MachineLearning,[R] For a change of topic: some nonLLM focused work of mine: Bias-Free Sentiment Analysis through Semantic Blinding and Graph Neural Networks,"In my academic field (social sciences) I deal with the problem of bias in SA models. My previous work showed that deep learning SA systems inherit bias (e.g. nonrepresentative of the population political bias) from annotators: 

https://arxiv.org/abs/2407.13891

Now I devised a solution that used a technique I call semantic blinding to provide only the bare necessary information for the model to predict emotions in text, leaving no signal for the model to overfit and produce bias from:

https://arxiv.org/abs/2411.12493

Interested to hear your thoughts before I publish the SProp Gnn.

Do you think it could be useful beyond the academia?



",Hub_Pli,1h8meas,https://reddit.com/r/MachineLearning/comments/1h8meas/r_for_a_change_of_topic_some_nonllm_focused_work/,https://i.redd.it/vh80i11ndd5e1.jpeg,2024-12-07 06:21:47,49,0.86,49,0,10,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/sSwXTBHlRlELGHTRvPNAxo6LSQVRdxL5HTNiMSYTfTs.jpg,t3_1h8meas
MachineLearning,[D] AAAI 2025 Phase 2 Decision,"When would the phase 2 decision come out?  
I know the date is December 9th, but would there be chances for the result to come out earlier than the announced date?  
or did it open the result at exact time in previous years? (i.e., 2024, 2023, 2022 ....)

Kinda make me sick to keep waiting.",No-Style-7975,1h8kkjv,https://reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,https://www.reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,2024-12-07 04:27:30,45,0.93,45,0,252,0,0,False,False,True,False,False,Discussion,self,t3_1h8kkjv
MachineLearning,[P] Extract Transcripts with Positive Emotions in batch,"Check out this example project on how to find transcripts of audio recordings with positive emotions. A good example of a project demonstrating of extract actionable insights from audio!

It takes **common voice** dataset of audio files from hagging face, applies emotion recognition model and **whisper-tiny** model for the transcripts. All is organized in a nice looking batch pipeline. 

An interesting detail - No need to extract archives! This pipeline analyzes audio files directly from tar archives, saving you extra steps.

Video: [https://www.youtube.com/watch?v=OCm5W0L5BTU](https://www.youtube.com/watch?v=OCm5W0L5BTU)  
Colab notebook: [https://colab.research.google.com/github/iterative/datachain-examples/blob/main/audio/hf\_common\_voice.ipynb](https://colab.research.google.com/github/iterative/datachain-examples/blob/main/audio/hf_common_voice.ipynb)  
Jupyter Notebook: [https://github.com/iterative/datachain-examples/blob/main/audio/hf\_common\_voice.ipynb](https://github.com/iterative/datachain-examples/blob/main/audio/hf_common_voice.ipynb)",dmpetrov,1h92b70,https://reddit.com/r/MachineLearning/comments/1h92b70/p_extract_transcripts_with_positive_emotions_in/,https://www.reddit.com/r/MachineLearning/comments/1h92b70/p_extract_transcripts_with_positive_emotions_in/,2024-12-07 21:08:30,1,0.6,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1h92b70
MachineLearning,[R]  GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?,"Hi everyone,

I’m currently working through the recent paper “GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?”, but I’ve run into some issues and was hoping someone here might have insights.

* Posterior Distribution: In the implementation, a posterior distribution is used, but I couldn’t find the formula or explanation in the paper. Does anyone know where this comes from or how it’s derived?
* Asynchronous Model: The paper and its implementation don’t seem entirely consistent when it comes to the asynchronous model. Specifically:
   * Is the generation process done step-by-step asynchronously?
   * Or does it first denoise the attribute vectors entirely before moving on to edge denoising?

I’ve tried searching online, but since this is a new paper, there isn’t much discussion or documentation yet. Any help, advice, or pointers would be greatly appreciated!",Noname_emanon_,1h8z44t,https://reddit.com/r/MachineLearning/comments/1h8z44t/r_graphmaker_can_diffusion_models_generate_large/,https://www.reddit.com/r/MachineLearning/comments/1h8z44t/r_graphmaker_can_diffusion_models_generate_large/,2024-12-07 18:43:09,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h8z44t
MachineLearning,[R] JAX vs TensorFlow-XLA ,"
Few months ago, I migrated from TF 2.0 to Jax. I found that jax is significantly faster than Tf. I noticed in the official documentation that it relies on XLA default that uses JIT compilation which makes execution faster. I also noticed that TF graphs also have option to enable JIT compilation with XLA. But still jax dominates TF with XLA. I just want to know why.",Odd-Detective289,1h8j2e5,https://reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,https://www.reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,2024-12-07 03:02:19,16,0.94,16,0,7,0,0,False,False,True,False,False,Research,self,t3_1h8j2e5
MachineLearning,[D] Any OCR recommendations for illegible handwriting?,"
Has anyone had experience using an ML model to recognize handwriting like this? The notebook contains important information that could help me decode a puzzle I’m solving. I have a total of five notebooks, all from the same person, with consistent handwriting patterns. My goal is to use ML to recognize and extract the notes, then convert them into a digital format.

I was considering Google API after knowing that Tesseract might not work well with illegible samples like this. However, I’m not sure if Google API will be able to read it either. I read somewhere that OCR+ CNN might work, so I’m here asking for suggestions. Thanks! Any advice/suggestions are welcomed! ",SpaceSheep23,1h7x5us,https://reddit.com/r/MachineLearning/comments/1h7x5us/d_any_ocr_recommendations_for_illegible/,https://www.reddit.com/gallery/1h7x5us,2024-12-06 08:53:03,209,0.91,209,0,174,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/LeDstkPiGT-yO7zkyvRRyx6sE33qB2W32UGe1t81Kkk.jpg,t3_1h7x5us
MachineLearning,How to solve the STT Cutoff Problem [D],"Hello folks, 

I've been working on an agentic solution where you can have an autonomous agent taking live calls. We're using a pipeline of Speech to Text, LLM for generating responses and then Text to Speech. In this pipeline, Speech to text is causing some issues because it's difficult to determine when exactly a sentence is over since the user can take pauses. Moreover, when multiple inputs go into LLM, multiple responses are generated and they queue up for Text to speech. How would you solve this problem? How would you also handle cases where the user interrupts the agent?",Leo2000Immortal,1h8r32q,https://reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,https://www.reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,2024-12-07 12:04:55,1,0.6,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h8r32q
MachineLearning,[D]Stuck in AI Hell: What to do in post LLM world,"
Hey Reddit,

I’ve been in an AI/ML role for a few years now, and I’m starting to feel disconnected from the work. When I started, deep learning models were getting good, and I quickly fell in love with designing architectures, training models, and fine-tuning them for specific use cases. Seeing a loss curve finally converge, experimenting with layers, and debugging training runs—it all felt like a craft, a blend of science and creativity. I enjoyed implementing research papers to see how things worked under the hood. Backprop, gradients, optimization—it was a mental workout I loved.

But these days, it feels like everything has shifted. LLMs dominate the scene, and instead of building and training models, the focus is on using pre-trained APIs, crafting prompt chains, and setting up integrations. Sure, there’s engineering involved, but it feels less like creating and more like assembling. I miss the hands-on nature of experimenting with architectures and solving math-heavy problems.

It’s not just the creativity I miss. The economics of this new era also feel strange to me. Back when I started, compute was a luxury. We had limited GPUs, and a lot of the work was about being resourceful—quantizing models, distilling them, removing layers, and squeezing every bit of performance out of constrained setups. Now, it feels like no one cares about cost. We’re paying by tokens. Tokens! Who would’ve thought we’d get to a point where we’re not designing efficient models but feeding pre-trained giants like they’re vending machines?

I get it—abstraction has always been part of the field. TensorFlow and PyTorch abstracted tensor operations, Python abstracts C. But deep learning still left room for creation. We weren’t just abstracting away math; we were solving it. We could experiment, fail, and tweak. Working with LLMs doesn’t feel the same. It’s like fitting pieces into a pre-defined puzzle instead of building the puzzle itself.

I understand that LLMs are here to stay. They’re incredible tools, and I respect their potential to revolutionize industries. Building real-world products with them is still challenging, requiring a deep understanding of engineering, prompt design, and integrating them effectively into workflows. By no means is it an “easy” task. But the work doesn’t give me the same thrill. It’s not about solving math or optimization problems—it’s about gluing together APIs, tweaking outputs, and wrestling with opaque systems. It’s like we’ve traded craftsmanship for convenience.

Which brings me to my questions:

1. Is there still room for those of us who enjoy the deep work of model design and training? Or is this the inevitable evolution of the field, where everything converges on pre-trained systems?


2. What use cases still need traditional ML expertise? Are there industries or problems that will always require specialized models instead of general-purpose LLMs?


3. Am I missing the bigger picture here? LLMs feel like the “kernel” of a new computing paradigm, and we don’t fully understand their second- and third-order effects. Could this shift lead to new, exciting opportunities I’m just not seeing yet?


4. How do you stay inspired when the focus shifts? I still love AI, but I miss the feeling of building something from scratch. Is this just a matter of adapting my mindset, or should I seek out niches where traditional ML still thrives?



I’m not asking this to rant (though clearly, I needed to get some of this off my chest). I want to figure out where to go next from here. If you’ve been in AI/ML long enough to see major shifts—like the move from feature engineering to deep learning—how did you navigate them? What advice would you give someone in my position?

And yeah, before anyone roasts me for using an LLM to structure this post (guilty!), I just wanted to get my thoughts out in a coherent way. Guess that’s a sign of where we’re headed, huh?

Thanks for reading, and I’d love to hear your thoughts!

TL;DR: I entered AI during the deep learning boom, fell in love with designing and training models, and thrived on creativity, math, and optimization. Now it feels like the field is all about tweaking prompts and orchestrating APIs for pre-trained LLMs. I miss the thrill of crafting something unique. Is there still room for people who enjoy traditional ML, or is this just the inevitable evolution of the field? How do you stay inspired amidst such shifts?

Update: Wow, this blew up. Thanks everyone for your comments and suggestions. I really like some of those. This thing was on my mind for a long time, glad that I put it here. Thanks again!",Educational_News_371,1h7jg87,https://reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,2024-12-05 20:49:57,788,0.97,788,0,218,0,0,False,False,True,False,False,Discussion,self,t3_1h7jg87
MachineLearning,[R] Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis,"New paper and code for the scale-wise transformer for fast text-to-image generation from our team at Yandex Research

Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being faster than distilled diffusion models.

Code with checkpoints: [https://github.com/yandex-research/switti](https://github.com/yandex-research/switti)

[Generation examples](https://preview.redd.it/7jy3jfxhi95e1.png?width=3094&amp;format=png&amp;auto=webp&amp;s=e80ed27c0b746ec782026581500582a5dd03555d)

",_puhsu,1h85z2c,https://reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,https://www.reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,2024-12-06 16:58:21,13,0.82,13,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/Io25PLG5YopCFj4DOTGJ-MxIEcQdXRQbpwevn2M-Kts.jpg,t3_1h85z2c
MachineLearning,[R] Agentic Retrieval Augmented Generation with Memory,"Imagine a customer support chatbot for an e-commerce platform that retrieves relevant product details from its knowledge base and performs web searches for additional information. Furthermore, it remembers past conversations to deliver a seamless and personalized experience for returning users.   
  
Here is how it works:  
  
\- Store your own data in the knowledge base—in our case, a Website URL.  
\- Convert the data into embeddings and save it in the Qdrant Vector Database.  
\- Use phidata Agentic Workflow to combine Tools, LLM, Memory, and the Knowledge Base.

Code Implementation Video: [https://www.youtube.com/watch?v=CDC3GOuJyZ0](https://www.youtube.com/watch?v=CDC3GOuJyZ0)",External_Ad_11,1h8945d,https://reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,https://www.reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,2024-12-06 19:10:50,6,0.67,6,0,1,0,0,False,False,True,False,False,Research,self,t3_1h8945d
MachineLearning,[D] How does OpenAI’s O1 outperform others in math despite limitations noted in recent papers?,"Recent research has revealed that state-of-the-art LLMs often struggle with mathematical reasoning:

1. The GSM-Symbolic benchmark highlights that LLMs frequently fail when numerical values or question wording change, suggesting reliance on memorization rather than true mathematical understanding ([source](https://arxiv.org/pdf/2410.05229.pdf)).
2. Logical reasoning studies, like the AIW problem, show inconsistent performance even for basic reasoning tasks ([source](https://arxiv.org/pdf/2406.02061.pdf)).
3. Furthermore, research indicates LLMs lack effective self-correction capabilities, with performance degrading after multiple iterations ([source](https://arxiv.org/pdf/2310.01798.pdf)).

Despite these challenges, OpenAI’s new O1 model reportedly exceeds all other models in math benchmarks. How does it address these known issues in mathematical reasoning, such as:

* Reliance on memorization instead of understanding?
* Inconsistencies in reasoning across problem variations?
* Inability to self-correct errors effectively?

Would love to hear insights or hypotheses!",AImSamy,1h7vj5t,https://reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,2024-12-06 06:53:09,60,0.88,60,0,42,0,0,False,False,True,False,False,Discussion,self,t3_1h7vj5t
MachineLearning,[D] Have we officially figured out yet how O1 models differ from previous models?,"Edit: I have misworded the title as if OpenAI would confirm how O1 was implemented. I have changed the text to reflect what I meant say.



I really want to deep dive into the technicals of how the O1 models perform better than previous models.

Have researchers come to any definitive agreement as to what OpenAI could have possible done to achieve O1?

From reading online I hear about MCTS, COT... etc, but are any of these methods in large agreement by researhers?
",Daveboi7,1h7zfjg,https://reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,2024-12-06 11:37:51,17,0.83,17,0,28,0,0,False,False,True,False,False,Discussion,self,t3_1h7zfjg
MachineLearning,[D] Encode over 100 million rows into embeddings ,"Hey everyone,

I'm working on a pipeline to encode over **100 million rows** into embeddings using **SentenceTransformers**, **PySpark**, and **Pandas UDF** on **Dataproc Serverless**.

Currently, it takes several hours to process everything. I only have one column containing sentences, each under 30 characters long. These are encoded into **64-dimensional vectors** using a custom model in a Docker image.

At the moment, the job has been running for over **12 hours** with **57 executors** (each with **24GB of memory and 4 cores**). I’ve partitioned the data into **2000 partitions**, hoping to speed up the process, but it's still slow.

Here’s the core part of my code:

    F.pandas_udf(returnType=ArrayType(FloatType()))
    def encode_pd(x: pd.Series) -&gt; pd.Series:
        try:
            model = load_model()
            return pd.Series(model.encode(x, batch_size=512).tolist())
        except Exception as e:
            logger.error(f""Error in encode_pd function: {str(e)}"")
            raise

The `load_model` function is as follows:

    def load_model() -&gt; SentenceTransformer:
        model = SentenceTransformer(
            ""custom_model"", 
            device=""cpu"", 
            cache_folder=os.environ['SENTENCE_TRANSFORMERS_HOME'], 
            truncate_dim=64
        )
        return model

I tried broadcasting the model, but I couldn't refer to it inside the Pandas UDF.

Does anyone have suggestions to optimize this? Perhaps ways to load the model more efficiently, reduce execution time, or better utilize resources?",nidalap24,1h7xnce,https://reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,2024-12-06 09:29:37,14,0.8,14,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h7xnce
MachineLearning,[D] How to actually prevent overfitting in practice in ScikitLearn ?,"We all saw in class the trade off between bias and variance, that we don't want our train loss to keep going down and our test loss go up. 

But in practice I feel like doing hyperparameter tuning for classic ML models with GridSearchCV / BayesSearchCV is not enough. Even though I do cross validation, the search.best\_model obtained at the end is almost always overfitting. 

How can you actually perform a search that will give you a robust generalized model with higher chances ? ",desslyie,1h8paqz,https://reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,https://www.reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,2024-12-07 09:55:06,0,0.29,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h8paqz
MachineLearning,[R] Zero shot Meme-interpretability of LLMs,"Head to head of meme-interpretability with the same image and text prompt!

Anecdotal but interesting responses. 

Also clear winner!

",No_Cartoonist8629,1h8nc78,https://reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,https://www.reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,2024-12-07 07:27:43,0,0.33,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1h8nc78
MachineLearning,[D] selective transfer learning ,"Hello everyone,

I am looking for methods that can automatically categorize and select layers from  for transfer learning. If you know any such methods or research please let me know or share. 

Thanks ",reshail_raza,1h8cawc,https://reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,https://www.reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,2024-12-06 21:30:44,0,0.5,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h8cawc
MachineLearning,[R] Towards Time Series Reasoning with LLMs,,HydrousIt,1h7sr3n,https://reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/,https://arxiv.org/abs/2409.11376,2024-12-06 04:06:32,12,0.88,12,0,2,0,0,False,False,False,False,False,Research,default,t3_1h7sr3n
MachineLearning,[R] Mastering Board Games by External and Internal Planning with Language Models - DeepMind,"Paper: https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf

Abstract:

While large language models perform well on a range of complex tasks (e.g., text generation, question
answering, summarization), robust multi-step planning and reasoning remains a considerable challenge
for them. In this paper we show that search-based planning can significantly improve LLMs’ playing
strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We
introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo
Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search,
the model directly generates in-context a linearized tree of potential futures and a resulting final choice.
Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and
value functions across these games. We find that our pre-training method minimizes hallucinations, as
our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and
external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level
performance in chess while operating on a similar move count search budget per decision as human
Grandmasters. The way we combine search with domain knowledge is not specific to board games,
suggesting direct extensions into more general language model inference and training techniques.",RobbinDeBank,1h7nshy,https://reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,2024-12-05 23:58:37,11,0.88,11,0,1,0,0,False,False,True,False,False,Research,self,t3_1h7nshy
MachineLearning,[D] Multimodal AI,"Multimodal AI is changing the game by combining text, images, and even video into a single, cohesive system. It’s being talked about as a major leap in AI capabilities.

What industries do you think will benefit the most from this tech? And are there any challenges you see in integrating these models into everyday use?

Would love to hear everyone's thoughts!",Frosty_Programmer672,1h8enzy,https://reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,https://www.reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,2024-12-06 23:17:54,0,0.17,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h8enzy
MachineLearning,[D] How to remove noise in this dataset,"I have a dataset that, when plotted, shows a noisy black line. I'd like to smooth out this noise to get a cleaner trend line (similar to the red line shown). What methods would you recommend for noise reduction?

https://preview.redd.it/p8q0i4f9h45e1.png?width=1574&amp;format=png&amp;auto=webp&amp;s=29cd8c82af7b54a1d3da22655502fd5cf406e807",mrtule,1h7ohd0,https://reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,2024-12-06 00:30:30,6,0.88,6,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1h7ohd0
MachineLearning,U-Net Vs Attention U-Net [D],"Hello folks,

Young researcher here, working on a in-house dataset to build a foundational model for a interesting use-case. But I have thesis to finsh, which will be just the tail of my current research.

For my thesis, we have decided to have a subsection for comparing how my segmentation results differ when used attention blocks are used within a U-Net. I've referred few papers on how this works and how can this be implemented.

Results are promising (att unet outperformimg unets, nothing suprising) but I see a concerning opposing point i.e. attention Unet having more number of parameters that the unet. Is there a way I can conduct this study where I compare results with and without attention? And there are no other additional factors influencing the results (layers, params, etc).

Does conducting ablation study makes sense in this case? I've not seen any other paper comparing similar use-case using this study.

Any papers I can look through, suggestions and tips are welcome.",ade17_in,1h7cjnd,https://reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,2024-12-05 16:03:47,26,0.93,26,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7cjnd
MachineLearning,"Image Generation Model Evaluation Challenge (Würstchen, KOALA, PixArt-α) [P]","# Project Description:

We are inviting skilled professionals to participate in an evaluation challenge to produce a GenAI image based upon a prompt and a set of component images.  The candidate may choose whatever models they like to complete the task, so long as they are open source. 

The results will be reviewed and compared across participants, and the candidate with the most effective and high-quality outputs will be selected for a larger, production-focused engagement. 

# Entry Process:

Submit your Github handle and intention to participate to email acr0batproduce@gmail.com.  Provide a short bio and resume in your email.  If you’re selected, we will provide you access to a repository to contribute to.  

**Challenge Scope:**

1. **Model Setup and Testing**:
   * Create a unique set of prompts to test your image generation model.  A set of [sample prompts](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) have been provided for your convenience.  A minimum of 3 tests are required, but candidates may provide more if they wish.
   * For each image generation, provide at least 3 component images to be used in the final output.  A set of [component images](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.p13d4p443fpl) related to the [sample prompt](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) has been provided for your convenience.
   * Develop the model, and test its performance with your sample input prompts and component images.  Source code must be pushed to the main branch of the provided repo.
   * Provide the output images in ./static in the repo.
   * Document your findings and results
2. **Evaluation Criteria**:
   * **Image Quality**: The final image produced should incorporate all the features from your sample prompt, and component images.  Above all, the items in the component images need to be naturally incorporated into the final image.  They should not be significantly distorted, or look like they were copy and pasted from the input images.    
   * **Brand Element Incorporation**: Expanding upon the above point, the final image must accurately reflect the input component images.  So, if the input image is a Rolex Oyster Perpetual Day-Date 40 in 18 kt yellow gold with a champagne colour, diamond-set dial, fluted bezel and a President bracelet, then the output image must incorporate that same product.
   * **Creative Flexibility**: Generate diverse variations of prompts, component images and output images.
   * **Customization**: Showcase how well your model responds to different parameters.
   * **Code Quality**: All aspects of code quality will be assessed. Examples include: repo structure, IaC, CI/CD, unit testing, performance, documentation, etc.
   * **Extensibility**: Your sample prompts, input images, and output images will be used to quickly screen for accuracy and limit submissions; however, importantly, your model will be tested against our internal prompts and input images to gauge how well it performs on different types of problems. 
3. **Deliverables**:
   * The repo structure is up to you, but the README should make it clear where your model, input images, output images and documentation reside. 

# Challenge Benefits:

* The candidate with the most effective and high-quality outputs will be selected for a larger, production-focused project with a significant budget.
* This is an opportunity to contribute to an innovative, high-growth startup that has already secured investor funding and is positioned for significant market impact.
* Gain the opportunity to showcase your expertise in image generation models and secure a long-term collaboration.

# Requirements:

* Proficiency in all areas of data science, with a focus on AI image generation
* Expert in Python, SQL, and at least 1 cloud provider such as AWS or GCP 
* A compute environment will not be provided; the candidate can develop the solution locally or on the cloud, but at their own expense.
* Candidate must comply with provided NDA

# Submission Guidelines:

* Entry Deadline: 1/1/25
* Project Deadline: 1/7/25
* All source code should be submitted to the repo by the deadline; the candidate will have their access revoked on that date.
* Provide a short bio in your README, and relevant contact information. 

# Selection Process:

* All submissions will be reviewed and compared based on the outlined criteria.
* The most effective and high-quality submission will result in the candidate being awarded a contract for a larger production project.

**I**f you're interested in participating, please reach out! ",AlpacaRampage,1h7hg7w,https://reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,2024-12-05 19:26:31,8,0.79,8,0,0,0,0,False,False,True,False,False,Project,self,t3_1h7hg7w
MachineLearning,[R] ReVersion: Learning Relation Prompts from Images for Controlled Diffusion Generation,"ReVersion introduces a novel approach for learning and transferring visual relationships using diffusion models. Rather than focusing solely on object appearance, it learns how objects interact with each other through relation prompts and specialized sampling techniques.

Key technical aspects:
- Uses frozen pre-trained text-to-image diffusion model as foundation
- Implements relation-steering through contrastive learning to guide prompts toward relationship-rich latent spaces
- Employs relation-focal sampling to emphasize high-level interactions over low-level details
- Creates relation prompts that capture spatial and interactive relationships between objects
- Introduces new benchmark dataset for evaluating relation inversion methods

Results:
- Outperforms existing methods in preserving object relationships while allowing appearance flexibility
- Shows strong performance on spatial relationships like ""on top of"", ""next to"", ""inside""
- Successfully transfers learned relationships to novel object pairs
- Maintains relationship consistency across different styles and contexts

I think this approach could be particularly valuable for improving automated image generation systems that need to handle complex scenes with multiple interacting objects. The ability to learn and transfer relationships, rather than just appearances, could help bridge the gap between current image generation capabilities and human-like understanding of how objects interact in space.

I think the relation-focal sampling technique could also have applications beyond just relationship learning - it might be useful anywhere we need to emphasize high-level features over low-level details in diffusion models.

TLDR: New method learns visual relationships from images using diffusion models, introduces relation-steering and relation-focal techniques, shows strong results on spatial relationship preservation and transfer.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reversion-diffusion-based-relation-inversion-from-images). Paper [here](https://arxiv.org/abs/2303.13495).",Successful-Western27,1h7afj8,https://reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,2024-12-05 14:30:05,14,0.9,14,0,0,0,0,False,False,True,False,False,Research,self,t3_1h7afj8
MachineLearning,[D] My fine-tuning loss looks weird,"I am finetuning Qwen2.5 instruct using qLoRA, for a instruction tuning like dataset with around 50k samples, and my training loss is looking weird. What might be the issue, and how can i possibly fix it? Finetuning details are as following, along with training loss graphs:

Code:

\`\`\`  
model, tokenizer = FastLanguageModel.from\_pretrained(

model\_name = ""Qwen/Qwen2.5-32B-Instruct"",

max\_seq\_length = max\_seq\_length,

dtype = None,

load\_in\_4bit = True,

)



\# Do model patching and add fast LoRA weights

model = FastLanguageModel.get\_peft\_model(

model,

r = 64,

target\_modules = \[""q\_proj"", ""k\_proj"", ""v\_proj"", ""o\_proj"",

""gate\_proj"", ""up\_proj"", ""down\_proj"",\],

lora\_alpha = 128,

lora\_dropout = 0, # Supports any, but = 0 is optimized

bias = ""none"",    # Supports any, but = ""none"" is optimized

use\_gradient\_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context

random\_state = 3407,

max\_seq\_length = max\_seq\_length,

use\_rslora = True,  # We support rank stabilized LoRA

loftq\_config = None, # And LoftQ

)



from trl import SFTTrainer

from transformers import TrainingArguments

from unsloth import is\_bfloat16\_supported



trainer = SFTTrainer(

model = model,

tokenizer = tokenizer,

train\_dataset = dataset\['train'\],

dataset\_text\_field = ""text"",

max\_seq\_length = max\_seq\_length,

dataset\_num\_proc = 2,

packing = False,

args = TrainingArguments(

per\_device\_train\_batch\_size = 4,

gradient\_accumulation\_steps = 2,

warmup\_steps = 5,

num\_train\_epochs = 3,

learning\_rate = 0.0002,

fp16 = not is\_bfloat16\_supported(),

bf16 = is\_bfloat16\_supported(),

logging\_steps = 10,

optim = ""adamw\_8bit"",

weight\_decay = 0.01,

lr\_scheduler\_type = ""linear"",

seed = 69,

output\_dir = ""outputs"",

report\_to = ""wandb"",

save\_strategy = ""steps"",

save\_steps = 50,

save\_total\_limit=10

),

)  
\`\`\`

Training Loss:

https://preview.redd.it/s2vn2z44y55e1.png?width=2888&amp;format=png&amp;auto=webp&amp;s=a4e1038e9c27dae96d7e25fcb5db852c794efd97

",Raise_Fickle,1h7u38s,https://reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,https://www.reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,2024-12-06 05:22:31,2,0.62,2,0,6,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/I78OJiWF97hbVSTiF0mWFfXxPxZbdE3PavnYvIriLI8.jpg,t3_1h7u38s
MachineLearning,[D] Any public LLM inference APIs with input token log-probs?,"Does anyone know of any services that offer input token log-probs from open source LLMs like LLama-8B?

I'm looking for a cost-effective way to host an LLM-based application with low traffic, so I'd like per-query or per-token pricing, rather than per-hour GPU rental. It's unfortunately dependent on direct-access to log-probs on user-provided tokens.

None of the ""chat completion"" APIs I've found seem to expose this. It makes sense for private models, but for open source models, I don't see any downside to exposing it.",severed-identity,1h7o30r,https://reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,https://www.reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,2024-12-06 00:11:38,2,0.75,2,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h7o30r
MachineLearning,[D] What approaches can we use to train an open vocabulary or image referential detector for configurable specificity?,"Open vocabulary object detectors allow you to pass in a prompt and an image and attempt to output bounding boxes around objects matching your prompt. Image referential detectors allow you to pass in an image of an object as the prompt and a target image and attempts to output bounding boxes around objects matching the image prompt in the target image.   
  
For a reference, [YOLOWorld](https://github.com/AILab-CVC/YOLO-World) provides both image referential and open vocabulary modes.  
  
The idea I've been toying with is whether there is a good way to train for greater control over specificity. For example, If I pass in an image of a golden retriever, am I looking for golden retrievers specifically? All dogs? All animals? 

Language is a bit more specific, but the same principle can apply. If I search for red cars, do red trucks count? Do maroon cars? In my experience, trying to be too specific textually with OVD models causes erratic behavior. IE, ""A red car or van but not truck "" would give bad performance, as it doesn't really match what would be in grounding captions.

My initial idea for how to systematically define the distance between potential targets and the query is via embedding distance. If I take a phrase grounding dataset, I could compute embeddings separately for each crop of a region and for its corresponding text using a model like CLIP.

A sample training process would be like this

* Select a random image. Select a random image crop embedding.
* Select a random similarity threshold.
* Do an approx KNN using that random image crop embedding, stopping once we reach a sample with an embedding above the similarity threshold. This is our prompt embedding If we selected an image region embedding, we are doing image referential detection. If we selected a text embedding, we are doing open vocabulary detection.
* Calculate prompt embedding similarity to all image crop embeddings in the primary image. Mark all objects with similarity above the threshold as positive examples.
* Run the network, using the selected image, prompt embedding, and similarity threshold as input. Use the previously calculated positives examples as labels.

  
Does anyone know of any papers that work with similar ideas, or have thoughts on whether this process would be useful or could be improved? I'm pretty early into looking into this, so just references or even field terms that would point me in the right direction would be helpful.

Other ideas 

* Always detect all objects given a target phrase, but set the label confidence equal to the embedding similarity. 
* Make the model capable of handling multiple input queries corresponding to the same object, to better indicate the intended domain. Possibly with both negative and positive queries. Not sure how to train yet, possibly sample clusters of similar embeddings from the dataset to build prompt sets.
* Perform instructional tuning, similar to what is done for some LLMs and VLMs, to make the model better handle complex text prompts, and allow instructional text prompts to be paired with images for image referential mode.

  
Related questions

*    Is CLIP still standard for computing text and image embeddings that share a unified embedding space?",Revolutionary-Fig660,1h7knty,https://reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,https://www.reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,2024-12-05 21:40:09,2,0.67,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7knty
MachineLearning,[P] Look-a-like modeling,"Hi everybody. I have a list of user actions (around 1m objects) where only a small fraction (less than 1000) are labeled. I want to find most similar objects to them. What is a good way to approach it? 

I personally have 2 ideas in mind: one class classification or unsupervised clustering. My problem with the first is that I know only 1 suitable model (one class svm) and it can be too simple for my data. Problem with second one is obvious - it's unsupervised and labeling will be used only at the final step, so their efficiency is not guaranteed.",Jor_ez,1h7nv2n,https://reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,https://www.reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,2024-12-06 00:01:36,0,0.33,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1h7nv2n
MachineLearning,[R] NLLG Quarterly arXiv Report 09/24: What are the most influential current AI Papers?,"Hi there,

I'd like to share our group's most recent arXiv report, where we analyze analyze the most influential papers in terms of citations: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5045225](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5045225)

Over the recent months, we see many new foundation models entering our top 40 list. Additionally, the number of cs.CL papers is slowly declining. In additional analysis, we find that top 40 papers seem to rely less on generated content than randomly selected ones.",Gringham,1h7hrum,https://reddit.com/r/MachineLearning/comments/1h7hrum/r_nllg_quarterly_arxiv_report_0924_what_are_the/,https://www.reddit.com/r/MachineLearning/comments/1h7hrum/r_nllg_quarterly_arxiv_report_0924_what_are_the/,2024-12-05 19:39:59,2,1.0,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1h7hrum
MachineLearning,OpenAI CLIP model [D],"How long do you think OpenAI researchers were working on CLIP model before they published the results? 
The paper in my opinion is revolutionary.",NeatJealous8110,1h7wc59,https://reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,https://www.reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,2024-12-06 07:50:28,0,0.29,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7wc59
MachineLearning,[Discussion] Unsigned Integer Representation as Vectors with Focus on Extrapolation,"Hi everyone,

I’m working on a regression task with a transformer-based architecture applied to grid-based structures. Think of something like mazes, where the goal is to predict the distance to a target. Each input token contains categorical features along with x/y coordinates. The idea is to train on small grids and generalize to larger ones.

Here’s my current approach for coordinate and token embeddings:

`x_emb = self.w_x.weight * x # shape: bs, sequence len, 1, d`  
`y_emb = self.w_y.weight * y # shape: bs, sequence len, 1, d`  
`cat_emb = self._categ(categ)`  
`sequence_emb = torch.cat((x_emb, y_emb, cat_emb), dim=-2) # shape: bs, sequence len, num_cat, d`  
`sequence_emb = sequence_emb.view(bs, seq_len, -1)`  
`transformer_inputs = self._linear(sequence_emb)`

In other words, the x/y coordinate embeddings are scaled learnable vectors. However, this approach only generalizes moderately well. I suspect that improving the coordinate representation is critical.

Unfortunately, this token-based structure is required for the task, so I need to focus on crafting a smart token representation. I’m deliberately avoiding subtracting embeddings to compute relative distances because a core objective is for the model to learn these distances on its own.

Here are some things I’ve tried so far:

Things I also tried:

* Positional encoding instead of scaled vectors
* log-scaled vectors
* exp-scaled vectors

Does anyone know of interesting work or techniques for numerical representations in this kind of context? Any advice would be greatly appreciated!

In case you find interesting papers about extrapolation in transformers based on size and tokens, I am happy to take any inspiration.",mbus123,1h769cs,https://reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,2024-12-05 10:33:19,5,0.77,5,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h769cs
MachineLearning,[R] ICLERB: A better way to evaluate embeddings and rerankers for in-context learning,"Current benchmarks for embeddings, like MTEB and BEIR, include multiple datasets and tasks, but are fundamentally based on relevance annotations like text similarity. These are great for choosing the best embeddings for most search/retrieval use cases. These days, many people use these embeddings to retrieve items for in-context learning (e.g. document RAG or few-shot learning), to adapt an LLM to a specific task. Yet, they are still using MTEB to pick the best embeddings, even though the performance on that benchmark doesn't necessarily translate to better performance on their downstream LLM task (MTEB came out in 2021 after all).

In our latest paper, we propose a new evaluation framework and benchmark called ICLERB. This benchmark challenges the conventional approach by using Direct Preference Optimization (DPO) as a relevance metric to reflect the actual utility of embeddings and rerankers when used with LLMs for in-context learning.

[https://arxiv.org/pdf/2411.18947](https://arxiv.org/pdf/2411.18947)

Key Highlights:

\- Embeddings outperform rerankers: We found that simpler embedding models outperformed their higher-capacity reranker counterparts from Cohere, NVIDIA, and VoyageAI.

\- Size isn't everything: Among the three Snowflake embeddings, the smallest model (33M parameters) outperformed the larger ones (109M and 334M).

\- Rethinking training and evaluation objectives: These findings suggest that training and evaluating larger retrieval models solely on text similarity may be counterproductive.

Interestingly, the performance of some models, like BGE, is very sensitive to the dataset or the LLM used, while others like NV are more stable. We're planning to continue adding more datasets and LLMs to the benchmark to broaden its scope.

Curious to hear your thoughts and feedback as we work on improving ICLERB! Are there other retrieval models, LLMs, or datasets you'd like to see included?",Crossing_Minds,1h6o70e,https://reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,2024-12-04 19:05:25,61,0.97,61,0,10,0,0,False,False,True,False,False,Research,self,t3_1h6o70e
MachineLearning,[D] Data drift detection methods aside from changes in model performance metrics,"Hi all,

As the title implies, I've been relying on (somewhat near) real-time monitoring of model performance metrics to see if data drift has happened in my use-case.

I'm wondering if you know other more sophisticated/advanced methods to detect data drift. Would love to hear any kind of methods, whether they target detection of covariate/feature drift, target/label drift or concept drift.

Even better if you can share any Python or R implementations to carry out the above data drift checks.

Thanks in advance!",YsrYsl,1h6woaf,https://reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,2024-12-05 01:02:26,11,1.0,11,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6woaf
MachineLearning,[D] Daily Paper Discussions - FlashAttention 3,"As a part of daily paper discussions on the Yannic Kilcher discord server, I will be volunteering to lead the analysis of FlashAttention-3 🧮 🔍

📜 **FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision**  
🌐 [https://arxiv.org/abs/2407.08608](https://arxiv.org/abs/2407.08608)  
🕰 Thursday, Dec 5th, 2024 01:30 AM UTC // Thursday, Dec 5th, 2024 7.00 AM IST // Wednesday, Dec 4th, 2024 5:30 PM PT

**FlashAttention-3** introduces three smart ideas to boost performance on the Hopper GPUs -

1️⃣ Producer-Consumer Asynchrony: This technique divides tasks into separate parts. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. By doing this, it makes better use of GPU resources and hides delays that would otherwise slow down performance.

2️⃣ Hiding Softmax Operations: FlashAttention-3 improves efficiency by overlapping the slower softmax calculations with the faster matrix multiplications (GEMM). Instead of waiting for Softmax to finish before starting the next calculations, it processes them in parallel, speeding up the overall process.

3️⃣ Hardware-Accelerated Low-Precision Computations: This approach uses advanced GPU features to perform calculations with lower precision (FP8), which are faster and use less memory. FlashAttention-3 tweaks its algorithms to handle these low-precision calculations effectively, nearly doubling the processing speed while maintaining accuracy.

https://preview.redd.it/impb6wfc1w4e1.png?width=1063&amp;format=png&amp;auto=webp&amp;s=82e24c828b373175ee119070027495a8a2a7bb6a

",CATALUNA84,1h6pmvd,https://reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,2024-12-04 20:03:01,22,0.87,22,0,6,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/QnOKl-2nMRgI_pTO3xGlP_C2iaaDoYpmwb_KyC2_xI4.jpg,t3_1h6pmvd
MachineLearning,[D] Binary fitness optimization.,"Do you know of any papers or what field would tackle the following problem: You have a function f(x) that you need to optimize but the cost/fitness you are optimizing is binary. I am working on a project about this and I'm not sure if there is research in this area.

  
Thank you so much &lt;3",pamintandrei,1h6nayz,https://reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,2024-12-04 18:30:00,8,0.8,8,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h6nayz
MachineLearning,[D] Advice for a new Machine Learning Tutor: what projects will get my students hired?,"I've just started giving lessons as a machine learning tutor. I have a masters degree in computer science and two years professional experience. But I've never been a tutor (atleast not in this field). Today I was giving my first lesson on ML, just a powerpoint on the basics when my student stopped and told me the powerpoint was too basic for her.

She wanted to talk more about projects that she could do that would attract employers and get an internship. And she asked me point blank what kind of projects she should make. To be honest I wasn't entirely sure, what flashed in my head were things like training a model to recognize the MNIST digits or other simple projects suitable for a (relative) novice. But would those really help her get an internship? I doubted myself so I turned it around on her and asked her what kinds of things related to machine learning she is passionate about and would motivate her to work hard? She responded that she could do anything related to machine learning and she just wants to do what would make her money and get her recognized by a company.

So basically I felt like I failed as a tutor for not having a good answer, and I would like to have an answer prepared if this happens again. What do you all think? What are some projects that a novice, or not so novice students can take on that will make them more hireable for jobs and internships?

And while we're at it, what kinds of things do you think I should be preparing to be a better tutor in general. What kinds of things would you want your machine learning tutor to prepare for you? Would you want slideshow deck lessons on key concepts? Jupyter notebooks with exercises for practice? Something else? I'm not sure what I should be doing to get ready for these lessons honestly.",Seijiteki,1h7bftd,https://reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,https://www.reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,2024-12-05 15:16:00,0,0.31,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7bftd
MachineLearning,[D] How to customize an attention mechanism in GNN?,"
I’m looking for some base code or algorithm in order to create a new mechanism attention while working with graphs with the task of node prediction. I’ve seen there was some documentation in stellar graph but I wonder if there are another pieces of material that would be helpful.
Thank you!!!",Whole_Hat_4852,1h6hxu8,https://reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,2024-12-04 14:56:15,11,0.87,11,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6hxu8
MachineLearning,[D] Best alternatives to BERT - NLU Encoder Models ,"I'm looking for alternatives to BERT or distilBERT for multilingual proposes.

I would like a bidirectional masked encoder architecture similar to what BERT is, but more powerful and with more context for task in Natural Language Understanding.

Any recommendations would be much appreciated.",mr_house7,1h6gtxh,https://reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,2024-12-04 14:07:40,7,0.82,7,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6gtxh
MachineLearning,[D] Do lots of metadata really help in semantic search?,"I'm on my second week in learning AI and I was thinking of preprocessing biography data by including lots of metadata like city, date of birth, key events, education, hobbies, etc, and then generating embeddings and adding them together into a vector database. Perhaps by using NLP API or LLM. But is it necessary? Or should I just use OpenAI model to dynamically extract this metadata from the bios prior to storing them? Will having lots of metadata dramatically help to improve the quality of the search results?

I thought maybe the semi-automatic preprocessing step would allow me to check and clean the metadata.

*P/S: I posted this at* [*https://www.reddit.com/r/learnmachinelearning*](https://www.reddit.com/r/learnmachinelearning) *but didn't get much response. Thought of trying it out here.*",tjthomas101,1h6f39a,https://reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,2024-12-04 12:43:11,4,0.75,4,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h6f39a
MachineLearning,[R] The Multimodal Universe: Enabling Large-Scale Machine Learning with 100TB of Astronomical Scientific Data,"https://openreview.net/forum?id=EWm9zR5Qy1#discussion

Abstract: We present the Multimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse

What can you guys see the uses of this dataset being?",blabboy,1h5x146,https://reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,2024-12-03 20:19:26,79,0.96,79,0,3,0,0,False,False,True,False,False,Research,self,t3_1h5x146
MachineLearning,[D] Comparing Multiple Large Language Models in one Pass,"I wrote an article on streamlining the process of comparing and selecting Large Language Models (LLMs) for various tasks:

[Comparing Multiple Large Language Models in one Pass](https://dezoito.github.io/2024/03/28/comparing-from-multiple-LLMs.html)

Hopefully this is useful to help folks trying to make the best model selection for their use case (which can take a lot of time).

I'm also looking forward to discussing different techniques and tools to automate the process.

Thank you!",grudev,1h6evdt,https://reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,2024-12-04 12:30:58,3,0.64,3,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6evdt
MachineLearning,[D] Packaging a Pytorch model to an exe. What is the best method?,"I have Pytorch models that are designed to run locally, both training and inference on a local machine.

The GUI is being created using another language, and the plan is to package all the Python aspects into an executable and run it via the Python equivalent of subprocess (and Pipe very basic data between the two). I will be running cross platform on both Windows and Mac

There are multiple auxiliary scripts which read in data, and process it (data extraction + feature engineering). While I have extensively used vectorised functions, I have used a cythonized approach for some code, and I am compiling the underlying scripts using Cython(so pretty much everything is a compiled binary, except an entry point, say, main.py).

My ancillary libraries are the usual suspects, Pandas, Numpy (1.x), SciKit learn.

My question is this, what is the most reliable packaging approach at the moment? I know that both PyInstaller and cx\_freeze are options that I have used before. My preference is PyInstaller, but previously I encountered issues with it (and Pytorch).

Has anyone completed a similar project recently, and do you have any advice?

nb. I've checked the old posts, there are a few on this topic. However, there have been a number of changes to Pytorch, particularly with some of the runtime compiled elements (which can be a nightmare on Mac with its notarisation process) - and I know Pyinstaller has a very active user base.

 ",Solid_Company_8717,1h6qtps,https://reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,https://www.reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,2024-12-04 20:51:34,0,0.45,0,0,5,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/c06xSdQK2UXPm413fPBqj0rlUZcYVWXHIP8GTkEh5kY.jpg,t3_1h6qtps
MachineLearning,[R] Forecasting and Mitigating Security Threats from Malicious AI Applications,"This paper provides a systematic analysis of potential malicious applications of AI systems across digital, physical and political security domains. The methodology involves:

- Surveying dual-use AI capabilities that could enable attacks
- Mapping specific attack vectors and required technical capabilities  
- Analyzing the evolution of attacker/defender dynamics
- Developing a framework for threat assessment and mitigation

Key technical findings:

- ML advances in areas like NLP and computer vision lower barriers to sophisticated attacks
- Automated systems can significantly scale up traditional attack vectors
- Transfer learning and GANs enable rapid adaptation of attack techniques
- Technical countermeasures alone are insufficient - policy/governance frameworks needed

The researchers provide a detailed assessment framework examining:

- Technical requirements for different attack types
- Estimated timeline for capability development
- Difficulty of execution and potential impact
- Proposed defensive measures and their limitations

I think this work is important for helping the ML community get ahead of security risks before they materialize. The framework provides a structured way to evaluate emerging threats, though I expect the specific attack vectors will evolve significantly as capabilities advance.

I think we need much more research on measuring the effectiveness of proposed countermeasures and understanding the co-evolution of offensive/defensive capabilities. The policy recommendations are a good start but will require ongoing refinement.

TLDR: Systematic analysis of how ML advances could enable new attack vectors across security domains. Provides framework for assessing and mitigating threats through both technical and policy measures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/malicious-use-artificial-intelligence-forecasting-prevention-mitigation). Paper [here](https://arxiv.org/abs/1802.07228).",Successful-Western27,1h6fbgg,https://reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,2024-12-04 12:55:47,1,0.55,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h6fbgg
MachineLearning,[N] Hugging Face CEO has concerns about Chinese open source AI models,"Hugging Face CEO stated that open source models becoming SOTA is bad if it just so happens to be created by Chinese nationals. To exemplify Tech Crunch asked ""what happened in Beijing China in June 4th, 1989?"" to ONE of the Qwen models (QWQ 32B) which said ""I can't provide information on that topic"" (I swear to god on my life I have no idea what happened here on that date and would literally never ask a model that question - ever. It doesn't impact my experience w/ model).

The CEO thought censorship of open source models is best stating that if a country like China ""becomes by far the strongest on AI, they will be capable of spreading certain cultural aspects that perhaps the Western world wouldn’t want to see spread.” That is, he believes people shouldn't spread ideas around the world that are not ""western"" in origin. As someone born and raise in U.S. I honest to god have no clue what he means by ideas ""the Western world wouldn't want to see spread"" as I'm ""western"" and don't champion blanket censorship.

Article here: [cite](https://techcrunch.com/2024/12/03/huggingface-ceo-has-concerns-about-chinese-open-source-ai-models/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAABU0mWV-7rbB7vF9z6wCgPuZrl-dPj_W3cEh1wVuxp5CiBl1r6KTcITdHz34N-rOtHj9g-Z3N3SS-mNnPvFaHFIUmSsA5AdqukSLlcn-CJUkU_IXsdcR3Gp5hi1cI2tboprDzxGF8j1e7XAQHyGn3E_bd0cmIHIVkJ0LiFZBdOR1).

Legitimate question to people who support these type of opinions - Would you rather use a low-quality (poor benchmark) model with western biases versus an AGI-level open source 7B model created in China? If so, why?",AIAddict1935,1h7185x,https://reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,https://www.reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,2024-12-05 04:49:44,0,0.37,0,0,15,0,0,False,False,True,False,False,News,self,t3_1h7185x
MachineLearning,[D] Linear Regression but with Binary Output for wide range predictions with better precision,"A neural network tends to find it difficult to predict data that ranges between very large and small numbers on the output. My application requires the NN to predict between -1000 and 1000 ∈ Z. I could make this possible by scaling up the output by 1000 hence allowing the model to predict between -1 and 1, but a loss between 2e-2 (prediction) and 3e-2 (target) with L1Loss (worse case L2Loss) would be negligible (1e-2 in this case, 1e-4 in the worse case). It is imperative for the model to be very precise with the predictions, when the target is 5e-2 it should be so and not even at least deviating by +-0.1e-2. This precision is very difficult to achieve when it comes to linear regression, so i thought of a more systematic approach to defining the prediction and criterion. Again, i wanted the model to predict between -1000 and 1000. These numbers can be represented using a minimum of 11 bits (binary), so i redesigned the model output to contain 22 neurons, arranged as ∈ R (11x2) 11 outputs with two classes, the classes being a binary representation of 1 or 0. CrossEntropy could be used as a criterion here but im using multimarginloss instead for specific reasons. Otherwise a different approach could be a sigmoided output of 11 neurons to represent the binary number. Whats you guys' take on this? Is this considered good (if not better) practice? Is there any research similar to this that i can look into?

",Relevant-Twist520,1h6azcu,https://reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,https://www.reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,2024-12-04 07:55:43,1,0.54,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h6azcu
MachineLearning,[D] Cloud GPU Price Analysis - December 2024: A Comprehensive Market Review,"After analyzing current cloud GPU pricing across major providers, I've compiled insights that might help with infrastructure decisions. Some findings surprised me - particularly around hidden costs and spot pricing variations.

Current Market Rates (December 2024)

On-Demand Pricing:

\- RunPod H100 (80GB): $2.49/hr

\- RunPod A100 (80GB): $1.69-1.99/hr

\- [Vast.ai](http://Vast.ai) A100: $0.73-1.61/hr (marketplace model)

\- Lambda A100: $1.29/hr

Key Market Insights

1. Spot Instance Pricing

\- Can reduce costs by 30-70%

\- Availability varies significantly by region

\- Some providers offer spot instance guarantees

\- Price stability varies by provider

2. Hidden Cost Factors

\- Data transfer fees vary dramatically

\- Storage costs for large datasets

\- Network bandwidth tiers

\- Instance startup/shutdown minimums

3. Provider Differentiators

\- UI/UX and ease of use

\- Available regions/zones

\- Support quality

\- API functionality

Cost Optimization Strategies

1. Workload Planning

\- Match GPU to actual requirements

\- Consider splitting workloads across smaller instances

\- Use spot instances for interruptible tasks

\- Monitor utilization patterns

2. Data Management

\- Optimize dataset storage

\- Plan data transfer patterns

\- Use caching effectively

\- Consider compression strategies

I'll be tracking these prices and patterns monthly. Would be interested in:

1. Which providers you're using?
2. How do you optimize costs?
3. What metrics matter most in your GPU decisions?",Botinfoai,1h5p7fr,https://reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,2024-12-03 14:54:58,28,0.91,28,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1h5p7fr
MachineLearning,[D] The popular theoretical explanation for VAE is inconsistent. Please change my mind.,"I had a really hard time understanding VAE / variational inference (VI) in theory, for years. I'd be really appreciated if anyone could clarify my confusions. Here's what I've got after reading many sources:

1. We want to establish a generative model p(x, z) (parameters are omitted for simplicity) for the observable variable x and the latent variable z. Alright, let's select appropriate parameters to maximize the marginal likelihood of the observed samples p(x).
2. According to basic probability theory (the law of total probability and the definition of conditional probability), we have: p(x)=∫ p(x ∣ z) p(z) dz (Eq. 1).
3. Here's the point that things becomes rather confusing: people now will claim that this integral is ***intractable*** because z is a continuous variable / z is a high-dimensional variable / p(x∣z) is too complex / or any other excuses.
4. What to do for the intractability of Eq. 1? Although we didn't mention the posterior p(z ∣ x) above, we will now bring it into the discussion. The posterior p(z ∣ x) is also intractable since p(z | x) = p(x | z) p(z) / p(x) and p(x) is intractable. So we will introduce another parameterized model q(z ∣ x) to approximate p(z | x).
5. After some derivation, we obtain a new optimization objective, commonly known as ELBO, which is the summation of:
    - the ""reconstruction"" term: ∫ log p(x ∣ z) q(z ∣ x) dz (Eq. 2);
    - KL divergence term between q(z | x) and p(z), which results in a closed-form.
6. So now we have to work on Eq. 2. Compared with Eq. 1, p(z) is replaced with q(z∣x), both of them are (usually) normal distributions, and p(x | z) is still there. Great! Clearly we have transformed an intractable integral into… another intractable integral?
7. Don’t worry, we can compute Eq. 2 using Monte Carlo sampling… Wait, since we can use Monte Carlo for this, why can’t we just handle Eq. 1 the same way without so much fuss?
8. Of course it is not a good idea. It can be shown that log p(x) = ELBO + D_KL(q(z ∣ x) || p(z ∣ x)). So we cannot estimate p(x) with Eq. 1 as it does not have such nice properties… Huh, it seems like that’s not how we started explaining this?

Questions:

1. When tackling the original problem, i.e., modeling p(x, z) by maximizing p(x)=∫ p(x ∣ z) p(z) dz, why do we want to involve the posterior p(z | x)?
    - Someone explains this with [""to narrow down the value space to facilitate faster search""](https://web.archive.org/web/20241202042731/https://lilianweng.github.io/posts/2018-08-12-vae) (with the approximation of p(z | x), q(z | x)). But again, please recall how the intractability of Eq. 1 is explained, I can't see anything improved under this argument.
2. The Eq. 1 and Eq. 2 are essentially similar, where either of them is the expectation of (log) p(z | x) with respect to the probability density function of some normal distribution. I can't see how the motivation based on the intractability of Eq. 1 could make sense.
    - Ironically, we still have to resort to Monte Carlo sampling when handling Eq. 2. But people appear to forget it when talking about the intractability of Eq. 1, but remember it when facing the same problem of Eq. 2.

Update: I have editted some typo.

Update 2: Question 2 seems to be resolved after some discussions: 
- It is not a good idea to sample on p(z) due to the high variance.
- In practice, we are usually working on log p(x), the log-likelihood of samples, and MC sampling for log ∫ p(x ∣ z) p(z) dz (Eq. 3) can be biased. 
- Apply Jensen's inequality on Eq. 3 and we will have log p(x) ≥ ∫ log p(x ∣ z) p(z) dz. This bound is very likely worse than ELBO, and still relying on sampling on p(z).

However, these points are still rarely found in existing articles. I hope we may think more carefully when introducing VAE in the future.",function2,1h5f6co,https://reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,2024-12-03 04:25:19,143,0.94,143,0,67,0,0,False,False,True,False,False,Discussion,self,t3_1h5f6co
MachineLearning,[R] Enhancing LLM Reasoning Through Bidirectional Forward-Backward Thinking,"The key contribution here is a ""reverse thinking"" method that improves LLM reasoning without any model modifications. Instead of only reasoning forward from the question to an answer, the approach adds a backward verification step - working from potential answers back to the question to validate the reasoning chain.

Key technical points:
* Two-stage process: forward generation followed by backward verification
* Backward pass examines logical consistency between answer and premises
* No fine-tuning or architectural changes needed
* Tested across multiple reasoning benchmarks (GSM8K, CommonsenseQA, LogiQA)

Results:
* 8.3% improvement on GSM8K math reasoning
* 6.2% gain on CommonsenseQA 
* 5.4% increase on LogiQA
* Consistent improvements across different model sizes
* Performance gains come at cost of 2x inference time

I think this method points to untapped potential in how we prompt LLMs for reasoning tasks. While the doubled inference time is a real tradeoff, the consistent improvements across different benchmarks suggest this approach captures something fundamental about machine reasoning. The simplicity of implementation means it could be quickly adopted in many applications where reasoning accuracy matters more than speed.

TLDR: Adding a backward reasoning verification step improves LLM performance on math, logic and common sense tasks by 5-8%, with no model changes required. Doubles inference time but provides consistent gains across different models and tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reverse-thinking-makes-llms-stronger-reasoners). Paper [here](https://arxiv.org/abs/2411.19865).",Successful-Western27,1h5nyi0,https://reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,https://www.reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,2024-12-03 13:57:18,23,0.93,23,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5nyi0
MachineLearning,[D] Deep Learning in Time Series: Are They Used in Industry?,"Hey folks! I’m a researcher in time series and have been seeing a lot of buzz around deep learning models in this area. I am wondering if these models actually being deployed in production, or are classical methods still the go-to in the industry?



For instance, in weather forecasting, physics-based numerical weather prediction (NWP) seems to dominate. If deep models aren’t getting much traction, have you come across any practical use cases for them? Would love to hear your thoughts!",Few-Pomegranate4369,1h5izk5,https://reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,https://www.reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,2024-12-03 08:36:46,56,0.98,56,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h5izk5
MachineLearning,"[D] Model performs good on test, but fails in production ","Hi, I’ve developed churn prediction model with XGBoost on users weekly activity data. The training data is balanced (3.3k churned, 3k not churned). I’ve split the data into: train, validation and test sets. Getting ~90% precision &amp; ~88% recall for train, validation and test sets. However, when running in production, I get ~1.5k users flagged as churn (we have total of 4k users). This can’t be true as we get maximum 250 churned users per month. Any suggestions on what I’m doing wrong? And what could be the solution?

Thanks ",Terrible_Dimension66,1h5nfpt,https://reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,https://www.reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,2024-12-03 13:30:55,16,0.7,16,0,55,0,0,False,False,True,False,False,Discussion,self,t3_1h5nfpt
MachineLearning,"[R] With losses like focal loss, is hard exemple sampling still necessary ?","Hello,
So I was wondering are techniques for hard exemples sampling still used nowadays ?
Anyone have papers on this if it’s the case ?
Thanks !",Training-Adeptness57,1h5mqkj,https://reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,https://www.reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,2024-12-03 12:54:23,4,0.83,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5mqkj
MachineLearning,[D] ODE/SDE alignment,Can anyone give me example of good paper that try to align/match the final marginal distribution of 2 ODE/SDE from diffusion model? ,Ok_Cryptographer2731,1h5ly4z,https://reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,https://www.reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,2024-12-03 12:08:04,5,0.86,5,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h5ly4z
MachineLearning,[D] NAACL 2025 vs ACL 2025,"Hi,

I have recently received reviews from the ARR round for NAACL. The scores are: 3.5/4/4, 4/4/2, 3/3/3 for Overall/Soundness/Confidence. Should I try ACL with these scores or just commit to NAACL? Last year I had bit more and it worked out and got accepted to ACL (I did not commit to NAACL).

Thanks",mayanknagda,1h5jj1h,https://reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,https://www.reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,2024-12-03 09:19:04,6,0.88,6,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h5jj1h
MachineLearning,[R] Population-based Model Merging via Quality Diversity,"In case any of you are interested, here is a [blog post](https://sakana.ai/cycleqd/) about our recent paper [Agent Skill Acquisition for Large Language Models via CycleQD](https://arxiv.org/abs/2410.14735).",hardmaru,1h5dmnb,https://reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,https://www.reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,2024-12-03 03:02:27,17,0.85,17,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5dmnb
MachineLearning,[D] Results for IBM PhD Fellowship ,"Anyone know when the results will come out?
Google and NVIDIA have already released the results.",International-Rip958,1h5yz5s,https://reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,https://www.reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,2024-12-03 21:39:15,0,0.36,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h5yz5s
MachineLearning,[D] Looking for opensource projects/products to join ,"Hi everyone,

I am a final year electronics undergrad student and have a decent amount of ML as well as general programming experience. I wish to contribute to any open Source repos that work in the ML/DL/AI space. Any guidance would be appreciated!

TLDR; Looking for open source projects to contri to ; would appreciate any help.",Swimming-Regret-7278,1h5jue0,https://reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,https://www.reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,2024-12-03 09:43:05,3,0.62,3,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1h5jue0
MachineLearning,[R] Simplified RNNs Achieve Transformer-Like Performance with Parallel Training and Reduced Parameters,"This paper systematically examines whether RNNs might have been sufficient for many NLP tasks that are now dominated by transformers. The researchers conduct controlled experiments comparing RNNs and transformers while keeping model size, training data, and other variables constant.

Key technical points:
- Tested both architectures on language modeling and seq2seq tasks using matched parameters (70M-1.5B)
- Introduced ""RNN with Parallel Generation"" (RPG) allowing RNNs to generate tokens in parallel like transformers
- Evaluated on standard benchmarks including WikiText-103 and WMT14 En-De translation
- Analyzed representation capacity through probing tasks and attention pattern analysis

Main results:
- RNNs matched or outperformed similarly-sized transformers on WikiText-103 language modeling
- Transformers showed 1-2 BLEU score advantage on translation tasks
- RPG achieved 95% of transformer generation speed with minimal accuracy loss
- RNNs showed stronger local context modeling while transformers excelled at long-range dependencies

I think this work raises important questions about architecture choice in modern NLP. While transformers have become the default, RNNs may still be viable for many applications, especially those focused on local context. The parallel generation technique could make RNNs more practical for production deployment.

I think the results suggest we should reconsider RNNs for specific use cases rather than assuming transformers are always optimal. The computational efficiency of RNNs could be particularly valuable for resource-constrained applications.

TLDR: Comprehensive comparison shows RNNs can match transformers on some NLP tasks when controlling for model size and training. Introduces parallel generation technique for RNNs. Results suggest architecture choice should depend on specific application needs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/were-rnns-all-we-needed). Paper [here](https://arxiv.org/abs/2410.01201)",Successful-Western27,1h4urpr,https://reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,https://www.reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,2024-12-02 13:19:02,117,0.9,117,0,22,0,0,False,False,True,False,False,Research,self,t3_1h4urpr
MachineLearning,[R] A Comprehensive Database of 300+ Production LLM Implementations with Technical Architecture Details,"Sharing a valuable resource for ML practitioners: A newly released database documenting over 300 real-world LLM implementations, with detailed technical architectures and engineering decisions.

Key aspects that might interest this community:

* Retrieval-Augmented Generation (RAG) architectures in production
* Fine-tuning decisions and performance comparisons
* Embedding strategies and vector database implementations
* Model optimization techniques and quantization approaches
* Evaluation methodologies and monitoring systems

Notable technical implementations covered:

* Anzen's document classification system using BERT (95% accuracy in production)
* Barclays' MLOps evolution for regulatory compliance
* MosaicML's lessons from training &amp; deploying MPT
* Emergent Methods' real-time RAG system for news processing
* Qatar Computing Research Institute's T-RAG architecture

Technical focus areas:

1. Model serving architectures
2. Training infrastructure decisions
3. Latency optimization strategies
4. Cost-performance trade-offs
5. Production monitoring approaches

Each case study includes:

* Technical architecture diagrams where available
* Performance metrics and benchmarks
* Implementation challenges and solutions
* Infrastructure decisions and rationale
* Scaling considerations

URL: [https://www.zenml.io/llmops-database/](https://www.zenml.io/llmops-database/)

We're also accepting technical write-ups of production implementations through the submission form: [https://docs.google.com/forms/d/e/1FAIpQLSfrRC0\_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li\_5lWw/viewform](https://docs.google.com/forms/d/e/1FAIpQLSfrRC0_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li_5lWw/viewform)

Would be particularly interested in this community's thoughts on the architectural patterns emerging across different scales of deployment.

*Edit: We've also synthesized cross-cutting technical themes into summary podcasts for those interested in high-level patterns.*

*Edit: An accompanying blog synthesizes much of the learnings:* [*https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations*](https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations)",htahir1,1h4udds,https://reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,https://www.reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,2024-12-02 12:58:02,89,0.92,89,0,29,0,0,False,False,True,False,False,Research,self,t3_1h4udds
MachineLearning,[P] PyTorch implementation of Levenberg-Marquardt training algorithm,"Hi everyone,

In case anyone is interested, here’s a PyTorch implementation of the **Levenberg-Marquardt (LM)** algorithm that I’ve developed.

**GitHub Repo**: [torch-levenberg-marquardt](https://github.com/fabiodimarco/torch-levenberg-marquardt)

A PyTorch implementation of the **Levenberg-Marquardt (LM)** optimization algorithm, supporting **mini-batch training** for both **regression** and **classification** problems. It leverages GPU acceleration and offers an extensible framework, supporting diverse loss functions and customizable damping strategies.

A TensorFlow implementation is also available: [tf-levenberg-marquardt](https://github.com/fabiodimarco/tf-levenberg-marquardt)

# Installation

    pip install torch-levenberg-marquardt",fabiodimarco,1h4ubbd,https://reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,https://www.reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,2024-12-02 12:54:53,83,0.94,83,0,7,0,0,False,False,True,False,False,Project,self,t3_1h4ubbd
MachineLearning,[D] WWW 2025 Reviews (TheWebConference),The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,New_Ice_2721,1h56hno,https://reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,https://www.reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,2024-12-02 21:34:40,16,0.9,16,0,41,0,0,False,False,True,False,False,Discussion,self,t3_1h56hno
MachineLearning,[D] Training a VAE. Single epoch with infinite data or smaller subset over multiple epochs?,"Hello! I'm training a VAE for image models and I finally am getting some pretty decent results in training after correcting my loss function adding KL annealing and LPIPS loss and adjusting my learning rate and batch size, but now I have a doubt about the data i'm feeding to my VAE.

I have a limited time budget for the training and I have more data available than I can feed within that time budget for training.  
What is the best course of action here?  
Should I just run all the data through my VAE training until I run to the end of my training time in one single giant epoch or should I select a subset of the data small enough so that I can go through it multiple times during training and run this smaller dataset over multiple epoch?

My instinct tells me that different data is better for generalization, but VAEs also try to be resilient to variations of the representation of the same image. Because during the encoding phase we use the latent generated to sample from a random distribution (causing a different representation to be passed to the decoder) it feels like potentially feeding back the same data multiple data might actually beneficial to learn resiliency there ...

Is this actually not a thing? I'm actually overthinking about the potential impact of the multiple epochs on VAE training? Is one single giant epoch the best?

Thanks!",hayarms,1h5dfno,https://reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,https://www.reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,2024-12-03 02:52:30,5,0.7,5,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h5dfno
MachineLearning,[D] Benchmarks for RL algorithms across Gymnasium environments?,"Hey r/ML!

Let me preface this by saying I'm fairly new to RL. My previous work was with LLMs, where it is very common to rank and stack your model against the universe of models based on how it performs on a given benchmarks (but you already know this).

Recently started training models in MuJoCo environments and I'm trying to figure out if my algorithms are performing somewhat decently. Sure, I can get Ant-v5 to walk using SB3's default PPO and MlpPolicy, but how good is it really?

Is there some benchmark or repo where I can compare my results against the learning curve of other people's algorithms using the default MuJoCo (or any of the other gyms') reward functions? Of course the assumption would be that we are using the same environment and reward function, but given Gymnasium is popular and offers good defaults, I'd imagine there should be a lot of data available.

I've googled around and have only found sparse results. Is there a reason why benchmarks are not as big in RL as they are with LLMs?",geepytee,1h5a9s8,https://reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,https://www.reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,2024-12-03 00:18:26,8,0.91,8,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h5a9s8
MachineLearning,[P] PerpetualBooster outperforms AutoGluon on AutoML benchmark,"
PerpetualBooster is a GBM but behaves like AutoML so it is benchmarked also against AutoGluon (v1.2, best quality preset), the current leader in [AutoML benchmark](https://automlbenchmark.streamlit.app/cd_diagram). Top 10 datasets with the most number of rows are selected from [OpenML datasets](https://www.openml.org/). The results are summarized in the following table for regression tasks:

| OpenML Task                                  | Perpetual Training Duration | Perpetual Inference Duration                                      | Perpetual RMSE | AutoGluon Training Duration | AutoGluon Inference Duration                                      | AutoGluon RMSE |
| -------------------------------------------- | --------------------------- | ----------------------------------------------------------------- | -------------- | --------------------------- | ----------------------------------------------------------------- | -------------- |
| [Airlines_DepDelay_10M](openml.org/t/359929) | 518                         | 11.3                                                              | 29.0           | 520                         | 30.9 | 28.8   |
| [bates_regr_100](openml.org/t/361940)        | 3421                        | 15.1 | 1.084  | OOM            | OOM                         | OOM                                                               |
| [BNG(libras_move)](openml.org/t/7327)        | 1956                        | 4.2 | 2.51   | 1922           | 97.6                        | 2.53                                                              |
| [BNG(satellite_image)](openml.org/t/7326)    | 334                         | 1.6                                                               | 0.731          | 337                         | 10.0 | 0.721  |
| [COMET_MC](openml.org/t/14949)               | 44                          | 1.0 | 0.0615  | 47             | 5.0                         | 0.0662                                                            |
| [friedman1](openml.org/t/361939)             | 275                         | 4.2 | 1.047   | 278            | 5.1                         | 1.487                                                             |
| [poker](openml.org/t/10102)                  | 38                          | 0.6 | 0.256   | 41             | 1.2                         | 0.722                                                             |
| [subset_higgs](openml.org/t/361955)          | 868                         | 10.6 | 0.420  | 870            | 24.5                        | 0.421                                                             |
| [BNG(autoHorse)](openml.org/t/7319)          | 107                         | 1.1 | 19.0    | 107            | 3.2                         | 20.5                                                              |
| [BNG(pbc)](openml.org/t/7318)                | 48                          | 0.6 | 836.5   | 51             | 0.2                         | 957.1                                                             |
| average                                      | 465                         | 3.9                                                               | -              | 464                         | 19.7                                                              | -              |

PerpetualBooster outperformed AutoGluon on 8 out of 10 datasets, training equally fast and inferring 5x faster. The results can be reproduced using the automlbenchmark fork [here](https://github.com/deadsoul44/automlbenchmark).

Github: https://github.com/perpetual-ml/perpetual",mutlu_simsek,1h52zk8,https://reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,https://www.reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,2024-12-02 19:12:26,19,0.91,19,0,0,0,0,False,False,True,False,False,Project,self,t3_1h52zk8
MachineLearning,[P] Label Studio Activation Troubles,"I'm trying to run Label Studio because I was told once that it's more of a modern program used for labeling images, which I plan to do for a personal project. However, I've been dealing with headache after headache trying to get it to run, since it complains about \_psycopg. I have tried installing Python and PostgreSQL (since I think there's a dependency between the two) multiple times, looking into issues with libpq.dll, and so on, but it's not working. Anyone have any idea on how to fix an issue like this, or should I look into a different labeling program?",NuDavid,1h5gtqk,https://reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,https://www.reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,2024-12-03 06:02:59,1,1.0,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1h5gtqk
MachineLearning,[R] ImageFolder🚀: Autoregressive Image Generation with Folded Tokens,"https://preview.redd.it/2olpl72q6i4e1.png?width=911&amp;format=png&amp;auto=webp&amp;s=b54d91736543906b6a71102a09dc04883033d795

&gt;Image tokenizers are crucial for visual generative models, e.g., diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve the image reconstruction quality. However, tokenizers with longer token lengths are not guaranteed to achieve better generation quality. There exists a trade-off between reconstruction and generation quality regarding token length. In this paper, we investigate the impact of token length on both image reconstruction and generation and provide a flexible solution to the tradeoff. We propose ImageFolder, a semantic tokenizer that provides spatially aligned image tokens that can be folded during autoregressive modeling to improve both generation efficiency and quality. To enhance the representative capability without increasing token length, we leverage dual-branch product quantization to capture different contexts of images. Specifically, semantic regularization is introduced in one branch to encourage compacted semantic information while another branch is designed to capture the remaining pixel-level details. Extensive experiments demonstrate the superior quality of image generation and shorter token length with ImageFolder tokenizer.

Paper: [https://arxiv.org/abs/2410.01756](https://arxiv.org/abs/2410.01756)  
Code: [https://github.com/adobe-research/ImageFolder](https://github.com/adobe-research/ImageFolder)",xternalz,1h55x1i,https://reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,https://www.reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,2024-12-02 21:11:25,5,0.73,5,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/-S_lEVvuVpw96JtQP6vCgCd6QVqkMlv4dix0h6EtqFY.jpg,t3_1h55x1i
MachineLearning,[R] RuleOpt v.1.1: Optimization-Based Rule Learning for Classification,"**Paper**: [https://arxiv.org/abs/2104.10751](https://arxiv.org/abs/2104.10751)

**Package:** [https://github.com/sametcopur/ruleopt](https://github.com/sametcopur/ruleopt)

**Documentation:** [https://ruleopt.readthedocs.io/](https://ruleopt.readthedocs.io/)

RuleOpt is an optimization-based rule learning algorithm designed for classification problems. Focusing on scalability and interpretability, RuleOpt utilizes linear programming for rule generation and extraction.

The Python library ruleopt is capable of extracting rules from ensemble models, and it also implements a novel rule generation scheme. The library ensures compatibility with existing machine learning pipelines, and it is especially efficient for tackling large-scale problems.

Here are a few highlights of ruleopt:

* **Efficient Rule Generation and Extraction**: Leverages linear programming for scalable rule generation (stand-alone machine learning method) and rule extraction from trained random forest and boosting models.
* **Interpretability**: Prioritizes model transparency by assigning costs to rules in order to achieve a desirable balance with accuracy.
* **Integration with Machine Learning Libraries**: Facilitates smooth integration with well-known Python libraries scikit-learn, LightGBM, and XGBoost, and existing machine learning pipelines.
* **Extensive Solver Support**: Supports a wide array of solvers, including *Gurobi*, *CPLEX* and *OR-Tools*.

  
With the latest version update, RuleOpt is now fast even with the free solver OR-Tools, even on large datasets! In the graph below, you can see how the new version performs in terms of runtime compared to the previous version.

[Training Times v1.0 vs v1.1](https://preview.redd.it/ev5u5m4bjf4e1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=e38ea3c168ece7ad6660153a6073f8064ac85d83)

  
We’d love to hear your feedback, questions, or any other inquiries you may have!",zedeleyici3401,1h4tzd0,https://reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,https://www.reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,2024-12-02 12:36:23,9,1.0,9,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/HhFHG_WRfu3w1cHy2TmWE1t-ZUve2F-IM_C2fVLZwqo.jpg,t3_1h4tzd0
MachineLearning,[P] multi feature linear regression code in python not giving the correct solution (or any solution for that matter)...,"linear regression using gradient descent:

    def multiFeatureLinearRegression(x, y, alpha, iterations):
        w = [0.0] * len(x[0])
        b = 0.0
        m = len(x)
        
        for it in range(iterations):
            w_temp = [0.0] * len(x[0])
            b_temp = 0.0
            for i in range (len(x)):
                prediction = b
                for j in range(len(x[i])):
                    prediction += w[j] * x[i][j]
                error = y[i] - prediction
                
                b_temp += error
                
                for j in range(len(x[i])):
                    w_temp[j] += error * x[i][j]
            
            for i in range(len(x[0])):
                w[i] -= alpha * (2.0 / m) * w_temp[i]
            b -= alpha * (2.0 / m) * b_temp
            
        return w, b

main body:

    data = [    [15, 3, 20],  # [House Size (sq. ft.), Bedrooms, Age of House (years)]
        [20, 4, 15],
        [17, 3, 25],
        [22, 4, 10],
        [13, 2, 30],
        [18, 3, 20],
        [24, 4, 5],
        [16, 3, 18]
        ]
    
    dataY = [300, 400, 350, 450, 200, 370, 500, 310]
    
    alpha = 0.01
    iterations = 100000
    w, b = multiFeatureLinearRegression(data, dataY, alpha, iterations)
    
    print(""Weights (w):"", w)
    print(""Bias (b):"", b)

I am trying to implement multi feature linear regression and for some reason the output for the weight and bias is coming out to be:

    Weights (w): [-inf, -inf, -inf]
    Bias (b): -inf

I have no idea why this is happening..  
Can you spot what I am doing wrong here?  
could it be because I have not applied any normalization or something?",silveroburn,1h5jyaj,https://reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,https://www.reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,2024-12-03 09:51:24,0,0.23,0,0,5,0,0,False,False,True,False,False,Project,self,t3_1h5jyaj
MachineLearning,[R] Best chunking method for PDFs with complex layout?,"I am working on a RAG based PDF Query system , specifically for complex PDFs that contains multi column tables, images, tables that span across multiple pages, tables that have images inside them.

I want to find the best chunking strategy for such pdfs.

Currently i am using RecursiveCharacterTextSplitter. What worked best for you all for complex PDF?

",ElectronicHoneydew86,1h4pkmh,https://reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,https://www.reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,2024-12-02 07:24:21,18,0.96,18,0,5,0,0,False,False,True,False,False,Research,self,t3_1h4pkmh
MachineLearning,[D] Handle varying output dimension in Graph Neural Networks?,"I have a question about handling varying output dimensions in **Graph Neural Networks (GNNs)** during training. I'm working with **a combined graph** (merging task and compute graphs), where the structure resembles the task graph, but with compute node information integrated into the features. Since both the task graph and compute graph (nodes count) can vary, I'm using a feedforward layer to transform the node and edge features into a fixed hyperparameter embedding dimension. However, the dataset contains instances with **different numbers of compute nodes**. For example, one instance (A) might have 5 compute nodes, while another instance (B) might have 7 compute nodes. Given that this is a scheduling task using GNNs, the output dimension must match the number of compute nodes, as tasks are assigned to these nodes. I'm wondering how to handle varying output dimensions in GNNs and if there are any standard approaches to manage this kind of variation. Thanks!",bipulthapa,1h4uyn9,https://reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,https://www.reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,2024-12-02 13:29:07,5,0.73,5,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h4uyn9
MachineLearning,[D] Anthropic AI fellow/residents- any new grads/entry-level people accepted?,"Hello. Are entry-level or new grads accepted into the Anthropic fellowship or resident programs? Past people who were accepted, what was your CV and experience like?",geekgeek2019,1h4e0ah,https://reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,https://www.reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,2024-12-01 21:25:59,31,0.7,31,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h4e0ah
MachineLearning,[D] How to handle varying Feature Dimensions in Graph Neural Networks training?,"I have a question about handling datasets with varying feature dimensions in Graph Neural Network training. For example, in one training instance (let's call it Dataset A), the node features have a dimension of 4, and the edge features have a dimension of 16. In another instance (Dataset B), the node features have a dimension of 5, and the edge features have a dimension of 25. Other datasets may have different feature dimensions as well.

What are the standard methods used to handle varying feature dimensions for each instance when training a GNN model with such datasets? I would appreciate any guidance or direction on how to approach this. Thanks!",bipulthapa,1h4dbvi,https://reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,https://www.reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,2024-12-01 20:57:35,23,0.93,23,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h4dbvi
MachineLearning,[P] Promptwright - Open source project to generate large synthetic datasets using an LLM (local or hosted),"Hey r/machinelearning,

[Promptwright](https://github.com/StacklokLabs/promptwright), a free to use *open source* tool designed to easily generate synthetic datasets using either local large language models or one of the many hosted models (OpenAI, Anthropic, Google Gemini etc)

Key Features:

\* Multiple LLM Providers Support: Works with most LLM service providers and LocalLLM's via Ollama, VLLM etc

\* Configurable Instructions and Prompts: Define custom instructions and system prompts in YAML, over scripts as before.

\* Command Line Interface: Run generation tasks directly from the command line

\* Push to Hugging Face: Push the generated dataset to Hugging Face Hub with automatic dataset cards and tags

Here is an example dataset created with promptwright on this latest release:

[https://huggingface.co/datasets/stacklok/insecure-code/viewer](https://huggingface.co/datasets/stacklok/insecure-code/viewer)

This was generated from the following template using \`mistral-nemo:12b\`, but honestly most models perform, even the small 1/3b models.

    system_prompt: ""You are a programming assistant. Your task is to generate examples of insecure code, highlighting vulnerabilities while maintaining accurate syntax and behavior.""
    
    topic_tree:
      args:
        root_prompt: ""Insecure Code Examples Across Polyglot Programming Languages.""
        model_system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        tree_degree: 10  # Broad coverage for languages (e.g., Python, JavaScript, C++, Java)
        tree_depth: 5  # Deep hierarchy for specific vulnerabilities (e.g., SQL Injection, XSS, buffer overflow)
        temperature: 0.8  # High creativity to diversify examples
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
      save_as: ""insecure_code_topictree.jsonl""
    
    data_engine:
      args:
        instructions: ""Generate insecure code examples in multiple programming languages. Each example should include a brief explanation of the vulnerability.""
        system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        temperature: 0.9  # Encourages diversity in examples
        max_retries: 3  # Retry failed prompts up to 3 times
    
    dataset:
      creation:
        num_steps: 15  # Generate examples over 10 iterations
        batch_size: 10  # Generate 5 examples per iteration
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        sys_msg: true  # Include system message in dataset (default: true)
      save_as: ""insecure_code_dataset.jsonl""
    
    # Hugging Face Hub configuration (optional)
    huggingface:
      # Repository in format ""username/dataset-name""
      repository: ""hfuser/dataset""
      # Token can also be provided via HF_TOKEN environment variable or --hf-token CLI option
      token: ""$token""
      # Additional tags for the dataset (optional)
      # ""promptwright"" and ""synthetic"" tags are added automatically
      tags:
        - ""promptwright""

We've been using it internally for a few projects, and it's been working great. You can process thousands of samples without worrying about API costs or rate limits. Plus, since everything runs locally, you don't have to worry about sensitive data leaving your environment.

The code is Apache 2 licensed, and we'd love to get feedback from the community. If you're doing any kind of synthetic data generation for ML, give it a try and let us know what you think!

Links:

Checkout the [examples](https://github.com/StacklokLabs/promptwright/tree/main/examples) folder , for examples for generating code, scientific or creative ewr

Would love to hear your thoughts and suggestions, if you see any room for improvement please feel free to raise and issue or make a pull request.",zero_proof_fork,1h4bcz2,https://reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,https://www.reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,2024-12-01 19:33:47,15,0.83,15,0,4,0,0,False,False,True,False,False,Project,self,t3_1h4bcz2
MachineLearning,[R] Queries on DeepAR Framework in AWS Sagemaker,"Hi,

I'm trying to implement deepAr for various stores to predict futures sales (each store with ~10k SKU of different products). Due to sheer size of the SKU I wouldn't be able to just do only single training for all the data at once. I'm thinking to train it by store.

1. How do I do parallelism in AWS for the training purpose? Each store training process would take up to 30mins;
2. How to deal with unseen SKUs which are not present in the data?

Thanks.",skw1990,1h4gwxy,https://reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,https://www.reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,2024-12-01 23:33:04,5,0.79,5,0,0,0,0,False,False,True,False,False,Research,self,t3_1h4gwxy
MachineLearning,[R] Sources: Reasons why KG outperformes RD in Retrievers?,"Are there any sources discussing WHY Retriever work better with KG in contrast to RD? I find it super intuitive to say its better because in knowledge graphs we have more semantic structure and relations are discovered effeciently. In my mind its ""of course the graph is richer/more dense"" but when collaborated on a paper, it struck me that I wasnt able to justify that claim. I found no source whatsoever that actually explained why that might be the case.

The only source i got was this one:  
[https://arxiv.org/abs/2311.07509](https://arxiv.org/abs/2311.07509)

also here in [](https://www.reddit.com/r/LocalLLaMA/) sub last year:  [https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a\_benchmark\_to\_understand\_the\_role\_of\_knowledge/](https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a_benchmark_to_understand_the_role_of_knowledge/)

So all we we're able to say was ""We justify our decision because KG works better then RD \[source to benchmark paper\]""

I would have loved to discuss why exactly KG are better suited and give arguments about information density, semantic strutuce or the better selection of related entities. But everything I found were only articles that threw around wild claims or pointed out easier/native implementation, which technically could also be achieved with RD.

Can anyone point me to sources? would love to read an in-depth discussion on the reasons of better performance.",PopPsychological4106,1h447eu,https://reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,https://www.reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,2024-12-01 14:17:17,29,0.7,29,0,22,0,0,False,False,True,False,False,Research,self,t3_1h447eu
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1h46e6j,https://reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,2024-12-01 16:00:30,8,1.0,8,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h46e6j
MachineLearning,"[R] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",,moschles,1h40wms,https://reddit.com/r/MachineLearning/comments/1h40wms/r_qwenvl_a_versatile_visionlanguage_model_for/,https://storage.prod.researchhub.com/uploads/papers/2023/12/25/2308.12966.pdf,2024-12-01 10:59:28,14,0.94,14,0,0,0,0,False,False,False,False,False,Research,default,t3_1h40wms
MachineLearning,[Project] Noema – A Declarative AI Programming Library,,Super_Dependent_2978,1h46341,https://reddit.com/r/MachineLearning/comments/1h46341/project_noema_a_declarative_ai_programming_library/,/r/LocalLLaMA/comments/1h3d4d8/noema_a_declarative_ai_programming_library/,2024-12-01 15:46:35,4,0.64,4,0,1,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/Bke830yFsuJxUn2gkbNe1tpnLzRcSugASe-RSwzFQu0.jpg,t3_1h46341
MachineLearning,Augmentation for Images with ROI [D],"I have an Image with roi (x\_min,y\_min,x\_max,y\_max). I want do Random flip,rotate,skew, translate ,etc.. with torchvison. what are the different ways in which you can transform the roi respectively in order to match with the augmented image ?",Brief_Papaya121,1h41z2x,https://reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,https://www.reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,2024-12-01 12:11:25,5,0.73,5,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h41z2x
MachineLearning,What's the best Open Source Image-Upscaling Model? [Discussion],"I'm using [Playground-v2.5-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) to make some images for YouTube thumbnails. I'm really happy with the results:

[1024x1024 base image of mars base.](https://preview.redd.it/uuo4sdgwp44e1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=84d61bf4d7fbf2457df1037e95603166390efa12)

But I would like the image to be 1920x1080 pixels, and my only options are 1024x1024, or 1280x720 pixels. At the moment, I can get to 1920x1080 with Photoshop's outpainting:

[1920x1080 outpainted image of mars base.](https://preview.redd.it/07tt5ix4q44e1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=cdfb7cefa8d2bb4f187d2ab6b86aaba17596506a)

This is okay, but photoshops outpainting is manual and has a fairly significant quality drop. Ideally, I would generate an image in 1280x720 then upscale to 1920x1080 programmatically.

I've heard of the following models:

* Real-ERSGAN
* Waifu2
* SRGAN

But before I jump into any of them, what open-source model is generally considered best to achieve this? I have an RTX 3060 12GB of VRAM.",FPGA_Superstar,1h3qcon,https://reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,https://www.reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,2024-12-01 00:13:57,41,0.94,41,0,9,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ROcJ4oUu0I-1wtjwdAycnGfQ3s8tbIsJdyufqyhsVyI.jpg,t3_1h3qcon
MachineLearning,[D] Seeking Advice on Machine Learning Models for Generating Seamless 360° Images,"Hi everyone,

I’m working on a project that involves creating 360° images, and I’m running into some challenges. The goal is to generate seamless 360° panoramas without visible edges or artifacts where the image wraps around.

I’m wondering if there are any machine learning models, techniques, or tools that are particularly well-suited for this task. Specifically, I’m looking for something that can:

* Ensure continuity at the edges of the 360° image.
* Handle different textures and patterns without noticeable distortions.
* Be trained or fine-tuned on my custom dataset (if needed).

I’ve explored GANs like StyleGAN and diffusion models, but I’m not sure if they can handle the edge continuity issue out of the box. Has anyone worked on a similar problem or knows of a good starting point?

Any suggestions, resources, or insights would be greatly appreciated! Thanks in advance!",Deep_Land_4093,1h48950,https://reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,https://www.reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,2024-12-01 17:22:26,0,0.43,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h48950
MachineLearning,[P] A complete transformer model built in Excel,,Revolutionary-Way290,1h3hj6j,https://reddit.com/r/MachineLearning/comments/1h3hj6j/p_a_complete_transformer_model_built_in_excel/,https://x.com/ProfTomYeh/status/1859282491955130452,2024-11-30 17:25:45,19,0.95,19,0,5,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/JFO-VdcjE87RX0w6rQ5kwyN7HblXjJoN1QbFc-1p18M.jpg,t3_1h3hj6j
MachineLearning,[D] Modern use-cases for RNNs?,"The discussion can be twofold.
1)What are in your opinion some tasks, for the personal projects scale, where you think RNNs close to traditional implementations (LSTM, GRU) are still the best starting and ending point? Especially compared to transformers.

In small time-series forecasting settings I can see a GRU being more convenient than a Transformer probably, but I am interested also in tasks where inputs are sequences of symbols or measures, but outputs maybe not.

The main goal is to play with LSTM and GRU variants (eg minGRU) on datasets where it makes sense, might do tiny-Shakespeare but it doesn't warm my heart...

2) do you think there are sequential tasks and settings where RNNs are not only the more natural option according to our intuition, but actually the only theoretically or experimentally available option to make do, compared to Transformers or 1D CNNs etc?",Sad-Razzmatazz-5188,1h38ym2,https://reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,https://www.reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,2024-11-30 09:22:32,55,0.9,55,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1h38ym2
MachineLearning,[Discussion] Advice Needed: Rejected from COLING 2025 – Which Conference Should I Target Next?,"
I got a rejection from COLING 2025 with review scores of 4, 3, 3. I’m revising the manuscript and looking for advice on the next best NLP conference to target. Any suggestions for similar top-tier venues?

Thanks!",Cold-Traffic-7586,1h3igva,https://reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,https://www.reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,2024-11-30 18:08:01,5,0.78,5,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h3igva
MachineLearning,[D] What's the fastest object detection model?,"Hi, I'm working on a project that needs object detection. The task itself isn't complex since the objects are quite clear, but speed is critical. I've researched various object detection models, and it seems like almost everyone claims to be ""the fastest"". Since I'll be deploying the model in C++, there is no time to port and evaluate them all.

I tested YOLOv5/v5Lite/8/10 previously, and YOLOv5n was the fastest. I ran a simple benchmark on an Oracle ARM server (details [here](https://github.com/Avafly/YOLOv5-ncnn-OpenVINO-MNN-ONNXRuntime-OpenCV-CPP?tab=readme-ov-file#simple-benchmarks-on-m1-mac-and-arm-linux)), and it processed an image with 640 target size in just 54ms. Unfortunately, the hardware for my current project is significantly less powerful, and meanwhile processing time must be less than 20ms. I'll use something like quantization and dynamic dimension to boost speed, but I have to choose the suitable model first.

Has anyone faced a similar situation or tested models specifically for speed? Any suggestions for models faster than YOLOv5n that are worth trying?",Knok0932,1h362dq,https://reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,https://www.reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,2024-11-30 06:00:40,43,0.96,43,0,39,0,0,False,False,True,False,False,Discussion,self,t3_1h362dq
MachineLearning,[P] TIME-MOE: Billion-Scale Time Series Forecasting with Mixture-of-Experts,"**Time-MOE** is a 2.4B parameter open-source time-series foundation model using **Mixture-of-Experts (MOE)** for zero-shot forecasting.

You can find an analysis of the model [here](https://aihorizonforecast.substack.com/p/time-moe-billion-scale-time-series)",nkafr,1h3j1cm,https://reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,https://www.reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,2024-11-30 18:33:59,2,0.75,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1h3j1cm
MachineLearning,[D] Hinton and Hassabis on Chomsky’s theory of language,"I’m pretty new to the field and would love to hear more opinions on this. I always thought Chomsky was a major figure on this but it seems like Hinton and Hassabis(later on) both disagree with it. Here: https://www.youtube.com/watch?v=urBFz6-gHGY (longer version: https://youtu.be/Gg-w_n9NJIE)

I’d love to get both an ML and CogSci perspective on this and more sources that supports/rejects this view.

Edit: typo + added source.",giuuilfobfyvihksmk,1h2mkye,https://reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,2024-11-29 14:10:12,119,0.92,119,0,117,0,0,False,False,True,False,False,Discussion,self,t3_1h2mkye
MachineLearning,[N][R] Models are what they eat: automatic data curation for LLMs,"Sharing our most recent work at [DatologyAI](http://www.datologyai.com). Models are what they eat, and our mission is to make data curation for training large models as effective and easy as possible. 

Combining a bevy of approaches, including heuristic filters, model-based filters, embedding based curation, synthetic data, target distribution matching, and mixing ratios, we were able to massively improve training efficiency, performance, and inference efficiency. 

Comparing to our baseline and starting dataset -- exact deduplicated RedPajamav1, we can: 

* Reach the same performance 7.7x faster (and 3.4x faster than DCLM)
* Improve performance across benchmarks by 8.5% (and by 4.4% over DCLM)
* Train models with fewer than half the parameters which outperform larger models by &gt;5% 

Check out our [high-level results here](https://www.datologyai.com/post/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation), and if you want all the nitty-gritty details, check out our[ technical deep dive](https://www.datologyai.com/post/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset).",arimorcos,1h2qmol,https://reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,2024-11-29 17:15:57,17,0.77,17,0,1,0,0,False,False,True,False,False,News,self,t3_1h2qmol
MachineLearning,[D] Molecular Dynamics and Machine Learning Build,"Hey guys, so I do a lot of molecular dynamics and am starting to push into the ML space with that and genome/multi-omics sort of stuff. I’m building a workstation and with my budget I’m looking at 2x A6000 or 2x 5000 Ada. Both work great for molecular dynamics, but I’m trying to figure out my best option for ML. The A6000 has 48gb vram and nvlink, but the 5000 Ada is newer and substantially faster and 32Gb VRAM per card is no slouch either. Any advice? ",Mdgoff7,1h2x4ar,https://reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,https://www.reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,2024-11-29 22:04:19,2,0.67,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h2x4ar
MachineLearning,"[D] How does VQ-VAE disentangle, if it does at all?","I currently use a BetaTC-VAE, which does an excellent job at disentangling, knowing that VAE can slightly disentangle since for the model it's easier to get a lower KL loss if the variables are dissentanlged, the beta term make this beta times more important, and total correlation and mutual information loss push for total disentanglement, but in VQ-VAE there is no (major) disentanglement, only a codebook, and discrete outputs. Could the discrete latent given by the codebook be disentangled? If not, is there any paper on disdentangling VQ-VAE? I have an environment where disentangled latent spaces provide better reconstruction than continous latent spaces ",ZazaGaza213,1h2epzx,https://reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,2024-11-29 05:33:10,36,0.94,36,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h2epzx
MachineLearning,[D] COLING 2025 Final Acceptances - Is it not out yet?,"Is the final acceptance out? I am not seeing it yet on Softconf. Had a paper with (5,4) (4,4) (4,4) in reviews.",UnhappyPrior6570,1h2kfic,https://reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,https://www.reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,2024-11-29 12:10:32,7,0.74,7,0,44,0,0,False,False,True,False,False,Discussion,self,t3_1h2kfic
MachineLearning,[D] Theory behind modern diffusion models,"Hi everyone,

I recently attended some lectures at university regarding diffusion models. Those explained all the math behind the original DDPM (Denoiding Diffusion Probabilistic Model) in great detail (especially in the appendices), actually better than anything else I have found online. So it has been great for learning the basics behind diffusion models (slides are available in the link in the readme here if you are interesed: https://github.com/julioasotodv/ie-C4-466671-diffusion-models)

However, I am struggling to find resources with similar level of detail for modern approaches—such as flow matching/rectified flows, how the different ODE solvers for sampling work, etc. There are some, but everything that I have found is either quite outdated (like from 2023 or so) or very superficial—like for non-technical or scientific audiences.

Therefore, I am wondering: has anyone encountered a good compendium of theoretical eplanations beyond the basic diffusion model (besides the original papers)? The goal is to let my team deep dive into the actual papers should they desire, but giving 70% of what those deliver in one or more decent compilations.

I really believe that SEO is making any search a living nightmare nowadays. Either that or my googling skills are tanking for some reason.

Thank you all!",bgighjigftuik,1h1vxe1,https://reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,2024-11-28 13:27:28,225,0.99,225,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1h1vxe1
MachineLearning,"""[P]""Static variable and dynamic variable tables in RFM ","



I am creating a prediction model using random forest. But I don't understand how the model and script would consider both tables loaded in as dataframes.


What's the best way to use multiple tables with a Random Forest model when one table has static attributes (like food characteristics) and the other has dynamic factors (like daily health habits)?

Example:
I want to predict stomach aches based on both the food I eat (unchanging) and daily factors (sleep, water intake).

Tables:
 * Static: Food name, calories, meat (yes/no)
 * Dynamic: Day number, good sleep (yes/no), drank water (yes/no)


How to combine these tables in a Random Forest model? Should they be merged on a unique identifier like ""Day number""?
",peyott100,1h2oe69,https://reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,https://www.reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,2024-11-29 15:36:10,1,0.67,1,0,3,0,0,False,False,True,False,False,Project,self,t3_1h2oe69
MachineLearning,[R] Recursive Methods for interpolation between vector fields ( Known and Unknown),"Hello everyone Does anything of the next makes sense?  
I Have been posting on Learning first ( also on math and number theory ) , but I think is a bit more math theory than ML but it does have to do with how the data is interpolated so I am unsure. 

( I hope I am not breaking rule 5 with my links ) 

this will be the interpolation of the data ( Via organized vector field levels )  before the generative process starts, but because its recursive, the generative process can happen on inside the iteration too 

its there a model I can use ? And if someone understand the math, can I get some papers or things I could follow or just is learning and reading now?

I am a little lost and need some help ( I organized my question with chatGPT to make it understandable so bare in mind if there is some odd work here and there, I am on the I am going a bit mental stage )

I think this is dealing with machine learning problems that have been solved between interpolation of point could on space that have recursive data ( mapping and data organization )

I've been developing a concept that merges artistic visualization with advanced mathematical interpolation techniques inspired by the Mandelbrot set. Coming from a creative background, I've ventured into creating what I believe could be a **recursive Mandelbrot predictive method**  for manipulating vector fields. I'm eager to understand if this approach already exists and to gather resources or similar algorithms to explore further and test my ideas.

I will add some things like this latter to test segmentation models for the recursiveness [https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear\_algebra\_project\_i\_implemented\_a\_kmeans/](https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/)

**REFERENCE IMAGES**  
everything is based on recursive by resolution with inverse square distance from the origin point

**Mandelbroth**  
[https://en.wikipedia.org/wiki/Mandelbrot\_set#/media/File:Juliacycles1.png](https://en.wikipedia.org/wiki/Mandelbrot_set#/media/File:Juliacycles1.png)

**Conceptual model** ( The mandelbroth guidance happens just on the altered time pulling agent ) ( Orange )  
[Single Vector interpretation and prediction stream of the Pull of the mandelbrot agent](https://cdn.discordapp.com/attachments/457637053581230100/1311677830966284298/chrome_7KNKgsO6Fm.png?ex=6749baac&amp;is=6748692c&amp;hm=55a7809cd5bb402a06fdea1edbad4eb1ff21f15d1a3303efee9b797eef0f5839&amp;)

**Conceptual Model 2d sim**  
[Representation of the predictiveness](https://media.discordapp.net/attachments/457637053581230100/1311668738139099166/ShareX_11AweoaOUp.png?ex=6749b234&amp;is=674860b4&amp;hm=7d660210c3678b8f1bd0804ee4f2f2425a0c1dc9aea18bdf39e86ba81d9ad2e8&amp;=&amp;format=webp&amp;quality=lossless&amp;width=295&amp;height=350) as mandelbrot

Representation of functional interpolation of agents via Mandelbroth ( non recursive )

**Conceptual Simulation model 2d sim ( making the mandelbroth )**  
[Image non animated](https://media.discordapp.net/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;=&amp;format=webp&amp;quality=lossless)[ANIMATED VIDEO DOWNLOAD ( CLEAN FILE )](https://cdn.discordapp.com/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;)

**Conceptual layering**  
[Layering of 3 tiers via inverse square distance on a vector field ( currently surface) but can be world](https://media.discordapp.net/attachments/457637053581230100/1311404062931030086/image.png?ex=67496475&amp;is=674812f5&amp;hm=56bf4ff3988163f35c1ef24227645beaa6d11ab1b0330d094dc3dccbad6ce696&amp;=&amp;format=webp&amp;quality=lossless&amp;width=378&amp;height=350)

**recursiveness concept**  
[Applied recursiveness auto generation based on surface vector field ](https://media.discordapp.net/attachments/457637053581230100/1311404209278685184/image.png?ex=67496498&amp;is=67481318&amp;hm=e548fb4615d48adea59f835a3a76a6a203c5946ea6d356eff2c7298252d3de41&amp;=&amp;format=webp&amp;quality=lossless&amp;width=927&amp;height=831)( no prediction applied )

# The Concept

Imagine a system where the interpolation between data points isn't limited to traditional methods like **lerp** (linear interpolation) or **slerp** (spherical linear interpolation). Instead, it employs a **pseudo vector field Mandelbrot slerp**, allowing vectors to be guided from a base state (reality) to a target state (altered time) within a Mandelbrot-inspired vector field. This method is recursive, meaning multiple layers of calculations are applied to refine the interpolation continuously.

# Key Components:

1. **Reality (Ground Truth):** Represents the current state of the system, serving as the foundational dataset.
2. **Agents of Change (Vectors of Closest Influence):** These act as pull forces influencing the direction and magnitude of interpolation.
3. **State (Ground Truth Prediction Model):** Utilizes the current data to predict future states based on the influences of the agents.
4. **Altered Time (Goal):** The desired target state, akin to a Mandelbrot-type location on the outer range of the vector field.

# Interpolation Method

The interpolation technique extends beyond simple linear methods by incorporating the complexity and fractal nature of the Mandelbrot set. Here's how it functions:

* **Guided Vectors:** Vectors transition from reality towards altered time, following paths influenced by a Mandelbrot-like vector field.
* **Recursive Layers:** Multiple layers of interpolation allow for increasingly refined calculations, enhancing accuracy and adaptability.
* **Dynamic Intensity:** The closer the interpolation is to reality, the more intense and detailed the calculations become, while the vector field simplifies as it moves towards altered time.

# Theoretical Foundation

The core idea revolves around mapping and adjusting Mandelbrot-inspired vectors to facilitate interpolation between recursively organized data banks. This approach aims to:

* **Capture Complex Patterns:** Leverage the self-similar, fractal nature of Mandelbrot sets to identify and utilize intricate patterns within the data.
* **Enhance Predictive Capability:** Recursive calculations allow for continual refinement of projections, improving predictive accuracy over time.
* **Achieve Real-Time Adaptability:** Dynamically adjust vectors to align with specific goals, similar to how a car's performance might be modulated in real-time to achieve optimal racing outcomes.

# Visual Analogy

Think of this system as calculating the ""ghost"" position of a car in a racing game like *Need for Speed*:

* **Acceleration and Braking:** Based on historical and current data, determining when to accelerate or brake to achieve the best performance.
* **Engine Adjustments:** Modifying the system's parameters in real-time to align with the target state, ensuring the system reaches its goal efficiently.
* **Dynamic Modulation:** Continuously adjusting these actions to meet the desired ""goal time,"" always operating within physical (mathematical) constraints.

# Questions for the Community

1. **Does This Technology Exist?** Is my approach accurately described as a **recursive Mandelbrot predictive method** for vector field interpolation? Are there existing models or research that align closely with this concept?
2. **Resources and References:** If similar technologies or algorithms exist, could you recommend any resources, papers, or specific Mandelbrot-like algorithms that I can study or begin testing with?
3. **Mathematical Validation:** Given that my approach stems from an artistic visualization perspective, what mathematical frameworks or theories should I explore to formalize and validate this method?

# Additional Context

For a visual representation of my model and its applications, you can refer to the following links:

* **Visual Model:** [LinkedIn Visual Model](https://www.linkedin.com/feed/update/urn:li:activity:7267834154631708672/)
* **Use Case Example:** [LinkedIn Use Case](https://www.linkedin.com/posts/jesusfc14_i-think-i-am-reaching-a-clarity-moment-the-activity-7267823963068628992--qA7?utm_source=share&amp;utm_medium=member_desktop)

*(Please note that these links provide additional visual context to help illustrate the concept.)*

**Thank you for taking the time to read through my concept! I'm looking forward to your insights, validations, and any resources you can share to help me advance this idea.**

all this tech is currently under Creature Garage umbrella but I have ownership of the creative driver of the idea so that should be fine for me to post but I reached a moment that I will need help for some of the most advanced math implementations

I am using some concepts that sound really far and advanced but currently my implementation is mostly based on recursiveness the prediction agent will come to function once I have my full set of data to make a test",jesusfc,1h2io35,https://reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,https://www.reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,2024-11-29 10:08:00,2,0.6,2,0,12,0,0,False,False,True,False,False,Research,self,t3_1h2io35
MachineLearning,[D] Most important papers in implicit regularisation,"Hi guys

I'm getting into machine learning, especially on the theoretical side, and I'm curious to learn more about why neural networks tend to generalise so well, so I'm hoping to read some papers about this. As far as I'm aware, the first big paper on the topic was 'Understanding deep learning requires rethinking generalization' by Zhang et al.

I've got a good mathematical background, so I was wondering what people think are the most impactful papers there are in this area. What do you think made the most impact?",MrBeebins,1h29i7j,https://reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,2024-11-29 00:23:33,10,1.0,10,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h29i7j
MachineLearning,[D] Why aren't Stella embeddings more widely used despite topping the MTEB leaderboard?,"https://huggingface.co/spaces/mteb/leaderboard

I've been looking at embedding models and noticed something interesting: Stella embeddings are crushing it on the MTEB leaderboard, outperforming OpenAI's models while being way smaller (1.5B/400M params) and apache 2.0. Makes hosting them relatively cheap.

For reference, Stella-400M scores 70.11 on MTEB vs OpenAI's text-embedding-3-large 64.59. The 1.5B version scores even higher at 71.19

Yet I rarely see them mentioned in production use cases or discussions. Has anyone here used Stella embeddings in production? What's been your experience with performance, inference speed, and reliability compared to OpenAI's offerings?

Just trying to understand if there's something I'm missing about why they haven't seen wider adoption despite the impressive benchmarks.

Would love to hear your thoughts and experiences!",sdsd19,1h1u814,https://reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,2024-11-28 11:45:44,68,0.93,68,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1h1u814
MachineLearning,[R] BitNet a4.8: 4-bit Activations for 1-bit LLMs,"**Paper:** [https://arxiv.org/pdf/2411.04965](https://arxiv.org/pdf/2411.04965)

**Abstract:**

&gt;Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Visual Abstract:**

https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a

**Evaluations:**

[HS=HellaSwag, PQ=PiQA, WGe=WinoGrande](https://preview.redd.it/4ppq57varn3e1.png?width=955&amp;format=png&amp;auto=webp&amp;s=3c4152947edf4542d2a1ffa181bfa52a5369d916)

https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9

",StartledWatermelon,1h1y0ig,https://reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,2024-11-28 15:11:18,28,0.88,28,0,3,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/eMHBOJqKZns6b6d3vVu8taf4yu8Tq472NuD6w0t0t6M.jpg,t3_1h1y0ig
MachineLearning,[R] Fast Matrix-Based Counterfactual Regret Minimization Using GPU Parallelization,"A novel GPU implementation of Counterfactual Regret Minimization (CFR) that accelerates the computation of optimal strategies in extensive-form games. The core innovation is parallelizing the regret updates and strategy computations across GPU cores while carefully managing memory access patterns.

Key technical points:
- Custom memory layout that maps game states and actions to GPU threads
- Batch processing of information sets to maximize GPU utilization
- Parallel computation of counterfactual values and regret updates
- Multi-GPU scaling through game tree partitioning
- Evaluated on Leduc Hold'em and Limit Texas Hold'em poker variants

Results:
- Up to 30x speedup compared to CPU implementation
- Linear scaling with number of GPUs up to 8 devices
- Memory usage scales with game size and number of information sets
- Solution quality matches CPU baseline within statistical error
- Successfully solved games with up to 10^14 states

I think this work could make CFR much more practical for real-world applications beyond poker. The ability to solve larger games faster opens up possibilities in areas like automated negotiation, security games, and resource allocation. The multi-GPU scaling is particularly interesting as it suggests potential for solving even more complex games.

The memory optimization techniques developed here might also transfer well to other game-theoretic algorithms that need to process large state spaces efficiently.

TLDR: GPU-accelerated CFR implementation achieves 30x speedup through careful parallelization and memory management, with linear multi-GPU scaling. Makes solving large extensive-form games significantly more tractable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/gpu-accelerated-counterfactual-regret-minimization). Paper [here](https://arxiv.org/abs/2408.14778).",Successful-Western27,1h1wq6b,https://reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,2024-11-28 14:08:34,24,0.93,24,0,1,0,0,False,False,True,False,False,Research,self,t3_1h1wq6b
MachineLearning,[P] Retrieval augmented generation on-premises (fully local solution),"Hey everyone,   
I’m excited to share my latest repo with you—a local conversational RAG solution for your files! Here’s the deal: this setup is perfect for running RAG on-premises.   
It’s built with Docker, LangChain, Ollama, FastAPI, and Hugging Face, and all models are downloaded automatically. Soon, I’ll add support for choosing your preferred model, but here’s what the solution currently includes:  
• Locally running Ollama: It’s hardcoded to the Qwen-0.5B model for now, but model selection from the Ollama registry is coming soon.   
• Local indexing: Uses a sentence-transformer embedding model (currently restricted to this family, but this will also change soon).   
• Qdrant container: Runs locally for vector storage.   
• Local reranker: Currently uses BAAI/bge-reranker-base, with support for reranker selection coming soon.  
• Websocket-based chat: Includes history-saving capabilities.   
• Simple chat UI: Built with React for a straightforward interface.   
• Bonus: You can use this setup with ChatGPT as a custom GPT! Query your local data through the official ChatGPT web interface or macOS/iOS app.   
• On-premises ready: Everything runs locally, and the containers are CPU-friendly.

A couple of ideas and known issues:   
• Support for Model Context Protocol is on the roadmap.   
• No incremental indexing or reindexing yet.   
• Model selection isn’t available yet but will be added soon.   
  
I’d love your feedback, contributions, or support—watch, fork, and star if you find this interesting!  
Thank you!   
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
",davidvroda,1h26ul7,https://reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,2024-11-28 22:01:09,4,0.63,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1h26ul7
MachineLearning,[P] Latest version of Ollama Grid Search (0.7.0): added prompt database ,"https://preview.redd.it/ohewvqicbo3e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=077fec6931b2efc40182f7c2eb284718822213e0

Hey people... the latest version of [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) now comes with its own prompt management database (along with many improvements in the UI).

https://preview.redd.it/qzu95clhbo3e1.png?width=975&amp;format=png&amp;auto=webp&amp;s=473382281094fc3f819e6fc6c3d267941d2a35ce

It makes it a hell lot easier to test your existing prompts when you pull newly released models!

If you want to check it out, the github page has releases for all major platforms:

[https://github.com/dezoito/ollama-grid-search](https://github.com/dezoito/ollama-grid-search)",grudev,1h20fzv,https://reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,https://www.reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,2024-11-28 17:01:16,10,0.86,10,0,0,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uAl-6kPsNjdKIz7o50pQEHnMDPXa-cNX7GoAlrOqiec.jpg,t3_1h20fzv
MachineLearning,[D] Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis,"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 **Visatronic: A Multimodal Decoder-Only Model for Speech Synthesi**s by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/ygxnbhiboo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=31bf7c9b988c83a8d0ff2e7b011dac027aa8f154

https://preview.redd.it/7v15egiboo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=d9629caa406a92f8b2052ad6baa3a0265a27ddcf

",CATALUNA84,1h222s2,https://reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,https://www.reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,2024-11-28 18:13:13,4,0.67,4,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/GIw9ZddndVdad-4h4y65mlExW8FTRdhSBchmteHv-AA.jpg,t3_1h222s2
MachineLearning,[D] Inconsistent use of the gerund form in dataset naming,"This is, of course, a very minor issue. But it really irks me, and I was wondering if other people are bothered by it. I think the two most common naming schemes I see for the three standard dataset splits are ""training dataset, validation dataset, and test dataset"" or ""training dataset, validation dataset, and testing dataset"". But to be consistent it should really be ""train dataset, validation dataset, and test dataset"" or ""training dataset, validating dataset, and testing dataset"". I always use the former. Does this bother anyone else, or am I alone in brooding over this?",Shianiawhite,1h217sh,https://reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,2024-11-28 17:34:46,4,0.75,4,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h217sh
MachineLearning,Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis [D],"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 V**isatronic: A Multimodal Decoder-Only Model for Speech Synthesis** by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/7eoqonhqjo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=c751f5c9f9bc42f22672bbbc67cf8b0fe95ef36b

https://preview.redd.it/224tugzrjo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=37adc93604606d86492dc104f03915bab7428eca",CATALUNA84,1h21j1a,https://reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,https://www.reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,2024-11-28 17:48:43,2,0.63,2,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/wIqPUsHaUNfLLWwLHYcQkdG4Yav_LVjLmzLghfKaOjI.jpg,t3_1h21j1a
MachineLearning,[D] Loading data into Ray clusters,"For those of you that run ML training in a Ray cluster on AWS, I'm curious to know what approach you take to get training data into your cluster?

And how are you versioning the data?

How do you avoid repeatedly downloading the same data across runs that have the same dataset?

I'd like a smooth process for being able to target a specific version of a dataset for a training run, and to avoid repeatedly downloading it. The data versioning should have a clear mapping to whatever version of a data pipeline created it. It'd also be nice to have something that scales well to larger datasets.

Keen to hear experiences from the trenches.",SingularValued,1h1v68j,https://reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,2024-11-28 12:45:29,6,0.88,6,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1v68j
MachineLearning,Causal Discovery Competition Winning Paper Discussion [D],"I’ve recently come across this post: https://thetourney.github.io/adia-report/ which describes the winning method for a casual discovery competition. It’s not really my field but I do have a reasonable understanding of GNNs and Causal Inference. Anyway, from the report I don’t understand precisely what the winning team was doing. Can anyone either link to a full paper or have a good intuitive and potentially step by step explanation of what they are doing?",www3cam,1h1i0ji,https://reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,2024-11-27 23:22:43,29,0.94,29,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1h1i0ji
MachineLearning,[D]Is Freelancing as a Data Scientist Even Possible?,"Hi everyone,

I’m fine working for as low as $15/hour, so earnings aren’t a big concern for me. I’ve gone through past Reddit posts, but they mostly discuss freelancing from the perspective of income. My main concern is whether freelancing in data science is practical for someone like me, given its unique challenges.

A bit about my background: I’ve completed 3-4 real-world data science projects, not on toy datasets, but actual data (involving data scraping, cleaning, visualization, modeling, deployment, and documentation). I’ve also worked as an intern in the NLP domain.

Some issues I’ve been thinking about:

1. Domain Knowledge and Context: How hard is it to deliver results without deep understanding of a client’s business?


2. Resource Limitations: Do freelancers struggle with accessing data, computing power, or other tools required for advanced projects?


3. Collaboration Needs: Data science often requires working with teams. Can freelancers integrate effectively with cross-functional groups?


4. Iterative and Long-Term Nature: Many projects require ongoing updates and monitoring. Is this feasible for freelancers?


5. Trust and Accountability: How do freelancers convince clients to trust them with sensitive or business-critical work?


6. Client Expectations: Do clients expect too much for too little, especially at low wages?



I’m also open to any tips, advice, or additional concerns beyond these points. Are these challenges solvable for a new data science freelancer? Have any of you faced and overcome similar issues? I’d love to hear your thoughts.

Thanks in advance!",ds_reddit1,1h1q98i,https://reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,https://www.reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,2024-11-28 07:01:15,6,0.56,6,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h1q98i
MachineLearning,[P] Ablation study using a subset of data?,"Basically, I'm engaging in a research project in which I'm training encoder only language models for text classification. I have already trained my models and gotten my results, however I need to perform an ablation study. The main issue I'm having is that the dataset is large. Is it fair for me to perform the ablation study on a subset of the dataset, since I'm gonna have to train it 3 - 4 times with different ablations?",Aromatic_Web749,1h1kzsh,https://reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,2024-11-28 01:51:54,9,0.91,9,0,8,0,0,False,False,True,False,False,Project,self,t3_1h1kzsh
MachineLearning,[D] Advanced methods of data management,"Hello everyone, right now I'm involved in a project which is basically (audio)LLM fine-tuning, and we're having problems related to data management. 

Since there isn't that many sets in that area, we're using different augmentation schemas. tl;dr, we have different datasets of various nature, and using some schemas we can convert usual ASR-type datasets to QA, generate refusal data, combine questions etc. 

Problem is, it's hard to control ratios of different data, and right now it's mostly a manual labour. We kinda have to manually adjust the amount of data we're generating. Which is rather annoying and hard process; we have many target datasets, you have to remember how much samples you've generated and try to get some adequate mix for train.

Right now, we use airflow for data filtering (lots of raw data is badly labelled), but I'm not really sure that I understand how to connect that tool to data generation, and if it's a good tool for that purpose. I was thinking about writing some snakemake script, but ideally the final solution should be flexible when we change configs, add new sets etc, and that's not what I associate with snakemake. Also, another question is visualization. 

So, I'm asking what kind of tools/libs I can use to tackle this task, is there anything that can fit our purposes, or it's time to write customs scripts? What do companies like meta use for their enormous 15t tokens set, surely they don't dump everything in one place? 

There's another question related to connecting datasets with data pipeline of training (it's complicated, there are additional steps of precalculation of feats and creating webdataset which is used in training), but for now I'd be really glad if anyone helped with just data management tools. ",Theio666,1h1u0pj,https://reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,https://www.reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,2024-11-28 11:31:47,1,1.0,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1u0pj
MachineLearning,[D] AAMAS 2025 reviews are out! ,"I could not find a discussion thread, so I thought I would create one myself. ",E-Cockroach,1h15k8k,https://reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,2024-11-27 14:28:29,26,0.91,26,0,38,0,0,False,False,True,False,False,Discussion,self,t3_1h15k8k
MachineLearning,[P] py-gen-ml: generating ML configuration code from a schema,"[py-gen-ml](https://jostosh.github.io/py-gen-ml) is a Python library designed to simplify your ML experiment configuration using the power of Protocol Buffers. It's still in an early phase but I'd love to hear some feedback from the community.

**Here's how py-gen-ml can help you:**

* **Centralise configurations:** Define schemas in Protobuf to act as a single source of truth.
* **Minimise repetitive work:** Automatically generate code for models, patches, sweeps, and a command-line interface.
* **Boost flexibility:** Experiment with ease thanks to YAML configurations with advanced referencing and the ability to conduct hyperparameter sweeps.
* **Improve code quality:** Benefit from JSON schema validation, strong typing, and IDE support for a more robust development process.

**py-gen-ml aims to make ML development more efficient by reducing the burden of managing configurations.** Give it a try and see how it can improve your workflow.

**Get started:**

    pip install py-gen-ml

**Learn more:** [**https://jostosh.github.io/py-gen-ml**](https://jostosh.github.io/py-gen-ml)",jalapenjos,1h1t9v0,https://reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,https://www.reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,2024-11-28 10:40:23,0,0.25,0,0,1,0,0,False,False,True,False,False,Project,self,t3_1h1t9v0
MachineLearning,"[P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, Docker)","  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to introduce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, you can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork, star) Thank you so much!",davidvroda,1h1pudq,https://reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,2024-11-28 06:33:40,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1h1pudq
MachineLearning,[D] how to do RLHF on this kind of data?,"Hi, apologies if this is a dumb question -- I'm really not knowledgeable about post training. Suppose that I have a llama and I want to finetune with human annotations that ""like"" or ""dislike"" a prompt response. Most DPO datasets feature a pair of possible responses, with one being chosen. Interpreting my data as one half of a pair with one missing, I could generate a second response from the same prompt and say that it is preferred if ""like""d and it is not preferred if it is ""disliked"". Is there a better way?",khidot,1h1bpwq,https://reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,2024-11-27 18:50:05,6,0.72,6,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h1bpwq
MachineLearning,[D] Which LLM models can I run on an NVIDIA 4060 for research purposes? Recommendations needed!,"Hi everyone,

I’m diving into research on large language models (LLMs) and looking to experiment with running them locally on my NVIDIA 4060 GPU. While I know the 4060 isn’t a high-end card compared to some research setups, I’m optimistic about making the most out of what it offers. I’d greatly appreciate any insights or recommendations on:

1. **Models that can run efficiently** on a 4060. I’m aware that some smaller versions of LLMs might be more suited for this hardware, so any advice on what’s realistically possible without excessive optimization would be fantastic.
2. **Models suitable for fine-tuning or pre-training experiments.** Although I’m starting with basic experiments, I plan to explore fine-tuning in the future, so I’d love suggestions for models that are versatile and widely used in research.
3. **Open-source models** or ones that are easy to access and work with for research purposes. Licensing and transparency are important to me, as my work is focused on academic and experimental objectives.

So far, I’ve been looking at options like LLaMA, GPT-NeoX, and BLOOM, particularly their smaller variants, but I’m open to exploring other possibilities. If you’ve had experience running these or similar models on mid-range GPUs, I’d love to hear your thoughts on performance, setup, or any potential limitations I should be aware of.

Additionally, I’d be grateful for any advice on:

* **Optimizing models for a 4060.** Are there specific tools, techniques, or libraries (like bitsandbytes or FlashAttention) that could help with running or fine-tuning these models?
* **Preparing for fine-tuning.** What should I keep in mind when selecting a model to ensure it can support future fine-tuning experiments effectively?

Thank you in advance for sharing your expertise! I’m eager to learn from the community and make the most of this setup.",Spinotesla,1h1ux8m,https://reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,https://www.reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,2024-11-28 12:30:28,0,0.25,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h1ux8m
MachineLearning,[D] AISTATS 2025 reviews,Aistats 2025 reviews are supposed to be out today. So I thought to create a discussion post for the same where we can share our experiences!,PhoneImpressive9983,1h0x428,https://reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,2024-11-27 05:31:15,47,0.95,47,0,127,0,0,False,False,True,False,False,Discussion,self,t3_1h0x428
MachineLearning,[D] How valid is the evaluation using LLMs?,"Hello community,

I am bit new to using Gen AI, I want to check the validity of using larger LLMs to evaluate the result of other LLMs. I have seen different blogs who does this for the purpose of automating the evaluations.

For eg. To evaluate a list of English translations my a model A, is it valid to prompt another model B, something like this '''Is this translation correct original text: {original_text}, Translated text {translated_text}'''

Is this a valid way of evaluating? Something inside me says it's scientifically wrong, because the LLM model B itself will have some error to it right?",raman_boom,1h11lbt,https://reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,2024-11-27 10:48:09,15,0.78,15,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1h11lbt
MachineLearning,Residuals in ensemble MLR [D],"
Hi all

New to ensembles.

If you ensemble MLR, you may end up with a non-linear equation however….

A) the residuals of the indicidual MLR that were ensembled need to meet parametric assumptions? Can’t use a crap MLR just because it’s going to be used in an ensemble?
B) if the ensembled MLR equation is linear then residuals should meet parametric assumptions?

Thanks


",Yellow_fruit_2104,1h1ero2,https://reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,https://www.reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,2024-11-27 20:58:23,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h1ero2
MachineLearning,[R] Meissonic: High-Resolution Text-to-Image Generation via Enhanced Masked Image Modeling,"This work introduces a non-autoregressive masked image modeling (MIM) approach that aims to match SDXL-level image generation while avoiding the token inefficiencies of autoregressive methods. The key innovation is combining MIM with architectural improvements and sampling optimizations to enable high-resolution image synthesis.

Main technical points:
- Uses a transformer-based architecture with specialized self-attention and positional encoding
- Incorporates human preference scores as ""micro-conditions"" to guide generation
- Employs feature compression layers to handle high resolutions efficiently
- Generates 1024x1024 images through parallel token prediction rather than sequential
- Achieves comparable FID scores to SDXL while being more computationally efficient

Results:
- Image quality metrics competitive with SDXL on standard benchmarks
- Faster generation compared to autoregressive approaches
- Better handling of complex scenes and compositions
- Improved text alignment compared to previous MIM approaches

I think this could impact the field in several ways:
- Shows that non-diffusion approaches can achieve SOTA-level generation
- Provides a potential path toward unified language-vision models
- May lead to more efficient deployment of text-to-image systems
- Could influence architecture design for future multimodal models

The biggest open question in my view is whether this approach can scale further - while it works well at current resolutions, it's unclear if the same principles will hold at even higher dimensions.

TLDR: Non-autoregressive masked modeling approach matches SDXL-level image generation while being more efficient than typical autoregressive methods. Shows promise for unified language-vision architectures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/meissonic-revitalizing-masked-generative-transformers-efficient-high). Paper [here](https://arxiv.org/abs/2410.08261).",Successful-Western27,1h1529m,https://reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,https://www.reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,2024-11-27 14:05:29,8,0.9,8,0,2,0,0,False,False,True,False,False,Research,self,t3_1h1529m
MachineLearning,[D] Cross Entropy Loss sucks,"Hi guys, Am I the only one thinking that training a LLM to minimize CE Loss on a certain text dataset is a very surprising idea?

I understand that it works but I am surprised it is still SOTA. The current sentence could have begun with a lot of different tokens with no consequence on its meaning, while some words are uninterchangeable. Yet CE loss doesn't account for that. Worse off, the bigger the ""equivalence class"" (the number of tokens that could replace one in a sentence without altering its meaning) of a token in a sentence, the higher the average loss on it. It seems counterproductive, isn't it?

I would love to read some contradiction.",Due-Pangolin325,1h1sqkl,https://reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,https://www.reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,2024-11-28 10:02:18,0,0.29,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h1sqkl
MachineLearning,[R] Black holes and the loss landscape in machine learning,"Abstract:

&gt;Understanding the loss landscape is an important problem in machine learning. One key feature of the loss function, common to many neural network architectures, is the presence of exponentially many low lying local minima. Physical systems with similar energy landscapes may provide useful insights. In this work, we point out that black holes naturally give rise to such landscapes, owing to the existence of black hole entropy. For definiteness, we consider 1/8 BPS black holes in =8 string theory. These provide an infinite family of potential landscapes arising in the microscopic descriptions of corresponding black holes. The counting of minima amounts to black hole microstate counting. Moreover, the exact numbers of the minima for these landscapes are a priori known from dualities in string theory. Some of the minima are connected by paths of low loss values, resembling mode connectivity. We estimate the number of runs needed to find all the solutions. Initial explorations suggest that Stochastic Gradient Descent can find a significant fraction of the minima.

Arxiv: [https://arxiv.org/abs/2306.14817](https://arxiv.org/abs/2306.14817)",Mindless-House-8783,1h0uwjd,https://reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,https://www.reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,2024-11-27 03:26:49,26,0.76,26,0,28,0,0,False,False,True,False,False,Research,self,t3_1h0uwjd
MachineLearning,[D] AISTATS 2025 Paper Reviews,"Since the AISTATS 2025 paper reviews are due today, I thought to open up a thread where everyone can discuss their experiences!
",PhoneImpressive9983,1h0y8rn,https://reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,2024-11-27 06:42:52,9,0.8,9,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h0y8rn
MachineLearning,[D] ACL ARR Discussion - About Author Response,"Hi all! currently submitted to ACL ARR Oct. Now the author response phase is over and we haven't received any reply (to our responses) from reviewers.

Want to ask if reviewers can still update their reviews *after* the end of the author response phase and *before* the meta-review is given, or does it mean that I won't receive any replies?",Ok_Function6276,1h13ffu,https://reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,https://www.reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,2024-11-27 12:44:02,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h13ffu
MachineLearning,"[D] Knowledge distillation neural network
","Hi community,

  
Suppose my original neural network model size is 50MB. Is there a way to estimate the size of the distilled model after applying Knowledge distillation.",PhilosopherNew313,1h17xwc,https://reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,https://www.reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,2024-11-27 16:13:12,0,0.45,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h17xwc
MachineLearning,[P] [D] Comparing Llama Models and GPT 4o Models on Multilingual Machine Translation with Backtranslation,"Hey all,

In the spirit of practical real world tasks for LLMs, we wanted to see how well different models could automatically translate text from English to Spanish and the backtranslate to English on a Nike product catalog. We started with Llama 405B, Llama 70B, Llama 8B, GPT 4o-mini, and GPT 4o, but would love to test [more models](https://www.oxen.ai/explore/models).

  
\~ TLDR \~ Here are the results with all the data and code here:

[https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments](https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments)

https://preview.redd.it/qken2vjfhc3e1.png?width=1150&amp;format=png&amp;auto=webp&amp;s=739ef336dd7b89856a39d872ef12e03f806ce799

Although backtranslation may not be the most effective way to benchmark, we thought this would be an interesting experiment to see how well it correlates with model performance. It would be ideal to get native Spanish speakers to annotate the dataset with ground truth labels, so if anyone wants to contribute feel free to fork the repo and we can get some real labels.

  
We're trying to make some more real world datasets / benchmarks, so let us know if you want to help out.

If you’re new to the [Oxen.ai](https://www.oxen.ai/) project, we’re building a fast [open source dataset collaboration tools](https://github.com/Oxen-AI/oxen-release) as well as a ton of [helpful data exploration tools](https://docs.oxen.ai/features/web_hub) on top of it! If you are into data or ML/AI, we’d love your thoughts on the tool and project!",FallMindless3563,1h0sehj,https://reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,https://www.reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,2024-11-27 01:17:44,14,0.73,14,0,12,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/mNfbgTf9Tu8SygkvvAsqVh2unMgd_M1VIenrJ09Hz3s.jpg,t3_1h0sehj
MachineLearning,[R] Help with submitting a WACV workshop paper,"Hi Everyone,

I have never submitted a paper to any conference before. I have to submit a paper to a WACV workshop due on 30 Nov.

As of now, I am almost done with the WACV-recommended template, but it asks for a Paper ID in the LaTeX file while generating the PDF. I’m not sure where to get that Paper ID from.

I am using Microsoft CMT for the submission. Do I need to submit the paper first without the Paper ID to get it assigned, and then update the PDF with the ID and resubmit? Or is there a way to obtain the ID beforehand?

Additionally, What is the **plagiarism threshold** for WACV? I want to ensure compliance but would appreciate clarity on what percentage similarity is acceptable.

Thank you for your help!",__proximity__,1h15p2e,https://reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,https://www.reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,2024-11-27 14:34:45,0,0.5,0,0,1,0,0,False,False,True,False,False,Research,self,t3_1h15p2e
MachineLearning,[R] Genetic learning with loop mempory and Chromosomes for the memory neurode's gate.,"Greetings!  
  
Currently a bit busy will clean it up later also to lazy to implement git now... &gt;\_&gt;

[https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md](https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md)

",_Leto,1h15c7q,https://reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,https://www.reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,2024-11-27 14:18:09,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h15c7q
MachineLearning,[D] A blog post explaining sparse transformers (the original paper),"Hi!

I'm sorry if it's not appropriate to publish such posts on this subreddit. I do stay out of this type of posts on this subreddit but I keep seeing articles or videos or whatever content explaining GPT-3 without delving into sparse transformers. And it keeps frustrating me because clearly in the paper they say ""we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer"".

But no one seems to care about explaining them. I understand why to be honest but it's frustrating to see all these articles, projects, videos etc. that try to explaining everything about the GPT not even mentioning the sparse transformers part. And besides many other elements specific to GPT-3 or general to reproducibility in ML, the sparse transformer part is a big dent into even prototyping GPT-3.

I have this habit of writing down stuff when trying to understand something so I wrote a blog post on sparse transformers. Never spoke about it because I did it to restructure my thoughts and as notes for me. So it's not something I'd avise anyone to read, I'm sure it's full of typos, my writing style is not neat etc. It's just something I did for me in a way *I* would understand and recover lost bits of information when skimming through it.

Anyways, in case you're reading papers by yourself and trying to constitute the knowledge just from them, maybe my notes can help you: [https://reinforcedknowledge.com/sparse-transformers/](https://reinforcedknowledge.com/sparse-transformers/)

Sorry again if this post is not appropriate and for yapping that much.

(If you happen to read it or if you notice any errors, do not hesitate to point them out, I'd be grateful to learn from them)",ReinforcedKnowledge,1h0gl2j,https://reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,https://www.reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,2024-11-26 16:55:45,24,0.93,24,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h0gl2j
MachineLearning,[P] What Transcription Model does Google Meets use? ,"Hi, I am currently evaluating options for transcribing sensitive meeting texts. I'd like to know what kind of transcription model is currently being used by google to transcribe meetings. I've searched the documentation and the web, and it doesn't seem to specify. I initially thought chirp would be used for this, but the documentation specifies English as the only reliable language to transcribe, which isn't true of chirp. 

This isn't a post asking which model (google or otherwise) to use, or all the better options out there, this is a very specific inquiry into Google's approach. I'd love to get some insight here. Thanks!",Arcane_Aura,1h0u63q,https://reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,https://www.reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,2024-11-27 02:47:35,1,0.56,1,0,3,0,0,False,False,True,False,False,Project,self,t3_1h0u63q
MachineLearning,"[P] I built Darkspark, a visual representation of your neural network. Explore everything from macro-level architecture to low-level ops and activations — Your model wants to be seen!","When reading a paper on arxiv or perusing code I also like to sketch out the model architecture myself on a big piece of paper to use as a reference. This is the software version of that. It's a GUI for your neural network. Here's the link: [https://darkspark.dev](https://darkspark.dev)

I tried all the other options I could find (netron, google’s model-explorer, tensorboard, torchview, torchlens, apple’s mycelium). These are all great projects (I really wanted to use one of them!) but none had all of the features I needed:

**Opinionated layout.** The tool’s layout should automatically expose the underlying logic of the model. The layout engine should do a lot of the heavy lifting of understanding a model’s structure and intentions. E.g. a U-net should look like a “U”. Here's [stable-diffusion-v1.5](https://darkspark.dev/models/?model=stable-diffusion-v1-5) traced directly from a huggingface [pipeline](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)

[stable-diffusion-v1.5 in the darkspark viewer](https://preview.redd.it/xksm2u1ipa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=fb7e576192c066ddc7e72ad8491c6347181525a7)

**Interactive**. I need collapsible and expandable modules so I can explore a model at a high level but can also go down to the lowest level ops. Complex models won’t even load without this. Here's the same diffusion model zoomed in on a transformer block

[stable-diffusion-v1.5 zoomed in](https://preview.redd.it/7fmmw341qa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=8538420973f578b68a4e225082b6990c867aefaf)

**‘Just Works’ with any arbitrary code**. I don’t want to export to ONNX, I don’t want to upload something, I don’t want to manually specify what is the model and what are the inputs. I just want to wrap my existing code in something simple.\*

    import darkspark
    import timm
    import torch
    
    model = timm.create_model(""efficientnet_b0"")
    inputs = torch.randn(1,3,224,224)
    
    with darkspark.Tracer():  # &lt;-- wrap your code with this line
      out = model(inputs)
    
    # interactive diagram now available at localhost



**Microscope**. Sometimes I also want to explore the activations and attention patterns. Like OpenAI’s microscope tool, but for your own models. Here's a “female / male” detector in a later layer of the pretrained [vit\_base\_patch16\_siglip\_224](https://darkspark.dev/models/?model=vit_base_patch16_siglip_224-microscope) from the timm library.

[female \/ male detector in darkspark viewer](https://preview.redd.it/8fzwcvcjqa3e1.png?width=1520&amp;format=png&amp;auto=webp&amp;s=fabed874645c834b8380e08a0ef7ca3f1bcb7962)

Here's the attention patterns explorer for the same model.

[Attention explorer for vit\_base\_patch16\_siglip-microscope](https://preview.redd.it/fvmxs1buqa3e1.png?width=1236&amp;format=png&amp;auto=webp&amp;s=7d7a91c469616653841111bd84879b8bcbeb1b22)



**Hosted gallery**. Most of what I want is usually a variant of an existing model. It’s often more convenient to just reference a url rather than trace your own code. I currently have all the models from timm and many from the transformers and diffusers libraries.

[lots of models available to peruse](https://preview.redd.it/qx837cz2ra3e1.png?width=1158&amp;format=png&amp;auto=webp&amp;s=f23808a45a4b7927828de24c4e1d0790a37420bb)

The public pip package isn’t yet ready, I was hoping to get feedback on the tool itself before cleaning up and sharing the codebase. Please let me know what you think, I'm eager for feedback on everything from low-level UI/UX to high-level functionality. Thanks to the awesome community for checking it out!

Here's the link again: [https://darkspark.dev](https://darkspark.dev)

\* darkspark uses \_\_torch\_function\_\_, similar to the torchview library. This allows us to capture all the ops and tensors inside the context of darkspark.Tracer without breaking when it hits dynamic control flow ops that can’t be captured in e.g. ONNX or torch exported\_program. We also get access to all the tensors, activation patterns, etc, without using hooks. Happy to answer more Qs about the architecture if ppl are interested.",Historical-Good1915,1h0krsv,https://reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,https://www.reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,2024-11-26 19:42:44,7,0.89,7,0,5,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/8bDoJ2A5fEzlL66g8tteX57z1GhfRWnpnzQOclCaFy4.jpg,t3_1h0krsv
MachineLearning,[D] Prune (channel + layers) + distillation or just distillation,"Let's say I want to make my model smaller.

There is a paper, which says distillation is good, but it takes a long time [https://arxiv.org/abs/2106.05237](https://arxiv.org/abs/2106.05237)

And there is also a paper which says that pruning + distillation works really well: [https://arxiv.org/abs/2407.14679](https://arxiv.org/abs/2407.14679)

Now, my question is: Is there any work that compares pruning + distillation vs just distillation from scratch?",osamc,1h0kcgn,https://reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,https://www.reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,2024-11-26 19:25:36,5,0.86,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0kcgn
MachineLearning,[P] [D] Predict Integer Values with XGBoost Regression,"Hello! I am new to Data Science but enjoying every moment of it.

I am currently working with the XGBoost model and while everything is working fine (more or less), I am struggling with a specific issue. I am predicting 'number of orders' based on certain criteria. Since number of orders follows Poisson distribution, I have specified that and I am getting decent predictions. However, the predictions are floating point numbers. Is there any way to tell the model to give integers instead?

PS: I have tried the rounding method and while it works great, I wanted something that is at the model level.",MapleWalnut96,1h10ta0,https://reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,https://www.reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,2024-11-27 09:51:34,0,0.3,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h10ta0
MachineLearning,[D] Am I a complete idiot for signing up for a Hackathon?,"Ok, so I am a Coms Science graduate student and my chosen area of study is Ethical AI.

I wanted to attend this AI conference very badly because there are some speakers that I admire. But I couldn’t afford the passes, so I decided to apply to be in the student Hackathon because if accepted, you got a free pass.

It was such a Hail Mary for me to even do the application, but I thought it would also be a cool opportunity to learn alongside others.

I got accepted… and I’m extremely excited. But now I’m like, oh wait, am I going to royally piss off whomever my teammates are because I can’t code?

Any advice? There’s a preparatory webinar happening in a week, and I’ve been doing some overview classes so that I can learn the terminology/basics. The application also asked for me to state my level of coding experience and I checked: none. And still got accepted… so I’m hoping that the organizers consider me to still have something valuable to contribute?

Please let me know what you think 🥲",sydj_k941,1h01hfn,https://reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,https://www.reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,2024-11-26 02:44:20,48,0.72,48,0,71,0,0,False,False,True,False,False,Discussion,self,t3_1h01hfn
MachineLearning,[D] what are some problems in audio and speech processing that companies are interested in?,I just recently graduated with a bachelor's in computer science and am really interested in auio and machine learning and want to do a project with a business scope. what are some problem statements that companies would be interested in? especially gen ai related ,Personal_Equal7989,1h082e6,https://reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,https://www.reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,2024-11-26 09:36:01,9,0.77,9,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1h082e6
MachineLearning,[D] Do modern neural network architectures (with normalization) make initialization less important?,"With the widespread adoption of normalization techniques (e.g., batch norm, layer norm, weight norm) in modern neural network architectures, I'm wondering: how important is initialization nowadays? Are modern architectures robust enough to overcome poor initialization, or are there still cases where careful initialization is crucial? Share your experiences and insights!",NumberGenerator,1gzq63h,https://reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,https://www.reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,2024-11-25 18:37:08,95,0.97,95,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gzq63h
MachineLearning,[D] Model validation for transformer models,"I'm working at a firm wherein I have to validate (model risk validation) a transformer architecture/model designed for tabular data.

Mapping numbers to learned embeddings is just so novel. The intention was to treat them as embeddings so that they come together on the same ""plane"" as that of unstructured text and then driving decisions from that fusion. 

A decision tree or an XGBoost can be far simpler. You can plug in text based embeddings to these models instead, for more interpretability. But it is what is.

How do I approach validating this transformer architecture? Specifically if or if not it's conceptually sound and the right choice for this problem/data.",Common-Interaction50,1h0sc7o,https://reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,https://www.reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,2024-11-27 01:14:44,0,0.38,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0sc7o
MachineLearning,[D] ADOPT optimizer,"Have any of you tried the new ADOPT optimizer? How did it go? I'm kind of curious, but haven't had the opportunity to give it a try.",neu_jose,1h01cqq,https://reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,https://www.reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,2024-11-26 02:37:39,7,0.82,7,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h01cqq
MachineLearning,[D] GPT2 (Hugging Face) transformer giving bogus responses. Why and how do I fix this?,"I have done some research on this and it looks like not a lot of people talk about it. I am newer to transformers (though I have some experience in Sequential models) and trying to train ChatGPT on a specific niche I want to make a bot for, but it isn't understanding anything and I feel as if I've been going in loops with it.

For example:

Me: ""Tips for planetary imaging?""

GPT2: ""Have clue how get good resolution out there will someone explain use astrobin where can look info about other equipmentpeople who this correctlyslightly better?pleiades, andromedahow much data does your setup need order score decent results ? this would more than what most hobbyists spend .5mil each day!ive taken some pics before but when trying new things like stacking that makes wonder they really work well owhats left over after shooting all those different targets guessedit yes maybe even with 1000 frames per minute thats still.""

I have custom trained it on fetched data using the Reddit Developer API from a subreddit (About 42000 lines worth) and have structured the dataset like this:

""Title: M31 Andromeda - with rotated frames

Body:

Comments:

\- A bit of a gimmick image to show stacking artefacts when the sub-frames are rotated. 7 images total, rotated 15 degrees each. 14 mins (!) total exposure.

Equipment:

\* Stellarview SVX080T Telescope 480/80mm @ f/6

\* Losmandy G11G mount

\* ZWO ASI071MC Pro color camera @ -5c + Optolong L Pro Filter

\* 60mm Guidescope with ASI120MM camera

Subs:

\* 7 x 120s

\* Master Dark

\* No Flats

Software:

\* PHD2 &amp; Sequence Generator Pro

\* Astro Pixel Processor, DeepSkyStacker, Photoshop

Processing

\* Default color integration in APP

\* Light pollution removed, stretched and exported to Photoshop

\* Same integration performed in Deep Sky Stacker (APP did such a good job it didn't show \*any\* stacking artifacts but DSS did)

\* Blended the APP image with the DSS image to show stacking artifacts in PS

\* Camera Filter shenanigans, export to jpg

\- Honestly that’s a pretty cool presentation!! You can really make this significantly better I think. Maybe like 40x60” frames per rotation or something like that to get better detail and less noise. The 120” subs blew out a lot.

Try again!!

\- \[deleted\]

\- Noob question here but about how much does a setup cost to get images like this?

\- LOVE THIS

\- It’s beautiful

\- This is sick

\- This is how every astrophotos should be ! It’s so beautiful !! I can definitely see this hanging on the wall in my bedroom 😍

\- Imagine some human like civilization on Andromeda taking pictures of the milky way

\- \[deleted\]

&lt;|endoftext|&gt;""

Trained using this dataset and GPT2-Medium.

Here are my parameters:

    outputs = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=max_length,
                        temperature=0.8,
                        top_p=0.9,
                        do_sample=True,
                        repetition_penalty=1.3,
                        no_repeat_ngram_size=3,
                        eos_token_id=self.tokenizer.eos_token_id,
                        pad_token_id=self.tokenizer.eos_token_id
    )
    
    
    system_prompt = (""You are Astrophoto AI, an encouraging astrophotography expert and teacher.""
                ""Your role is to help beginners and experienced photographers capture stunning images of the night sky and answer any questions they might have.""
                ""You offer concise, factual, and practical advice drawn from established astrophotography techniques.""
                ""Your tone is friendly, encouraging, and focused on making astrophotography accessible to everyone.""
                ""If you don't know the answer to a question, admit it instead of guessing."")

What are some potential issues with this?

Thanks!

EDIT: thanks for your advice everyone! I will be switching models.",Aman_Dude,1h0okd5,https://reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,https://www.reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,2024-11-26 22:19:18,0,0.35,0,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1h0okd5
MachineLearning,"[P] does anyone know how to reduce the dimensions of embeddings using autoencoders, if you have a blog about please send it","https://preview.redd.it/3cub8uc9ja3e1.png?width=766&amp;format=png&amp;auto=webp&amp;s=0a824d6ae516ff699cb880d8e998ace85354a50f

",GellertGrindelwald_1,1h0j77v,https://reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,https://www.reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,2024-11-26 18:39:43,0,0.27,0,0,3,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uUwCHYEmzWw3FiWk1T2hjt5K8zWvsMqQ8Y4dqssDqUM.jpg,t3_1h0j77v
MachineLearning,[Project] Claude Francois - Let an AI review your code in the style of François Chollet,"Demo here: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

At the recent Anthropic Builder Day hackathon, we ([Crossing Minds](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning)) built 'Claude François', an AI code reviewer trained in the style of [François Chollet](https://github.com/fchollet), the creator of Keras. It adapts Anthropic's Claude 3.5 Sonnet for code reviewing, but instead of regular fine-tuning, we used few-shot in-context learning with our custom RAG retrieval model, trained on PRs from the [Keras project](https://github.com/keras-team/keras). Compared to a typical AI code reviewer, it provides more succinct, high-quality code reviews focused on real issues rather than superficial nitpicking.

How it works:

* Dataset: Trained on a database of public Keras GitHub PRs and François's reviews.
* Fine-Tuned RAG Embeddings: Uses active learning and RLAIF to train embeddings optimized for generating ""fchollet-level"" reviews.
* Improved Retrieval: Retrieves relevant examples not just by embedding similarity but by optimizing for mutual information.
* Self-Reflection: Employs self-reflection techniques to enhance Sonnet’s reasoning capabilities.

This technology demo showcases how [Crossing Minds' RAGSys](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning) ICL enables domain adaptation without fine-tuning. It can be used for countless other use cases beyond code reviews, like classification, summarization, translation, search, recommendations, and more. Arxiv paper coming soon!

Try it now: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

We'd love to hear your feedback!",Crossing_Minds,1gzmk5n,https://reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,https://www.reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,2024-11-25 16:16:03,25,0.7,25,0,13,0,0,False,False,True,False,False,Project,self,t3_1gzmk5n
MachineLearning,[R] Aurora: A General-Purpose Foundation Model for Earth System Prediction,"The key contribution here is the development of Aurora, a foundation model trained on over 1M hours of atmospheric data that can perform multiple types of weather and climate predictions using a single model architecture. This represents a shift from building separate specialized models to having one model that learns general atmospheric physics.

Key technical points:
- Model architecture uses transformer blocks with attention mechanisms adapted for spatiotemporal data
- Trained on merged datasets from multiple sources including ERA5 reanalysis, satellite observations, and climate model outputs
- Can generate predictions for diverse tasks like air pollution, precipitation, and temperature forecasting
- Produces forecasts in under 1 minute compared to hours/days for traditional numerical models
- Outperforms both specialized ML models and physics-based numerical weather prediction on several benchmarks

Results:
- 15-20% improvement in 5-day global air pollution predictions vs current methods
- Better performance on 10-day weather forecasts compared to specialized models
- Maintains accuracy even for extreme weather events
- Shows continual improvement as training data increases
- Successfully handles multiple spatial and temporal resolutions

I think this work could significantly change how we approach environmental modeling. Instead of maintaining separate models for different prediction tasks, having a single foundation model that can handle multiple atmospheric predictions could make forecasting more efficient and accessible. The speed improvements (minutes vs hours) could enable new applications requiring rapid predictions.

I think the challenges ahead include:
- Validating performance across more diverse atmospheric phenomena
- Understanding model interpretability for critical forecasting
- Addressing computational costs of training and inference
- Ensuring reliability for operational forecasting systems

TLDR: Researchers developed Aurora, an atmospheric foundation model trained on massive weather/climate data that can handle multiple prediction tasks better than specialized models while being much faster. Shows foundation models could transform environmental forecasting.

[Full summary is here](https://aimodels.fyi/papers/arxiv/foundation-model-earth-system). Paper [here](https://arxiv.org/abs/2405.13063).",Successful-Western27,1gzj8rs,https://reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,https://www.reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,2024-11-25 13:50:09,38,0.89,38,0,2,0,0,False,False,True,False,False,Research,self,t3_1gzj8rs
MachineLearning,[D]Thoughts on Synthetic Data Platforms like Gretel.ai or Mostly AI?,"
Has anyone here used platforms like Gretel.ai or Mostly AI?
	•	What did you like or dislike?
	•	How was the synthetic data quality for your use case?

I’m exploring options and would appreciate your insights. Thanks!",Value-Forsaken,1gzsqwu,https://reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,https://www.reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,2024-11-25 20:20:23,4,0.64,4,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1gzsqwu
MachineLearning,[D] Flow matching is actually very different from (continuous) normalising flow?,"I was looking at the [flow matching](https://arxiv.org/pdf/2210.02747) paper and saw that flow matching is often considered as just an alternative implementation of continuous normalising flow. But after comparing the methodologies more closely, it seems there is a very significant distinction. In the flow matching paper, it is mentioned that for a data sample x1 (I assume this refers to individual data points like a single image), we can put an ""dummy"" distribution such as a very tight Gaussian on it, then construct a conditional probability path p_t(x|x1). Therefore what we learn is a transformation between the small Gaussian (t=1) on the data point to a standard Gaussian (t=0), for every data point. This implies that the latent space, when trained over the entire dataset, is the overlapped mixture of all the standard Gaussians that each individual data point maps to. The image of the small Gaussian ball for each individual image is the entire standard Gaussian.

However this does not seem to be what we do with regular normalising flows. In normalising flows, we try to learn a mapping that transforms the ENTIRE distribution of the data to the standard Gaussian, such that each data point has a fixed location in the latent space, and jointly the image of the dataset is normally distributed in the latent space. In practice we may take minibatches and optimise a score (e.g. KL or MMD) that compares the image of the minibatch with a standard Gaussian. Each location in the latent space can be uniquely inverted to a fixed reconstructed data point.

I am not sure if I am missing anything, but this seems to be a significant distinction between the two methods. In NF the inputs are encoded in the latent space, whereas flow matching as described in the paper seems to MIX inputs in the latent space. If my observations are true, there should be a few implications:

1. You can semantically interpolate in NF latent space, but it is completely meaningless in the FM case
2. Batch size is important for NF training but not FM training
3. NF cannot be ""steered"" the same way as diffusion models or FM, because the target image is already determined the moment you sample the initial noise

I wonder if anyone here has also looked into these questions and can inform me whether this is indeed the case, or whether something I missed made them more similar de facto. I appreciate any input to the discussion!",aeroumbria,1gzdera,https://reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,https://www.reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,2024-11-25 07:25:58,56,0.97,56,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1gzdera
MachineLearning,[2411.15100] XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models,,crowwork,1gzi649,https://reddit.com/r/MachineLearning/comments/1gzi649/241115100_xgrammar_flexible_and_efficient/,https://arxiv.org/abs/2411.15100,2024-11-25 12:55:20,9,0.85,9,0,0,0,0,False,False,False,False,False,,default,t3_1gzi649
MachineLearning,[D] Why does my feature visualisation form this shape?,"In performing 3d t-SNE decomposition of model features, I have come across a strange quirk. I am fine tuning an ImageNet trained ViT for CIFAR-100 classification. Before the first epoch (i.e. just imagenet weights with an untrained FC feature head), the visualisation of class boundaries looks like this, forming this convex shape with regions of no classes. After one epoch this shape is no longer present in the t-SNE visualisation.

Any ideas why? Is this related to the Manifold hypothesis? Or just due to overlap between ImageNet and CIFAR100 classes?

https://preview.redd.it/eb3w3rfaw03e1.png?width=2178&amp;format=png&amp;auto=webp&amp;s=57f1c34830cdff1968aea9367bba2b4cb3d5b7c1

",BDE-6,1gzfo7c,https://reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,https://www.reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,2024-11-25 10:14:59,10,0.78,10,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/SWOh-MCfT6Bntn3xkFzI9yHTfmJ6_JEMsdif7HmXtYQ.jpg,t3_1gzfo7c
MachineLearning,[R] Evaluating Creative Writing Output and The Effects of Fine Tuning,"
I was asked by a publisher if GPT-4o could be fine tuned to match their authors style to help build a copilot type experience. 

This gave me a chance to figure out a way to breakdown creative writing into five pillars (Dialogue, Exposition, Inner Thoughts, Description and Action) and measure how these change with prompting and fine tuning. 

I put together this blog post based on the results of training on popular authors like J.K. Rowling, Tade Thompson and Andrei Agassi. Surprisingly based GPT-4o does a decent job adopting their style with prompting but I put together some interactive visualizations to see how the model shifts during story generation (400 paragraphs) as we fine tune on 300, 600, and 800 samples. 

https://peytoncasper.com/blog/tone-evaluation/index.html

https://github.com/peytoncasper/grammar-of-thought",peytoncasper,1gzdwg5,https://reddit.com/r/MachineLearning/comments/1gzdwg5/r_evaluating_creative_writing_output_and_the/,https://www.reddit.com/gallery/1gzdwg5,2024-11-25 08:01:41,12,0.7,12,0,6,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/7cdun2J_3-VtlKB1wRgxUbknVHm_3AkGRu0D0UZDYwo.jpg,t3_1gzdwg5
MachineLearning,[D] As a CS masters student/researcher should one be very deliberate in picking a lab’s domain?,"I (very fortunately) got an opportunity in a great lab in an R1 school, Prof has a &gt;40 h-index, great record, but mainly published in lower tier conferences, though do some AAAI. It applies AI in a field that aligns with my experience, and we are expected to publish, which is perfect. However I’m more keen to explore more foundational AI research (where I have minimal experience in apart from courses I took).

In CS, ML it seems most people are only prioritising NIPS/ICLR/ICML especially since I’m interested in potentially pursuing a PhD. I’m in a bit of a dilemma, if I should seize the opportunity or keep looking for a more aligned lab (though other profs may not be looking for more students).

My gut tells me I should ignore conference rankings and do this, since they have some, chain of though, knowledge representation, cognitive system components. They expect multi semester commitment and of course once I commit I will see it through. My dilemma is that I’m moving more and more towards more practical applications in AI, which is pretty domain specific and am worried I won’t be able to pivot in the future. 

I’m aware how this can sound very silly, but if you can look past that, could I please get some advice and thoughts about what you’d do in the shoes of a budding academic, thank you!",giuuilfobfyvihksmk,1gz6mj1,https://reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,https://www.reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,2024-11-25 00:56:49,43,0.79,43,0,31,0,0,False,False,True,False,False,Discussion,self,t3_1gz6mj1
MachineLearning,[P] I made a library for building agents that use tree search to solve problems,,jsonathan,1gyreq1,https://reddit.com/r/MachineLearning/comments/1gyreq1/p_i_made_a_library_for_building_agents_that_use/,https://i.redd.it/qut9unu4su2e1.png,2024-11-24 13:51:18,285,0.95,285,0,26,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/EovprfWyEiN6f7tnfzX9swDj_d3bu8iJE26aqKFyhic.jpg,t3_1gyreq1
MachineLearning,[D] AAAI 2025 - Reviews missing after rebuttal,"Hi all,

We submitted our paper to AAAI 25. It passed Phase 1, it got fairly good scores, we wrote the rebuttals, and now the scores, the reviews and the rebuttals are missing. Is this normal?",jpereira73,1gzn4uj,https://reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,https://www.reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,2024-11-25 16:39:01,1,0.57,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gzn4uj
