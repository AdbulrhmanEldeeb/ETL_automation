subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1gyhfxm,https://reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/,2024-11-24 03:15:10,35,0.85,35,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gyhfxm
MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

&gt;Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

&gt;Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&amp;#x200B;

Please remember that this community is geared towards those with experience.",AutoModerator,1ftdkmb,https://reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/,https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/,2024-10-01 02:30:17,28,0.86,28,0,31,0,0,False,False,True,False,False,Discussion,self,t3_1ftdkmb
MachineLearning,[D] Modern use-cases for RNNs?,"The discussion can be twofold.
1)What are in your opinion some tasks, for the personal projects scale, where you think RNNs close to traditional implementations (LSTM, GRU) are still the best starting and ending point? Especially compared to transformers.

In small time-series forecasting settings I can see a GRU being more convenient than a Transformer probably, but I am interested also in tasks where inputs are sequences of symbols or measures, but outputs maybe not.

The main goal is to play with LSTM and GRU variants (eg minGRU) on datasets where it makes sense, might do tiny-Shakespeare but it doesn't warm my heart...

2) do you think there are sequential tasks and settings where RNNs are not only the more natural option according to our intuition, but actually the only theoretically or experimentally available option to make do, compared to Transformers or 1D CNNs etc?",Sad-Razzmatazz-5188,1h38ym2,https://reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,https://www.reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,2024-11-30 09:22:32,22,0.89,22,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1h38ym2
MachineLearning,[D] What's the fastest object detection model?,"Hi, I'm working on a project that needs object detection. The task itself isn't complex since the objects are quite clear, but speed is critical. I've researched various object detection models, and it seems like almost everyone claims to be ""the fastest"". Since I'll be deploying the model in C++, there is no time to port and evaluate them all.

I tested YOLOv5/v5Lite/8/10 previously, and YOLOv5n was the fastest. I ran a simple benchmark on an Oracle ARM server (details [here](https://github.com/Avafly/YOLOv5-ncnn-OpenVINO-MNN-ONNXRuntime-OpenCV-CPP?tab=readme-ov-file#simple-benchmarks-on-m1-mac-and-arm-linux)), and it processed an image with 640 target size in just 54ms. Unfortunately, the hardware for my current project is significantly less powerful, and meanwhile processing time must be less than 20ms. I'll use something like quantization and dynamic dimension to boost speed, but I have to choose the suitable model first.

Has anyone faced a similar situation or tested models specifically for speed? Any suggestions for models faster than YOLOv5n that are worth trying?",Knok0932,1h362dq,https://reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,https://www.reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,2024-11-30 06:00:40,17,0.95,17,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h362dq
MachineLearning,[D] Hinton and Hassabis on Chomsky’s theory of language,"I’m pretty new to the field and would love to hear more opinions on this. I always thought Chomsky was a major figure on this but it seems like Hinton and Hassabis(later on) both disagree with it. Here: https://www.youtube.com/watch?v=urBFz6-gHGY (longer version: https://youtu.be/Gg-w_n9NJIE)

I’d love to get both an ML and CogSci perspective on this and more sources that supports/rejects this view.

Edit: typo + added source.",giuuilfobfyvihksmk,1h2mkye,https://reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,2024-11-29 14:10:12,97,0.92,97,0,83,0,0,False,False,True,False,False,Discussion,self,t3_1h2mkye
MachineLearning,[N][R] Models are what they eat: automatic data curation for LLMs,"Sharing our most recent work at [DatologyAI](http://www.datologyai.com). Models are what they eat, and our mission is to make data curation for training large models as effective and easy as possible. 

Combining a bevy of approaches, including heuristic filters, model-based filters, embedding based curation, synthetic data, target distribution matching, and mixing ratios, we were able to massively improve training efficiency, performance, and inference efficiency. 

Comparing to our baseline and starting dataset -- exact deduplicated RedPajamav1, we can: 

* Reach the same performance 7.7x faster (and 3.4x faster than DCLM)
* Improve performance across benchmarks by 8.5% (and by 4.4% over DCLM)
* Train models with fewer than half the parameters which outperform larger models by &gt;5% 

Check out our [high-level results here](https://www.datologyai.com/post/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation), and if you want all the nitty-gritty details, check out our[ technical deep dive](https://www.datologyai.com/post/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset).",arimorcos,1h2qmol,https://reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,2024-11-29 17:15:57,14,0.77,14,0,0,0,0,False,False,True,False,False,News,self,t3_1h2qmol
MachineLearning,[D] Molecular Dynamics and Machine Learning Build,"Hey guys, so I do a lot of molecular dynamics and am starting to push into the ML space with that and genome/multi-omics sort of stuff. I’m building a workstation and with my budget I’m looking at 2x A6000 or 2x 5000 Ada. Both work great for molecular dynamics, but I’m trying to figure out my best option for ML. The A6000 has 48gb vram and nvlink, but the 5000 Ada is newer and substantially faster and 32Gb VRAM per card is no slouch either. Any advice? ",Mdgoff7,1h2x4ar,https://reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,https://www.reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,2024-11-29 22:04:19,1,0.57,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h2x4ar
MachineLearning,"[D] How does VQ-VAE disentangle, if it does at all?","I currently use a BetaTC-VAE, which does an excellent job at disentangling, knowing that VAE can slightly disentangle since for the model it's easier to get a lower KL loss if the variables are dissentanlged, the beta term make this beta times more important, and total correlation and mutual information loss push for total disentanglement, but in VQ-VAE there is no (major) disentanglement, only a codebook, and discrete outputs. Could the discrete latent given by the codebook be disentangled? If not, is there any paper on disdentangling VQ-VAE? I have an environment where disentangled latent spaces provide better reconstruction than continous latent spaces ",ZazaGaza213,1h2epzx,https://reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,2024-11-29 05:33:10,33,0.92,33,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h2epzx
MachineLearning,[D] COLING 2025 Final Acceptances - Is it not out yet?,"Is the final acceptance out? I am not seeing it yet on Softconf. Had a paper with (5,4) (4,4) (4,4) in reviews.",UnhappyPrior6570,1h2kfic,https://reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,https://www.reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,2024-11-29 12:10:32,8,0.79,8,0,41,0,0,False,False,True,False,False,Discussion,self,t3_1h2kfic
MachineLearning,[P] Python Implementation of Softmax that takes integer input ,"Hey, So I am working on a project whereby I have to quantize my model's weights and biases to integers and perform subsequent operations using integers. The output of my model can be either (int8 or int16) values (in this case, logits) and I need to call softmax on this logits output/array. I was able to find an integer implementation of softmax written in C ([https://github.com/ARM-software/CMSIS-NN/tree/main/Source/SoftmaxFunctions](https://github.com/ARM-software/CMSIS-NN/tree/main/Source/SoftmaxFunctions)). The problem I'm having is trying to evaluate that this C implementation is accurate (or more specifically, that I am using it accurately). The way I'm thinking of doing that is detailed below:

\*\*In Python\*\*  
Take my integer logits -&gt; call an integer python implementation of softmax on the logits -&gt; get a result  
(\*\*python\_integer\_prediction\_probabilities\*\*).

\*\* In C (using CMSIS-NN's )  
Take the same integer logits -&gt; call the C softmax implementation on my logits -&gt; get a result (\*\*CMSIS\_NN\_prediction\_probabilities\*\*)

Finally, I compare these two results to see if they are close enough. The main problem I'm having is, I assumed there would be information about how to implement a softmax function that takes integer inputs in Python, but I can't find anything online. Does anyone have an idea of how to implement this in python or is aware of resources that I could use to figure this out? thank you.",Individual_Ad_1214,1h318gl,https://reddit.com/r/MachineLearning/comments/1h318gl/p_python_implementation_of_softmax_that_takes/,https://www.reddit.com/r/MachineLearning/comments/1h318gl/p_python_implementation_of_softmax_that_takes/,2024-11-30 01:24:56,0,0.4,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1h318gl
MachineLearning,[D] Theory behind modern diffusion models,"Hi everyone,

I recently attended some lectures at university regarding diffusion models. Those explained all the math behind the original DDPM (Denoiding Diffusion Probabilistic Model) in great detail (especially in the appendices), actually better than anything else I have found online. So it has been great for learning the basics behind diffusion models (slides are available in the link in the readme here if you are interesed: https://github.com/julioasotodv/ie-C4-466671-diffusion-models)

However, I am struggling to find resources with similar level of detail for modern approaches—such as flow matching/rectified flows, how the different ODE solvers for sampling work, etc. There are some, but everything that I have found is either quite outdated (like from 2023 or so) or very superficial—like for non-technical or scientific audiences.

Therefore, I am wondering: has anyone encountered a good compendium of theoretical eplanations beyond the basic diffusion model (besides the original papers)? The goal is to let my team deep dive into the actual papers should they desire, but giving 70% of what those deliver in one or more decent compilations.

I really believe that SEO is making any search a living nightmare nowadays. Either that or my googling skills are tanking for some reason.

Thank you all!",bgighjigftuik,1h1vxe1,https://reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,2024-11-28 13:27:28,205,0.99,205,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h1vxe1
MachineLearning,[R] Recursive Methods for interpolation between vector fields ( Known and Unknown),"Hello everyone Does anything of the next makes sense?  
I Have been posting on Learning first ( also on math and number theory ) , but I think is a bit more math theory than ML but it does have to do with how the data is interpolated so I am unsure. 

( I hope I am not breaking rule 5 with my links ) 

this will be the interpolation of the data ( Via organized vector field levels )  before the generative process starts, but because its recursive, the generative process can happen on inside the iteration too 

its there a model I can use ? And if someone understand the math, can I get some papers or things I could follow or just is learning and reading now?

I am a little lost and need some help ( I organized my question with chatGPT to make it understandable so bare in mind if there is some odd work here and there, I am on the I am going a bit mental stage )

I think this is dealing with machine learning problems that have been solved between interpolation of point could on space that have recursive data ( mapping and data organization )

I've been developing a concept that merges artistic visualization with advanced mathematical interpolation techniques inspired by the Mandelbrot set. Coming from a creative background, I've ventured into creating what I believe could be a **recursive Mandelbrot predictive method**  for manipulating vector fields. I'm eager to understand if this approach already exists and to gather resources or similar algorithms to explore further and test my ideas.

I will add some things like this latter to test segmentation models for the recursiveness [https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear\_algebra\_project\_i\_implemented\_a\_kmeans/](https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/)

**REFERENCE IMAGES**  
everything is based on recursive by resolution with inverse square distance from the origin point

**Mandelbroth**  
[https://en.wikipedia.org/wiki/Mandelbrot\_set#/media/File:Juliacycles1.png](https://en.wikipedia.org/wiki/Mandelbrot_set#/media/File:Juliacycles1.png)

**Conceptual model** ( The mandelbroth guidance happens just on the altered time pulling agent ) ( Orange )  
[Single Vector interpretation and prediction stream of the Pull of the mandelbrot agent](https://cdn.discordapp.com/attachments/457637053581230100/1311677830966284298/chrome_7KNKgsO6Fm.png?ex=6749baac&amp;is=6748692c&amp;hm=55a7809cd5bb402a06fdea1edbad4eb1ff21f15d1a3303efee9b797eef0f5839&amp;)

**Conceptual Model 2d sim**  
[Representation of the predictiveness](https://media.discordapp.net/attachments/457637053581230100/1311668738139099166/ShareX_11AweoaOUp.png?ex=6749b234&amp;is=674860b4&amp;hm=7d660210c3678b8f1bd0804ee4f2f2425a0c1dc9aea18bdf39e86ba81d9ad2e8&amp;=&amp;format=webp&amp;quality=lossless&amp;width=295&amp;height=350) as mandelbrot

Representation of functional interpolation of agents via Mandelbroth ( non recursive )

**Conceptual Simulation model 2d sim ( making the mandelbroth )**  
[Image non animated](https://media.discordapp.net/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;=&amp;format=webp&amp;quality=lossless)[ANIMATED VIDEO DOWNLOAD ( CLEAN FILE )](https://cdn.discordapp.com/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;)

**Conceptual layering**  
[Layering of 3 tiers via inverse square distance on a vector field ( currently surface) but can be world](https://media.discordapp.net/attachments/457637053581230100/1311404062931030086/image.png?ex=67496475&amp;is=674812f5&amp;hm=56bf4ff3988163f35c1ef24227645beaa6d11ab1b0330d094dc3dccbad6ce696&amp;=&amp;format=webp&amp;quality=lossless&amp;width=378&amp;height=350)

**recursiveness concept**  
[Applied recursiveness auto generation based on surface vector field ](https://media.discordapp.net/attachments/457637053581230100/1311404209278685184/image.png?ex=67496498&amp;is=67481318&amp;hm=e548fb4615d48adea59f835a3a76a6a203c5946ea6d356eff2c7298252d3de41&amp;=&amp;format=webp&amp;quality=lossless&amp;width=927&amp;height=831)( no prediction applied )

# The Concept

Imagine a system where the interpolation between data points isn't limited to traditional methods like **lerp** (linear interpolation) or **slerp** (spherical linear interpolation). Instead, it employs a **pseudo vector field Mandelbrot slerp**, allowing vectors to be guided from a base state (reality) to a target state (altered time) within a Mandelbrot-inspired vector field. This method is recursive, meaning multiple layers of calculations are applied to refine the interpolation continuously.

# Key Components:

1. **Reality (Ground Truth):** Represents the current state of the system, serving as the foundational dataset.
2. **Agents of Change (Vectors of Closest Influence):** These act as pull forces influencing the direction and magnitude of interpolation.
3. **State (Ground Truth Prediction Model):** Utilizes the current data to predict future states based on the influences of the agents.
4. **Altered Time (Goal):** The desired target state, akin to a Mandelbrot-type location on the outer range of the vector field.

# Interpolation Method

The interpolation technique extends beyond simple linear methods by incorporating the complexity and fractal nature of the Mandelbrot set. Here's how it functions:

* **Guided Vectors:** Vectors transition from reality towards altered time, following paths influenced by a Mandelbrot-like vector field.
* **Recursive Layers:** Multiple layers of interpolation allow for increasingly refined calculations, enhancing accuracy and adaptability.
* **Dynamic Intensity:** The closer the interpolation is to reality, the more intense and detailed the calculations become, while the vector field simplifies as it moves towards altered time.

# Theoretical Foundation

The core idea revolves around mapping and adjusting Mandelbrot-inspired vectors to facilitate interpolation between recursively organized data banks. This approach aims to:

* **Capture Complex Patterns:** Leverage the self-similar, fractal nature of Mandelbrot sets to identify and utilize intricate patterns within the data.
* **Enhance Predictive Capability:** Recursive calculations allow for continual refinement of projections, improving predictive accuracy over time.
* **Achieve Real-Time Adaptability:** Dynamically adjust vectors to align with specific goals, similar to how a car's performance might be modulated in real-time to achieve optimal racing outcomes.

# Visual Analogy

Think of this system as calculating the ""ghost"" position of a car in a racing game like *Need for Speed*:

* **Acceleration and Braking:** Based on historical and current data, determining when to accelerate or brake to achieve the best performance.
* **Engine Adjustments:** Modifying the system's parameters in real-time to align with the target state, ensuring the system reaches its goal efficiently.
* **Dynamic Modulation:** Continuously adjusting these actions to meet the desired ""goal time,"" always operating within physical (mathematical) constraints.

# Questions for the Community

1. **Does This Technology Exist?** Is my approach accurately described as a **recursive Mandelbrot predictive method** for vector field interpolation? Are there existing models or research that align closely with this concept?
2. **Resources and References:** If similar technologies or algorithms exist, could you recommend any resources, papers, or specific Mandelbrot-like algorithms that I can study or begin testing with?
3. **Mathematical Validation:** Given that my approach stems from an artistic visualization perspective, what mathematical frameworks or theories should I explore to formalize and validate this method?

# Additional Context

For a visual representation of my model and its applications, you can refer to the following links:

* **Visual Model:** [LinkedIn Visual Model](https://www.linkedin.com/feed/update/urn:li:activity:7267834154631708672/)
* **Use Case Example:** [LinkedIn Use Case](https://www.linkedin.com/posts/jesusfc14_i-think-i-am-reaching-a-clarity-moment-the-activity-7267823963068628992--qA7?utm_source=share&amp;utm_medium=member_desktop)

*(Please note that these links provide additional visual context to help illustrate the concept.)*

**Thank you for taking the time to read through my concept! I'm looking forward to your insights, validations, and any resources you can share to help me advance this idea.**

all this tech is currently under Creature Garage umbrella but I have ownership of the creative driver of the idea so that should be fine for me to post but I reached a moment that I will need help for some of the most advanced math implementations

I am using some concepts that sound really far and advanced but currently my implementation is mostly based on recursiveness the prediction agent will come to function once I have my full set of data to make a test",jesusfc,1h2io35,https://reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,https://www.reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,2024-11-29 10:08:00,4,0.7,4,0,7,0,0,False,False,True,False,False,Research,self,t3_1h2io35
MachineLearning,"""[P]""Static variable and dynamic variable tables in RFM ","



I am creating a prediction model using random forest. But I don't understand how the model and script would consider both tables loaded in as dataframes.


What's the best way to use multiple tables with a Random Forest model when one table has static attributes (like food characteristics) and the other has dynamic factors (like daily health habits)?

Example:
I want to predict stomach aches based on both the food I eat (unchanging) and daily factors (sleep, water intake).

Tables:
 * Static: Food name, calories, meat (yes/no)
 * Dynamic: Day number, good sleep (yes/no), drank water (yes/no)


How to combine these tables in a Random Forest model? Should they be merged on a unique identifier like ""Day number""?
",peyott100,1h2oe69,https://reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,https://www.reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,2024-11-29 15:36:10,1,0.67,1,0,3,0,0,False,False,True,False,False,Project,self,t3_1h2oe69
MachineLearning,[D] Why aren't Stella embeddings more widely used despite topping the MTEB leaderboard?,"https://huggingface.co/spaces/mteb/leaderboard

I've been looking at embedding models and noticed something interesting: Stella embeddings are crushing it on the MTEB leaderboard, outperforming OpenAI's models while being way smaller (1.5B/400M params) and apache 2.0. Makes hosting them relatively cheap.

For reference, Stella-400M scores 70.11 on MTEB vs OpenAI's text-embedding-3-large 64.59. The 1.5B version scores even higher at 71.19

Yet I rarely see them mentioned in production use cases or discussions. Has anyone here used Stella embeddings in production? What's been your experience with performance, inference speed, and reliability compared to OpenAI's offerings?

Just trying to understand if there's something I'm missing about why they haven't seen wider adoption despite the impressive benchmarks.

Would love to hear your thoughts and experiences!",sdsd19,1h1u814,https://reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,2024-11-28 11:45:44,64,0.93,64,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1h1u814
MachineLearning,[D] Most important papers in implicit regularisation,"Hi guys

I'm getting into machine learning, especially on the theoretical side, and I'm curious to learn more about why neural networks tend to generalise so well, so I'm hoping to read some papers about this. As far as I'm aware, the first big paper on the topic was 'Understanding deep learning requires rethinking generalization' by Zhang et al.

I've got a good mathematical background, so I was wondering what people think are the most impactful papers there are in this area. What do you think made the most impact?",MrBeebins,1h29i7j,https://reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,2024-11-29 00:23:33,5,0.86,5,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h29i7j
MachineLearning,[R] BitNet a4.8: 4-bit Activations for 1-bit LLMs,"**Paper:** [https://arxiv.org/pdf/2411.04965](https://arxiv.org/pdf/2411.04965)

**Abstract:**

&gt;Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Visual Abstract:**

https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a

**Evaluations:**

[HS=HellaSwag, PQ=PiQA, WGe=WinoGrande](https://preview.redd.it/4ppq57varn3e1.png?width=955&amp;format=png&amp;auto=webp&amp;s=3c4152947edf4542d2a1ffa181bfa52a5369d916)

https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9

",StartledWatermelon,1h1y0ig,https://reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,2024-11-28 15:11:18,31,0.92,31,0,2,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/eMHBOJqKZns6b6d3vVu8taf4yu8Tq472NuD6w0t0t6M.jpg,t3_1h1y0ig
MachineLearning,[R] Fast Matrix-Based Counterfactual Regret Minimization Using GPU Parallelization,"A novel GPU implementation of Counterfactual Regret Minimization (CFR) that accelerates the computation of optimal strategies in extensive-form games. The core innovation is parallelizing the regret updates and strategy computations across GPU cores while carefully managing memory access patterns.

Key technical points:
- Custom memory layout that maps game states and actions to GPU threads
- Batch processing of information sets to maximize GPU utilization
- Parallel computation of counterfactual values and regret updates
- Multi-GPU scaling through game tree partitioning
- Evaluated on Leduc Hold'em and Limit Texas Hold'em poker variants

Results:
- Up to 30x speedup compared to CPU implementation
- Linear scaling with number of GPUs up to 8 devices
- Memory usage scales with game size and number of information sets
- Solution quality matches CPU baseline within statistical error
- Successfully solved games with up to 10^14 states

I think this work could make CFR much more practical for real-world applications beyond poker. The ability to solve larger games faster opens up possibilities in areas like automated negotiation, security games, and resource allocation. The multi-GPU scaling is particularly interesting as it suggests potential for solving even more complex games.

The memory optimization techniques developed here might also transfer well to other game-theoretic algorithms that need to process large state spaces efficiently.

TLDR: GPU-accelerated CFR implementation achieves 30x speedup through careful parallelization and memory management, with linear multi-GPU scaling. Makes solving large extensive-form games significantly more tractable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/gpu-accelerated-counterfactual-regret-minimization). Paper [here](https://arxiv.org/abs/2408.14778).",Successful-Western27,1h1wq6b,https://reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,2024-11-28 14:08:34,24,0.93,24,0,1,0,0,False,False,True,False,False,Research,self,t3_1h1wq6b
MachineLearning,[P] Retrieval augmented generation on-premises (fully local solution),"Hey everyone,   
I’m excited to share my latest repo with you—a local conversational RAG solution for your files! Here’s the deal: this setup is perfect for running RAG on-premises.   
It’s built with Docker, LangChain, Ollama, FastAPI, and Hugging Face, and all models are downloaded automatically. Soon, I’ll add support for choosing your preferred model, but here’s what the solution currently includes:  
• Locally running Ollama: It’s hardcoded to the Qwen-0.5B model for now, but model selection from the Ollama registry is coming soon.   
• Local indexing: Uses a sentence-transformer embedding model (currently restricted to this family, but this will also change soon).   
• Qdrant container: Runs locally for vector storage.   
• Local reranker: Currently uses BAAI/bge-reranker-base, with support for reranker selection coming soon.  
• Websocket-based chat: Includes history-saving capabilities.   
• Simple chat UI: Built with React for a straightforward interface.   
• Bonus: You can use this setup with ChatGPT as a custom GPT! Query your local data through the official ChatGPT web interface or macOS/iOS app.   
• On-premises ready: Everything runs locally, and the containers are CPU-friendly.

A couple of ideas and known issues:   
• Support for Model Context Protocol is on the roadmap.   
• No incremental indexing or reindexing yet.   
• Model selection isn’t available yet but will be added soon.   
  
I’d love your feedback, contributions, or support—watch, fork, and star if you find this interesting!  
Thank you!   
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
",davidvroda,1h26ul7,https://reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,2024-11-28 22:01:09,4,0.63,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1h26ul7
MachineLearning,[P] Latest version of Ollama Grid Search (0.7.0): added prompt database ,"https://preview.redd.it/ohewvqicbo3e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=077fec6931b2efc40182f7c2eb284718822213e0

Hey people... the latest version of [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) now comes with its own prompt management database (along with many improvements in the UI).

https://preview.redd.it/qzu95clhbo3e1.png?width=975&amp;format=png&amp;auto=webp&amp;s=473382281094fc3f819e6fc6c3d267941d2a35ce

It makes it a hell lot easier to test your existing prompts when you pull newly released models!

If you want to check it out, the github page has releases for all major platforms:

[https://github.com/dezoito/ollama-grid-search](https://github.com/dezoito/ollama-grid-search)",grudev,1h20fzv,https://reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,https://www.reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,2024-11-28 17:01:16,8,0.79,8,0,0,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uAl-6kPsNjdKIz7o50pQEHnMDPXa-cNX7GoAlrOqiec.jpg,t3_1h20fzv
MachineLearning,[D] Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis,"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 **Visatronic: A Multimodal Decoder-Only Model for Speech Synthesi**s by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/ygxnbhiboo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=31bf7c9b988c83a8d0ff2e7b011dac027aa8f154

https://preview.redd.it/7v15egiboo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=d9629caa406a92f8b2052ad6baa3a0265a27ddcf

",CATALUNA84,1h222s2,https://reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,https://www.reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,2024-11-28 18:13:13,3,0.67,3,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/GIw9ZddndVdad-4h4y65mlExW8FTRdhSBchmteHv-AA.jpg,t3_1h222s2
MachineLearning,[D] Loading data into Ray clusters,"For those of you that run ML training in a Ray cluster on AWS, I'm curious to know what approach you take to get training data into your cluster?

And how are you versioning the data?

How do you avoid repeatedly downloading the same data across runs that have the same dataset?

I'd like a smooth process for being able to target a specific version of a dataset for a training run, and to avoid repeatedly downloading it. The data versioning should have a clear mapping to whatever version of a data pipeline created it. It'd also be nice to have something that scales well to larger datasets.

Keen to hear experiences from the trenches.",SingularValued,1h1v68j,https://reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,2024-11-28 12:45:29,6,0.87,6,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h1v68j
MachineLearning,[D] Inconsistent use of the gerund form in dataset naming,"This is, of course, a very minor issue. But it really irks me, and I was wondering if other people are bothered by it. I think the two most common naming schemes I see for the three standard dataset splits are ""training dataset, validation dataset, and test dataset"" or ""training dataset, validation dataset, and testing dataset"". But to be consistent it should really be ""train dataset, validation dataset, and test dataset"" or ""training dataset, validating dataset, and testing dataset"". I always use the former. Does this bother anyone else, or am I alone in brooding over this?",Shianiawhite,1h217sh,https://reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,2024-11-28 17:34:46,2,0.67,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h217sh
MachineLearning,Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis [D],"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 V**isatronic: A Multimodal Decoder-Only Model for Speech Synthesis** by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/7eoqonhqjo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=c751f5c9f9bc42f22672bbbc67cf8b0fe95ef36b

https://preview.redd.it/224tugzrjo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=37adc93604606d86492dc104f03915bab7428eca",CATALUNA84,1h21j1a,https://reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,https://www.reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,2024-11-28 17:48:43,0,0.33,0,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/wIqPUsHaUNfLLWwLHYcQkdG4Yav_LVjLmzLghfKaOjI.jpg,t3_1h21j1a
MachineLearning,[D]Is Freelancing as a Data Scientist Even Possible?,"Hi everyone,

I’m fine working for as low as $15/hour, so earnings aren’t a big concern for me. I’ve gone through past Reddit posts, but they mostly discuss freelancing from the perspective of income. My main concern is whether freelancing in data science is practical for someone like me, given its unique challenges.

A bit about my background: I’ve completed 3-4 real-world data science projects, not on toy datasets, but actual data (involving data scraping, cleaning, visualization, modeling, deployment, and documentation). I’ve also worked as an intern in the NLP domain.

Some issues I’ve been thinking about:

1. Domain Knowledge and Context: How hard is it to deliver results without deep understanding of a client’s business?


2. Resource Limitations: Do freelancers struggle with accessing data, computing power, or other tools required for advanced projects?


3. Collaboration Needs: Data science often requires working with teams. Can freelancers integrate effectively with cross-functional groups?


4. Iterative and Long-Term Nature: Many projects require ongoing updates and monitoring. Is this feasible for freelancers?


5. Trust and Accountability: How do freelancers convince clients to trust them with sensitive or business-critical work?


6. Client Expectations: Do clients expect too much for too little, especially at low wages?



I’m also open to any tips, advice, or additional concerns beyond these points. Are these challenges solvable for a new data science freelancer? Have any of you faced and overcome similar issues? I’d love to hear your thoughts.

Thanks in advance!",ds_reddit1,1h1q98i,https://reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,https://www.reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,2024-11-28 07:01:15,7,0.57,7,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1h1q98i
MachineLearning,Causal Discovery Competition Winning Paper Discussion [D],"I’ve recently come across this post: https://thetourney.github.io/adia-report/ which describes the winning method for a casual discovery competition. It’s not really my field but I do have a reasonable understanding of GNNs and Causal Inference. Anyway, from the report I don’t understand precisely what the winning team was doing. Can anyone either link to a full paper or have a good intuitive and potentially step by step explanation of what they are doing?",www3cam,1h1i0ji,https://reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,2024-11-27 23:22:43,27,0.94,27,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h1i0ji
MachineLearning,[P][R] Looking for Multimodal Classification Examples Using Perceiver IO (Audio + Image + Text),"I'm exploring Perceiver IO for a project that involves processing multiple data modalities (audio, image, and text) simultaneously for a binary classification tasks. I’m looking for any GitHub repositories or resources where it has been used to handle these modalities together. Thanks a lot for your help!",kernel_KP,1h204ag,https://reddit.com/r/MachineLearning/comments/1h204ag/pr_looking_for_multimodal_classification_examples/,https://www.reddit.com/r/MachineLearning/comments/1h204ag/pr_looking_for_multimodal_classification_examples/,2024-11-28 16:46:35,1,0.67,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h204ag
MachineLearning,[P] Ablation study using a subset of data?,"Basically, I'm engaging in a research project in which I'm training encoder only language models for text classification. I have already trained my models and gotten my results, however I need to perform an ablation study. The main issue I'm having is that the dataset is large. Is it fair for me to perform the ablation study on a subset of the dataset, since I'm gonna have to train it 3 - 4 times with different ablations?",Aromatic_Web749,1h1kzsh,https://reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,2024-11-28 01:51:54,8,0.84,8,0,8,0,0,False,False,True,False,False,Project,self,t3_1h1kzsh
MachineLearning,[D] Advanced methods of data management,"Hello everyone, right now I'm involved in a project which is basically (audio)LLM fine-tuning, and we're having problems related to data management. 

Since there isn't that many sets in that area, we're using different augmentation schemas. tl;dr, we have different datasets of various nature, and using some schemas we can convert usual ASR-type datasets to QA, generate refusal data, combine questions etc. 

Problem is, it's hard to control ratios of different data, and right now it's mostly a manual labour. We kinda have to manually adjust the amount of data we're generating. Which is rather annoying and hard process; we have many target datasets, you have to remember how much samples you've generated and try to get some adequate mix for train.

Right now, we use airflow for data filtering (lots of raw data is badly labelled), but I'm not really sure that I understand how to connect that tool to data generation, and if it's a good tool for that purpose. I was thinking about writing some snakemake script, but ideally the final solution should be flexible when we change configs, add new sets etc, and that's not what I associate with snakemake. Also, another question is visualization. 

So, I'm asking what kind of tools/libs I can use to tackle this task, is there anything that can fit our purposes, or it's time to write customs scripts? What do companies like meta use for their enormous 15t tokens set, surely they don't dump everything in one place? 

There's another question related to connecting datasets with data pipeline of training (it's complicated, there are additional steps of precalculation of feats and creating webdataset which is used in training), but for now I'd be really glad if anyone helped with just data management tools. ",Theio666,1h1u0pj,https://reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,https://www.reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,2024-11-28 11:31:47,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h1u0pj
MachineLearning,[D] AAMAS 2025 reviews are out! ,"I could not find a discussion thread, so I thought I would create one myself. ",E-Cockroach,1h15k8k,https://reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,2024-11-27 14:28:29,25,0.89,25,0,30,0,0,False,False,True,False,False,Discussion,self,t3_1h15k8k
MachineLearning,[P] py-gen-ml: generating ML configuration code from a schema,"[py-gen-ml](https://jostosh.github.io/py-gen-ml) is a Python library designed to simplify your ML experiment configuration using the power of Protocol Buffers. It's still in an early phase but I'd love to hear some feedback from the community.

**Here's how py-gen-ml can help you:**

* **Centralise configurations:** Define schemas in Protobuf to act as a single source of truth.
* **Minimise repetitive work:** Automatically generate code for models, patches, sweeps, and a command-line interface.
* **Boost flexibility:** Experiment with ease thanks to YAML configurations with advanced referencing and the ability to conduct hyperparameter sweeps.
* **Improve code quality:** Benefit from JSON schema validation, strong typing, and IDE support for a more robust development process.

**py-gen-ml aims to make ML development more efficient by reducing the burden of managing configurations.** Give it a try and see how it can improve your workflow.

**Get started:**

    pip install py-gen-ml

**Learn more:** [**https://jostosh.github.io/py-gen-ml**](https://jostosh.github.io/py-gen-ml)",jalapenjos,1h1t9v0,https://reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,https://www.reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,2024-11-28 10:40:23,0,0.25,0,0,1,0,0,False,False,True,False,False,Project,self,t3_1h1t9v0
MachineLearning,"[P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, Docker)","  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to introduce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, you can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork, star) Thank you so much!",davidvroda,1h1pudq,https://reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,2024-11-28 06:33:40,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1h1pudq
MachineLearning,[D] how to do RLHF on this kind of data?,"Hi, apologies if this is a dumb question -- I'm really not knowledgeable about post training. Suppose that I have a llama and I want to finetune with human annotations that ""like"" or ""dislike"" a prompt response. Most DPO datasets feature a pair of possible responses, with one being chosen. Interpreting my data as one half of a pair with one missing, I could generate a second response from the same prompt and say that it is preferred if ""like""d and it is not preferred if it is ""disliked"". Is there a better way?",khidot,1h1bpwq,https://reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,2024-11-27 18:50:05,8,0.78,8,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h1bpwq
MachineLearning,[D] Which LLM models can I run on an NVIDIA 4060 for research purposes? Recommendations needed!,"Hi everyone,

I’m diving into research on large language models (LLMs) and looking to experiment with running them locally on my NVIDIA 4060 GPU. While I know the 4060 isn’t a high-end card compared to some research setups, I’m optimistic about making the most out of what it offers. I’d greatly appreciate any insights or recommendations on:

1. **Models that can run efficiently** on a 4060. I’m aware that some smaller versions of LLMs might be more suited for this hardware, so any advice on what’s realistically possible without excessive optimization would be fantastic.
2. **Models suitable for fine-tuning or pre-training experiments.** Although I’m starting with basic experiments, I plan to explore fine-tuning in the future, so I’d love suggestions for models that are versatile and widely used in research.
3. **Open-source models** or ones that are easy to access and work with for research purposes. Licensing and transparency are important to me, as my work is focused on academic and experimental objectives.

So far, I’ve been looking at options like LLaMA, GPT-NeoX, and BLOOM, particularly their smaller variants, but I’m open to exploring other possibilities. If you’ve had experience running these or similar models on mid-range GPUs, I’d love to hear your thoughts on performance, setup, or any potential limitations I should be aware of.

Additionally, I’d be grateful for any advice on:

* **Optimizing models for a 4060.** Are there specific tools, techniques, or libraries (like bitsandbytes or FlashAttention) that could help with running or fine-tuning these models?
* **Preparing for fine-tuning.** What should I keep in mind when selecting a model to ensure it can support future fine-tuning experiments effectively?

Thank you in advance for sharing your expertise! I’m eager to learn from the community and make the most of this setup.",Spinotesla,1h1ux8m,https://reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,https://www.reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,2024-11-28 12:30:28,0,0.27,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h1ux8m
MachineLearning,[D] AISTATS 2025 reviews,Aistats 2025 reviews are supposed to be out today. So I thought to create a discussion post for the same where we can share our experiences!,PhoneImpressive9983,1h0x428,https://reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,2024-11-27 05:31:15,44,0.95,44,0,83,0,0,False,False,True,False,False,Discussion,self,t3_1h0x428
MachineLearning,[D] How valid is the evaluation using LLMs?,"Hello community,

I am bit new to using Gen AI, I want to check the validity of using larger LLMs to evaluate the result of other LLMs. I have seen different blogs who does this for the purpose of automating the evaluations.

For eg. To evaluate a list of English translations my a model A, is it valid to prompt another model B, something like this '''Is this translation correct original text: {original_text}, Translated text {translated_text}'''

Is this a valid way of evaluating? Something inside me says it's scientifically wrong, because the LLM model B itself will have some error to it right?",raman_boom,1h11lbt,https://reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,2024-11-27 10:48:09,15,0.78,15,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h11lbt
MachineLearning,Residuals in ensemble MLR [D],"
Hi all

New to ensembles.

If you ensemble MLR, you may end up with a non-linear equation however….

A) the residuals of the indicidual MLR that were ensembled need to meet parametric assumptions? Can’t use a crap MLR just because it’s going to be used in an ensemble?
B) if the ensembled MLR equation is linear then residuals should meet parametric assumptions?

Thanks


",Yellow_fruit_2104,1h1ero2,https://reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,https://www.reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,2024-11-27 20:58:23,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h1ero2
MachineLearning,[P] Search query content safety moderation model selection,"Hi there, I am making a mobile application with a search feature. After string cleaning &amp; validation I want to classify the query into one or more of several categories for content safety moderation similar to what Google offers for SafeSearchAnnotations on images or Meta offers in their Llama Guard for LLM prompts/responses.

I need something very fast (&lt;100 ms) as obviously the actual search and data fetching needs to occur with low latency (&lt;500 ms) after this pre-filtering. I expect to have 1000...2000 labelled sample search queries and another 5000...10000 unlabelled sample search queries for model validation. I may also have a list of stop words prior to this that runs on the client and doesn't allow the user to send the query until all stop words are removed. The categories will likely have two parents (user/admin) with five children each. The user categories can be adusted by the user and if a query falls into an admin category this would be flagged and trigger an audit. I need the model to provide a score for all categories.

Please don't recommend any LLM's/GPT's as these will not be fast enough, I am looking for something like BERT or its variants but am unsure which one. English only. At present I am really looking at Google Cloud's Model Garden specifically MobileBERT Classifier or RoBERTa-large (PEFT) as a lot of my stack is GC heavy. I don't want something complicated to setup and deploy. Please note this is different to determining ""toxicity"" like in Google's Perceptive API.",AxelrodWins,1h1dof1,https://reddit.com/r/MachineLearning/comments/1h1dof1/p_search_query_content_safety_moderation_model/,https://www.reddit.com/r/MachineLearning/comments/1h1dof1/p_search_query_content_safety_moderation_model/,2024-11-27 20:11:08,2,1.0,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1h1dof1
MachineLearning,[R] Meissonic: High-Resolution Text-to-Image Generation via Enhanced Masked Image Modeling,"This work introduces a non-autoregressive masked image modeling (MIM) approach that aims to match SDXL-level image generation while avoiding the token inefficiencies of autoregressive methods. The key innovation is combining MIM with architectural improvements and sampling optimizations to enable high-resolution image synthesis.

Main technical points:
- Uses a transformer-based architecture with specialized self-attention and positional encoding
- Incorporates human preference scores as ""micro-conditions"" to guide generation
- Employs feature compression layers to handle high resolutions efficiently
- Generates 1024x1024 images through parallel token prediction rather than sequential
- Achieves comparable FID scores to SDXL while being more computationally efficient

Results:
- Image quality metrics competitive with SDXL on standard benchmarks
- Faster generation compared to autoregressive approaches
- Better handling of complex scenes and compositions
- Improved text alignment compared to previous MIM approaches

I think this could impact the field in several ways:
- Shows that non-diffusion approaches can achieve SOTA-level generation
- Provides a potential path toward unified language-vision models
- May lead to more efficient deployment of text-to-image systems
- Could influence architecture design for future multimodal models

The biggest open question in my view is whether this approach can scale further - while it works well at current resolutions, it's unclear if the same principles will hold at even higher dimensions.

TLDR: Non-autoregressive masked modeling approach matches SDXL-level image generation while being more efficient than typical autoregressive methods. Shows promise for unified language-vision architectures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/meissonic-revitalizing-masked-generative-transformers-efficient-high). Paper [here](https://arxiv.org/abs/2410.08261).",Successful-Western27,1h1529m,https://reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,https://www.reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,2024-11-27 14:05:29,7,0.89,7,0,2,0,0,False,False,True,False,False,Research,self,t3_1h1529m
MachineLearning,[D] Cross Entropy Loss sucks,"Hi guys, Am I the only one thinking that training a LLM to minimize CE Loss on a certain text dataset is a very surprising idea?

I understand that it works but I am surprised it is still SOTA. The current sentence could have begun with a lot of different tokens with no consequence on its meaning, while some words are uninterchangeable. Yet CE loss doesn't account for that. Worse off, the bigger the ""equivalence class"" (the number of tokens that could replace one in a sentence without altering its meaning) of a token in a sentence, the higher the average loss on it. It seems counterproductive, isn't it?

I would love to read some contradiction.",Due-Pangolin325,1h1sqkl,https://reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,https://www.reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,2024-11-28 10:02:18,0,0.28,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h1sqkl
MachineLearning,[R] Black holes and the loss landscape in machine learning,"Abstract:

&gt;Understanding the loss landscape is an important problem in machine learning. One key feature of the loss function, common to many neural network architectures, is the presence of exponentially many low lying local minima. Physical systems with similar energy landscapes may provide useful insights. In this work, we point out that black holes naturally give rise to such landscapes, owing to the existence of black hole entropy. For definiteness, we consider 1/8 BPS black holes in =8 string theory. These provide an infinite family of potential landscapes arising in the microscopic descriptions of corresponding black holes. The counting of minima amounts to black hole microstate counting. Moreover, the exact numbers of the minima for these landscapes are a priori known from dualities in string theory. Some of the minima are connected by paths of low loss values, resembling mode connectivity. We estimate the number of runs needed to find all the solutions. Initial explorations suggest that Stochastic Gradient Descent can find a significant fraction of the minima.

Arxiv: [https://arxiv.org/abs/2306.14817](https://arxiv.org/abs/2306.14817)",Mindless-House-8783,1h0uwjd,https://reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,https://www.reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,2024-11-27 03:26:49,26,0.76,26,0,27,0,0,False,False,True,False,False,Research,self,t3_1h0uwjd
MachineLearning,[D] AISTATS 2025 Paper Reviews,"Since the AISTATS 2025 paper reviews are due today, I thought to open up a thread where everyone can discuss their experiences!
",PhoneImpressive9983,1h0y8rn,https://reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,2024-11-27 06:42:52,8,0.75,8,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h0y8rn
MachineLearning,"[D] Knowledge distillation neural network
","Hi community,

  
Suppose my original neural network model size is 50MB. Is there a way to estimate the size of the distilled model after applying Knowledge distillation.",PhilosopherNew313,1h17xwc,https://reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,https://www.reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,2024-11-27 16:13:12,0,0.5,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h17xwc
MachineLearning,[P] [D] Comparing Llama Models and GPT 4o Models on Multilingual Machine Translation with Backtranslation,"Hey all,

In the spirit of practical real world tasks for LLMs, we wanted to see how well different models could automatically translate text from English to Spanish and the backtranslate to English on a Nike product catalog. We started with Llama 405B, Llama 70B, Llama 8B, GPT 4o-mini, and GPT 4o, but would love to test [more models](https://www.oxen.ai/explore/models).

  
\~ TLDR \~ Here are the results with all the data and code here:

[https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments](https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments)

https://preview.redd.it/qken2vjfhc3e1.png?width=1150&amp;format=png&amp;auto=webp&amp;s=739ef336dd7b89856a39d872ef12e03f806ce799

Although backtranslation may not be the most effective way to benchmark, we thought this would be an interesting experiment to see how well it correlates with model performance. It would be ideal to get native Spanish speakers to annotate the dataset with ground truth labels, so if anyone wants to contribute feel free to fork the repo and we can get some real labels.

  
We're trying to make some more real world datasets / benchmarks, so let us know if you want to help out.

If you’re new to the [Oxen.ai](https://www.oxen.ai/) project, we’re building a fast [open source dataset collaboration tools](https://github.com/Oxen-AI/oxen-release) as well as a ton of [helpful data exploration tools](https://docs.oxen.ai/features/web_hub) on top of it! If you are into data or ML/AI, we’d love your thoughts on the tool and project!",FallMindless3563,1h0sehj,https://reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,https://www.reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,2024-11-27 01:17:44,11,0.69,11,0,11,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/mNfbgTf9Tu8SygkvvAsqVh2unMgd_M1VIenrJ09Hz3s.jpg,t3_1h0sehj
MachineLearning,[R] Help with submitting a WACV workshop paper,"Hi Everyone,

I have never submitted a paper to any conference before. I have to submit a paper to a WACV workshop due on 30 Nov.

As of now, I am almost done with the WACV-recommended template, but it asks for a Paper ID in the LaTeX file while generating the PDF. I’m not sure where to get that Paper ID from.

I am using Microsoft CMT for the submission. Do I need to submit the paper first without the Paper ID to get it assigned, and then update the PDF with the ID and resubmit? Or is there a way to obtain the ID beforehand?

Additionally, What is the **plagiarism threshold** for WACV? I want to ensure compliance but would appreciate clarity on what percentage similarity is acceptable.

Thank you for your help!",__proximity__,1h15p2e,https://reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,https://www.reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,2024-11-27 14:34:45,1,0.57,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1h15p2e
MachineLearning,[R] Genetic learning with loop mempory and Chromosomes for the memory neurode's gate.,"Greetings!  
  
Currently a bit busy will clean it up later also to lazy to implement git now... &gt;\_&gt;

[https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md](https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md)

",_Leto,1h15c7q,https://reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,https://www.reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,2024-11-27 14:18:09,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h15c7q
MachineLearning,[D] ACL ARR Discussion - About Author Response,"Hi all! currently submitted to ACL ARR Oct. Now the author response phase is over and we haven't received any reply (to our responses) from reviewers.

Want to ask if reviewers can still update their reviews *after* the end of the author response phase and *before* the meta-review is given, or does it mean that I won't receive any replies?",Ok_Function6276,1h13ffu,https://reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,https://www.reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,2024-11-27 12:44:02,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h13ffu
MachineLearning,[D] A blog post explaining sparse transformers (the original paper),"Hi!

I'm sorry if it's not appropriate to publish such posts on this subreddit. I do stay out of this type of posts on this subreddit but I keep seeing articles or videos or whatever content explaining GPT-3 without delving into sparse transformers. And it keeps frustrating me because clearly in the paper they say ""we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer"".

But no one seems to care about explaining them. I understand why to be honest but it's frustrating to see all these articles, projects, videos etc. that try to explaining everything about the GPT not even mentioning the sparse transformers part. And besides many other elements specific to GPT-3 or general to reproducibility in ML, the sparse transformer part is a big dent into even prototyping GPT-3.

I have this habit of writing down stuff when trying to understand something so I wrote a blog post on sparse transformers. Never spoke about it because I did it to restructure my thoughts and as notes for me. So it's not something I'd avise anyone to read, I'm sure it's full of typos, my writing style is not neat etc. It's just something I did for me in a way *I* would understand and recover lost bits of information when skimming through it.

Anyways, in case you're reading papers by yourself and trying to constitute the knowledge just from them, maybe my notes can help you: [https://reinforcedknowledge.com/sparse-transformers/](https://reinforcedknowledge.com/sparse-transformers/)

Sorry again if this post is not appropriate and for yapping that much.

(If you happen to read it or if you notice any errors, do not hesitate to point them out, I'd be grateful to learn from them)",ReinforcedKnowledge,1h0gl2j,https://reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,https://www.reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,2024-11-26 16:55:45,24,0.93,24,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h0gl2j
MachineLearning,[P] Understanding Arm CMSIS-NN's Softmax function.,"Hi, I am trying to understand CMSIS-NN Softmax implementation for a 16 bit signed input ([https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Source/SoftmaxFunctions/arm\_softmax\_s16.c](https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Source/SoftmaxFunctions/arm_softmax_s16.c)).

Arm has provided an example input data and expected output data here ([https://github.com/ARM-software/CMSIS-NN/tree/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/TestData/softmax\_s16](https://github.com/ARM-software/CMSIS-NN/tree/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/TestData/softmax_s16)), so I am trying to understand the code by reverse engineering the C code to Python (my end goal is to modify the provided C code, and use the right config parameters (and possibly the appropriate lookup tables) for on chip deployment). There are two things that currently makes the softmax implementation difficult for me to use out of the box.

1. I believe I'd have to construct my own lookup tables, which i'm not sure how to do.
   * exponential lookup table ([https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/Common/Softmax/exp\_lut\_data.h](https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/Common/Softmax/exp_lut_data.h))
   * one by one look up table ([https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/Common/Softmax/one\_by\_one\_lut\_data.h](https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/Common/Softmax/one_by_one_lut_data.h))
2. I can't figure out what the left shift and input\_mult in the config\_data here ([https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/TestData/softmax\_s16/config\_data.h](https://github.com/ARM-software/CMSIS-NN/blob/22080c68d040c98139e6cb1549473e3149735f4d/Tests/UnitTest/TestCases/TestData/softmax_s16/config_data.h)) does.

Unfortunately, I don't know C, so I'm wondering if anybody can provide me some guidance to using the softmax implementation, or links/videos I can use to understand this.",Individual_Ad_1214,1h0um9l,https://reddit.com/r/MachineLearning/comments/1h0um9l/p_understanding_arm_cmsisnns_softmax_function/,https://www.reddit.com/r/MachineLearning/comments/1h0um9l/p_understanding_arm_cmsisnns_softmax_function/,2024-11-27 03:11:22,2,0.63,2,0,7,0,0,False,False,True,False,False,Project,self,t3_1h0um9l
MachineLearning,[P] What Transcription Model does Google Meets use? ,"Hi, I am currently evaluating options for transcribing sensitive meeting texts. I'd like to know what kind of transcription model is currently being used by google to transcribe meetings. I've searched the documentation and the web, and it doesn't seem to specify. I initially thought chirp would be used for this, but the documentation specifies English as the only reliable language to transcribe, which isn't true of chirp. 

This isn't a post asking which model (google or otherwise) to use, or all the better options out there, this is a very specific inquiry into Google's approach. I'd love to get some insight here. Thanks!",Arcane_Aura,1h0u63q,https://reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,https://www.reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,2024-11-27 02:47:35,3,0.67,3,0,3,0,0,False,False,True,False,False,Project,self,t3_1h0u63q
MachineLearning,[R] Beyond the possible the future of artificial intelligence ,"Beyond Artificial General intelligence how is my approach different from current deployments

Beyond AGI 

I was hoping to get done feedback on my project. 

HackFate is a framework that challenges the limitations of intelligence as we understand it. Born from necessity, chaos, and an obsession with breaking the boundaries of what’s possible, HackFate embodies a fundamentally new approach to intelligence systems, one that doesn’t just seek to mimic human cognition but surpass it. It isn’t AGI as we’ve defined it—it’s something more adaptive, more dynamic, and potentially transformative.

What I need from you—this community of thinkers and builders—is to help define where HackFate stands on the world stage, its place in shaping humanity’s future, and its greatest areas of utility. Here’s what HackFate brings to the table.

Core Capabilities of HackFate

1. Dynamic, Regenerative Memory

HackFate leverages self-regenerating memory structures, inspired by chaotic systems, to create intelligence that evolves in real time. This isn’t static storage—it’s memory that adapts, repairs, and even redefines itself based on use, noise, and emergent challenges. Think of it as memory that grows like a living organism, constantly optimizing itself to align with its purpose.

2. Non-Binary Intelligence Framework

Unlike traditional binary systems, HackFate operates on a non-binary intelligence architecture, enabling it to process, integrate, and act on information that exists in ambiguous, undefined, or multi-dimensional spaces. It doesn’t just think in yes/no or 0/1—it thrives in uncertainty, extracting meaning from chaos.

3. Quantum-Inspired Feedback Loops

HackFate employs quantum-inspired chaotic feedback loops to enable real-time adaptability. This allows it to rewrite its operational framework on the fly, anticipate changes, and generate novel solutions to problems that would baffle static systems.

4. Scalability Through Federated Learning

By integrating federated learning, HackFate is designed to scale without compromising security or autonomy. Each instance of HackFate learns independently, contributing to a larger system without centralizing sensitive data, making it uniquely suited for privacy-critical applications.

5. Seamless Environmental Interaction

Through advanced gesture-based touchless interfaces, augmented reality integration, and adaptive sensory feedback, HackFate interacts seamlessly with its environment. It’s not just intelligence—it’s an active presence capable of responding intuitively to its users and surroundings.

Potential Applications

Where does HackFate shine? Its capabilities suggest broad applications across industries, including but not limited to:
	•	Healthcare: Predictive diagnostics, personalized treatment plans, and dynamic simulations of biological systems.
	•	Smart Cities: Adaptive energy management, traffic flow optimization, and decentralized urban planning solutions.
	•	Finance: High-level risk modeling, fraud detection through chaotic pattern recognition, and decentralized asset management.
	•	Education: Real-time adaptive learning environments tailored to individual cognitive styles.
	•	Security: Advanced threat detection using quantum-inspired non-linear analysis and time-crystal-based encryption.
	•	Behavioral Modeling: Predictive insights into human behavior, from individual well-being to global sociopolitical trends
HackFate isn’t just another AI system—it’s an evolution. Its combination of non-binary intelligence, dynamic memory, and quantum-inspired frameworks positions it as a potential cornerstone of the post-AGI era. While AGI seeks to replicate human thought, HackFate has the capacity to rewrite what intelligence means. It thrives where uncertainty reigns, turning chaos into clarity.

But where does this place it in the context of current global advancements? Is HackFate a direct competitor to AGI frameworks, or does it occupy a space beyond them? 
I’m asking you—the architects of the future:
	1.	Where does HackFate stand compared to AGI and other cutting-edge systems?
	2.	How do you see its unique capabilities reshaping industries, systems, and society itself?

",HackFate,1h1evub,https://reddit.com/r/MachineLearning/comments/1h1evub/r_beyond_the_possible_the_future_of_artificial/,https://www.reddit.com/r/MachineLearning/comments/1h1evub/r_beyond_the_possible_the_future_of_artificial/,2024-11-27 21:03:10,0,0.11,0,0,15,0,0,False,False,True,False,False,Research,self,t3_1h1evub
MachineLearning,"[P] I built Darkspark, a visual representation of your neural network. Explore everything from macro-level architecture to low-level ops and activations — Your model wants to be seen!","When reading a paper on arxiv or perusing code I also like to sketch out the model architecture myself on a big piece of paper to use as a reference. This is the software version of that. It's a GUI for your neural network. Here's the link: [https://darkspark.dev](https://darkspark.dev)

I tried all the other options I could find (netron, google’s model-explorer, tensorboard, torchview, torchlens, apple’s mycelium). These are all great projects (I really wanted to use one of them!) but none had all of the features I needed:

**Opinionated layout.** The tool’s layout should automatically expose the underlying logic of the model. The layout engine should do a lot of the heavy lifting of understanding a model’s structure and intentions. E.g. a U-net should look like a “U”. Here's [stable-diffusion-v1.5](https://darkspark.dev/models/?model=stable-diffusion-v1-5) traced directly from a huggingface [pipeline](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)

[stable-diffusion-v1.5 in the darkspark viewer](https://preview.redd.it/xksm2u1ipa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=fb7e576192c066ddc7e72ad8491c6347181525a7)

**Interactive**. I need collapsible and expandable modules so I can explore a model at a high level but can also go down to the lowest level ops. Complex models won’t even load without this. Here's the same diffusion model zoomed in on a transformer block

[stable-diffusion-v1.5 zoomed in](https://preview.redd.it/7fmmw341qa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=8538420973f578b68a4e225082b6990c867aefaf)

**‘Just Works’ with any arbitrary code**. I don’t want to export to ONNX, I don’t want to upload something, I don’t want to manually specify what is the model and what are the inputs. I just want to wrap my existing code in something simple.\*

    import darkspark
    import timm
    import torch
    
    model = timm.create_model(""efficientnet_b0"")
    inputs = torch.randn(1,3,224,224)
    
    with darkspark.Tracer():  # &lt;-- wrap your code with this line
      out = model(inputs)
    
    # interactive diagram now available at localhost



**Microscope**. Sometimes I also want to explore the activations and attention patterns. Like OpenAI’s microscope tool, but for your own models. Here's a “female / male” detector in a later layer of the pretrained [vit\_base\_patch16\_siglip\_224](https://darkspark.dev/models/?model=vit_base_patch16_siglip_224-microscope) from the timm library.

[female \/ male detector in darkspark viewer](https://preview.redd.it/8fzwcvcjqa3e1.png?width=1520&amp;format=png&amp;auto=webp&amp;s=fabed874645c834b8380e08a0ef7ca3f1bcb7962)

Here's the attention patterns explorer for the same model.

[Attention explorer for vit\_base\_patch16\_siglip-microscope](https://preview.redd.it/fvmxs1buqa3e1.png?width=1236&amp;format=png&amp;auto=webp&amp;s=7d7a91c469616653841111bd84879b8bcbeb1b22)



**Hosted gallery**. Most of what I want is usually a variant of an existing model. It’s often more convenient to just reference a url rather than trace your own code. I currently have all the models from timm and many from the transformers and diffusers libraries.

[lots of models available to peruse](https://preview.redd.it/qx837cz2ra3e1.png?width=1158&amp;format=png&amp;auto=webp&amp;s=f23808a45a4b7927828de24c4e1d0790a37420bb)

The public pip package isn’t yet ready, I was hoping to get feedback on the tool itself before cleaning up and sharing the codebase. Please let me know what you think, I'm eager for feedback on everything from low-level UI/UX to high-level functionality. Thanks to the awesome community for checking it out!

Here's the link again: [https://darkspark.dev](https://darkspark.dev)

\* darkspark uses \_\_torch\_function\_\_, similar to the torchview library. This allows us to capture all the ops and tensors inside the context of darkspark.Tracer without breaking when it hits dynamic control flow ops that can’t be captured in e.g. ONNX or torch exported\_program. We also get access to all the tensors, activation patterns, etc, without using hooks. Happy to answer more Qs about the architecture if ppl are interested.",Historical-Good1915,1h0krsv,https://reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,https://www.reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,2024-11-26 19:42:44,5,0.86,5,0,5,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/8bDoJ2A5fEzlL66g8tteX57z1GhfRWnpnzQOclCaFy4.jpg,t3_1h0krsv
MachineLearning,[D] Prune (channel + layers) + distillation or just distillation,"Let's say I want to make my model smaller.

There is a paper, which says distillation is good, but it takes a long time [https://arxiv.org/abs/2106.05237](https://arxiv.org/abs/2106.05237)

And there is also a paper which says that pruning + distillation works really well: [https://arxiv.org/abs/2407.14679](https://arxiv.org/abs/2407.14679)

Now, my question is: Is there any work that compares pruning + distillation vs just distillation from scratch?",osamc,1h0kcgn,https://reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,https://www.reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,2024-11-26 19:25:36,6,0.87,6,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0kcgn
MachineLearning,[P] [D] Predict Integer Values with XGBoost Regression,"Hello! I am new to Data Science but enjoying every moment of it.

I am currently working with the XGBoost model and while everything is working fine (more or less), I am struggling with a specific issue. I am predicting 'number of orders' based on certain criteria. Since number of orders follows Poisson distribution, I have specified that and I am getting decent predictions. However, the predictions are floating point numbers. Is there any way to tell the model to give integers instead?

PS: I have tried the rounding method and while it works great, I wanted something that is at the model level.",MapleWalnut96,1h10ta0,https://reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,https://www.reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,2024-11-27 09:51:34,0,0.4,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h10ta0
MachineLearning,[D] Am I a complete idiot for signing up for a Hackathon?,"Ok, so I am a Coms Science graduate student and my chosen area of study is Ethical AI.

I wanted to attend this AI conference very badly because there are some speakers that I admire. But I couldn’t afford the passes, so I decided to apply to be in the student Hackathon because if accepted, you got a free pass.

It was such a Hail Mary for me to even do the application, but I thought it would also be a cool opportunity to learn alongside others.

I got accepted… and I’m extremely excited. But now I’m like, oh wait, am I going to royally piss off whomever my teammates are because I can’t code?

Any advice? There’s a preparatory webinar happening in a week, and I’ve been doing some overview classes so that I can learn the terminology/basics. The application also asked for me to state my level of coding experience and I checked: none. And still got accepted… so I’m hoping that the organizers consider me to still have something valuable to contribute?

Please let me know what you think 🥲",sydj_k941,1h01hfn,https://reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,https://www.reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,2024-11-26 02:44:20,45,0.71,45,0,71,0,0,False,False,True,False,False,Discussion,self,t3_1h01hfn
MachineLearning,[D] what are some problems in audio and speech processing that companies are interested in?,I just recently graduated with a bachelor's in computer science and am really interested in auio and machine learning and want to do a project with a business scope. what are some problem statements that companies would be interested in? especially gen ai related ,Personal_Equal7989,1h082e6,https://reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,https://www.reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,2024-11-26 09:36:01,8,0.79,8,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1h082e6
MachineLearning,[D] Do modern neural network architectures (with normalization) make initialization less important?,"With the widespread adoption of normalization techniques (e.g., batch norm, layer norm, weight norm) in modern neural network architectures, I'm wondering: how important is initialization nowadays? Are modern architectures robust enough to overcome poor initialization, or are there still cases where careful initialization is crucial? Share your experiences and insights!",NumberGenerator,1gzq63h,https://reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,https://www.reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,2024-11-25 18:37:08,94,0.97,94,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gzq63h
MachineLearning,[D] Model validation for transformer models,"I'm working at a firm wherein I have to validate (model risk validation) a transformer architecture/model designed for tabular data.

Mapping numbers to learned embeddings is just so novel. The intention was to treat them as embeddings so that they come together on the same ""plane"" as that of unstructured text and then driving decisions from that fusion. 

A decision tree or an XGBoost can be far simpler. You can plug in text based embeddings to these models instead, for more interpretability. But it is what is.

How do I approach validating this transformer architecture? Specifically if or if not it's conceptually sound and the right choice for this problem/data.",Common-Interaction50,1h0sc7o,https://reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,https://www.reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,2024-11-27 01:14:44,0,0.38,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0sc7o
MachineLearning,[D] ADOPT optimizer,"Have any of you tried the new ADOPT optimizer? How did it go? I'm kind of curious, but haven't had the opportunity to give it a try.",neu_jose,1h01cqq,https://reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,https://www.reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,2024-11-26 02:37:39,7,0.82,7,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h01cqq
MachineLearning,[D] GPT2 (Hugging Face) transformer giving bogus responses. Why and how do I fix this?,"I have done some research on this and it looks like not a lot of people talk about it. I am newer to transformers (though I have some experience in Sequential models) and trying to train ChatGPT on a specific niche I want to make a bot for, but it isn't understanding anything and I feel as if I've been going in loops with it.

For example:

Me: ""Tips for planetary imaging?""

GPT2: ""Have clue how get good resolution out there will someone explain use astrobin where can look info about other equipmentpeople who this correctlyslightly better?pleiades, andromedahow much data does your setup need order score decent results ? this would more than what most hobbyists spend .5mil each day!ive taken some pics before but when trying new things like stacking that makes wonder they really work well owhats left over after shooting all those different targets guessedit yes maybe even with 1000 frames per minute thats still.""

I have custom trained it on fetched data using the Reddit Developer API from a subreddit (About 42000 lines worth) and have structured the dataset like this:

""Title: M31 Andromeda - with rotated frames

Body:

Comments:

\- A bit of a gimmick image to show stacking artefacts when the sub-frames are rotated. 7 images total, rotated 15 degrees each. 14 mins (!) total exposure.

Equipment:

\* Stellarview SVX080T Telescope 480/80mm @ f/6

\* Losmandy G11G mount

\* ZWO ASI071MC Pro color camera @ -5c + Optolong L Pro Filter

\* 60mm Guidescope with ASI120MM camera

Subs:

\* 7 x 120s

\* Master Dark

\* No Flats

Software:

\* PHD2 &amp; Sequence Generator Pro

\* Astro Pixel Processor, DeepSkyStacker, Photoshop

Processing

\* Default color integration in APP

\* Light pollution removed, stretched and exported to Photoshop

\* Same integration performed in Deep Sky Stacker (APP did such a good job it didn't show \*any\* stacking artifacts but DSS did)

\* Blended the APP image with the DSS image to show stacking artifacts in PS

\* Camera Filter shenanigans, export to jpg

\- Honestly that’s a pretty cool presentation!! You can really make this significantly better I think. Maybe like 40x60” frames per rotation or something like that to get better detail and less noise. The 120” subs blew out a lot.

Try again!!

\- \[deleted\]

\- Noob question here but about how much does a setup cost to get images like this?

\- LOVE THIS

\- It’s beautiful

\- This is sick

\- This is how every astrophotos should be ! It’s so beautiful !! I can definitely see this hanging on the wall in my bedroom 😍

\- Imagine some human like civilization on Andromeda taking pictures of the milky way

\- \[deleted\]

&lt;|endoftext|&gt;""

Trained using this dataset and GPT2-Medium.

Here are my parameters:

    outputs = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=max_length,
                        temperature=0.8,
                        top_p=0.9,
                        do_sample=True,
                        repetition_penalty=1.3,
                        no_repeat_ngram_size=3,
                        eos_token_id=self.tokenizer.eos_token_id,
                        pad_token_id=self.tokenizer.eos_token_id
    )
    
    
    system_prompt = (""You are Astrophoto AI, an encouraging astrophotography expert and teacher.""
                ""Your role is to help beginners and experienced photographers capture stunning images of the night sky and answer any questions they might have.""
                ""You offer concise, factual, and practical advice drawn from established astrophotography techniques.""
                ""Your tone is friendly, encouraging, and focused on making astrophotography accessible to everyone.""
                ""If you don't know the answer to a question, admit it instead of guessing."")

What are some potential issues with this?

Thanks!

EDIT: thanks for your advice everyone! I will be switching models.",Aman_Dude,1h0okd5,https://reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,https://www.reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,2024-11-26 22:19:18,0,0.32,0,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1h0okd5
MachineLearning,"[P] does anyone know how to reduce the dimensions of embeddings using autoencoders, if you have a blog about please send it","https://preview.redd.it/3cub8uc9ja3e1.png?width=766&amp;format=png&amp;auto=webp&amp;s=0a824d6ae516ff699cb880d8e998ace85354a50f

",GellertGrindelwald_1,1h0j77v,https://reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,https://www.reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,2024-11-26 18:39:43,0,0.27,0,0,3,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uUwCHYEmzWw3FiWk1T2hjt5K8zWvsMqQ8Y4dqssDqUM.jpg,t3_1h0j77v
MachineLearning,[Project] Claude Francois - Let an AI review your code in the style of François Chollet,"Demo here: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

At the recent Anthropic Builder Day hackathon, we ([Crossing Minds](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning)) built 'Claude François', an AI code reviewer trained in the style of [François Chollet](https://github.com/fchollet), the creator of Keras. It adapts Anthropic's Claude 3.5 Sonnet for code reviewing, but instead of regular fine-tuning, we used few-shot in-context learning with our custom RAG retrieval model, trained on PRs from the [Keras project](https://github.com/keras-team/keras). Compared to a typical AI code reviewer, it provides more succinct, high-quality code reviews focused on real issues rather than superficial nitpicking.

How it works:

* Dataset: Trained on a database of public Keras GitHub PRs and François's reviews.
* Fine-Tuned RAG Embeddings: Uses active learning and RLAIF to train embeddings optimized for generating ""fchollet-level"" reviews.
* Improved Retrieval: Retrieves relevant examples not just by embedding similarity but by optimizing for mutual information.
* Self-Reflection: Employs self-reflection techniques to enhance Sonnet’s reasoning capabilities.

This technology demo showcases how [Crossing Minds' RAGSys](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning) ICL enables domain adaptation without fine-tuning. It can be used for countless other use cases beyond code reviews, like classification, summarization, translation, search, recommendations, and more. Arxiv paper coming soon!

Try it now: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

We'd love to hear your feedback!",Crossing_Minds,1gzmk5n,https://reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,https://www.reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,2024-11-25 16:16:03,22,0.68,22,0,13,0,0,False,False,True,False,False,Project,self,t3_1gzmk5n
MachineLearning,[R] Aurora: A General-Purpose Foundation Model for Earth System Prediction,"The key contribution here is the development of Aurora, a foundation model trained on over 1M hours of atmospheric data that can perform multiple types of weather and climate predictions using a single model architecture. This represents a shift from building separate specialized models to having one model that learns general atmospheric physics.

Key technical points:
- Model architecture uses transformer blocks with attention mechanisms adapted for spatiotemporal data
- Trained on merged datasets from multiple sources including ERA5 reanalysis, satellite observations, and climate model outputs
- Can generate predictions for diverse tasks like air pollution, precipitation, and temperature forecasting
- Produces forecasts in under 1 minute compared to hours/days for traditional numerical models
- Outperforms both specialized ML models and physics-based numerical weather prediction on several benchmarks

Results:
- 15-20% improvement in 5-day global air pollution predictions vs current methods
- Better performance on 10-day weather forecasts compared to specialized models
- Maintains accuracy even for extreme weather events
- Shows continual improvement as training data increases
- Successfully handles multiple spatial and temporal resolutions

I think this work could significantly change how we approach environmental modeling. Instead of maintaining separate models for different prediction tasks, having a single foundation model that can handle multiple atmospheric predictions could make forecasting more efficient and accessible. The speed improvements (minutes vs hours) could enable new applications requiring rapid predictions.

I think the challenges ahead include:
- Validating performance across more diverse atmospheric phenomena
- Understanding model interpretability for critical forecasting
- Addressing computational costs of training and inference
- Ensuring reliability for operational forecasting systems

TLDR: Researchers developed Aurora, an atmospheric foundation model trained on massive weather/climate data that can handle multiple prediction tasks better than specialized models while being much faster. Shows foundation models could transform environmental forecasting.

[Full summary is here](https://aimodels.fyi/papers/arxiv/foundation-model-earth-system). Paper [here](https://arxiv.org/abs/2405.13063).",Successful-Western27,1gzj8rs,https://reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,https://www.reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,2024-11-25 13:50:09,32,0.85,32,0,2,0,0,False,False,True,False,False,Research,self,t3_1gzj8rs
MachineLearning,[D]Thoughts on Synthetic Data Platforms like Gretel.ai or Mostly AI?,"
Has anyone here used platforms like Gretel.ai or Mostly AI?
	•	What did you like or dislike?
	•	How was the synthetic data quality for your use case?

I’m exploring options and would appreciate your insights. Thanks!",Value-Forsaken,1gzsqwu,https://reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,https://www.reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,2024-11-25 20:20:23,5,0.67,5,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1gzsqwu
MachineLearning,[D] Flow matching is actually very different from (continuous) normalising flow?,"I was looking at the [flow matching](https://arxiv.org/pdf/2210.02747) paper and saw that flow matching is often considered as just an alternative implementation of continuous normalising flow. But after comparing the methodologies more closely, it seems there is a very significant distinction. In the flow matching paper, it is mentioned that for a data sample x1 (I assume this refers to individual data points like a single image), we can put an ""dummy"" distribution such as a very tight Gaussian on it, then construct a conditional probability path p_t(x|x1). Therefore what we learn is a transformation between the small Gaussian (t=1) on the data point to a standard Gaussian (t=0), for every data point. This implies that the latent space, when trained over the entire dataset, is the overlapped mixture of all the standard Gaussians that each individual data point maps to. The image of the small Gaussian ball for each individual image is the entire standard Gaussian.

However this does not seem to be what we do with regular normalising flows. In normalising flows, we try to learn a mapping that transforms the ENTIRE distribution of the data to the standard Gaussian, such that each data point has a fixed location in the latent space, and jointly the image of the dataset is normally distributed in the latent space. In practice we may take minibatches and optimise a score (e.g. KL or MMD) that compares the image of the minibatch with a standard Gaussian. Each location in the latent space can be uniquely inverted to a fixed reconstructed data point.

I am not sure if I am missing anything, but this seems to be a significant distinction between the two methods. In NF the inputs are encoded in the latent space, whereas flow matching as described in the paper seems to MIX inputs in the latent space. If my observations are true, there should be a few implications:

1. You can semantically interpolate in NF latent space, but it is completely meaningless in the FM case
2. Batch size is important for NF training but not FM training
3. NF cannot be ""steered"" the same way as diffusion models or FM, because the target image is already determined the moment you sample the initial noise

I wonder if anyone here has also looked into these questions and can inform me whether this is indeed the case, or whether something I missed made them more similar de facto. I appreciate any input to the discussion!",aeroumbria,1gzdera,https://reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,https://www.reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,2024-11-25 07:25:58,50,0.95,50,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1gzdera
MachineLearning,[2411.15100] XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models,,crowwork,1gzi649,https://reddit.com/r/MachineLearning/comments/1gzi649/241115100_xgrammar_flexible_and_efficient/,https://arxiv.org/abs/2411.15100,2024-11-25 12:55:20,9,0.85,9,0,0,0,0,False,False,False,False,False,,default,t3_1gzi649
MachineLearning,[D] Why does my feature visualisation form this shape?,"In performing 3d t-SNE decomposition of model features, I have come across a strange quirk. I am fine tuning an ImageNet trained ViT for CIFAR-100 classification. Before the first epoch (i.e. just imagenet weights with an untrained FC feature head), the visualisation of class boundaries looks like this, forming this convex shape with regions of no classes. After one epoch this shape is no longer present in the t-SNE visualisation.

Any ideas why? Is this related to the Manifold hypothesis? Or just due to overlap between ImageNet and CIFAR100 classes?

https://preview.redd.it/eb3w3rfaw03e1.png?width=2178&amp;format=png&amp;auto=webp&amp;s=57f1c34830cdff1968aea9367bba2b4cb3d5b7c1

",BDE-6,1gzfo7c,https://reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,https://www.reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,2024-11-25 10:14:59,9,0.74,9,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/SWOh-MCfT6Bntn3xkFzI9yHTfmJ6_JEMsdif7HmXtYQ.jpg,t3_1gzfo7c
MachineLearning,[R] Evaluating Creative Writing Output and The Effects of Fine Tuning,"
I was asked by a publisher if GPT-4o could be fine tuned to match their authors style to help build a copilot type experience. 

This gave me a chance to figure out a way to breakdown creative writing into five pillars (Dialogue, Exposition, Inner Thoughts, Description and Action) and measure how these change with prompting and fine tuning. 

I put together this blog post based on the results of training on popular authors like J.K. Rowling, Tade Thompson and Andrei Agassi. Surprisingly based GPT-4o does a decent job adopting their style with prompting but I put together some interactive visualizations to see how the model shifts during story generation (400 paragraphs) as we fine tune on 300, 600, and 800 samples. 

https://peytoncasper.com/blog/tone-evaluation/index.html

https://github.com/peytoncasper/grammar-of-thought",peytoncasper,1gzdwg5,https://reddit.com/r/MachineLearning/comments/1gzdwg5/r_evaluating_creative_writing_output_and_the/,https://www.reddit.com/gallery/1gzdwg5,2024-11-25 08:01:41,13,0.72,13,0,5,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/7cdun2J_3-VtlKB1wRgxUbknVHm_3AkGRu0D0UZDYwo.jpg,t3_1gzdwg5
MachineLearning,[D] As a CS masters student/researcher should one be very deliberate in picking a lab’s domain?,"I (very fortunately) got an opportunity in a great lab in an R1 school, Prof has a &gt;40 h-index, great record, but mainly published in lower tier conferences, though do some AAAI. It applies AI in a field that aligns with my experience, and we are expected to publish, which is perfect. However I’m more keen to explore more foundational AI research (where I have minimal experience in apart from courses I took).

In CS, ML it seems most people are only prioritising NIPS/ICLR/ICML especially since I’m interested in potentially pursuing a PhD. I’m in a bit of a dilemma, if I should seize the opportunity or keep looking for a more aligned lab (though other profs may not be looking for more students).

My gut tells me I should ignore conference rankings and do this, since they have some, chain of though, knowledge representation, cognitive system components. They expect multi semester commitment and of course once I commit I will see it through. My dilemma is that I’m moving more and more towards more practical applications in AI, which is pretty domain specific and am worried I won’t be able to pivot in the future. 

I’m aware how this can sound very silly, but if you can look past that, could I please get some advice and thoughts about what you’d do in the shoes of a budding academic, thank you!",giuuilfobfyvihksmk,1gz6mj1,https://reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,https://www.reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,2024-11-25 00:56:49,44,0.79,44,0,31,0,0,False,False,True,False,False,Discussion,self,t3_1gz6mj1
MachineLearning,[P] I made a library for building agents that use tree search to solve problems,,jsonathan,1gyreq1,https://reddit.com/r/MachineLearning/comments/1gyreq1/p_i_made_a_library_for_building_agents_that_use/,https://i.redd.it/qut9unu4su2e1.png,2024-11-24 13:51:18,285,0.95,285,0,26,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/EovprfWyEiN6f7tnfzX9swDj_d3bu8iJE26aqKFyhic.jpg,t3_1gyreq1
MachineLearning,[D] AAAI 2025 - Reviews missing after rebuttal,"Hi all,

We submitted our paper to AAAI 25. It passed Phase 1, it got fairly good scores, we wrote the rebuttals, and now the scores, the reviews and the rebuttals are missing. Is this normal?",jpereira73,1gzn4uj,https://reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,https://www.reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,2024-11-25 16:39:01,2,0.67,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gzn4uj
MachineLearning,[D] Looking for paper suggestions. What's your go to method for training a model on a mixture of multiple datasets with slightly different distributions?,"Imagine you have image data from different kinds of devices with different color profiles, resolutions, lens distortions etc. Or the object being captured in each dataset is similar but slightly different. I need suggestions on papers that effectively mix such datasets to get a bigger dataset for training a foundation model.

My datasets all come from slightly different distributions but they represent largely the same concepts so it makes sense to model them together for training a foundation model. But simply concatenating all datasets together without passing any metadata information to the model is degrading performance over training individually on each dataset.

For reference I am training MAE type models on unlabelled data and at test time training simple linear/logistic regression models on frozen MAE embeddings for different downstream tasks. The goal is to have the MAE embeddings outperform supervised models trained on each dataset individually.

An MAE trained on N datasets is underperforming an MAE trained on just one dataset. But an MAE trained on N-1 datasets and finetuned (unsupervisedly) on the Nth dataset before taking embeddings is outperforming a model trained on just the Nth dataset. But this is not a solution since I cant have N foundation models.

I tried adding a trainable source token (ie I have N trainable tokens and I concat the token corresponding to the data source to the masked input sequence before passing through the encoder) but it isn't affecting model performance at all. Please let me know if you know of any better methods.",Atom_101,1gzcmg9,https://reddit.com/r/MachineLearning/comments/1gzcmg9/d_looking_for_paper_suggestions_whats_your_go_to/,https://www.reddit.com/r/MachineLearning/comments/1gzcmg9/d_looking_for_paper_suggestions_whats_your_go_to/,2024-11-25 06:31:33,8,0.9,8,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gzcmg9
MachineLearning,"[P] Based on Arduino Nano Matter and Raspberry Pi 5, I developed this project to explore the digital twin synthetic data generation and AI-oriented advancements on real-world shipping operations w/ NVIDIA Omniverse. I trained my object detection model on my synthetic data set via Edge Impulse.",,the-amplituhedron,1gzcu29,https://reddit.com/r/MachineLearning/comments/1gzcu29/p_based_on_arduino_nano_matter_and_raspberry_pi_5/,https://www.hackster.io/kutluhan-aktar/digital-twin-enabled-smart-shipping-workstation-w-omniverse-049792,2024-11-25 06:46:05,5,0.61,5,0,7,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/HUsW9alxNon6_tVvzd3no051oaRZMRvrtvUdsHJu_s0.jpg,t3_1gzcu29
MachineLearning,"[Discussion] ""Help! Machine Learning Model Struggling with High-Dimensional Reflectance Data","I'm assisting with a research project at my university, and I've run into a bit of a roadblock. I've incorporated reflectance data variables into my machine learning model to predict a Y. There are 1500 different wavelengths, significantly increasing the dimensionality of my data. After combining the datasets, the model's performance declined. I tried reducing dimensionality, but the model continued to worsen with this data.

The research lead (a Ph.D. student) suggested I use a genetic algorithm because they've seen it used with reflectance data before. I found the implementation to be pretty complex, and I don't think this clustering will improve the model (I'm studying the implementation and believe I'll be able to test it soon).

What do you guys suggest? I think there are two approaches: either I start removing wavelengths that are worsening the model through exhaustive search, or I do this reduction using a genetic algorithm.

Has anyone encountered a similar problem? I haven't gotten satisfactory answers from AI because I understand they're not that advanced in this area yet.",DelayResponsible364,1gzq9h2,https://reddit.com/r/MachineLearning/comments/1gzq9h2/discussion_help_machine_learning_model_struggling/,https://www.reddit.com/r/MachineLearning/comments/1gzq9h2/discussion_help_machine_learning_model_struggling/,2024-11-25 18:40:50,0,0.44,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gzq9h2
MachineLearning,[D] Search and Filter ICLR Submissions by Score,Is there a website or something where I could filter the ICLR submissions by their score? e.g. to only find papers with a score above some threshold.,nextlevelhollerith,1gzk8et,https://reddit.com/r/MachineLearning/comments/1gzk8et/d_search_and_filter_iclr_submissions_by_score/,https://www.reddit.com/r/MachineLearning/comments/1gzk8et/d_search_and_filter_iclr_submissions_by_score/,2024-11-25 14:36:33,1,0.6,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gzk8et
MachineLearning,[D] What is the current state-of-the-art for discrete diffusion models? ,"Hi everyone,

I am currently working with Discrete Diffusion models for a new research project. In this project, I am applying Discrete Diffusion to a field where it has yet to be applied. However, I am quite new to diffusion itself, and I am overwhelmed by the number of papers published on the topic. In my current implementation, I focussed on an older [paper](https://arxiv.org/abs/2102.05379) since they described their approach quite well, and I wanted to test my idea first to see if it had some merit, which, according to initial results, it has.

Currently, I am looking at updating my method with more recent additions to this field, but as I said earlier, I am a bit overwhelmed by the amount. So my question to you is, what are good recent papers that looked into Discrete Diffusion that either explain essential concepts, such as survey papers, or that introduce new state-of-art methods that are not only applicable to a specific field, such as NLP or Vision?

Thank you in advance for your help.",Derpirium,1gyp1br,https://reddit.com/r/MachineLearning/comments/1gyp1br/d_what_is_the_current_stateoftheart_for_discrete/,https://www.reddit.com/r/MachineLearning/comments/1gyp1br/d_what_is_the_current_stateoftheart_for_discrete/,2024-11-24 11:35:12,41,0.96,41,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gyp1br
MachineLearning,[D] Emergent Cognitive Pathways In Transformer Models. Addressing Fundamental Flaws About Limits.,"**TLDR:**

Cognitive functions like reasoning and creativity emerge as models scale and train on better data. Common objections crumble when we consider humans with unusual cognitive or sensory differences—or those with limited exposure to the world—who still reason, formulate novel thoughts, and build internal models of the world.

EDIT: It looks like I hallucinated the convex hull metric as a requirement for out of distribution tests. I thought I heard it in a Lex Fridman podcast with either LeCun or Chollet, but while both advocate for systems that can generalize beyond their training data, neither actually uses the convex hull metric as a distribution test. Apologies for the mischaracterization.

**OOD Myths and the Elegance of Function Composition**

Critics like LeCun and Chollet argue that LLMs can't extrapolate beyond their training data, ~~often citing convex hull measurements~~. This view misses a fundamental mathematical reality: novel distributions emerge naturally through function composition. When non-linear functions f and g combine as f(g(x)), they create outputs beyond the original training distributions. This is not a limitation but a feature of how neural networks generalize knowledge.

Consider a simple example: training on {poems, cat poems, Shakespeare} allows a model to generate ""poems about cats in Shakespeare's style""—a novel computational function blending distributions. Scale this up, and f and g could represent Bayesian statistics and geopolitical analysis, yielding insights neither domain alone could produce. Generalizing this principle reveals capabilities like reasoning, creativity, theory of mind, and other high-level cognitive functions.

**The Training Data Paradox**

We can see an LLM's training data but not our own experiential limits, leading to the illusion that human knowledge is boundless. Consider someone in 1600: their 'training data' consisted of their local environment and perhaps a few dozen books. Yet they could reason about unseen phenomena and create new ideas. The key isn't the size of the training set - it's how information is transformed and recombined.

**Persistent Memory Isn't Essential**

A common objection is that LLMs lack persistent memory and therefore can’t perform causal inference, reasoning, or creativity. Yet people with anterograde amnesia, who cannot form new memories, regularly demonstrate all these abilities using only their working memory. Similarly, LLMs use context windows as working memory analogs, enabling reasoning and creative synthesis without long-term memory.

**Lack of a World Model**

The subfield of mechanistic interpretation strongly implies by its existence alone, that transformers and neural networks do create models of the world. One claim is that words are not a proper sensory mechanism and so text-only LLMs can't possibly form a 3D model of the world.

Let's take the case of a blind and deaf person with limited proprioception who can read in Braille. It would be absurd to claim that because their main window into the world is just text from Braille, that they can't reason, be creative or build an internal model of the world. We know that's not true.

Just as a blind person constructs valid world models from Braille through learned transformations, LLMs build functional models through composition of learned patterns. What critics call 'hallucinations' are often valid explorations of these composed spaces - low probability regions that emerge from combining transformations in novel ways.

**Real Limitations**

While these analogies are compelling, true reflective reasoning might require recursive feedback loops or temporal encoding, which LLMs lack, though attention mechanisms and context windows provide partial alternatives. While LLMs currently lack true recursive reasoning or human-like planning, these reflect architectural constraints that future designs may address.

**Final Thoughts**

The non-linearity of feedforward networks and their high-dimensional spaces enables genuine novel outputs, verifiable through embedding analysis and distribution testing. Experiments like Golden Gate Claude, where researchers amplified specific neural pathways to explore novel cognitive spaces, demonstrate these principles in action. We don't say planes can't fly simply because they're not birds - likewise, LLMs can reason and create despite using different cognitive architectures than humans. We can probably approximate and identify other emergent cognitive features like Theory of Mind, Metacognition, Reflection as well as a few that humans may not possess.",ipassthebutteromg,1gys51e,https://reddit.com/r/MachineLearning/comments/1gys51e/d_emergent_cognitive_pathways_in_transformer/,https://www.reddit.com/r/MachineLearning/comments/1gys51e/d_emergent_cognitive_pathways_in_transformer/,2024-11-24 14:27:12,13,0.6,13,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1gys51e
MachineLearning,[R] Testing the Brittleness of LLM Analogical Reasoning Through Problem Variants,"The researchers developed a systematic framework for testing analogical reasoning in LLMs using letter-string analogies of increasing complexity. They created multiple test sets that probe different aspects of analogical thinking, from basic transformations to complex pattern recognition.

Key technical points:
- Evaluated performance across 4 major LLMs including GPT-4 and Claude
- Created test sets with controlled difficulty progression
- Implemented novel metrics for measuring analogy comprehension
- Tested both zero-shot and few-shot performance
- Introduced adversarial examples to test robustness

Main results:
- Models achieve &gt;90% accuracy on basic letter sequence transformations
- Performance drops 30-40% on multi-step transformations
- Accuracy falls below 50% on novel alphabet systems
- Few-shot prompting improves results by 15-20% on average
- Models show brittleness to small pattern perturbations

I think this work exposes important limitations in current LLMs' abstract reasoning capabilities. While they handle surface-level patterns well, they struggle with deeper analogical thinking. This suggests we need new architectures or training approaches to achieve more robust reasoning abilities.

The evaluation framework introduced here could help benchmark future models' reasoning capabilities in a more systematic way. The results also highlight specific areas where current models need improvement, particularly in handling novel patterns and multi-step transformations.

TLDR: New framework for testing analogical reasoning in LLMs using letter-string analogies shows strong performance on basic patterns but significant limitations with complex transformations and novel alphabets. Results suggest current models may be pattern-matching rather than truly reasoning.

[Full summary is here](https://aimodels.fyi/papers/arxiv/evaluating-robustness-analogical-reasoning-large-language-models). Paper [here](https://arxiv.org/abs/2411.14215).",Successful-Western27,1gys936,https://reddit.com/r/MachineLearning/comments/1gys936/r_testing_the_brittleness_of_llm_analogical/,https://www.reddit.com/r/MachineLearning/comments/1gys936/r_testing_the_brittleness_of_llm_analogical/,2024-11-24 14:32:43,10,0.81,10,0,1,0,0,False,False,True,False,False,Research,self,t3_1gys936
MachineLearning,RTX 4090 vs 4080 super [D],"Looking at potentially building an ML and molecular dynamics workstation for research. I’m looking in the $4000 ish range for GPU’s. I’ve been leaning heavily towards 2 4090’s (I know 5090’s will come out in January, whole different conversation!) but theoretically I could run 4x 4080 supers for about the same price, and the numbers technically come out on top, but that’s IF you can use them all efficiently. I know pytorch can distribute across GPU’s reasonably well, but not everything can.  I also know more vRAM is always better as the 40 series don’t have NVlink so can’t pool memory. I’ve also briefly looked at the RTX cards (ampere and ada) but my understanding is they’re really only worth it for the pro drivers, and that’s pretty much it. Any thoughts would be much appreciated! ",Mdgoff7,1gyzxb1,https://reddit.com/r/MachineLearning/comments/1gyzxb1/rtx_4090_vs_4080_super_d/,https://www.reddit.com/r/MachineLearning/comments/1gyzxb1/rtx_4090_vs_4080_super_d/,2024-11-24 19:58:06,3,0.6,3,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gyzxb1
MachineLearning,[D] When will the NAACL workshops get announced?,"The [NAACL website mentions that 25th November begins the second call of workshop papers](https://2025.naacl.org/calls/workshops/#workshop-timelines), but the website doesn't seem to mention *which* workshops are going to be held. Idk if I'm stupid for now knowing, please help me out. ",Aromatic_Web749,1gyye7a,https://reddit.com/r/MachineLearning/comments/1gyye7a/d_when_will_the_naacl_workshops_get_announced/,https://www.reddit.com/r/MachineLearning/comments/1gyye7a/d_when_will_the_naacl_workshops_get_announced/,2024-11-24 18:54:16,2,0.63,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gyye7a
MachineLearning,[R] Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,"
Abstract:
Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [0,1]  and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [-1,1]. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.

https://arxiv.org/abs/2411.12537",iltruma,1gy0hbh,https://reddit.com/r/MachineLearning/comments/1gy0hbh/r_unlocking_statetracking_in_linear_rnns_through/,https://arxiv.org/abs/2411.12537,2024-11-23 14:11:53,90,0.95,90,0,4,0,0,False,False,False,False,False,Research,default,t3_1gy0hbh
MachineLearning,"[D] This is my first blog on medium, and they are about, How Modern Binary Hopfield Networks are just Hamming Distance Auto completers in disguise",,StoneSteel_1,1gy4qpv,https://reddit.com/r/MachineLearning/comments/1gy4qpv/d_this_is_my_first_blog_on_medium_and_they_are/,https://medium.com/@kanishq.vijay/modern-hopfield-networks-are-just-fancy-hamming-distance-calculators-and-i-can-prove-it-e9f538ee908e,2024-11-23 17:22:10,27,0.8,27,0,0,0,0,False,False,False,False,False,Discussion,https://a.thumbs.redditmedia.com/9FY02BNeYiTA4V1t16gIVBAkQJrL_tL5YxXhNMuurn0.jpg,t3_1gy4qpv
MachineLearning,[D] where find a good benchmark for all consumer gpu,"hi guys, I was wondering what gpu is the best in general for machine learning, include with openVino(only for intel), with new introduction of rochm and obviusly the queen nvidia, exist some benchmark full focuss on ML with various type of library",FewVEVOkuruta,1gyvari,https://reddit.com/r/MachineLearning/comments/1gyvari/d_where_find_a_good_benchmark_for_all_consumer_gpu/,https://www.reddit.com/r/MachineLearning/comments/1gyvari/d_where_find_a_good_benchmark_for_all_consumer_gpu/,2024-11-24 16:45:29,0,0.33,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gyvari
MachineLearning,[D] ACL Rolling Review October 2024,Discussion thread for ACL 2024 (ARR Oct) reviews.,AffectionateTip521,1gy8ekt,https://reddit.com/r/MachineLearning/comments/1gy8ekt/d_acl_rolling_review_october_2024/,https://www.reddit.com/r/MachineLearning/comments/1gy8ekt/d_acl_rolling_review_october_2024/,2024-11-23 20:01:19,12,1.0,12,0,32,0,0,False,False,True,False,False,Discussion,self,t3_1gy8ekt
MachineLearning,"[D] Accepted NeurIPS 2024 paper claimed to be solving a novel problem as first work, but ignores 5 prior works","At NeurIPS 2024 I found a paper that got accepted that positions its main contribution in the form of “Existing algorithms for X ignore Y. We adapt algorithm Z for X to account for Y”.

On OpenReview I see that the reviewers in particular praised the novelty of the work, and recognised Y as an important aspect that had been ignored in the field of X.

Now the interesting bit: co-authors and I published a paper in Springer’s Machine Learning journal in 2023 that also proposes an algorithm for X that account for Y. We were also not the first to study the problem setting of X with Y: our paper’s related work section discusses 4 papers that have all proposed algorithms for X that account for Y. One is even from NeurIPS (2017), and the oldest one dates back to 2012 (an AAAI paper).

The authors of this 2024 NeurIPS paper completely missed all this prior literature and believed they were the first, and so did all the reviewers.

This week I e-mailed the authors of this NeurIPS 2024 paper and they acknowledged that these works (mine + the 4 others) indeed were all working on the same problem setting, mentioned that they were unaware of all these works, and acknowledged that they can no longer claim novelty of the problem setting.

NeurIPS allows updating the camera ready paper after the conference, and the authors promised to use this opportunity to incorporate those related works and modify their contribution statements to no longer claim novelty of a first solution of X with Y.

At the one hand, it makes me happy that our work will get credited appropriately.

At the other hand I have my doubts about the ethics of severely modifying contribution statements post-review. The authors will no longer claim novelty, but the reviewers in particular praised this novelty, which makes me uncertain whether reviewers would have recommended acceptance had they known that this paper will ultimately no longer be able to claim the novelty that it claimed to have in the reviewed version.

Moreover this makes me wonder about the experimental section. Almost surely, reviewers would have demanded comparison to those 5 prior works as baselines. This paper did not compare against baselines, which will have seemed reasonable to a reviewer who reviewed this work under the assumption that the problem setting was completely novel and no prior methods exist that could function as a baseline.

Asking the group here about any thoughts on how such cases should get resolved:
- should the paper be retracted?
- should the area chair / program committee be informed? who may or may not take action
- should the paper just get updated by authors in the way that was promised, and that is it?
- something else?

I redacted X, Y and Z in order to not publicly shame the authors, as they have engaged with my e-mails and I am convinced that there is no foul play and they truly were unaware of those works.",TaXxER,1gxooqv,https://reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/,https://www.reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/,2024-11-23 01:58:58,272,0.94,272,0,65,0,0,False,False,True,False,False,Discussion,self,t3_1gxooqv
MachineLearning,[D] Recommendations needed: image-to-3D diffusion models,"Hey all, I'm evaluating different open source image-to-3D diffusion models for a project and could use some real-world insights. I've been digging through papers but would love to hear from people who've actually implemented these.

My main requirements:

1. Quality is the top priority - looking for clean, accurate reconstructions
2. Need mesh-based output (not point clouds or neurals fields) that isn't astronomically large
3. Inference time isn't critical - happy to wait up to a minute per generation

I've looked at Zero123, Wonder3D, and a few others but curious what's working well for people in practice. Especially interested in:

* Which models are actually maintainable in production
* Any gotchas with mesh generation quality
* Real-world inference times you're seeing
* How much post-processing is typically needed

Would really appreciate hearing about your experiences, especially from anyone who's deployed these in actual projects. Thanks!",ESCNOptimist,1gy9dqd,https://reddit.com/r/MachineLearning/comments/1gy9dqd/d_recommendations_needed_imageto3d_diffusion/,https://www.reddit.com/r/MachineLearning/comments/1gy9dqd/d_recommendations_needed_imageto3d_diffusion/,2024-11-23 20:44:56,8,0.91,8,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gy9dqd
MachineLearning,[R] Resource: Precision Knowledge Editing (PKE) for Reducing Toxicity in LLMs,"Sharing a project and paper for those interested in AI safety and improving LLMs. The method, called Precision Knowledge Editing (PKE), focuses on reducing toxic outputs in LLMs by identifying and modifying specific neurons or layers responsible for generating toxic content.

Key highlights:

\- Uses techniques like neuron weight tracking and activation pathway tracing to locate ""toxic hotspots.""

\- Applies a custom loss function to reduce toxicity while preserving model performance.

\- Tested on models like Llama2-7b and Llama-3-8B with significant improvements in toxicity management (e.g., lower Attack Success Rate).

The paper is available here: [https://arxiv.org/pdf/2410.03772](https://arxiv.org/pdf/2410.03772)

GitHub repo with a demo Jupyter Notebook: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models)

Might be useful for researchers, developers, or anyone exploring ways to improve LLM safety. Would be interesting to hear what others think of the approach.",Reagane371,1gxyhwb,https://reddit.com/r/MachineLearning/comments/1gxyhwb/r_resource_precision_knowledge_editing_pke_for/,https://www.reddit.com/r/MachineLearning/comments/1gxyhwb/r_resource_precision_knowledge_editing_pke_for/,2024-11-23 12:25:10,28,0.9,28,0,11,0,0,False,False,True,False,False,Research,self,t3_1gxyhwb
MachineLearning,[P] Creating Custom Music Genres Using Unsupervised Learning ,"so i had this random thought to create new music genres/spotify daylists using unsupervised learning. my idea is more towards creating a custom genre but not something necessarily as hyper-personalized as daylists. this is very much just an idea for now, will be developing into it soon tho. so the idea is in two phases:

1. take music data with audio features/embeddings/mfccs/create own features and use unsupervised learning to create clusters of those using something like knns
2. take out the audio features of the centre of the clusters and feed that to an llm to generate a custom phrase/name for that particular cluster. this can be something customized like character names for a play/use data like what time frame particular clusters of songs were played more to create something a lil more personalized like daylists/anything for that matter. haven't given much thought into this part for now.

i found a lot of papers/articles for the former phase but couldn't find much for the latter as of now. i am reading more into how spotify makes their daylists to see if anything strikes of interest.

i would live to have suggestions on how this can be improved/ recommendations for research papers/articles on anything relevant to this.

note: i know this is not very well framed and is messy but tbf i am drunk at 2 am and suddenly struck with my long lost passion for musicso please help a girl out (⁠´⁠ ⁠.⁠ ⁠.̫⁠ ⁠.⁠ ⁠`⁠)",Personal_Equal7989,1gyb62h,https://reddit.com/r/MachineLearning/comments/1gyb62h/p_creating_custom_music_genres_using_unsupervised/,https://www.reddit.com/r/MachineLearning/comments/1gyb62h/p_creating_custom_music_genres_using_unsupervised/,2024-11-23 22:05:00,4,0.83,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1gyb62h
MachineLearning,[R] Iterative Narrowing: A Visual Prompting Framework for Enhanced GUI Location Grounding,"This paper introduces an iterative narrowing approach for GUI element grounding that processes visual and textual information in multiple refinement steps rather than a single pass. The key insight is breaking down element identification into coarse-to-fine stages that mirror how humans visually search interfaces.

Key technical points:
* **Two-stage architecture**: Initial region proposal network followed by focused refinement
*  Visual and text encoders process features in parallel before cross-attention alignment
* Progressive narrowing through multiple passes reduces false positives
* Handles nested GUI elements through hierarchical representation
* Trained on a dataset of 77K GUI screenshots with natural language queries

Results show:
* 15% improvement in grounding accuracy vs single-pass baseline
* Better handling of ambiguous queries
* Reduced computational overhead compared to exhaustive search
* Strong performance on complex nested interfaces
* Effective transfer to unseen GUI layouts

I think this approach could meaningfully improve accessibility tools and GUI automation by making element identification more robust. The iterative refinement mirrors human visual search patterns, which could lead to more natural interaction with interfaces.

I think the main limitation is handling highly dynamic interfaces, where elements move or change frequently. The multi-pass nature also introduces some latency that would need optimization for real-time applications.

TLDR: New GUI grounding method uses multiple refinement passes to identify interface elements more accurately, achieving 15% better accuracy through an approach that mimics human visual search patterns.

[Full summary is here](https://aimodels.fyi/papers/arxiv/improved-gui-grounding-via-iterative-narrowing). Paper [here](https://arxiv.org/abs/2411.13591).",Successful-Western27,1gy8tgs,https://reddit.com/r/MachineLearning/comments/1gy8tgs/r_iterative_narrowing_a_visual_prompting/,https://www.reddit.com/r/MachineLearning/comments/1gy8tgs/r_iterative_narrowing_a_visual_prompting/,2024-11-23 20:19:49,5,0.86,5,0,0,0,0,False,False,True,False,False,Research,self,t3_1gy8tgs
MachineLearning,[D] [Project] JAX ML Framework; Write neural networks and more; shorter and faster; What are your thoughts?,"Made a JAX framework for machine learning because I wanted to code faster &amp; shorter so I made zephyr. I hope it might be helpful to you guys too and wanted to hear some feedback.

Link in the comments.

Nothing wrong with current frameworks, this is just another way of doing things. 

NNs or ML algorithms to me, are just pure/mathematical functions and so I wanted that to reflect in my code. With other frameworks it comes in at least 2 steps: initialization in the constructor and a computation in the forward/call body. This seems fine at first but when models become larger, it's 2 places where I have to synchronize code. - If I change a computation, I might need to change a hyperparameter somewhere, or if I change a hyperparameter, I might need to change a computation - or if i have to re-read my code, i have to read in at least 2 places. I usually use a small window for an editor and so jumping between these could a hassle (putting them side by side is another solution).

Another thing I was experiencing was that if I was doing something that is not neural networks, for example if an algorithm was easier to do with a recursive call (but with different trainable weights for each call), that would be challenging in other frameworks. So while they generic computational graph-frameworks, some computations are hard to do. 

To me, computations was about passing data around and getting them to transform, so this \`act\` of transforming data should be that focus of the framework. That's what I did with zephyr. Mathematical functions are python functions, no need for initialization in a constructor. You use the functions(networks or layers, etc) when you need them. No need for constructors, allows recursions, allows you to focus on the transformations or operations. Zephyr handles weight creation and management for you - it is explicit tho unlike other frameworks; you carry around a \`params\` tree, and that should be no problem, since that's a core of the computation and shouldn't be hidden away.

In short, zephyr is  short but readable aimed at people developing research ideas about ML. The README has a few samples for neural networks. I hope you guys like it and try it.

",Pristine-Staff-5250,1gxqag6,https://reddit.com/r/MachineLearning/comments/1gxqag6/d_project_jax_ml_framework_write_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1gxqag6/d_project_jax_ml_framework_write_neural_networks/,2024-11-23 03:25:31,33,0.84,33,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gxqag6
MachineLearning,[R] Llama 3.2 Interpretability with Sparse Autoencoders,,RandomHexCode,1gxzsbp,https://reddit.com/r/MachineLearning/comments/1gxzsbp/r_llama_32_interpretability_with_sparse/,https://github.com/PaulPauls/llama3_interpretability_sae,2024-11-23 13:37:14,3,1.0,3,0,0,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/geyS4I_5fqQ5-NrpJOf-awV-cU5tO-EH7u-ZGL5X41Y.jpg,t3_1gxzsbp
MachineLearning,[R] Entropy-Guided Critical Neuron Pruning for Efficient Spiking Neural Networks,"This paper introduces a pruning method for Spiking Neural Networks (SNNs) based on neuroscience principles of criticality. The key insight is using neuronal avalanche analysis to identify neurons that have the most significant impact on network dynamics, similar to how critical neurons function in biological brains.

Key technical points:
* Monitors spike propagation patterns to identify critical neurons
* Introduces adaptive pruning schedule based on network stability metrics
* Achieves 90% compression while maintaining accuracy on MNIST/CIFAR-10
* Works across different SNN architectures (feed-forward, CNN)
* Uses stability measures to prevent catastrophic forgetting during pruning

Main results:
* Outperforms existing pruning methods on accuracy retention
* Shows better energy efficiency compared to unpruned networks
* Maintains temporal dynamics important for SNN operation
* Demonstrates scalability across different network sizes
* Validates biological inspiration through avalanche analysis

I think this approach could be particularly important for deploying SNNs in resource-constrained environments like edge devices. The adaptive pruning schedule seems especially promising since it automatically adjusts based on network behavior rather than requiring manual tuning.

I think there are some open questions about computational overhead of the avalanche analysis that need to be addressed for very large networks. However, the biological principles behind the method suggest it could generalize well to other architectures and tasks.

TLDR: Novel pruning method for SNNs based on neuroscience principles of criticality. Uses neuronal avalanche analysis to identify important neurons and achieves 90% compression while maintaining accuracy. Introduces adaptive pruning schedule that adjusts based on network stability.

[Full summary is here](https://aimodels.fyi/papers/arxiv/brain-inspired-efficient-pruning-exploiting-criticality-spiking). Paper [here](https://arxiv.org/abs/2311.16141).",Successful-Western27,1gx86i0,https://reddit.com/r/MachineLearning/comments/1gx86i0/r_entropyguided_critical_neuron_pruning_for/,https://www.reddit.com/r/MachineLearning/comments/1gx86i0/r_entropyguided_critical_neuron_pruning_for/,2024-11-22 13:45:14,46,0.88,46,0,5,0,0,False,False,True,False,False,Research,self,t3_1gx86i0
MachineLearning,[D] Optimization Algorithm that focuses on solving model parameters ,"i tried googling for any sources but does anyone have info on where i can start looking for optimization algorithms that focus on optimizing model parameters by solving for a specific parameter (or parameters), given the input and target for a sample? Or the name for this kind of optimization algorithm? E.g. solve for model parameters a,b,c for the function y = ax\^2 + bx + c , x and y being the input and target respectively. Surely this algorithm has a name in the context of ml.

EDIT:

I believe what im asking is a bit ambiguous. As opposed to gradient descent, which focuses on finding the derivative of model parameters to the loss provided, the optimization algorithm i specified above focuses on a number of parameters and somehow figures out (or solves the values of these parameters) to approximately match the output. Like the same way you have 5x = 10 and solve for x, so the algortithm figures that x=2. For more data samples and more parameters you have 5x + c = 12 and 2x + c = 6, x and c being model parameters and 10 ad 4 being the desired output. The algorithm figures x = 2 and c = 2 somehow. Its a bit of a stretch but even im starting to doubt my sanity enough to believe that what im asking is basically all if not most of what ml optimization algorithms do.",Relevant-Twist520,1gy6jy6,https://reddit.com/r/MachineLearning/comments/1gy6jy6/d_optimization_algorithm_that_focuses_on_solving/,https://www.reddit.com/r/MachineLearning/comments/1gy6jy6/d_optimization_algorithm_that_focuses_on_solving/,2024-11-23 18:40:43,0,0.39,0,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gy6jy6
MachineLearning,"[D] We’ve crowd-sourced,  open-sourced, and made it easy to find so many tools to build with, but where is all this effort for context/scraping?","We have so many repos and libraries available to us for building, deploying, and using LLMs for tasks. We have hubs for models, plug-in-play libraries for things like LoRA and RAG, containerization for deploying models with APIs, extensions to integrate LLMs into IDEs and workflows, and plenty more. There’s stuff for managing and  orchestrating agents. 

Suffice to say, we have tons to open source tools to work to start working on both niche and general uses for LLMs.

That’s all great, but what I’m always having to build from scratch is getting context. Be that tools for online searches, webpage parsing (even common webpages that I know people would love to be easier to use for context), document parsing, etc.

I’ve been seen more cool projects pop up, but I’ve been seeing those projects provide details or implementation less and less on how they are finding, accessing, retrieving, and processing context.

There are plenty libraries to build tools for this purpose, but I just see less and less people sharing those.

Now I understand the context different projects need can be pretty niche, so reusability could be sparse. 

But is my perception wrong? Are there open-source resources for finding existing context extraction/scraping implementations or places to submit your own to make it easier for others to find?",Oscilla,1gxbrdm,https://reddit.com/r/MachineLearning/comments/1gxbrdm/d_weve_crowdsourced_opensourced_and_made_it_easy/,https://www.reddit.com/r/MachineLearning/comments/1gxbrdm/d_weve_crowdsourced_opensourced_and_made_it_easy/,2024-11-22 16:26:26,14,0.78,14,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gxbrdm
MachineLearning,[D] Historical archive of generative AI media output?,"Is there an archive or research paper that shows examples of the progress in generative AI media output over time?

I want to gather examples of multimedia outputs (text, images, video, sound) generated over time to help evaluate how the field has progressed in each area over time. 

Of course I can grab whatever results from different sources by searching, but I'm wondering if there is a more organized and consistent repository for this?",searchresults,1gxh7dm,https://reddit.com/r/MachineLearning/comments/1gxh7dm/d_historical_archive_of_generative_ai_media_output/,https://www.reddit.com/r/MachineLearning/comments/1gxh7dm/d_historical_archive_of_generative_ai_media_output/,2024-11-22 20:15:06,6,0.8,6,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gxh7dm
MachineLearning,[D] How to make more reliable reports using AI — A Technical Guide,,phicreative1997,1gy1t45,https://reddit.com/r/MachineLearning/comments/1gy1t45/d_how_to_make_more_reliable_reports_using_ai_a/,https://medium.com/firebird-technologies/how-to-make-more-reliable-reports-using-ai-a-technical-guide-672b2d01cb2a,2024-11-23 15:14:13,0,0.29,0,0,0,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/-AZkwBJl93JyKODja6Nv557NXqR_dwFniH8tEBkkuoQ.jpg,t3_1gy1t45
MachineLearning,[P] Registrations are open for the 2025 EY Open Science AI &amp; Data Challenge,"Use AI for good and help create more vital, sustainable communities when you join the [2025 EY Open Science AI &amp; Data Challenge](https://challenge.ey.com/2025). A phenomenon known as the urban heat island effect is becoming a significant issue as our cities continue to grow and develop. Dense development and lack of green space create heat islands that take a toll on our health and increase our energy use. But your skills and vision can help. [Register now.](https://challenge.ey.com/2025) \#EY #BetterWorkingWorld #AI #OpenScience #ShapeTheFutureWithConfidence",fofxy,1gx9htl,https://reddit.com/r/MachineLearning/comments/1gx9htl/p_registrations_are_open_for_the_2025_ey_open/,https://www.reddit.com/r/MachineLearning/comments/1gx9htl/p_registrations_are_open_for_the_2025_ey_open/,2024-11-22 14:48:28,11,0.74,11,0,1,0,0,False,False,True,False,False,Project,self,t3_1gx9htl
MachineLearning,[R] Say What You Mean: A Response to 'Let Me Speak Freely',"Will here from .txt, the team behind [Outlines](https://github.com/dottxt-ai/outlines) an open source library that enables open LLMs to perform structured generation, ensuring their outputs always adhere to a predefined format.

We are passionate about structured generation, and truly believe it has the potential to transform the work being done with LLMs in profound ways. 

However a recent paper, [Let Me Speak Freely](https://arxiv.org/abs/2408.02442) was published reporting some misinformation around the performance of structured generation on a series of evaluations. 

We've recently publish a rebuttal to this paper on our blog: [Say What You Mean: A Response to 'Let Me Speak Freely'](https://blog.dottxt.co/say-what-you-mean.html) and thought the community here might find it interesting. It covers not only issues with the original paper, but also dives into the nature of structured generation and how to get the most out of your models with prompting for structured generation.",CountBayesie,1gwswn7,https://reddit.com/r/MachineLearning/comments/1gwswn7/r_say_what_you_mean_a_response_to_let_me_speak/,https://www.reddit.com/r/MachineLearning/comments/1gwswn7/r_say_what_you_mean_a_response_to_let_me_speak/,2024-11-21 22:57:45,89,0.9,89,0,24,0,0,False,False,True,False,False,Research,self,t3_1gwswn7
MachineLearning,[D] Anyone else work in Infrastructure/MLOps?,"I've been working as an AI Architect at a small/medium company (\~300 people). I've been having more success focusing on the deployment of models using KServe and Kubeflow rather than pure ML. The trend that I'm seeing is that there's so many open-source models that custom ML solutions are less relevant, so it's becoming more valuable to simply deploy open-source models that keep getting massive improvements, at least in the space that I'm in.   
  
Curious who else may be working on the Infrastructure/MLOps side of things. I've found development to sometimes be pretty isolating, since I'm the only one in this company focusing on this and haven't found a community outside of this. It would be nice to hear stories from others who are also working in this area. Cheers!",Milwookie123,1gxbh9b,https://reddit.com/r/MachineLearning/comments/1gxbh9b/d_anyone_else_work_in_infrastructuremlops/,https://www.reddit.com/r/MachineLearning/comments/1gxbh9b/d_anyone_else_work_in_infrastructuremlops/,2024-11-22 16:14:51,2,0.76,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gxbh9b
MachineLearning,[R]Geometric aperiodic fractal organization in Semantic Space : A Novel Finding About How Meaning Organizes Itself ,"Hey friends! I'm sharing this here because I think it warrants some attention, and I'm using methods that intersect from different domains, with Machine Learning being one of them.

Recently I read Tegmark &amp; co.'s paper on Geometric Concepts [https://arxiv.org/abs/2410.19750](https://arxiv.org/abs/2410.19750) and thought that it was fascinating that they were finding these geometric relationships in llms and wanted to tinker with their process a little bit, but I didn't really have access or expertise to delve into LLM innards, so I thought I might be able to find something by mapping its output responses with embedding models to see if I can locate any geometric unity underlying how llms organize their semantic patterns. Well I did find that and more...

I've made what I believe is a significant discovery about how meaning organizes itself geometrically in semantic space, and I'd like to share it with you and invite collaboration.

**The Initial Discovery**

While experimenting with different dimensionality reduction techniques (PCA, UMAP, t-SNE, and Isomap) to visualize semantic embeddings, I noticed something beautiful and striking; a consistent ""flower-like"" pattern emerging across all methods and combinations thereof. I systematically weeded out the possibility that this was the behavior of any single model(either embedding or dimensional reduction model) or combination of models and what I've found is kind of wild to say the least. It turns out that this wasn't just a visualization artifact, as it appeared regardless of:

\- The reduction method used

\- The embedding model employed

\- The input text analyzed

https://preview.redd.it/pdyq50s1ob2e1.png?width=907&amp;format=png&amp;auto=webp&amp;s=b9ecf9206c1c2b43881341e8ad51950cf73b345c

https://preview.redd.it/b2u3uz93ob2e1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=6448776ebaeb5620b2079c7fed6992b3a813d619

[cross-section of the convergence point\(Organic\) hulls](https://preview.redd.it/t59tzz2qob2e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=a9a0cd3132191db5a2ea163c87e8dfe336f9320c)

[a step further, showing how they form with self similarity.](https://preview.redd.it/q0pmaveqob2e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=863dd23a1899efc8bf266c0702cf3258643859c3)

**Verification Through Multiple Methods**

To verify this isn't just coincidental, I conducted several analyses, rewrote the program and math 4 times and did the following:

1. Pairwise Similarity Matrices

Mapping the embeddings to similarity matrices reveals consistent patterns:

\- A perfect diagonal line (self-similarity = 1.0)

\- Regular cross-patterns at 45° angles

\- Repeating geometric structures

https://preview.redd.it/ft89ukpaob2e1.png?width=460&amp;format=png&amp;auto=webp&amp;s=9900f9113fad02841e5e18cb0bc5f9b6b66275e1

https://preview.redd.it/f2yzbvnbob2e1.png?width=433&amp;format=png&amp;auto=webp&amp;s=4a13a8e910794c64375ab0628f6f34006c31fb2f

Relevant Code:  
python

def analyze\_similarity\_structure(embeddings):

similarity\_matrix = cosine\_similarity(embeddings)

eigenvalues = np.linalg.eigvals(similarity\_matrix)

sorted\_eigenvalues = sorted(eigenvalues, reverse=True)

return similarity\_matrix, sorted\_eigenvalues

2. Eigenvalue Analysis

The eigenvalue progression as more text is added, regardless of content or languages shows remarkable consistency like the following sample:

First Set of eigenvalues while analyzing The Red Book by C.G. Jung in pieces:  
\[35.39, 7.84, 6.71\]

Later Sets:  
\[442.29, 162.38, 82.82\]

\[533.16, 168.78, 95.53\]

\[593.31, 172.75, 104.20\]

\[619.62, 175.65, 109.41\]

https://preview.redd.it/hesf440job2e1.png?width=1088&amp;format=png&amp;auto=webp&amp;s=b531499fe8043e0b41390229bd0b04017373c49b

Key findings:

\- The top 3 eigenvalues consistently account for most of the variance

\- Clear logarithmic growth pattern

\- Stable spectral gaps i.e: (35.79393)

3. Organic Hull Visualization

The geometric structure becomes particularly visible when visualizing through organic hulls:

Code for generating data visualization through sinusoidal sphere deformations:  
python

def generate\_organic\_hull(points, method='pca'):

phi = np.linspace(0, 2\*np.pi, 30)

theta = np.linspace(-np.pi/2, np.pi/2, 30)

phi, theta = np.meshgrid(phi, theta)

center = np.mean(points, axis=0)

spread = np.std(points, axis=0)

x = center\[0\] + spread\[0\] \* np.cos(theta) \* np.cos(phi)

y = center\[1\] + spread\[1\] \* np.cos(theta) \* np.sin(phi)

z = center\[2\] + spread\[2\] \* np.sin(theta)

return x, y, z

\`\`\`

What the this discovery suggests is that meaning in semantic space has inherent geometric structure that organizes itself along predictable patterns and shows consistent mathematical self-similar relationships that exhibit golden ratio behavior like a penrose tiling, hyperbolic coxeter honeycomb etc and these patterns persist across combinations of different models and methods. I've run into an inverse of the problem that you have when you want to discover something; instead of finding a needle in a haystack, I'm trying to find a single piece of hay in a stack of needles, in the sense that nothing I do prevents these geometric unity from being present in the semantic space of all texts. The more text I throw at it, the more defined the geometry becomes.

https://preview.redd.it/3hho1avzob2e1.png?width=1239&amp;format=png&amp;auto=webp&amp;s=a446d6b71ba0166c842e9537c6cd228662bb2682

I think I've done what I can so far on my own as far as cross-referencing results across multiple methods and collecting significant raw data that reinforces itself with each attempt to disprove it.

So I'm making a call for collaboration:

I'm looking for collaborators interested in:

1. Independently verifying these patterns
2. Exploring the mathematical implications
3. Investigating potential applications
4. Understanding the theoretical foundations

My complete codebase is available upon request, including:

\- Visualization tools

\- Analysis methods

\- Data processing pipeline

\- Metrics collection

If you're interested in collaborating or would like to verify these findings independently, please reach out. This could have significant implications for our understanding of how meaning organizes itself and potentially for improving language models, cognitive science, data science and more.

\*TL;DR: Discovered consistent geometric patterns in semantic space across multiple reduction methods and embedding models, verified through similarity matrices and eigenvalue analysis. Looking for interested collaborators to explore this further and/or independently verify.

\##EDIT##: I

I need to add some more context I guess,  because it seems that I'm being painted as a quack or a liar without being given the benefit of the doubt. Such is the nature of social media though I guess.

This is a cross-method, cross-model discovery using semantic embeddings that retain human interpretable relationships. i.e. for the similarity matrix visualizations, you can map the sentences to the eigenvalues and read them yourself. Theres nothing spooky going on here, its plain for your eyes and brain to see.

Here are some other researchers who are like-minded and do it for a living.

(Athanasopoulou et al.) supports our findings:

""The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it is organized in such a way that interesting semantic relations can be exported from manifolds of much lower dimensionality embedded in this high dimensional space."" [https://aclanthology.org/C14-1069.pdf](https://aclanthology.org/C14-1069.pdf)

A neuroscience paper(Alexander G. Huth 2013) reinforces my findings about geometric organization:""An efficient way for the brain to represent object and action categories would be to organize them into a continuous space that reflects the semantic similarity between categories.""  
[https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/)

""We use a novel eigenvector analysis method inspired from Random Matrix Theory and show that semantically coherent groups not only form in the row space, but also the column space.""  
[https://openreview.net/pdf?id=rJfJiR5ooX](https://openreview.net/pdf?id=rJfJiR5ooX)

I'm getting some hate here, but its unwarranted and comes from a lack of understanding. The automatic kneejerk reaction to completely shut someone down is not constructive criticism, its entirely unhelpful and unscientific in its closed-mindedness.",Own_Dog9066,1gwqvt2,https://reddit.com/r/MachineLearning/comments/1gwqvt2/rgeometric_aperiodic_fractal_organization_in/,https://www.reddit.com/r/MachineLearning/comments/1gwqvt2/rgeometric_aperiodic_fractal_organization_in/,2024-11-21 21:29:58,59,0.75,59,0,63,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/FrCUFqNtP_G0uh0Hibb8d0zw6XWHs9AUdUXetjPd1zM.jpg,t3_1gwqvt2
MachineLearning,[P] Machine learning project on chem,"It is called SMILES

[Simplified Molecular Input Line Entry System - Wikipedia](https://en.wikipedia.org/wiki/Simplified_Molecular_Input_Line_Entry_System)

I am not sure if I can train a model so that it can interpret the correct structure as well as naming after feeding labelled dataset?",uartimcs,1gx4ezg,https://reddit.com/r/MachineLearning/comments/1gx4ezg/p_machine_learning_project_on_chem/,https://www.reddit.com/r/MachineLearning/comments/1gx4ezg/p_machine_learning_project_on_chem/,2024-11-22 09:59:23,2,0.6,2,0,7,0,0,False,False,True,False,False,Project,self,t3_1gx4ezg
MachineLearning,[D] Do I need to connect more?,"I am currently finishing up my third year PhD and would most probably be graduating in another year.
So far I been pretty much working on my own. 

As a matter of fact, all my publications are first-author and most of the contributions to these papers are from PI or Co-PI who does very surface-level checks (grammars etc). I basically did not get involved with any work with other people and have been working solely on my own for these 3 years. Thus I don’t have much connections outside my lab group ( I actually don’t even know them well at all ). I see a lot of publications have quite a handful of authors, and most comprising of multiple organisations.

The thing is I was hoping to get an industrial job after graduation, would me being relatively unknown/unheard actually be a problem? 

I find it hard to find connections outside my lab. Most of my labmates don’t work in similar research areas. How do you guys actually connect or find collaborators outside of your lab zone? 

Would an overseas attachment help me? 
This is also one of the main reason why I am trying to find an internship (still very competitive), which is to collaborate or to be mentored by experienced people.
",AmbitiousSeesaw3330,1gwn0rn,https://reddit.com/r/MachineLearning/comments/1gwn0rn/d_do_i_need_to_connect_more/,https://www.reddit.com/r/MachineLearning/comments/1gwn0rn/d_do_i_need_to_connect_more/,2024-11-21 18:51:12,32,0.9,32,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gwn0rn
MachineLearning,[P] Python Windows Screenshot Analyzer,"I want to build a python project to analyse windows screehots. Suppose an app is open then the screenshot should tell everything going on in the app. For example in the Microsoft Teams Who are the participants, ongoing duration etc. What all apps are open in the taskbar what's the time in the screenshot etc. How can I achieve it I want to use open source resources only.",Ok-Bar5416,1gxh51z,https://reddit.com/r/MachineLearning/comments/1gxh51z/p_python_windows_screenshot_analyzer/,https://www.reddit.com/r/MachineLearning/comments/1gxh51z/p_python_windows_screenshot_analyzer/,2024-11-22 20:12:23,0,0.3,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1gxh51z
MachineLearning,[R] BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games,"Tired of saturated benchmarks? Want scope for a significant leap in capabilities? 

Introducing BALROG: a Benchmark for Agentic LLM and VLM Reasoning On Games!

BALROG is a challenging benchmark for LLM agentic capabilities, designed to stay relevant for years to come.

  
Check it out!

GitHub: [https://github.com/balrog-ai/BALROG](https://github.com/balrog-ai/BALROG)

Leaderboard: [https://balrogai.com](https://balrogai.com)

Paper: [https://arxiv.org/abs/2411.13543](https://arxiv.org/abs/2411.13543)",pagggga,1gwhnf8,https://reddit.com/r/MachineLearning/comments/1gwhnf8/r_balrog_benchmarking_agentic_llm_and_vlm/,https://www.reddit.com/r/MachineLearning/comments/1gwhnf8/r_balrog_benchmarking_agentic_llm_and_vlm/,2024-11-21 14:33:20,49,0.95,49,0,6,0,0,False,False,True,False,False,Research,self,t3_1gwhnf8
MachineLearning,[D] Next big thing in Time series?,"In NLP, we’ve seen major milestones like transformers, GPT, and LLMs, which have revolutionized the field. Time series research seems to be borrowing a lot from NLP and CV—like transformer-based models, self-supervised learning, and now even foundation models specifically for time series. But there doesn’t seem to be a clear consensus yet on what works best. For example, NLP has well-accepted pretraining strategies like masked language modeling or next-token prediction, but nothing similar has become a standard for time series.  
  
Lately, there’s been a lot of talk about adapting LLMs for time series or even building foundation models specifically for the purpose. On the other hand, some research indicates that LLMs are not helpful for time series. 

So I just wanna know what can be a game changer for time series!",Few-Pomegranate4369,1gwbhxq,https://reddit.com/r/MachineLearning/comments/1gwbhxq/d_next_big_thing_in_time_series/,https://www.reddit.com/r/MachineLearning/comments/1gwbhxq/d_next_big_thing_in_time_series/,2024-11-21 08:20:38,116,0.96,116,0,55,0,0,False,False,True,False,False,Discussion,self,t3_1gwbhxq
MachineLearning,[D] Noise injection,"Can anyone give me some recommendations, about paper that identify the relationship between accumulated output noise over time (e.g. something like time series model with the noise injection to the input)? ",Careless-Top-2411,1gwy1m7,https://reddit.com/r/MachineLearning/comments/1gwy1m7/d_noise_injection/,https://www.reddit.com/r/MachineLearning/comments/1gwy1m7/d_noise_injection/,2024-11-22 03:05:09,2,0.75,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwy1m7
MachineLearning,[D] Struggling to Transition to PhD,"**“Undergrad is about answering questions, while a PhD is about finding one.”** —Someone

I'm a first-year CS PhD student, but I feel stuck in the mindset of an undergrad. I excel at solving problems, as shown by my perfect GPA. However, when it comes to research, I struggle. If I enter a new area, **I typically read a lot of papers, take notes, and end up capable of writing a decent survey—but I rarely generate fresh ideas.**

Talking to other PhD students only adds to my frustration; one of them claims they can even come up with LLM ideas during a Latin class. My advisor says research is more about perseverance than talent, but I feel like I’m in a loop: I dive into a new field, produce a survey, and get stuck there.

I’m confident in my intelligence, but I’m questioning whether my workflow is flawed (e.g., maybe I should start experimenting earlier?) or if I’m just not cut out for research. Coming up with marginal improvements or applying A to B feels uninspiring, and I struggle to invest time in such ideas.

How do you CS (ML) PhD students come up with meaningful research ideas? Any advice on breaking out of this cycle?",StraightSpeech9295,1gw61tk,https://reddit.com/r/MachineLearning/comments/1gw61tk/d_struggling_to_transition_to_phd/,https://www.reddit.com/r/MachineLearning/comments/1gw61tk/d_struggling_to_transition_to_phd/,2024-11-21 02:50:51,146,0.94,146,0,56,0,0,False,False,True,False,False,Discussion,self,t3_1gw61tk
MachineLearning,[P] Comparing Machine Unlearning Techniques and PKE (Precision Knowledge Editing),"PKE focuses on enhancing the model's knowledge and positive output rather just identifying neuron activiations. While DINM is a great method for neural suppression and to aid real time modification, I really wanted to build on top of that and created PKE (Precision Knowledge Editing)which emphasizes neural reinforcement and enhancing the model's knowledge and positive output rather than just identifying neuron activiations.There's lots of current Machine unlearning techniques that can make LLMs safer right now like:

1. **Exact Unlearning:** This method involves retraining the model from scratch after removing the undesired data. While it ensures complete removal of the data's influence, it is computationally expensive and time-consuming, especially for large models.
2. **Approximate Unlearning:**
   1. **Fine-Tuning:** adjusting the model using the remaining data to mitigate the influence of the removed data. However, this may not completely eliminate the data's impact.
   2. **Gradient Ascent:** applying gradient ascent on the loss function concerning the data to be forgotten, effectively 'unlearning' it. This method can be unstable and may degrade model performance.

PKE is better for the following reasons:

1. **Fine-Grained Identification of Toxic Parameters:** PKE employs neuron weight tracking and activation pathway tracing to accurately pinpoint specific regions in the model responsible for generating toxic or harmful content. This precision allows for targeted interventions, reducing the risk of unintended alterations to the model's overall behavior.
2. **Maintaining Model Performance:** By focusing edits on identified toxic regions, PKE minimizes the impact on the model's general performance. This approach ensures that the model retains its capabilities across various tasks while effectively mitigating the generation of undesirable content.
3. **Scalability Across Different Model Architectures:** PKE has demonstrated effectiveness across various LLM architectures, including models like Llama2-7b and Llama-3-8b-instruct. This scalability makes it a versatile tool for enhancing safety in diverse AI systems.

Would love to hear your guys' thoughts on this project and how to continue to improve this methodology. If interested, here's the Github link: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models) and [paper](https://arxiv.org/pdf/2410.03772) .",lial4415,1gwwt6y,https://reddit.com/r/MachineLearning/comments/1gwwt6y/p_comparing_machine_unlearning_techniques_and_pke/,https://www.reddit.com/r/MachineLearning/comments/1gwwt6y/p_comparing_machine_unlearning_techniques_and_pke/,2024-11-22 02:02:44,2,1.0,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1gwwt6y
MachineLearning,[D] Research Topics in Conformal Prediction,"My background is in econometrics and soon I'll start to work in my master's thesis (already have a supervisor but would like to come up with some ideas that I could integrate in my research). One thing that recently got my attention were uncertainty quantification methods, specifically Conformal Prediction.

One thing that seems particularly cool is that it can be adapted to ensure coverage across specific groups in the covariates or even the labels. Additionally, 'recently', the research community was able to tackle the most limiting assumption, that of exchangeability, meaning it can be applied, for example, to time-series data.

My questions are two-fold (one out of curiosity and the other for personal interest):

1. What are some real-world scenarios that you've seen Conformal Prediction shine? And if there is some scenario that you'd think it would work but didn't.
2. And what do you think are some interesting questions yet to be addressed?

Any thoughts or general feedback very welcome! Thanks in advance!",HamsterExpress8688,1gwomft,https://reddit.com/r/MachineLearning/comments/1gwomft/d_research_topics_in_conformal_prediction/,https://www.reddit.com/r/MachineLearning/comments/1gwomft/d_research_topics_in_conformal_prediction/,2024-11-21 19:56:24,5,0.86,5,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gwomft
MachineLearning,"[D] Curious, how do you manage the full ML lifecycle ?","Hi guys! I’ve been pondering with a specific question/idea that I would like to pose as a discussion, it concerns the idea of more quickly going from idea to production with regards to ML/AI apps.

My experience in building ML apps and whilst talking to friends and colleagues has been something along the lines of you get data, that tends to be really crappy, so you spend about 80% of your time cleaning this, performing EDA, then some feature engineering including dimension reduction etc. All this mostly in notebooks using various packages depending on the goal. During this phase there are couple of tools that one tends to use to manage and version data e.g DVC etc

Thereafter one typically connects an experiment tracker such as MLFlow when conducting model building for various metric evaluations. Then once consensus has been reached on the optimal model, the Jupyter Notebook code usually has to be converted to pure python code and wrapped around some API or other means of serving the model. Then there is a whole operational component with various tools to ensure the model gets to production and amongst a couple of things it’s monitored for various data and model drift.

Now the ecosystem is full of tools for various stages of this lifecycle which is great but can prove challenging to **operationalize** and as we all know sometimes the results we get when adopting ML can be supar :(

I’ve been playing around with various platforms that have the ability for an end-to-end flow from cloud provider platforms such as AWS SageMaker, Vertex , Azure ML. Popular opensource frameworks like MetaFlow and even tried DagsHub. With the cloud providers it always feels like a jungle, clunky and sometimes overkill e.g maintenance. Furthermore when asking for platforms or tools that can really help one explore, test and investigate without too much setup it just feels lacking, as people tend to recommend tools that are great but only have one part of the puzzle. The best I have found so far is Lightning AI, although when it came to experiment tracking it was lacking.

So I’ve been playing with the idea of a truly out-of-the-box end-to-end platform, the idea is not to to re-invent the wheel but combine many of the good tools in an end-to-end flow powered by collaborative AI agents to help speed up the workflow across the ML lifecycle for faster prototyping and iterations. You can check out my current project over here [https://envole.ai](https://envole.ai)

This is still in the early stages so the are a couple of things to figure out, but would love to hear your feedback on the above hypothesis, how do you you solve this today ?",Lumiere-Celeste,1gwhdpl,https://reddit.com/r/MachineLearning/comments/1gwhdpl/d_curious_how_do_you_manage_the_full_ml_lifecycle/,https://www.reddit.com/r/MachineLearning/comments/1gwhdpl/d_curious_how_do_you_manage_the_full_ml_lifecycle/,2024-11-21 14:20:57,14,0.76,14,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gwhdpl
MachineLearning,[D]  Does anyone remember the machine learning in 2023 wrap-up meme video?,"Around this time last year someone posted a video of stitched-together memes about machine learning in 2023. I cannot remember all of the memes but there was definitely one about NLP professors needing to learn about RL, and one about Anthropic's appearance in front of some part of the US government.

Two questions.

1. Does anyone else remember this video and have a link? I cannot find it using Google because ""Machine learning in 2023"" is not a very discriminative search query.
2. Will there be a 2024 edition? I hope so!",votadini_,1gwh8yk,https://reddit.com/r/MachineLearning/comments/1gwh8yk/d_does_anyone_remember_the_machine_learning_in/,https://www.reddit.com/r/MachineLearning/comments/1gwh8yk/d_does_anyone_remember_the_machine_learning_in/,2024-11-21 14:14:51,13,0.81,13,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwh8yk
MachineLearning,[R] The Complexity Dynamics of Grokking,"# [https://openreview.net/pdf?id=07N9jCfIE4](https://openreview.net/pdf?id=07N9jCfIE4)

Though OpenReviewers [don't seem impressed](https://openreview.net/forum?id=07N9jCfIE4) by this paper, I found it very interesting.  I like the concepts / ideas involved: 

1. Regularization methods (e.g. norms) usually measure capacity, not complexity.
2. Attempt regularization closer to Kolmogorov complexity.
3. relate model complexity to generalization &amp; grokking.",marojejian,1gwju03,https://reddit.com/r/MachineLearning/comments/1gwju03/r_the_complexity_dynamics_of_grokking/,https://www.reddit.com/r/MachineLearning/comments/1gwju03/r_the_complexity_dynamics_of_grokking/,2024-11-21 16:48:46,5,0.73,5,0,9,0,0,False,False,True,False,False,Research,self,t3_1gwju03
MachineLearning,[D] Machine Learning as a DSL-design problem? Is this a thing? ,"I'll probably sound confusing or unclear, and that's because I don't even know what I want to ask about, but the general direction is: instead of learning numbers that combine/compute in a certain way is there a way to learn a DSL over a problem in guided/principled way? (I have no idea how to give an example, since I'm not even sure this is currently possible). This is different from an NLP-model learning to use a programming language. 

  
I do know a problem with this, and it's just that it's hard to evaluate an arbitrary program (halting problem). Neural nets are basically vector programs with a set amount compute steps. I have seen program search algorithms over a small crafted DSL, but I was thinking of, learning the DSL directly - but this is a chicken and egg problem since what learns the DSL ?",Pristine-Staff-5250,1gwwitr,https://reddit.com/r/MachineLearning/comments/1gwwitr/d_machine_learning_as_a_dsldesign_problem_is_this/,https://www.reddit.com/r/MachineLearning/comments/1gwwitr/d_machine_learning_as_a_dsldesign_problem_is_this/,2024-11-22 01:48:20,2,0.6,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gwwitr
MachineLearning,[Discussion] VQVAE Reconstruction Issue: Grayscale Output and Gradient Flow Insights,"I recently trained a VQVAE model on the COCO dataset, and it successfully converged, generating images with fine details. However, during inference, I noticed something unusual.

When I tried to reconstruct an image using only the quantized vector and passed it through the decoder, the generated output was a grayscale image that preserved the edges of the original.

As shown in the attached image, the second image is the reconstructed one. In this case, I included the codebook gradient flow term as follows:

    x = x + (x - z).detach()

With this modification, the generated image appears much cleaner.

I'm curious to know if this behavior is expected with VQVAE or if there might be an issue with my implementation. If anyone has experience working with VQVAE, your insights would be greatly appreciated!

https://preview.redd.it/g961r7f3ka2e1.png?width=1688&amp;format=png&amp;auto=webp&amp;s=94d94e51c76a508949b2dee7e4341b6ec079dab1

",Logical-Passenger471,1gwl7c9,https://reddit.com/r/MachineLearning/comments/1gwl7c9/discussion_vqvae_reconstruction_issue_grayscale/,https://www.reddit.com/r/MachineLearning/comments/1gwl7c9/discussion_vqvae_reconstruction_issue_grayscale/,2024-11-21 17:41:18,4,0.83,4,0,6,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/xa_-CXxF248GR7gjMuLQB5rjtDa81-Z37-Czne4FTJA.jpg,t3_1gwl7c9
MachineLearning,"[D] Train and Val Dice Score gets zero for a long time and then increases, while loss keeps on decreasing. Wondering why?",,__proximity__,1gwhf4o,https://reddit.com/r/MachineLearning/comments/1gwhf4o/d_train_and_val_dice_score_gets_zero_for_a_long/,https://www.reddit.com/gallery/1gvxoga,2024-11-21 14:22:45,8,0.72,8,0,8,0,0,False,False,False,False,False,Discussion,default,t3_1gwhf4o
MachineLearning,[R] Agentic AI test suites. All the test environments used to benchmark BALROG.,"# BabyAI

+ Purpose is to facilitate research on *grounded language learning.*  The current domain of BabyAI is a 2D gridworld in which synthetic natural-looking instructions (e.g. “put the red ball next to the box on your left”) require the agent to navigate the world including unlocking doors) and move objects to specified locations.  

https://openreview.net/forum?id=rJeXCo0cYX

----

# Crafter

+ Crafter features randomly generated 2D worlds where the player needs to forage for food and water, find shelter to sleep, defend against monsters, collect materials, and build tools.

https://github.com/danijar/crafter?tab=readme-ov-file


----

# TextWorld

+ Microsoft TextWorld is an open-source, extensible engine that both generates and simulates text games. You can use it to train reinforcement learning (RL) agents to learn skills such as language understanding and grounding, combined with sequential decision making.

https://www.microsoft.com/en-us/research/project/textworld/

https://github.com/microsoft/TextWorld

https://arxiv.org/pdf/1806.11532

----

# Baba is AI 

+ Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives.    We test three ***state-of-the-art multi-modal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically*** when generalization requires that the rules of the game must be manipulated and combined. 


https://github.com/nacloos/baba-is-ai

https://arxiv.org/abs/2407.13729

----

# MiniHack

+ MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforcement Learning (RL).   The motivation behind MiniHack is to be able to perform RL experiments in a controlled setting while being able to increasingly scale the complexity of the tasks.

https://github.com/facebookresearch/minihack

https://minihack.readthedocs.io/en/latest/


----

# NetHack

+ NetHack is an attractive research platform as it contains hundreds of enemy and object types, has complex and stochastic environment dynamics, and has a clearly defined goal (descend the dungeon, retrieve an amulet, and ascend) which can be achieved in a diverse set of ways. The game is considered one of the hardest in the world1, with winning episodes lasting 100,000s of steps, and a permadeath setting that starts agents at the beginning in a whole new world if they die in the dungeon. NetHack is even difficult to master for human players who often rely on external knowledge.


https://proceedings.neurips.cc/paper_files/paper/2023/file/764ba7236fb63743014fafbd87dd4f0e-Paper-Conference.pdf

https://github.com/upiterbarg/hihack

https://arxiv.org/pdf/2203.11889

https://www.youtube.com/watch?v=8L8LiQ-cIWA",moschles,1gwqe0m,https://reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/,https://www.reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/,2024-11-21 21:09:01,2,0.67,2,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwqe0m
MachineLearning,[D] GNNs with applications to science Labs?,"I'm graduating this december with a degree in data science (sort of equivalent to a B.sc.+ M.sc.). I'm looking to pursue a PhD in an área similar as to what I did my thesis on, which was an application of GNNs to biomedical data. 

My advisor is relatively new in this area (the lab is more of a traditional Systems/Network biology), so he doesnt have international contacts in this particular area. I'm waiting to see if I'm granted with a PhD scholarship where I live, but science funding in my country has been really cut down, so I'm looking for options overseas.

Any directions to good labs working on this stuff would be greatly appreciated",maximusdecimus__,1gwogns,https://reddit.com/r/MachineLearning/comments/1gwogns/d_gnns_with_applications_to_science_labs/,https://www.reddit.com/r/MachineLearning/comments/1gwogns/d_gnns_with_applications_to_science_labs/,2024-11-21 19:49:49,2,0.76,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gwogns
MachineLearning,How to efficiently generate text from RNNs and Transformers during inference [P],"Most of the notebooks I see do something like this to generate code

    text = ['start']
    for _ in range(num_to_gen):
      token = model(text)
      text.append(token)

But this clearly is inefficient when it would be better to pass in each token one at a time to the model as it's generated while preserving the hidden state. What is the cleanest / industry accepted way to do this with pytorch models?

I see a tutorial on pytorch that has the model return both the output and the hidden state, and then you pass the hidden state back into the model. This feels really clunky and for large hidden states, it's inefficient to keep passing the hidden state of every rnn layer out to the end of the model.

Like I'm trying to work with Mamba currently which according to it's paper tries it's best to not materialize the full hidden state in memory.  
The only other way I can think of to do this is have the model persist the hidden state through forward calls and maybe have a reset function. But I'm not sure if this is an accepted way to do things. Also, I don't think I see the hidden state in the state\_dict of the open source mamba model, so I feel like Mamba doesn't do this but not entirely sure. I tried reading the Mamba code but found it difficult to understand.

I would appreciate seeing what the industry standard is to do this properly and an example in some open source code that's explained. As a bonus, if anyone can help me understand how Mamba does it in the state-spaces/mamba repo on github that would be great, but maybe I'll just post a comment on the repo.

Also just curious how transformer models deal with this as well.",No_Effective734,1gwkrir,https://reddit.com/r/MachineLearning/comments/1gwkrir/how_to_efficiently_generate_text_from_rnns_and/,https://www.reddit.com/r/MachineLearning/comments/1gwkrir/how_to_efficiently_generate_text_from_rnns_and/,2024-11-21 17:24:44,3,0.8,3,0,2,0,0,False,False,True,False,False,Project,self,t3_1gwkrir
MachineLearning,[D] How much can we revise paper during rebuttal?,"I'm currently preparing for ICLR rebuttal, and revising paper is an option. I have fixed mostly typo, slightly change the notation of 1 or 2 variable a bit (index, transposed ,..). Can anyone give me some advices, can this result in negative impression to the reviewer/AC? Should we revise at all or not?",Competitive_Newt_100,1gwf6t6,https://reddit.com/r/MachineLearning/comments/1gwf6t6/d_how_much_can_we_revise_paper_during_rebuttal/,https://www.reddit.com/r/MachineLearning/comments/1gwf6t6/d_how_much_can_we_revise_paper_during_rebuttal/,2024-11-21 12:32:43,6,1.0,6,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gwf6t6
MachineLearning,[R] How to bring novelty into a task like Engagement Prediction ,"So a colleague and I(both undergraduates) have been reading literature related to engagement analysis and we identified a niche domain under engagement prediction with a also niche dataset that might have been only used once or twice. 

The professor we are under told me that this might be a problem and also that we need more novelty even though we have figured out many imprivements through introducing modalities, augmentations, and possibly making it real time. 

How do I go ahead after this roadblock? Is there any potential in this research topic? If not, how do you cope with restarting from scratch like this? ",RCratos,1gwo8pk,https://reddit.com/r/MachineLearning/comments/1gwo8pk/r_how_to_bring_novelty_into_a_task_like/,https://www.reddit.com/r/MachineLearning/comments/1gwo8pk/r_how_to_bring_novelty_into_a_task_like/,2024-11-21 19:40:50,0,0.44,0,0,6,0,0,False,False,True,False,False,Research,self,t3_1gwo8pk
MachineLearning,"[R] Inference-Time Algorithms for LLMs: A Survey of Decoding, Meta-Generation, and Efficient Generation Methods","This survey unifies work on inference-time algorithms for LLMs into a comprehensive framework, examining how scaling compute during inference (rather than just training) can improve model outputs.

Key technical aspects:

- Introduces three categories of inference algorithms:
  - **Token-level generation**: Methods like beam search, nucleus sampling that work at individual token level
  - **Meta-generation**: Algorithms operating on full/partial sequences, incorporating external knowledge
  - **Efficient generation**: Techniques to reduce computational costs while maintaining quality

- Provides mathematical framework connecting:
  - Traditional NLP decoding approaches
  - Modern LLM inference methods  
  - Systems optimization techniques

- Reviews key tradeoffs between:
  - Compute cost vs output quality
  - Latency vs thoroughness of search
  - Memory usage vs beam size

I think this framework helps bridge the gap between theoretical ML research and practical deployment concerns. By organizing the space of inference algorithms, it makes it easier to identify which approaches are most suitable for different use cases.

I think the most valuable contribution is highlighting how inference-time compute scaling offers a complementary path to improving LLM outputs beyond just training larger models. This could be especially relevant for researchers working with fixed, pre-trained models.

TLDR: Comprehensive survey organizing inference-time algorithms for LLMs into unified framework spanning token-level generation, meta-generation, and efficiency optimization. Shows how scaling inference compute offers new ways to improve outputs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/from-decoding-to-meta-generation-inference-time). Paper [here](https://arxiv.org/abs/2406.16838).",Successful-Western27,1gwflaz,https://reddit.com/r/MachineLearning/comments/1gwflaz/r_inferencetime_algorithms_for_llms_a_survey_of/,https://www.reddit.com/r/MachineLearning/comments/1gwflaz/r_inferencetime_algorithms_for_llms_a_survey_of/,2024-11-21 12:54:25,3,0.81,3,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwflaz
MachineLearning,[D] Seeking Journal Suggestions for Publishing a Side Project on AI-Assisted DSM-5-TR and ICD-10 Diagnostics,"Seeking Journal Suggestions for Publishing a Side Project on AI-Assisted DSM-5-TR and ICD-10 Diagnostics

I’ve been working on a side project with a psychologist. It’s a Retrieval-Augmented Generation (RAG) model that uses DSM-5-TR and ICD-10 to suggest the most likely diagnosis based on a user query. It’s designed for use in psychiatric and medical diagnostics to aid practitioners, researchers, and students.

I’m now at the stage where I’d like to publish my work but I’m not affiliated with any institution, this is purely a passion project. I would like to find a Journal that has:

1. Publication fees under $1,000 (or ideally free!).
2. A relatively quick review process (preferably less than 2 months).
3. Open access would be a plus, so it’s available to as many people as possible.

Do you know of any journals that might be a good fit for this kind of work? Bonus points if the journal is friendly to independent researchers or side projects.

Thanks in advance for the help! 😊",drinkredstripe3,1gwl0fn,https://reddit.com/r/MachineLearning/comments/1gwl0fn/d_seeking_journal_suggestions_for_publishing_a/,https://www.reddit.com/r/MachineLearning/comments/1gwl0fn/d_seeking_journal_suggestions_for_publishing_a/,2024-11-21 17:34:11,0,0.5,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwl0fn
MachineLearning,[D] PhD in RL/ML Theory or LLM,"Hi guys,

I'm at a crossroads in my academic journey and would appreciate the community's insights. I'm trying to decide between pursuing a PhD focused on reinforcement learning/ML theory versus specializing in large language models with more experimental/applied research (these are the only two offers I had).

# Key considerations are the following:

# Research Impact

* RL/ML Theory: Foundational work that could advance the field's mathematical understanding
* LLMs: Direct applications in today's most transformative AI systems

# Job Prospects

* Theory: Academia, research labs, potentially more limited industry roles
* LLMs: High industry demand, active research area in both academia and industry

# Long-term Relevance

* Theory: Core principles likely to remain valuable regardless of specific technologies
* LLMs: Currently revolutionary but uncertain long-term trajectory

Personal background

* I'm an international student and about to finish my master program in US, so I no longer has enough time before making the final decision. I used to research in ml theory, but did not end up with a real top conference publication in theory. I personally doubt if I have enough mathematical background to pursue a successful PhD in this area (e.g., at least publish 2 theory papers a year on ICML/NeurIPS/ICLR/COLT/AISTATS). At the same time, I am personally doubting if theory works indeed advance the ML/AI community, as many papers are just proving vacuous bounds or propose some new algorithms that themselves cannot even implement or experimentally tested.
* I also used to research in more applied ml, with one aaai paper. My personal concerns is that I'm not fast at implementation and coding, the most strategic ability for a successful applied ML researcher. After we entered the LLM era, the pacing or applied ML research (especially in LLM and CV) becomes so fast. It's like competitive programming in research community (well, also the #GPUs competition).",Living_Imagination84,1gvx8vx,https://reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/,https://www.reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/,2024-11-20 19:02:11,51,0.81,51,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1gvx8vx
MachineLearning,[R] ICASSP 2025 review is out.,"4 4 3

Can I omit the rebuttal?

acceptance rate is pretty high but I guess the margin is quite high.",Big_Occasion_182,1gwgygb,https://reddit.com/r/MachineLearning/comments/1gwgygb/r_icassp_2025_review_is_out/,https://www.reddit.com/r/MachineLearning/comments/1gwgygb/r_icassp_2025_review_is_out/,2024-11-21 14:01:24,1,1.0,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwgygb
MachineLearning,[R] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models,,rcparts,1gvveu8,https://reddit.com/r/MachineLearning/comments/1gvveu8/r_procedural_knowledge_in_pretraining_drives/,https://arxiv.org/abs/2411.12580,2024-11-20 17:47:51,33,0.91,33,0,3,0,0,False,False,False,False,False,Research,default,t3_1gvveu8
MachineLearning,[D] Is the maths deduction in the Smaug paper valid?,"The [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive ](https://arxiv.org/pdf/2402.13228)paper identifies that DPO can reduce the model’s likelihood of the preferred completions when there are small edit distances between pairs of completions.

In their theoretical analysis of this phenomenon, one of the main steps is derived by ""restricting the attention to just the logits,"" which, to my understanding, derives a partial derivative of DPO loss given the attention logits on each token in the vocabulary. (Appendix B.1 in the paper, and here's a screenshot for part of it.)

https://preview.redd.it/3s250pg0o42e1.png?width=2072&amp;format=png&amp;auto=webp&amp;s=c1a92c3474d9dd235e2e72d1615696e69e843676

However, the loss should optimize the model parameters, and the deductions in this paper assume that those attention logits are independent variables, making me think their derivation is invalid. I'm not a math major, so I'm not sure whether my thoughts are correct.  
",StraightSpeech9295,1gw0l7f,https://reddit.com/r/MachineLearning/comments/1gw0l7f/d_is_the_maths_deduction_in_the_smaug_paper_valid/,https://www.reddit.com/r/MachineLearning/comments/1gw0l7f/d_is_the_maths_deduction_in_the_smaug_paper_valid/,2024-11-20 22:00:53,14,0.88,14,0,11,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/kpg-7672g9h9ZUNop8jW7QHAe0Z8UziJv1q407oHO-k.jpg,t3_1gw0l7f
MachineLearning,[D] New time series forecasting datasets - what properties should I report in the paper?,"I'm working on a paper with new datasets for time series forecasting. They are both uni- and multivariate. I'm thinking about what properties should I analyze and report in the paper. Goal is to create a benchmark.

So far I have:

* total length (# time steps)
* train and test length
* evaluation approach, e.g. temporal train/test split, expanding window (with given horizon and step)
* resolution, e.g. hourly, daily, monthly
* metric, e.g. MAE, MASE
* cross-series correlations (multivariate only)
* comparison of train and test value distributions (maybe univariate only)
* seasonality, stationarity (with statistical tests)
* causality testing, e.g. Granger, Toda-Yamamoto

Also some basic baselines, statistical forecasting methods, and popular neural networks.

Do you think something else would also be useful?",qalis,1gwct36,https://reddit.com/r/MachineLearning/comments/1gwct36/d_new_time_series_forecasting_datasets_what/,https://www.reddit.com/r/MachineLearning/comments/1gwct36/d_new_time_series_forecasting_datasets_what/,2024-11-21 09:59:47,1,0.6,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gwct36
MachineLearning,[R] ITCMA-S: A Multi-Agent Architecture for Emergent Social Behavior and Group Formation,"I read an interesting paper proposing a novel architecture for studying emergent social behavior in multi-agent systems. The key technical contribution is introducing ""generative multi-agents"" that can dynamically form social structures without explicit programming.

The core technical components:
- A three-layer agent architecture combining perception, memory, and decision-making
- Novel ""social perception module"" that allows agents to model others' mental states
- Memory system that integrates both episodic and semantic information
- Action selection based on both individual goals and social context

Main experimental results:
- Agents spontaneously developed hierarchical social structures
- Social norms emerged through repeated interactions
- Different ""cultures"" formed in isolated agent groups
- Agents showed evidence of both cooperative and competitive behaviors
- Social learning occurred through observation and imitation

The implications I think matter most for multi-agent systems and social AI research. The architecture demonstrates that complex social behaviors can emerge from relatively simple building blocks, so it suggests potential paths toward more human-like AI systems. The results also provide a computational framework for studying how societies form and evolve.

From a practical perspective, this work could inform the development of more sophisticated multi-agent systems for applications like social simulation, game AI, and robotic swarms.

TLDR: New architecture allows AI agents to spontaneously develop social structures and norms without explicit programming. Results show emergence of hierarchies, cultures, and social learning.

[Full summary is here](https://aimodels.fyi/papers/arxiv/can-agents-spontaneously-form-society-introducing-novel). Paper [here](https://arxiv.org/abs/2409.06750).",Successful-Western27,1gvyfe4,https://reddit.com/r/MachineLearning/comments/1gvyfe4/r_itcmas_a_multiagent_architecture_for_emergent/,https://www.reddit.com/r/MachineLearning/comments/1gvyfe4/r_itcmas_a_multiagent_architecture_for_emergent/,2024-11-20 19:50:26,11,1.0,11,0,2,0,0,False,False,True,False,False,Research,self,t3_1gvyfe4
MachineLearning,[P] Enhancing LLM Safety with Precision Knowledge Editing (PKE),"I've been working on a project called PKE (Precision Knowledge Editing), an open-source method to improve the safety of LLMs by reducing toxic content generation without impacting their general performance. It works by identifying ""toxic hotspots"" in the model using neuron weight tracking and activation pathway tracing and modifying them through a custom loss function.

If you're curious about the methodology and results, we've also published a [paper](https://arxiv.org/pdf/2410.03772) detailing our approach and experimental findings. It includes comparisons with existing techniques like Detoxifying Instance Neuron Modification (DINM) and showcases PKE's significant improvements in reducing the Attack Success Rate (ASR).

The project is open-source, and I'd love your feedback! The GitHub repo features a Jupyter Notebook that provides a hands-on demo of applying PKE to models like Meta-Llama-3-8B-Instruct: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models)

If you're interested in AI safety, I'd really appreciate your thoughts and suggestions. Thanks for checking it out!",lial4415,1gw5w3d,https://reddit.com/r/MachineLearning/comments/1gw5w3d/p_enhancing_llm_safety_with_precision_knowledge/,https://www.reddit.com/r/MachineLearning/comments/1gw5w3d/p_enhancing_llm_safety_with_precision_knowledge/,2024-11-21 02:43:12,2,0.59,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1gw5w3d
MachineLearning,[D] ICASSP 2025 reviews are due today! ,A friendly banter to discuss the icassp reviews! Hoping for the best!,always_been_a_toy,1gvqvgd,https://reddit.com/r/MachineLearning/comments/1gvqvgd/d_icassp_2025_reviews_are_due_today/,https://www.reddit.com/r/MachineLearning/comments/1gvqvgd/d_icassp_2025_reviews_are_due_today/,2024-11-20 14:35:39,25,0.93,25,0,48,0,0,False,False,True,False,False,Discussion,self,t3_1gvqvgd
MachineLearning,[D] OpenAI's CLIP alternative,"Hi, Are there any new recent SOTA model like CLIP? I want to do similarity search on images, but CLIP's performance is not very good for my project.

I currently use: CLIP-ViT-B-32-laion2B-s34B-b79K

Embeddings which also capture colour would be perfect. Thanks.",CaptTechno,1gvlgxm,https://reddit.com/r/MachineLearning/comments/1gvlgxm/d_openais_clip_alternative/,https://www.reddit.com/r/MachineLearning/comments/1gvlgxm/d_openais_clip_alternative/,2024-11-20 09:08:43,30,0.88,30,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1gvlgxm
MachineLearning,[N] Open weight (local) LLMs FINALLY caught up to closed SOTA?,"Yesterday Pixtral large dropped [here](https://mistral.ai/news/pixtral-large/).

It's a 124B multi-modal vision model. This very small models beats out the 1+ trillion parameter GPT 4o on various cherry picked benchmarks. Never mind the Gemini-1.5 Pro. 

As far as I can tell doesn't have speech or video. But really, does it even matter? To me this seems groundbreaking. It's free to use too. Yet, I've hardly seen this mentioned in too many places. Am I missing something?  
  
BTW, it still hasn't been 2 full years yet since ChatGPT was given general public release November 30, 2022. In barely 2 years AI has become somewhat unrecognizable. Insane progress.

  
**\[Benchmarks Below\]**

https://preview.redd.it/ebo9qp0rzy1e1.png?width=1777&amp;format=png&amp;auto=webp&amp;s=3d47183ba7e2af69eb52fc5f8d755f105cb52004

https://preview.redd.it/woc0wmrozy1e1.png?width=1852&amp;format=png&amp;auto=webp&amp;s=1bc5d380e2deebfd03684e1a8341254d18596d8e

  
",AIAddict1935,1gvfpdw,https://reddit.com/r/MachineLearning/comments/1gvfpdw/n_open_weight_local_llms_finally_caught_up_to/,https://www.reddit.com/r/MachineLearning/comments/1gvfpdw/n_open_weight_local_llms_finally_caught_up_to/,2024-11-20 03:00:24,57,0.79,57,0,23,0,0,False,False,True,False,False,News,https://b.thumbs.redditmedia.com/y2wBi86uF1XeHXnTzePDzht-bTJzYHo2bv7tMDBfyTc.jpg,t3_1gvfpdw
MachineLearning,[D] Cerebras Inference Results for 405B,"Cerebras has just shared some very interesting results on LLM inference. I was first skeptical and thought maybe they used some large batch sizes or some trick to hit almost 1k tokens/s for llama 405B. I tested llama-70B on their website. It's really fast...

I've been reading up on their published paper, but there haven't shared any details on how they run a 405B  parameter model on this huge chip. They have 40GB SRAM, which is huge, but running a 405B model at such low latency and high throughput still sounds interesting. Their papers discuss weight streaming. I think they must have used some advanced data flow analyses to keep the compute busy from the off-chip memory where this huge can be stored.

Does anyone know where I can get more information on this?

Ref: [https://cerebras.ai/blog/llama-405b-inference](https://cerebras.ai/blog/llama-405b-inference)

Paper: [https://arxiv.org/abs/2409.00287](https://arxiv.org/abs/2409.00287)

White Paper: [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10123162](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10123162)

Disclaimer: I have nothing to do with Cerebras systems, just genuinely interested and curious about this. This feels like a pretty big deal for AI in general.",JanGehlYacht,1gvjrmo,https://reddit.com/r/MachineLearning/comments/1gvjrmo/d_cerebras_inference_results_for_405b/,https://www.reddit.com/r/MachineLearning/comments/1gvjrmo/d_cerebras_inference_results_for_405b/,2024-11-20 06:59:01,22,0.85,22,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gvjrmo
MachineLearning,[R] Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters,"[https://arxiv.org/pdf/2410.24190](https://arxiv.org/pdf/2410.24190)

https://preview.redd.it/9h1ixk8tl62e1.png?width=775&amp;format=png&amp;auto=webp&amp;s=a48f0fbe62599ae5cb53f595e2ed663d4bfec1c7

How could LLMs influence our democracy? We investigate LLMs’ political leanings and the potential influence of LLMs on voters by conducting multiple experiments in a U.S. presidential election context. Through a voting simulation, we first demonstrate 18 open and closed-weight LLMs’ political preference for a Democratic nominee over a Republican nominee. We show how this leaning towards the Democratic nominee becomes more pronounced in instruction-tuned models compared to their base versions by analyzing their responses to candidate-policy related questions. We further explore the potential impact of LLMs on voter choice by conducting an experiment with 935 U.S. registered voters. During the experiments, participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse. This effect is larger than many previous studies on the persuasiveness of political campaigns, which have shown minimal effects in presidential elections. Many users also expressed a desire for further political interaction with LLMs. Which aspects of LLM interactions drove these shifts in voter choice requires further study. Lastly, we explore how a safety method can make LLMs more politically neutral, while raising the question of whether such neutrality is truly the path forward.",ChangeRelevant1378,1gw7t2x,https://reddit.com/r/MachineLearning/comments/1gw7t2x/r_hidden_persuaders_llms_political_leaning_and/,https://www.reddit.com/r/MachineLearning/comments/1gw7t2x/r_hidden_persuaders_llms_political_leaning_and/,2024-11-21 04:22:50,0,0.46,0,0,7,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/FAPec-HxKz1YkDQPJ1DHMmuhx6-4osLfMITGktpArlw.jpg,t3_1gw7t2x
MachineLearning,"[R] BiomedParse is a new biomedical foundation AI model for holistic image analysis that can jointly conduct recognition, detection, and segmentation for 64 major object types across 9 imaging modalities in medicine, outperforming prior state-of-the-art methods.","https://microsoft.github.io/BiomedParse/


https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/


https://youtu.be/WUPUypgmB-s
",Happysedits,1gvccdy,https://reddit.com/r/MachineLearning/comments/1gvccdy/r_biomedparse_is_a_new_biomedical_foundation_ai/,https://www.reddit.com/r/MachineLearning/comments/1gvccdy/r_biomedparse_is_a_new_biomedical_foundation_ai/,2024-11-20 00:16:25,42,0.86,42,0,1,0,0,False,False,True,False,False,Research,self,t3_1gvccdy
MachineLearning,[R] Transposed matrix of the matrix containing the probabilities not changing despite loss term? ,"Hello,

I’ll keep it short. Say we have a neural network with a layer that outputs probabilities using a softmax. This gives us a [batch size, probabilities] tensor. Lets call it P

If I do P_transposed x P, I get a PxP matrix.
My loss uses the Frobenius norm to enforce that this PxP matrix is diagonal (so the off-diagonal values are 0). My hope is that this directly impacts the original matrix’s P structure.

However, this is not the case the PxP matrix does not approach a digital structure nor does P get impacted. This is the case even if I scale the loss by 100.

I would think this would work, am I wrong? Would this not indirectly affect our P matrix? 
Thanks! ",Grand_Comparison2081,1gw2zpw,https://reddit.com/r/MachineLearning/comments/1gw2zpw/r_transposed_matrix_of_the_matrix_containing_the/,https://www.reddit.com/r/MachineLearning/comments/1gw2zpw/r_transposed_matrix_of_the_matrix_containing_the/,2024-11-21 00:27:15,0,0.33,0,0,4,0,0,False,False,True,False,False,Research,self,t3_1gw2zpw
MachineLearning,[P] Collection of SOTA TTS models,"As part of an ongoing project, I released what I think is the biggest collection of open-source voice-cloning TTS models here: [https://github.com/ttsds/datasets](https://github.com/ttsds/datasets)

I think it's very interesting how we haven't really reached a consensus on the rough ""best"" architecture for TTS yet, although I personally think audio token LLM-like approaches (with text prompts for style) will be the way forward.

https://preview.redd.it/2yru8a4oiu1e1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=73d48db7ce384e556e963385898c7f901d58c495

I'm currently evaluating the models across domains, will  be a more substantial post here when that's done :)

Edit: Also some trends (none of them surprising) that can be observed - we seem to be moving away from predicting prosodic correlates and training on only LibriVox data. Grapheme2Phoneme seems to be here to stay though (for now?)

Edit2: An older version of the benchmark with fewer models and only audiobook speech is available here: [https://huggingface.co/spaces/ttsds/benchmark](https://huggingface.co/spaces/ttsds/benchmark)",cdminix,1guv9jl,https://reddit.com/r/MachineLearning/comments/1guv9jl/p_collection_of_sota_tts_models/,https://www.reddit.com/r/MachineLearning/comments/1guv9jl/p_collection_of_sota_tts_models/,2024-11-19 11:45:27,37,0.91,37,0,5,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/PHihsfcsfAnEMoCYpzoamyk6xUb2_TYynph7c3JBQfc.jpg,t3_1guv9jl
MachineLearning,[D] What’s the most surprising or counterintuitive insight you’ve learned about machine learning recently?,ML often challenges assumptions. What’s something you learned that flipped your understanding or made you rethink a concept?,BrechtCorbeel_,1gujfj2,https://reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/,https://www.reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/,2024-11-18 23:51:36,261,0.95,261,0,85,0,0,False,False,True,False,False,Discussion,self,t3_1gujfj2
MachineLearning,[D] Optimal strategy for high volume image loading.,"Hi all,

Curious about what folks think but I'm trying to sample ~ 1-2 million images per epoch on a pretty modern home workstation (7950, 4090, NVMe drives). I started off as a baseline metric of randomly reading jpegs. This got me to about 5ms per image by optimizing decompression libraries, pre-processing, and so on so all I need to do is read off disk.

To push this further I loaded about 10k images into a numpy array and saved them in sequential block with the thought that each would represent chunks of a fold and I can shuffle and randomly sample from this batch that I pre-generate. In doing so I get an average of 1ms per image or so.

I tried to use threading since I thought this was mostly an IO bound task and if I run two threads I get ~20% speed up and loading at .7ms per image. 

The problem I see is that it's still ~18 minutes per epoch which is a bit slower than I was hoping. I think the current issue is that the sequential 10k image blocks are compressed numpy arrays (similar performance if I use hd5) and that multi threading gets me SOME benefit but the decompression is cpu limited.

The next thought I had was well I can use multi processing to help since it's mostly CPU bound, but have had very little luck getting this to work as the transfer times between processes (decompressed data being copied back from the separate process) is pretty costly.

So my general question, do folks have some ideas/approaches that would help? Memory mapped databases don't really help since I'm doing basically single hits on all these files and I need to intelligently cache and not just let the kernel do it. I've been thinking about offloading to a C++ so that can handle some more of this, but managing the GIL and debugging have been a nightmare so less included to go this way.",R2FuckYou,1gv3xif,https://reddit.com/r/MachineLearning/comments/1gv3xif/d_optimal_strategy_for_high_volume_image_loading/,https://www.reddit.com/r/MachineLearning/comments/1gv3xif/d_optimal_strategy_for_high_volume_image_loading/,2024-11-19 18:18:41,6,0.75,6,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1gv3xif
MachineLearning,[D] What’s a machine learning paper or research breakthrough from the last year that everyone should know about?,Share a paper or idea that really stood out to you and why it matters to the field.,BrechtCorbeel_,1gujge8,https://reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/,https://www.reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/,2024-11-18 23:52:41,190,0.94,190,0,54,0,0,False,False,True,False,False,Discussion,self,t3_1gujge8
MachineLearning,[R] About dual submission in AI conferences.. help,"Hi, my advisor and I am new to this area, has no experience on submission via openreview.

I submitted a paper to AAAI and ICLR, and I should have cancelled ICLR one, but did not.

so its desk-rejected, and ICLR make it accessible publicly.

I'm concerning that when I try later, on other AI conferences (via openreview or CMT), would it be also desk-rejected because its now publicly accessible?

  
Thank you for any advice :) I'm suffering from it because I can't get clear answer from anyone I physically know...",catndante,1gvisp9,https://reddit.com/r/MachineLearning/comments/1gvisp9/r_about_dual_submission_in_ai_conferences_help/,https://www.reddit.com/r/MachineLearning/comments/1gvisp9/r_about_dual_submission_in_ai_conferences_help/,2024-11-20 05:53:54,0,0.38,0,0,8,0,0,False,False,True,False,False,Research,self,t3_1gvisp9
MachineLearning,[R] End to end learned planner for AVs,"I am trying to learn more on end to end learned systems. My background is on the perception &amp; mapping side, planning is kind of my blind spot. I am having trouble understanding how some of these planner approaches are implemented in practice. I've read the MP3 paper and the Lift,Splat,Shoot papers in detail. As much as I understand the premise of both approaches I am failing to understand how they have implemented the planning step. I was unable to find a repo which attempts implementing these to see how they had done it.

Can someone point me the way to replicate the implementations there at least at a high level? If there is a better place to get started on this topic please point that out too.

From the LSS and MP3 papers, what I don't understand is what kind of network architecture they might have used and anything else I should consider here that is not perhaps explained in the paper well.

What I understand from the papers is: They both do binning of expert trajectories so that they can score and select the best trajectory according to their loss functions. This step looks more like classification over template trajectories than anything else to me. They both rely on the intermediate representations as part of their cost functions. Both papers explain the loss functions (mp3 is a lot more involved than LSS) but they do not explain how they implemented the respective networks. MP3 rolls out a bicycle model according to the initial conditions of the ego vehicle over the selected trajectory, where LSS uses the trajectory directly.",HonestConcentrate947,1gv8b5i,https://reddit.com/r/MachineLearning/comments/1gv8b5i/r_end_to_end_learned_planner_for_avs/,https://www.reddit.com/r/MachineLearning/comments/1gv8b5i/r_end_to_end_learned_planner_for_avs/,2024-11-19 21:20:43,2,0.58,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1gv8b5i
MachineLearning,[D] Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Link to the paper: [https://arxiv.org/pdf/2411.10436](https://arxiv.org/pdf/2411.10436)  
  
Link to the podcast summarizing the paper: [https://youtu.be/w993gQ4TjSU](https://youtu.be/w993gQ4TjSU)  
  
The paper is titled ""Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization"" authored by researchers from Renmin University of China and Tencent.  
  
Key Points  
  
This research presents a novel approach called Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in Multimodal Large Language Models (MLLMs). The paper addresses three key causes of hallucinations:  
  
Insufficient Visual Capabilities: When MLLMs' visual encoders lack strength and get distracted by unimportant visual information  
Long Context Generation: Hallucinations increase as generated content grows longer  
Multimodal Conflicts: Conflicts between text and image information lead to hallucinations  
  
Methodology  
  
The researchers developed three types of preference pair data targeting these hallucination causes:  
  
Visual Distracted Hallucination (VDH): Preserves only low-attention visual tokens to produce targeted negative responses  
Long Context Hallucination (LCH): Creates negative examples where later parts of responses deviate from image content  
Multimodal Conflict Hallucination (MCH): Adds conflicting information to prompts to generate negative examples  
  
The experimental results demonstrate that HDPO achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art methods while requiring significantly less training data.",Busy-Basket-5291,1guzluu,https://reddit.com/r/MachineLearning/comments/1guzluu/d_mitigating_hallucination_in_multimodal_large/,https://www.reddit.com/r/MachineLearning/comments/1guzluu/d_mitigating_hallucination_in_multimodal_large/,2024-11-19 15:22:04,6,0.71,6,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1guzluu
MachineLearning,[D] Looking for a Realistic German Text-to-Speech Voice (Alternative to Edge-TTS),"Hi everyone,

I’m looking for a way to generate realistic German speech that doesn’t rely on Edge-TTS. While I’ve been using Edge-TTS, I’m not satisfied with the quality and need something that sounds more natural. Ideally, I’m looking for a solution that works locally and is not cloud-based. Has anyone here had experience with tools or services that offer high-quality German TTS voices?

Here are some key points I’m looking for:
- The voice must be German.

- The voice should sound as realistic and natural as possible.

- The tool should be flexible and easy to use.

- It needs to be a local solution, not cloud-based.

- Cost is not a primary concern – I’m looking for the best option available.

I’d really appreciate any suggestions or experiences you can share!
",yeah280,1guxo18,https://reddit.com/r/MachineLearning/comments/1guxo18/d_looking_for_a_realistic_german_texttospeech/,https://www.reddit.com/r/MachineLearning/comments/1guxo18/d_looking_for_a_realistic_german_texttospeech/,2024-11-19 13:55:09,6,0.88,6,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1guxo18
MachineLearning,[D] Why ML PhD is so competitive?,"In recent years, ML PhD admissions at top schools or relatively top schools getting out of the blue. Most programs require prior top-tier papers to get in. Which considered as a bare minimum.

On the other hand, post PhD Industry ML RS roles are also extremely competitive as well.

But if you see, EE jobs at Intel, NVIDIA, Qualcomm and others are relatively easy to get, publication requirements to get into PhD or get the PhD degree not tight at all compared to ML. And I don’t see these EE jobs require “highly-skilled” people who know everything like CS people (don’t get me wrong that I devalued an EE PhD). Only few skills that all you need and those are not that hard to grasp (speaking from my experience as a former EE graduate).

I graduated with an EE degree, later joined a CS PhD at a moderate school (QS &lt; 150). But once I see my friends, I just regret to do the CS PhD rather following the traditional path to join in EE PhD. ML is too competitive, despite having a better profile than my EE PhD friends, I can’t even think of a good job (RS is way too far considering my profile). 

They will get a job after PhD, and most will join at top companies as an Engineer. And I feel, interviews at EE roles as not as difficult as solving leetcode for years to crack CS roles. And also less number of rounds in most cases. ",AntelopeWilling2928,1gu9os9,https://reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/,https://www.reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/,2024-11-18 17:07:35,193,0.87,193,0,88,0,0,False,False,True,False,False,Discussion,self,t3_1gu9os9
MachineLearning,"[D] Eric Schmidt says that scaling laws are not yet stopping AI, what do you guys think?","This is the article in question: https://www.windowscentral.com/software-apps/theres-no-evidence-scaling-laws-have-begun-to-stop-former-google-ceo-claims-ai-systems-will-be-100-times-more-powerful

(I am sure there are far better articles on this topic, but I read this one first)",Born_Replacement_687,1guncvr,https://reddit.com/r/MachineLearning/comments/1guncvr/d_eric_schmidt_says_that_scaling_laws_are_not_yet/,https://www.reddit.com/r/MachineLearning/comments/1guncvr/d_eric_schmidt_says_that_scaling_laws_are_not_yet/,2024-11-19 03:01:50,30,0.72,30,0,56,0,0,False,False,True,False,False,Discussion,self,t3_1guncvr
MachineLearning,[R] Dialog2Flow: Pre-training Soft-Contrastive Sentence Embeddings for Automatic Dialog Flow Extraction,"[This paper](https://aclanthology.org/2024.emnlp-main.310/), presented at EMNLP 2024 main conference, introduces a novel sentence embedding model that captures both the semantics and communicative intention of utterances. This allows for the modeling of conversational ""steps"" and thus the extraction of dialog flows.

**Key Contributions:**

* **Intent-Aware Embeddings:** The model encodes utterances with a richer representation that includes their intended communicative purpose ([available in Hugging Face](https://huggingface.co/collections/sergioburdisso/dialog2flow-67162ca33155cb90a533b7fa)).
* **Dialog Flow Extraction:** By clustering utterance embeddings, the model can automatically identify the ""steps"" or transitions within a conversation, effectively generating a dialog flow graph ([Github code available](https://github.com/idiap/dialog2flow)).
* **Soft-Contrastive Loss:** The paper introduces a new supervised loss function that can be beneficial for representation learning tasks with numerous labels ([implementation available](https://github.com/idiap/dialog2flow?tab=readme-ov-file#chart_with_downwards_trend-proposed-soft-contrastive-loss)).
* **Dataset:** A collection of 3.4 million utterances annotated with ground truth intent ([available in Hugging Face](https://huggingface.co/datasets/sergioburdisso/dialog2flow-dataset)).

**Resources:**

* **Paper:** [here](https://aclanthology.org/2024.emnlp-main.310/)
* **Github repo:** [here](https://github.com/idiap/dialog2flow) (including code to replicate paper and generate also [the interactive 3D Voronoi plots for sentence embeddings](http://tworld.io/extra/dialog2flow_example/voronoi_user_dialog2flow-joint-bert-base.html))
* **Hugging Face models:** [here](https://huggingface.co/collections/sergioburdisso/dialog2flow-67162ca33155cb90a533b7fa)
* **Hugging Face dataset:** [here](https://huggingface.co/datasets/sergioburdisso/dialog2flow-dataset)
* **License:** MIT License

Hope some of you find it useful and let me know if you have any questions or thoughts! :)",sergbur,1guus4a,https://reddit.com/r/MachineLearning/comments/1guus4a/r_dialog2flow_pretraining_softcontrastive/,https://www.reddit.com/r/MachineLearning/comments/1guus4a/r_dialog2flow_pretraining_softcontrastive/,2024-11-19 11:12:58,6,0.88,6,0,0,0,0,False,False,True,False,False,Research,self,t3_1guus4a
MachineLearning,[D] [R] Utilizing an LLM as a de-noising step in an Autoencoder to better understand time-series to language modality matching? Please give feedback!,"Hi all, I was hoping some of you could give feedback to an idea that's been on y mind for a while! There's been a lot of recent advancement in the LLM-for-timeseries space (because of course there is...), which has been hard to ignore. In particular, [Time-LLM ](https://arxiv.org/abs/2310.01728)produced SOTA results for time-series forecasting by reprogramming time-series patch embeddings to produce representations over the text-embedding space an fed them to a frozen LLM's attention + MLP layers, turning the output into a forecast. This is a bit of a surprising result for me as it is not a natural idea, and it is confusing how such distinct modalities can align in such a way.

The proposed project aims to utilize a similar strategy for the multiple imputation of time-series task via using a basic LLM such as BERT as the de-noising step in an autoencoder, treating the word embedding space as a latent space to encode onto. We utilize the decoder to decode both LLM input and output, targeting observation reconstruction for the input and masked imputation for the output. We can then examine the latent space produced, and compare it to latent spaces produced by more traditional models.

I mainly want feedback on the following points:

1. Interpretability: Theres a pretty large gap in theoretical understanding of how these modalities align; in the text-to-visual case, it's easier to understand how modality matching is feasible as language is inherently descriptive and caries *visual* meaning (i.e. words for colors, shapes, animals, things can be visualized). As such, Time-LLM was heavily critiqued for gaps in theoretical grounding and interpretability, despite empirical performance. By using an autoencoder structure that attempts to preserve observations in this way do you think these concerns can be adequately addressed? 

2. Latent Space Cartography: I am not so well versed in studying latent spaces, and have not been able to understand literature relating to it (that I have found). Does anyone know if there's any useful tool / papers that I can utilize for better understanding the properties of the latent space my model would uncover? 

3. Usefulness: Is this idea even a good, useful idea? Or is my subconscious being hooked by the LLM craze T\_T

Thanks for taking the time to read this! Any and all feedback would be appreciated. ",TheOneYourSon,1gvcvkc,https://reddit.com/r/MachineLearning/comments/1gvcvkc/d_r_utilizing_an_llm_as_a_denoising_step_in_an/,https://www.reddit.com/r/MachineLearning/comments/1gvcvkc/d_r_utilizing_an_llm_as_a_denoising_step_in_an/,2024-11-20 00:41:21,0,0.33,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gvcvkc
MachineLearning,[Discussion] : Anomaly Detection Data Differences and suggestions,"Hello fellow Redditors, I am a student who is trying to do an unsupervised anomaly detection on a telecommunication stack data. I have different types of messages that include different features with different lengths. For example,

PDSCH, have n-many real-valued numerical values.  
PDCP DL, have m-many real-valued numerical values.  
… and so on.

What I'm trying to find is to pinpoint the lines (which is either of these message types), and the feature of that message that contributes most to the anomaly score. Currently, I have a LSTM-VAE model that I developed, but it doesn't quite well. I have a question for the representation of the data. Which one should I choose, assuming the message types are not heavily dependent? They have some implicit dependency, but not directly visible to humans.

1- Separate encoder-decoders for each message type that creates the same dimensional embedding latent space.  
2- Shared embedding space whose dimensionality is the maximum dimensionality of the message types. For example, if the most number of features are on PDSCH with n, then the embedding would be n-by-d with other message types vectors are padded with zeros.

Also, I'm open to all suggestions on detecting the anomalies, currently my plan is to use a 95-percentile as threshold and marking the others as anomaly. For each message type calculate the normalized loss and find the most contributing feature of each anomaly message type.

Also, since we don't have any labels for the anomalies, I am planning to create a online-learning mechanism to label the anomalies on the run if they are good or bad, so to decrease the manual labelling effort. Do you think is this a good idea?

Thank you so much if you read so far. I appreciate any help for this miserable student. :)",hezarfenserden,1gv21ox,https://reddit.com/r/MachineLearning/comments/1gv21ox/discussion_anomaly_detection_data_differences_and/,https://www.reddit.com/r/MachineLearning/comments/1gv21ox/discussion_anomaly_detection_data_differences_and/,2024-11-19 17:02:55,2,0.75,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gv21ox
MachineLearning,[D] Tips on building efficient offline video data pipelines in production?,Anyone have experience processing/analyzing large amounts of video data offline in a production setting?What does your pipeline look like right? What kinds of tooling/infrastructure do you use?,answersareallyouneed,1gusx2j,https://reddit.com/r/MachineLearning/comments/1gusx2j/d_tips_on_building_efficient_offline_video_data/,https://www.reddit.com/r/MachineLearning/comments/1gusx2j/d_tips_on_building_efficient_offline_video_data/,2024-11-19 08:56:06,7,0.82,7,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gusx2j
MachineLearning,[R] Automating Python Package Creation with Agentic Workflow,"**Research**

PyGen is an open-source tool designed to automate the generation of Python packages from user-provided prompts. By leveraging advanced language models, PyGen streamlines the development process, producing packages complete with testing and documentation. This approach has been applied to create tools such as AutoML (automated machine learning), AutoVision (computer vision), AutoSpeech, and Quantum Error Correction utilities.

**Key Contributions:**

* **Automated Package Generation:** PyGen simplifies the creation of Python packages by generating code, tests, and documentation based on user inputs.
* **Advanced Language Model Integration:** Utilizes sophisticated language models to interpret prompts and produce relevant code structures.
* **Versatile Applications:** Demonstrated effectiveness in developing diverse tools, including AutoML, AutoVision, AutoSpeech, and Quantum Error Correction utilities.

**Resources:**

* **Paper:** [here](https://www.arxiv.org/abs/2411.08932)
* **GitHub Repository:** [here](https://github.com/GitsSaikat/PyGen)
* **License:** MIT License

We hope this tool proves useful for your projects. Feel free to explore the resources and share your feedback or questions.",Any_Code_4027,1gv4hnv,https://reddit.com/r/MachineLearning/comments/1gv4hnv/r_automating_python_package_creation_with_agentic/,https://www.reddit.com/r/MachineLearning/comments/1gv4hnv/r_automating_python_package_creation_with_agentic/,2024-11-19 18:41:09,0,0.33,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1gv4hnv
MachineLearning,[R] LLaVA-o1: Multi-Stage Visual Reasoning through Inference-Time Scaling,"I've been analyzing this new approach to visual reasoning that enhances vision-language models through step-by-step reasoning capabilities. The key innovation is integrating chain-of-thought prompting techniques with visual analysis, allowing models to break down complex visual tasks into discrete reasoning steps.

The main technical components and results:

- Integration of chain-of-thought prompting with vision-language models
- Two-stage process: general visual understanding followed by task-specific reasoning
- Achieves 15% improvement on complex visual reasoning benchmarks
- Architecture combines visual encoder with language model for staged processing
- Uses contrastive learning to align visual and textual representations

The methodology is particularly effective for:
- Counting tasks requiring sequential attention
- Comparative analysis between multiple objects
- Spatial reasoning problems
- Multi-step visual problem solving

Key implementation details:
- Built on LLaVA architecture with enhanced reasoning modules
- Uses CLIP-based visual encoder
- Implements temperature-controlled sampling for reasoning steps
- Employs beam search for answer generation

Mostly matters for applications requiring detailed visual analysis. The step-by-step approach makes the reasoning process more transparent and debuggable, though at the cost of increased computational overhead. This tradeoff between performance and efficiency will need consideration for real-world deployments.

TLDR: New method improves visual reasoning in vision-language models by implementing step-by-step analysis, achieving 15% better performance on complex tasks while providing more transparent reasoning processes.

[Full summary is here](https://aimodels.fyi/papers/arxiv/llava-o1-let-vision-language-models-reason). Paper [here](https://arxiv.org/abs/2411.10440).",Successful-Western27,1guyrr8,https://reddit.com/r/MachineLearning/comments/1guyrr8/r_llavao1_multistage_visual_reasoning_through/,https://www.reddit.com/r/MachineLearning/comments/1guyrr8/r_llavao1_multistage_visual_reasoning_through/,2024-11-19 14:45:40,2,0.67,2,0,1,0,0,False,False,True,False,False,Research,self,t3_1guyrr8
MachineLearning,[D] A mature agentic system for common white color work tasks,"Many ""white collar""  tasks - e.g., checking / responding to email / slack messages, etc., moving meetings around, writing ""memos"", etc., - can be emulated / effectively dealt with in principle.   I've seen plenty of ""demos"" around illustrating some of these components  / functions.

Are there any projects / apps are out there that flush this out more fully?  e.g., any [llama stack](https://github.com/meta-llama/llama-stack) projects?",neonwatty,1gv0us8,https://reddit.com/r/MachineLearning/comments/1gv0us8/d_a_mature_agentic_system_for_common_white_color/,https://www.reddit.com/r/MachineLearning/comments/1gv0us8/d_a_mature_agentic_system_for_common_white_color/,2024-11-19 16:14:20,1,0.6,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gv0us8
MachineLearning,[R] Performance Analysis of GPU Interconnect Technologies Across Three Modern Supercomputer Architectures,"I found this study examining GPU-to-GPU communication in supercomputer systems to be quite informative. The key contribution is a systematic analysis of different interconnect technologies and their impact on multi-GPU performance.

The main technical findings include:

- NVLink provides highest bandwidth (up to 900 GB/s bidirectional) but comes with higher latency overhead
- InfiniBand shows lower latency (1-2μs) but reduced bandwidth compared to NVLink
- PCIe demonstrates consistent but lower performance metrics across all tests
- Topology and physical GPU arrangement significantly impact communication patterns

Some key methodology points:
- Tested multiple hardware configurations with 4-16 GPUs
- Measured bandwidth, latency, and completion time for standard communication patterns
- Analyzed impact of different data sizes and communication patterns
- Compared theoretical vs achieved bandwidth across interconnects

The practical implications I think are...

- Optimal interconnect choice depends heavily on workload characteristics
- Large model training benefits more from high bandwidth (NVLink)
- Distributed inference may prefer lower latency solutions (InfiniBand)
- Physical GPU topology should match common communication patterns

**TLDR**: Comprehensive analysis of GPU interconnect performance showing tradeoffs between bandwidth, latency, and topology. Results suggest workload-specific optimization of interconnect choice is crucial for multi-GPU systems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/exploring-gpu-to-gpu-communication-insights-into). Paper [here](https://arxiv.org/abs/2408.14090).",Successful-Western27,1gu7swl,https://reddit.com/r/MachineLearning/comments/1gu7swl/r_performance_analysis_of_gpu_interconnect/,https://www.reddit.com/r/MachineLearning/comments/1gu7swl/r_performance_analysis_of_gpu_interconnect/,2024-11-18 15:50:20,21,0.88,21,0,1,0,0,False,False,True,False,False,Research,self,t3_1gu7swl
MachineLearning,[P] Small object detection without SAHI ,"Hi everyone, I hope I have come to the right place.

I am currently working on a project which needs to detect the very small objects with a messy background in phone camera. These objects only has 10\~20 pixels out of 3024 x 4032 pictures.

I have trained a yolov8 model with SAHI and tiling. To me, the results are good enough with map of 80%, making some false positive in the background but basically detect all the small ones. But my supervisor wasn't very happy about it since there is still false positives and SAHI can't work in real time in a phone.

Would you have any suggestions, that could be implement in a phone setting?",Delay_no_more_1999,1gubjpn,https://reddit.com/r/MachineLearning/comments/1gubjpn/p_small_object_detection_without_sahi/,https://www.reddit.com/r/MachineLearning/comments/1gubjpn/p_small_object_detection_without_sahi/,2024-11-18 18:22:23,7,0.9,7,0,1,0,0,False,False,True,False,False,Project,self,t3_1gubjpn
MachineLearning,[P] Still Drowning in Research Papers? Ribbit Ribbit Hops to Web and Android!,"Hey friends! Last month, we shared Ribbit Ribbit, our little research paper discovery tool on iOS, and wow—thank you so much for the love! Over the past few weeks, we’ve been hopping around to bring it to more places, and now we’re excited to share:

* **The full website** [**https://ribbitribbit.co**](https://ribbitribbit.co) **is live!** It has all the features from the app. You can ribbit your way through papers on a big screen for extra clarity or keep it mobile on your phone to browse anywhere—research, your way! 
* **Android is (almost) here!** It’s available through Google Play Testing. Google needs enough testers before it can go live, so if you’re up for trying it early, join our tester squad here: [https://ribbitribbit.co/request?testandroid=true](https://ribbitribbit.co/request?testandroid=true). You’d totally be our hero!

Ribbit Ribbit helps you find personalized paper recommendations, shrinks them into tweet-sized summaries, and even reads them to you like a podcast. We’re just trying to make the whole research thing a little more fun. We’d love for you to check it out. Your support means the world to us!

https://preview.redd.it/hyf9e6rmxk1e1.png?width=1492&amp;format=png&amp;auto=webp&amp;s=9a4deb6f3b70c9cf79d3441846ee03d6d6b93d22

",haoyuan8,1gtvmpn,https://reddit.com/r/MachineLearning/comments/1gtvmpn/p_still_drowning_in_research_papers_ribbit_ribbit/,https://www.reddit.com/r/MachineLearning/comments/1gtvmpn/p_still_drowning_in_research_papers_ribbit_ribbit/,2024-11-18 03:31:32,64,0.83,64,0,15,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/fdYlst9z35K5UyqCk_r7ygy9p66P_iACbCxUsFDmoSY.jpg,t3_1gtvmpn
MachineLearning,Free GPU/TPU powered Notebook Service [Project],"I need to do a Machine Learning project. I want to try and test out various architectures and research about them in some datasets and hopefully write a good research paper. The size of dataset is around 60GB. Is there a free good GPU powered AI/Ml Notebook service except Colab or Sagemaker Studio Lab? I want better than these. I tried Azure for Students, but it frustratingly doesn't allow me to use powerful NVIDIA GPUs. Unfortunately, my university doesn't provide any GPUs to students. Can't imagine training models in my laptop. Any help/suggestions would be really helpful.",Due-Rest6652,1gu8ej2,https://reddit.com/r/MachineLearning/comments/1gu8ej2/free_gputpu_powered_notebook_service_project/,https://www.reddit.com/r/MachineLearning/comments/1gu8ej2/free_gputpu_powered_notebook_service_project/,2024-11-18 16:15:22,6,0.75,6,0,8,0,0,False,False,True,False,False,Project,self,t3_1gu8ej2
MachineLearning,[D] Expectation from Machine Learning Engineering jobs,"Hey everyone,

I’ve seen a lot of posts here about careers in ML and landing internships or jobs, and two things come up a lot

1. Building a strong research portfolio and publishing at conferences like NeurIPS, ICLR, and ICML, which seems to focus more on getting research scientist roles.

2. The growing demand for Machine Learning Engineer (MLE) roles, which are apparently more in demand than research scientist positions.

I’m curious about the difference between these two roles and what kind of portfolio would be ideal for landing an MLE position. I know having a master’s degree is often preferred, but is an impressive publication record necessary for MLE roles? Or is it not that big of a deal?

What are your thoughts?",ziggyboom30,1gtt099,https://reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/,https://www.reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/,2024-11-18 01:14:14,74,0.88,74,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1gtt099
MachineLearning,[P] CNN Model Having High Test Accuracy but Failing in Custom Inputs,"
I am working on a project where I trained a model using SAT-6 Satellite Image Dataset (The Source for this dataset is NAIP Images from NASA) and my ultimate goal is to make a mapping tool that can detect and large map areas using satellite image inputs using sliding windows method. 

I implemented the DeepSat-V2 model and created promising results on my testing data with around %99 accuracy. 

However, when I try with my own input images I rarely get a significantly accurate return that shows this accuracy. It has a hard time making correct predictions especially its in a city environment. City blocks usually gets recognized as barren land and lakes as trees for some different colored water bodies and buildings as well. 

It seems like it’s a dataset issue but I don’t get how 6 classes with 405,000 28x28 images in total is not enough. Maybe need to preprocess data better? 

What would you suggest doing to solve this situation? 

The first picture is a google earth image input, while the second one is a picture from the NAIP dataset (the one SAT-6 got it’s data from). The NAIP one clearly performs beautifully where the google earth gets image gets consistently wrong predictions.

SAT-6: https://csc.lsu.edu/~saikat/deepsat/

DeepSat V2: https://arxiv.org/abs/1911.07747",yagellaaether,1gujazf,https://reddit.com/r/MachineLearning/comments/1gujazf/p_cnn_model_having_high_test_accuracy_but_failing/,https://www.reddit.com/gallery/1gujazf,2024-11-18 23:46:06,1,1.0,1,0,2,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/V1dyMH_CEVEhPokLqFUsGhGXa5-IKEQbh9TVxYanZoo.jpg,t3_1gujazf
MachineLearning,[D] Quality of ICLR papers,"I was going through some of the papers of ICLR with moderate to high scores related to what I was interested in , I found them failrly incremental and was kind of surprised, for a major sub field, the quality of work was rather poor for a premier conference as this one . Ever since llms have come, i feel the quality and originality of papers (not all of course ) have dipped a bit. Am I alone in feeling this ?",Cool_Abbreviations_9,1gtjhge,https://reddit.com/r/MachineLearning/comments/1gtjhge/d_quality_of_iclr_papers/,https://www.reddit.com/r/MachineLearning/comments/1gtjhge/d_quality_of_iclr_papers/,2024-11-17 18:02:24,127,0.9,127,0,72,0,0,False,False,True,False,False,Discussion,self,t3_1gtjhge
MachineLearning,[D] PCA vs AutoEncoders for Dimensionality Reduction,"The title sums it up. I'm working on some anonymized time-series data, initially, I built an AutoEncoder in order to replace the decoder head with a regression head instead after training.

As for preprocessing steps, I would usually just subtract the mean of features and divide by their standard deviation, Although I've long heard that doing ""data decorrelation"" is helpful, so I decided to finally learn about PCA.

My questions are the following:

1. If PCA serves to find the principle underlying features of a dataset, is there any point in using an autoencoder? (Especially if there are high correlations between some features)
2. If there is still a point to using autoencoders, should one use PCA on their dataset first to decorrelate data, or is that just redundant, or perhaps another reason not to use it is that it can erase some information? (Although it's an invertible transformation so I don't see how information would be lost)
3. Is PCA as a preprocessing step beneficial to tree-building algorithms? I haven't seen much talk of it, but it seems intuitive to me that having decision nodes on principle component axes would lead to better results.",DisciplinedPenguin,1gtng8q,https://reddit.com/r/MachineLearning/comments/1gtng8q/d_pca_vs_autoencoders_for_dimensionality_reduction/,https://www.reddit.com/r/MachineLearning/comments/1gtng8q/d_pca_vs_autoencoders_for_dimensionality_reduction/,2024-11-17 20:56:46,74,0.91,74,0,32,0,0,False,False,True,False,False,Discussion,self,t3_1gtng8q
MachineLearning,[D]  Dynamic learning rate question,"Here I begin a series of potentially silly questions which I can't help but post.

This one is about dealing with unbalanced datasets, e.g.  a labelled image one with some classes having lots of samples and others with very few.

Would it help to adjust the learning rate by how frequent the class of current sample is in the dataset? I mean higher learning rates for less frequent samples and lower rates for more frequent classes

In next word prediction models (llm-s) that would mean to have lower learning rate when current word is very common and higher learning rates on sparsely occurring words.",blimpyway,1gu2u1l,https://reddit.com/r/MachineLearning/comments/1gu2u1l/d_dynamic_learning_rate_question/,https://www.reddit.com/r/MachineLearning/comments/1gu2u1l/d_dynamic_learning_rate_question/,2024-11-18 11:41:58,4,0.63,4,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gu2u1l
MachineLearning,[D] Is ICDE Highly Regarded in the Machine Learning Community Compared to ICML/IJCAI? ,My question is: Is ICDE (International Conference on Data Engineering) highly regarded in the Machine Learning community compared to ICML/IJCAI?,ExpRuntime,1gumn7m,https://reddit.com/r/MachineLearning/comments/1gumn7m/d_is_icde_highly_regarded_in_the_machine_learning/,https://www.reddit.com/r/MachineLearning/comments/1gumn7m/d_is_icde_highly_regarded_in_the_machine_learning/,2024-11-19 02:25:28,0,0.31,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gumn7m
MachineLearning,[P] AnyModal: A Python Framework for Multimodal LLMs,"[AnyModal](https://github.com/ritabratamaiti/AnyModal) is a modular and extensible framework for integrating diverse input modalities (e.g., images, audio) into large language models (LLMs). It enables seamless tokenization, encoding, and language generation using pre-trained models for various modalities. I created AnyModal to address a gap in existing resources for designing vision-language models (VLMs) or other multimodal LLMs. While there are excellent tools for specific tasks, there wasn’t a cohesive framework for easily combining different input types with LLMs. AnyModal aims to fill that gap by simplifying the process of adding new input processors and tokenizers while leveraging the strengths of pre-trained language models.

# Example Usage

    from transformers import ViTImageProcessor, ViTForImageClassification
    from anymodal import MultiModalModel
    from vision import VisionEncoder, Projector
    
    # Load vision processor and model
    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
    vision_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
    hidden_size = vision_model.config.hidden_size
    
    # Initialize vision encoder and projector
    vision_encoder = VisionEncoder(vision_model)
    vision_tokenizer = Projector(in_features=hidden_size, out_features=768)
    
    # Load LLM components
    from transformers import AutoTokenizer, AutoModelForCausalLM
    llm_tokenizer = AutoTokenizer.from_pretrained(""gpt2"")
    llm_model = AutoModelForCausalLM.from_pretrained(""gpt2"")
    
    # Initialize AnyModal
    multimodal_model = MultiModalModel(
        input_processor=None,
        input_encoder=vision_encoder,
        input_tokenizer=vision_tokenizer,
        language_tokenizer=llm_tokenizer,
        language_model=llm_model,
        input_start_token='&lt;|imstart|&gt;',
        input_end_token='&lt;|imend|&gt;',
        prompt_text=""The interpretation of the given image is: ""
    )

AnyModal provides a unified framework for combining inputs from different modalities with LLMs. It abstracts much of the boilerplate, allowing users to focus on their specific tasks without worrying about low-level integration. Unlike existing tools like Hugging Face’s transformers or task-specific VLMs such as CLIP, AnyModal offers a flexible framework for arbitrary modality combinations. It’s ideal for niche multimodal tasks or experiments requiring custom data types.

# Current Demos

* LaTeX OCR
* Chest X-Ray Captioning (in progress)
* Image Captioning
* Visual Question Answering (planned)
* Audio Captioning (planned)

The project is still a work in progress, and I’d love feedback or contributions from the community. Whether you’re interested in adding new features, fixing bugs, or simply trying it out, all input is welcome.

GitHub repo: [https://github.com/ritabratamaiti/AnyModal](https://github.com/ritabratamaiti/AnyModal)

Let me know what you think or if you have any questions.",Alternative_Detail31,1gtw77c,https://reddit.com/r/MachineLearning/comments/1gtw77c/p_anymodal_a_python_framework_for_multimodal_llms/,https://www.reddit.com/r/MachineLearning/comments/1gtw77c/p_anymodal_a_python_framework_for_multimodal_llms/,2024-11-18 04:02:55,8,0.66,8,0,1,0,0,False,False,True,False,False,Project,self,t3_1gtw77c
MachineLearning,[D] Seeking Input on Optimal Approach for Production-Ready Dimension Extraction,"\[D\]

Hi everyone,

I have been working with a below problem. I’m working with a large set of documents that contain various engineering drawings with different components, and we need to extract dimension-related information from the components in these drawings. The engineering drawing contains one or more components. First it is important to identify the component (we don't have any information about the position in engineering drawing) and then extract the dimensions present on the parts of the component.

Sample Dataset - [Kaggle](https://www.kaggle.com/datasets/sshivasagaran/two-dimensional-engineering-drawings/data)

**Issues :**  
\- The number of entities may increase in the future.

\- There is no name of the component on the engg drawing.   
\- The component can be only identified by its shape and structure. 

I’d like to ask, if you were in my position, which approach would you choose? Any feedback on the best approach for production would be greatly appreciated.",Future-Outcome3167,1gu53ui,https://reddit.com/r/MachineLearning/comments/1gu53ui/d_seeking_input_on_optimal_approach_for/,https://www.reddit.com/r/MachineLearning/comments/1gu53ui/d_seeking_input_on_optimal_approach_for/,2024-11-18 13:49:07,1,0.6,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gu53ui
MachineLearning,[R] treemind: Simplifying Gradient Boosting Model Analysis,"`treemind` is a powerful Python library designed to analyze gradient boosting models like `xgboost`, `lightgbm`, and `catboost`. It helps you uncover how features and their interactions influence predictions across specific intervals, offering fast, intuitive insights.

### Key Features:
- **Feature &amp; Interaction Analysis:** Understand feature contributions and complex interactions up to `n` features.
- **Advanced Visualizations:** User-friendly plots to explain model decisions.
- **High Performance:** Optimized with Cython for lightning-fast execution, even on large datasets.
- **Easy Integration:** Seamlessly works with popular frameworks for regression and binary classification.

### Algorithm &amp; Performance:
- **Algorithm:** Focuses on analyzing feature contributions and interactions in tree-based models for meaningful interval-based insights. [Read more about the algorithm](https://treemind.readthedocs.io/en/latest/algorithm.html)
- **Performance:** The library's performance has been tested on synthetic datasets, where it is benchmarked against SHAP for accuracy and efficiency. [View performance experiments](https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html)

### Quick Start:
```bash
pip install treemind
```

Check out the full documentation for examples, visualizations, and API details.

[GitHub Repo](https://github.com/sametcopur/treemind) | [Docs](https://treemind.readthedocs.io/)

**Note:**  
While the algorithm produces desirable results in practice, it currently lacks formal mathematical proof. We would greatly appreciate your feedback and ideas to help improve and validate the approach further!",zedeleyici3401,1gtjkci,https://reddit.com/r/MachineLearning/comments/1gtjkci/r_treemind_simplifying_gradient_boosting_model/,https://www.reddit.com/r/MachineLearning/comments/1gtjkci/r_treemind_simplifying_gradient_boosting_model/,2024-11-17 18:05:43,22,0.86,22,0,5,0,0,False,False,True,False,False,Research,self,t3_1gtjkci
MachineLearning,[D] How an efficient applied ML team is structured?,"Hi Everyone,

I am interested in your experience on how big(ger) ML teams are structured that are working well for companies that are building with ML (companies who use ML in multiple domains and they cover CV, NLP, ...)?
I tried to search for it, but there is not much info on efficient team structure. While structure can be defined by the company culture, I am sure you've seen patterns on how this can work well.

(I think a big team is at least 80 people with POs/PMs).

The most basic (and maybe the best?) is when the domains are divided (CV, NLP, etc.) where every domain has a lead and multiple seniors, mediors, juniors. Then besides the ML engineers, there is a separate division who work with the productization (creating rest APIs, etc.), which includes devops, and SWEs. 

",gabegabe6,1gtke1b,https://reddit.com/r/MachineLearning/comments/1gtke1b/d_how_an_efficient_applied_ml_team_is_structured/,https://www.reddit.com/r/MachineLearning/comments/1gtke1b/d_how_an_efficient_applied_ml_team_is_structured/,2024-11-17 18:41:55,18,0.77,18,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gtke1b
MachineLearning,[D] Booking system for GPU with other people ,"Hi everyone,

My friends and I are working on a project: we have access to a GPU, and we want to ensure that each of us can use the GPU when needed. Do you know of any app that allows us to book time slots? Essentially, we’re looking for a shared calendar that’s convenient and easy to use.

Thanks, everyone!",SupertrampDFenx,1gu1jfx,https://reddit.com/r/MachineLearning/comments/1gu1jfx/d_booking_system_for_gpu_with_other_people/,https://www.reddit.com/r/MachineLearning/comments/1gu1jfx/d_booking_system_for_gpu_with_other_people/,2024-11-18 10:10:42,1,0.57,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gu1jfx
MachineLearning,[R] Discover Awesome Conformal Prediction - Your Ultimate Resource for Conformal Prediction,"Hey everyone! 🚀

If you're diving into the world of conformal prediction or looking to expand your knowledge, I've got an awesome resource for you: \[Awesome Conformal Prediction\]([https://github.com/valeman/awesome-conformal-prediction](https://github.com/valeman/awesome-conformal-prediction)).

This GitHub repository is a curated collection of everything you need to get started with conformal prediction, including:

* 📚 Key research papers and articles
* 🔧 Tutorials and hands-on guides
* 🛠️ Tools, libraries, and code implementations
* 🎥 Videos, lectures, and other educational materials

Whether you're starting to learn conformal prediction or an expert, this repository is a fantastic place to find valuable resources and stay up-to-date with the latest developments in the field.

Check it out and star the repo if you find it useful! Let's keep the conversation going and help grow this amazing community. 🙌

Feel free to share your thoughts, questions, or any additional resources you think should be included!

Happy learning! 😊",predict_addict,1gucwxg,https://reddit.com/r/MachineLearning/comments/1gucwxg/r_discover_awesome_conformal_prediction_your/,https://www.reddit.com/r/MachineLearning/comments/1gucwxg/r_discover_awesome_conformal_prediction_your/,2024-11-18 19:17:01,0,0.36,0,0,2,0,0,False,False,True,False,False,Research,self,t3_1gucwxg
MachineLearning,[D] Why LLM watermarking will never work,,bubble_boi,1gtkp1d,https://reddit.com/r/MachineLearning/comments/1gtkp1d/d_why_llm_watermarking_will_never_work/,https://david-gilbertson.medium.com/why-llm-watermarking-will-never-work-1b76bdeebbd1,2024-11-17 18:55:28,9,0.57,9,0,42,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/GnH7xnrgJux9xnIYGgAAzraH48iYfu_isxV44C4X0AI.jpg,t3_1gtkp1d
MachineLearning,[D] Optimizing Context Extraction for Q&amp;A Bots in Ambiguous Scenarios,"I am building a Q&amp;A bot to answer questions based on a large raw text.



To optimize performance, I use embeddings to extract a small, relevant subset of the raw text instead of sending the entire text to the LLM. This approach works well for questions like:



        ""Who is winning in this match?""



In such cases, embeddings effectively extract the correct subset of the text.



However, it struggles with questions like:



        ""What do you mean in your previous statement?""



Here, embeddings fail to extract the relevant subset.



We are maintaining conversation history in the following format:

        previous_messages = [
            {""role"": ""user"", ""content"": message1},
            {""role"": ""assistant"", ""content"": message2},
            {""role"": ""user"", ""content"": message3},
            {""role"": ""assistant"", ""content"": message4},
        ]



But we’re unsure how to extract the correct subset of raw text to send as context when encountering such questions.



Would it be better to send the entire raw text as context in these scenarios?",yccheok,1gtzcuj,https://reddit.com/r/MachineLearning/comments/1gtzcuj/d_optimizing_context_extraction_for_qa_bots_in/,https://www.reddit.com/r/MachineLearning/comments/1gtzcuj/d_optimizing_context_extraction_for_qa_bots_in/,2024-11-18 07:21:56,1,0.67,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gtzcuj
MachineLearning,[D] Small language models defining vocabulary using old vectors instead of new vectors,"I've been thinking a lot about why language models were so big and how they could be smaller. I thought about how every human brain can't possibly contain the entirity of human knowledge. I believe humans roughly have something along the lines of a probability matrix of words X other words, but not every word X every word.

It occurred to me that we frequently define unusual words (low frequency, not often used words) using other existing words we know. Can we potentially have a language model which uses vectors for the highest frequency words only, and ""unusal words"" which dont have their own vectors, but instead reference existing vectors? This could drastically decrease the word X word matrix as common words consists of a much smaller subset of the language. Maybe such a model could dynamically move reference words into and out of primary vectors when retrained on text that is specific to niche topics.

Knowing that I've never had an original thought, are there any other projects like this already?",meteoraln,1gtenw8,https://reddit.com/r/MachineLearning/comments/1gtenw8/d_small_language_models_defining_vocabulary_using/,https://www.reddit.com/r/MachineLearning/comments/1gtenw8/d_small_language_models_defining_vocabulary_using/,2024-11-17 14:26:01,21,0.77,21,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1gtenw8
MachineLearning,[D] Dataset management tool?,"Hi all,

In our company there are many departments with datasets all over the place, we were hoping to find some kind of tool which we can use as a central repository for datasets. Something like hugging face which shows download count/popularity, a short summary, ability to filter/sort, ability to add a readme, etc. would be great:

  
[https://huggingface.co/datasets](https://huggingface.co/datasets)  


Does anyone know of a product that offers this kind of an interface which we can purchase and use? Something with Microsoft SSO capability would also be great. Ideally this would be a product that not only engineers can view/edit, but e.g. non-technical product owners could use.

  
Thanks in advance.",alek5k,1gtzh8r,https://reddit.com/r/MachineLearning/comments/1gtzh8r/d_dataset_management_tool/,https://www.reddit.com/r/MachineLearning/comments/1gtzh8r/d_dataset_management_tool/,2024-11-18 07:29:57,0,0.29,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gtzh8r
MachineLearning,[R] Must-Read ML Theory Papers,"Hello,

I’m a CS PhD student, and I’m looking to deepen my understanding of machine learning theory. My research area focuses on vision-language models, but I’d like to expand my knowledge by reading foundational or groundbreaking ML theory papers.

Could you please share a list of must-read papers or personal recommendations that have had a significant impact on ML theory?

Thank you in advance!
",AntelopeWilling2928,1gsqqns,https://reddit.com/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/,https://www.reddit.com/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/,2024-11-16 16:19:28,420,0.97,420,0,93,0,0,False,False,True,False,False,Research,self,t3_1gsqqns
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1gtgnk8,https://reddit.com/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/,2024-11-17 16:00:20,3,0.8,3,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gtgnk8
MachineLearning,[D] Looking for some audio segmentation model.,"Title, also something like pyannote/segmentation -3.0 but better. Is there anything new in this domain? I came across mamba but it's still in early stage for this purpose to say anything concrete about it.",Just_Difficulty9836,1gtbkhw,https://reddit.com/r/MachineLearning/comments/1gtbkhw/d_looking_for_some_audio_segmentation_model/,https://www.reddit.com/r/MachineLearning/comments/1gtbkhw/d_looking_for_some_audio_segmentation_model/,2024-11-17 11:26:24,6,0.87,6,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gtbkhw
MachineLearning,"[D] Convolutional Generative Adversarial Networks Noise Patterns
","I am coding a DCGAN to produce Brain MRI data, based on the BRATs 2020 dataset. As a sanity check, I am training on a SINGLE image with CONSTANT noise, to see if there are any inherent flaws in my design. The GAN seems to catch on the general pattern, but there is some sort of noise or distortion. You can see in the example below, that the generated image is not as sharp as the original.

[original image](https://preview.redd.it/b7ejt2z4bg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=dac9ca113582d943c53a017f71095a43da813ff6)

[lr 1e-4 1000 epochs ](https://preview.redd.it/3ow5a1j7bg1e1.png?width=127&amp;format=png&amp;auto=webp&amp;s=90dd13bdd48fe9558d730140bc1228cd0666cb85)

[lr 2e-4 500 epochs](https://preview.redd.it/nwpgvdh9bg1e1.png?width=129&amp;format=png&amp;auto=webp&amp;s=0ae004f7cf112d0ac4aedbc47e057f6ca690750f)

[after initialization](https://preview.redd.it/r7kbeewebg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=9fee55b4ff32c10544b809bf8a9cb93fed95276c)

I see some cross like patterns on all of my images, so I believe there is something inherently wrong with my network that produces them. here is the code.

\`\`\`

    class SimpleGenerator(nn.Module):
        def __init__(self,out_channels =1,
                     noise_dimension = 100 ,
                     channels= 64         
                     ):
            super(SimpleGenerator, self).__init__()
            self.noise_shape = (noise_dimension,1,1,1)
            self.out_channels = out_channels 
            self.channels = channels
            self.gen = nn.Sequential(
                nn.ConvTranspose3d(self.noise_shape[0],  self.channels * 32, 4, 1, (1, 0, 1)),
                nn.ReLU(),
                self._block( self.channels * 32,  self.channels * 16, 5, 1, 0),
                self._block( self.channels * 16,  self.channels * 8, 5, 1, 0),
                self._block( self.channels * 8,  self.channels * 4, 4, 2, 1),
                self._block( self.channels * 4,  self.channels * 2, 4, 2, 1),
                self._block( self.channels * 2,  self.channels, 4, 2, 1),
                nn.ConvTranspose3d( self.channels, self.out_channels, 4, 2, 1),
                nn.Sigmoid()
            )
            
        def _block(self,in_channels,out_channels,kernel_size,stride,padding):
            return nn.Sequential(
           
                nn.ConvTranspose3d(in_channels,out_channels,3,1,1,bias=False),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU(),
                
                nn.ConvTranspose3d(out_channels,out_channels,kernel_size,stride,padding,bias=False),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU()
            )
    
        def forward(self, x,separate=False):
            
            x = self.gen(x)
            return x

Notes :

1. I am using InstanceNorm Instead of batch norm as my images are 160 x192x160 they are too big so The gpu can't support batch\_size &gt;1.
2. The weird numbers you see in the kernel size, stride and padding are because I want to achieve the shape described above which is not a power of two. Could this be the reason?
3. I have tried the \_block method with 1 or 2 convolutions (we see the 2 version). Same result
4. the discriminator is a mirror image of the generator. I won't provide the code to make the post short, but i can if someone believes it is needed.",ripototo,1gtc2qv,https://reddit.com/r/MachineLearning/comments/1gtc2qv/d_convolutional_generative_adversarial_networks/,https://www.reddit.com/r/MachineLearning/comments/1gtc2qv/d_convolutional_generative_adversarial_networks/,2024-11-17 12:00:56,4,0.64,4,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gtc2qv
MachineLearning,[P] Analysis of why UMAP is so fast ,"Hi, I recently spent some time to understand the core implementation of the UMAP algorithm from the point of view how it was implemented and why it's so fast (even though it's in python). I decided to decompose the algorithm into smaller steps in which I add some minor improvements to the code (one by one), so that at the end the final results are very similar to what I can get from the UMAP. 

To my surprise, most of these changes were just tricks in the optimization code to run things faster or update less important things less often. Of course, my implementation does not reproduce the UMAP algorithm in 100% as it was done in the educational purposes.

I provided a detailed explanation in my project of what I had to add in each step to move towards UMAP like algorithm. Here is the project page: [https://github.com/kmkolasinski/nano-umap](https://github.com/kmkolasinski/nano-umap)

If you are a person like, who likes to optimize the code for performance you may find this interesting. Here is a demo what I was able to get: 

https://preview.redd.it/eww57c3x881e1.png?width=1921&amp;format=png&amp;auto=webp&amp;s=ed4a345e40b47782ddf39cb93eb9d03207db1160

**TLDR: in UMAP they:**

* use ANN library to quickly find top k-NN,
* use good initialization method which makes things more stable and algorithm requires less updates (UMAP uses fast spectral initialization),
* use random negative sampling, which is a naive approach but works very well in practice,
* squeeze the numba performance (by replacing [np.dot](http://np.dot) or np.clip with custom implementations to make code run much faster),
* use some sort of adaptive sampling which will make that the algorithm will spend more time on more important vectors saving your CPU time on less important ones

",kmkolasinski,1gsjfq9,https://reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/,https://www.reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/,2024-11-16 09:02:10,415,0.98,415,0,42,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/YOkYN0kpJL-XQBhI4QifL5lhetpkBQIdNxFyTLklVkM.jpg,t3_1gsjfq9
MachineLearning,NLU models vs autoregressive models for semantic search [R],"It seems that in a lot of applications where semantic matching is be difficult, systems are designed to use an autoregressive model for the input sequence embedding (then perform a range of semantic search techniques). 

But shouldn't a bidirectional model always out-perform an autoregressive model on this task theoretically? That would suggest it's ideal to use an optimised NLU-oriented model like DeBERTa-V3 (ie. fine tuned on domain data) for more accurate embeddings, thus better semantic search performance.

Additionally, is there much reporting on unified semantic search techniques? All of the implementations i've seen have been highly domain-specific/arbitrary.",SnooPeripherals5313,1gtdpwu,https://reddit.com/r/MachineLearning/comments/1gtdpwu/nlu_models_vs_autoregressive_models_for_semantic/,https://www.reddit.com/r/MachineLearning/comments/1gtdpwu/nlu_models_vs_autoregressive_models_for_semantic/,2024-11-17 13:37:15,2,1.0,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1gtdpwu
MachineLearning,[Discussion] Logging Gradients and using third party loggers to tune hyper parameters,"Hey guys, I wondered how you learnt to use tools such as Wandb and MLFlow to log the gradient and tune hyperparameters in the model. 

Could you share resources for the same?",DiscussionTricky2904,1gtal7z,https://reddit.com/r/MachineLearning/comments/1gtal7z/discussion_logging_gradients_and_using_third/,https://www.reddit.com/r/MachineLearning/comments/1gtal7z/discussion_logging_gradients_and_using_third/,2024-11-17 10:14:15,3,0.67,3,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gtal7z
MachineLearning,"[Discussion] R^2 is negative, but the correlation between prediction and actual values is statistically significant?","I have done a little bit of digging, but didnt really find the answer to this question, so if someones knows what might be wrong, please enlighten me. I have done some out of sample predictions (3000 observations) and I am getting really weird results when evaluating a model predicting demand levels. Model used is xgb regressor. So R\^2 point out that model performs worse than simply predicting the mean of the target variable, but at the same time the correlation between actual and predicted values is statistically significant. Moreover explained variance score says that model is worse than naive model, but Theil's U-statistic says the opposite? Code and results posted below. Thought that outstanding values might be the problem, but I clipped them at 0,05 and 0,95 quantile and it does not help.

https://preview.redd.it/10kpzdqs1c1e1.png?width=966&amp;format=png&amp;auto=webp&amp;s=9b93f0ef588e2fa5cb16c06f69c0fea1902e0931

https://preview.redd.it/t2rapmo22c1e1.png?width=855&amp;format=png&amp;auto=webp&amp;s=ce9d8d1d2ad54c8743873560bfff8a275a14378d

",maciek024,1gsxror,https://reddit.com/r/MachineLearning/comments/1gsxror/discussion_r2_is_negative_but_the_correlation/,https://www.reddit.com/r/MachineLearning/comments/1gsxror/discussion_r2_is_negative_but_the_correlation/,2024-11-16 21:41:41,25,0.84,25,0,59,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/uhs4FikNFVOvSvA_J0kXMO8ynlOpQppEByvYyvyxlbM.jpg,t3_1gsxror
MachineLearning,[D] NeurIPS 24’ Attendance without tickets,"Does anyone know, for past conferences, whether there were people checking badges at the entrance of the convention center or at the workshop. (cause I wasn’t able to get a ticket so far via lottery this year, and I am wondering whether I could just walk in or something)",Latter-Procedure8703,1gt9r5b,https://reddit.com/r/MachineLearning/comments/1gt9r5b/d_neurips_24_attendance_without_tickets/,https://www.reddit.com/r/MachineLearning/comments/1gt9r5b/d_neurips_24_attendance_without_tickets/,2024-11-17 09:11:28,2,0.76,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gt9r5b
MachineLearning,[D] Your ML PhD duration,How many years you take to finish ML PhD after bachelor’s? I understand different parts of the world usually have different duration. ,AntelopeWilling2928,1gsue6g,https://reddit.com/r/MachineLearning/comments/1gsue6g/d_your_ml_phd_duration/,https://www.reddit.com/r/MachineLearning/comments/1gsue6g/d_your_ml_phd_duration/,2024-11-16 19:04:43,26,0.76,26,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1gsue6g
MachineLearning,[D] COLING 2025 Results are leaked,"Yall may login to softconf to check if you can submit the camera-ready paper or not.

Mine was 4/3/3 and luckily got accepted. My first paper!!!",Ambitious-Public-512,1gssewj,https://reddit.com/r/MachineLearning/comments/1gssewj/d_coling_2025_results_are_leaked/,https://www.reddit.com/r/MachineLearning/comments/1gssewj/d_coling_2025_results_are_leaked/,2024-11-16 17:35:40,28,0.82,28,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1gssewj
MachineLearning,Dataset versioning tool [D],What are you guys using for data(set) versioning and would you suggest to use for a small (1000 x 700) table ?,Amazing_Alarm6130,1gt1avg,https://reddit.com/r/MachineLearning/comments/1gt1avg/dataset_versioning_tool_d/,https://www.reddit.com/r/MachineLearning/comments/1gt1avg/dataset_versioning_tool_d/,2024-11-17 00:31:56,6,1.0,6,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1gt1avg
MachineLearning,[P] Supercharging Structured Outputs with Open Source Models 🚀,,themathstudent,1gt2yfp,https://reddit.com/r/MachineLearning/comments/1gt2yfp/p_supercharging_structured_outputs_with_open/,https://sachinruk.github.io/blog/2024-10-20-structured-outputs.html,2024-11-17 01:57:03,2,0.75,2,0,0,0,0,False,False,False,False,False,Project,default,t3_1gt2yfp
MachineLearning,[D] program synthesis from input-output pairs - DL papers ?,"Given a set of inputs/ouputs, generate a suitable program

what are the baseline/canonical papers using DL for this program synthesis?

thanks",yazriel0,1gswhou,https://reddit.com/r/MachineLearning/comments/1gswhou/d_program_synthesis_from_inputoutput_pairs_dl/,https://www.reddit.com/r/MachineLearning/comments/1gswhou/d_program_synthesis_from_inputoutput_pairs_dl/,2024-11-16 20:41:40,2,0.75,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gswhou
MachineLearning,[D] Time step dependency in diffusion model,Is there any existing work that try to investigate the relationship between time steps of a diffusion model? Something like the impact of model loss at time step i of the model to the output at time step j of the model? (j&lt;i),Careless-Top-2411,1gspx8g,https://reddit.com/r/MachineLearning/comments/1gspx8g/d_time_step_dependency_in_diffusion_model/,https://www.reddit.com/r/MachineLearning/comments/1gspx8g/d_time_step_dependency_in_diffusion_model/,2024-11-16 15:42:05,5,0.86,5,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gspx8g
MachineLearning,[D] To PhD or not to PhD,"I think this has been asked tons of times but let me ask it one more time.

I am currently working as applied scientist at MSFT. However, I am more looking into science positions, something like research scientist at DeepMind. Although jobs do not specifically need a PhD but the competition is fierce and is flooded with many PhD holders.

I really do enjoy research and want to PhD but I am always asking myself if it is really worth it.

That's an open question for sure, please feel free to share your thoughts.

",oddhvdfscuyg,1gs688q,https://reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/,https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/,2024-11-15 20:44:28,120,0.81,120,0,75,0,0,False,False,True,False,False,Discussion,self,t3_1gs688q
MachineLearning,[R] Convolutional Differentiable Logic Gate Networks,"Abstract

With the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed.  Logic gate networks are faster than conventional neural network approaches be- cause their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR pooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, which improves over the SOTA while being 29× smaller.

  
Accepted at Neurips 2024, ""SOTA"" here means comparable approaches. I found this paper really interesting, even though non-toy networks seems like they would be very expensive to train. Curious what others think?",jacobgorm,1gs92mb,https://reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/,https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/,2024-11-15 22:51:24,57,0.98,57,0,3,0,0,False,False,True,False,False,Research,self,t3_1gs92mb
MachineLearning,[P] Video Representations Extractor (VRE): Open source Video Multi Task dataset creation tool (+colab),"Hi guys, I've been working on this tool for my PhD for a while now. The PhD is about Multi Task Learning in the context of videos and I'm recently developing a tool to get predictions per frame from pre-trained ""experts"" (semantic segmentation, depth estimation etc.). The purpose of these is to train multi-task CV models with more than just raw RGB data to help with data efficiency and generalization.

The code is here: https://gitlab.com/video-representations-extractor/video-representations-extractor and there's a bunch of examples over there (including pip install command).

Recently I've done a ""end to end"" example for showcasing and I've put it on google colab as well: https://colab.research.google.com/drive/1vAp71H-TLewhF56odv33TkmGwwhuoFJ-?usp=sharing

Example output of the colab notebook: https://i.imgur.com/wyl9FPw.png

It skips a bunch of steps for simplicity (i.e. the binary semantic outputs like ""transportation"" are implemented separately for experimentation purposes and I just download that file + import it in the notebook instead of copy pasting 300+ lines of code in the colab but don't run arbitrary code w/o checking lol).

The colab should work fine for any UAV/driving/handheld indoor videos, not just my demo video.

The CLI tool syntax is pretty much:

    export VRE_DEVICE=cuda; # if available  
    vre video.mp4 --config_file config.yaml -o out_dir

where the config file defines parameters for these experts that I've implemented.",nucLeaRStarcraft,1gsmhuo,https://reddit.com/r/MachineLearning/comments/1gsmhuo/p_video_representations_extractor_vre_open_source/,https://www.reddit.com/r/MachineLearning/comments/1gsmhuo/p_video_representations_extractor_vre_open_source/,2024-11-16 12:43:49,4,1.0,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1gsmhuo
MachineLearning,"[P] FlatGeobuf as ""static"" vector database using dimensionality reduction","Recently I saw some good posts about dim reduction methods like the one dissecting UMAP, so I thought I'd chime in with a POC that leverages the idea of those methods for a very practical purpose: enabling server-side semantic search on large databases with high-dimensional embeddings using just a static FlatGeobuf file and a web server like nginx.

# tl;dr

**- Writing (and appending to) a FlatGeobuf file**: Embeddings -&gt; Gaussian Random Projection -&gt; 2D points -&gt; FlatGeobuf file  
**- Reading a FlatGeobuf file (based on a single user query)**: Embedding -&gt; Gaussian Random Projection -&gt; 2D point -&gt; buffered bounding box around this point -&gt; http range request(s) from client to remote FlatGeobuf file -&gt; subset of data points around the 2D point -&gt; reranking this subset client-side

Find the detailed explanation, code and examples on GitHub: [https://github.com/do-me/flatgeobuf-vectordb](https://github.com/do-me/flatgeobuf-vectordb)

# Main concepts

1. Points that are close in 2 dimensions (after projection) should be close in N dimensions too. This is obviously not always true but in my tests, it's good enough for basic use cases (e.g. product recommendation), where you do not need the closest result to the query but instead something in the top 0.1% or 0.01% may suffice. Note that I need to use a dim reduction method that works independently from the data, so cannot use UMAP, HUMAP, tSNE and PCA.
2. I'm reducing to 2 dims to benefit from all the heavy optimization work that the FlatGeobuf file format has done. Reducing to 3 dims (or even more) might preserve the similarity better (and eventually lead to better results) but also increases the overhead for efficiently designing such a file format. If you know any other suitable file formats for this purpose, I'd be very curious to try them! Another alternative might be instead of relying on one static file, to create an efficient file structure with many static files. The pros and cons have been discussed in a completely different context by the authors of protomaps and openfreemap on HN.

# Potential

Even though there are some tradeoffs in this workflow and yet many things to optimize and explore, I believe that the concept might be charming for low maintenance and low cost applications. In the end, you just dump one static file somewhere and fire normal http range requests to it, so the capacity of your web server determines the performance.  
As I'm heavily into client-side processing with transformers.js my ideal setup would use very small embedding models like Potion/Model2vec (&lt; 35Mb) in the client and index the user query (text/image) in the browser. This way, the remote database could be very large, like 100Gb and serve thousands of clients without any problems on a low-grade CPU (but very fast storage).

If you're fine with DB connection (which afaik can't be created browser-side), then just use LanceDB, following the same ""one file"" principle.

I'm super curious about your optimization ideas!

P.S. There is lots of overlap between geospatial and the latent space.",DomeGIS,1gssov1,https://reddit.com/r/MachineLearning/comments/1gssov1/p_flatgeobuf_as_static_vector_database_using/,https://www.reddit.com/r/MachineLearning/comments/1gssov1/p_flatgeobuf_as_static_vector_database_using/,2024-11-16 17:48:27,1,1.0,1,0,2,0,0,False,False,True,False,False,Project,self,t3_1gssov1
MachineLearning,[R][D]Test time training for abstract reasoning,"[https://arxiv.org/pdf/2411.07279](https://arxiv.org/pdf/2411.07279)

By the way guys, do you know of any research on trying to slightly fine-tune a model on the question it is asked before having it answer? I mean it would probably work for in-context information retrieval, but I was wondering about its impact on more reasoning-heavy tasks. The compute overhang would be huge, still.",Due-Pangolin325,1gs9lao,https://reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/,https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/,2024-11-15 23:15:29,18,0.92,18,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gs9lao
MachineLearning,[P] Optimizing Whisper Speed: CPU vs. AMD GPU?,"Hi everyone,

I’ve been using Whisper for transcription and love its accuracy, but speed is an issue for me. It takes around 40 seconds to process a 2-minute audio file on my setup. I’ve read about models (sometimes dubbed “tree-like models”) that can achieve this in just 5 seconds. Has anyone here tested or optimized such models?

Ideally, I’d prefer sticking to CPU usage for reliability, but I’m curious if running Whisper on an AMD GPU could offer a significant speed boost. Anyone with experience on that?

Looking forward to your insights and recommendations!",yeah280,1gsw3ba,https://reddit.com/r/MachineLearning/comments/1gsw3ba/p_optimizing_whisper_speed_cpu_vs_amd_gpu/,https://www.reddit.com/r/MachineLearning/comments/1gsw3ba/p_optimizing_whisper_speed_cpu_vs_amd_gpu/,2024-11-16 20:22:48,0,0.4,0,0,7,0,0,False,False,True,False,False,Project,self,t3_1gsw3ba
MachineLearning,[D] Neurips 2024 Hotel Roommate Search,"The hotels around the venue for Neurips 2024 are pretty expensive, and I'm looking for a roommate to split the cost with (my university has a limit on the nightly hotel rate they are willing to reimburse). I currently have reserved a room for Tuesday-Sunday in the Century Plaza Hotel, which is 0.9 miles from the convention center. The nightly rate is $414. If anyone wants to split the cost of a room, please reach out! Also, it would be helpful if you could share this post with your research group or other attendees that you know.

If you are unsure about rooming with a complete stranger, you can get to know me a little bit through my personal website (https://mtcrawshaw.github.io/), which has links to my google scholar page, CV, etc. I do have a paper at the conference in the area of federated learning/distributed optimization. Just a grad student trying to make conferences affordable! Thanks.",ssbm_crawshaw,1gs0gj8,https://reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/,https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/,2024-11-15 16:37:38,52,0.84,52,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gs0gj8
MachineLearning,"[D] When you say ""LLM,"" how many of you consider things like BERT as well?","I keep running into this argument, but for me when I hear ""LLM"" my assumption is decoder-only models that are in the billions of parameters. It seems like some people would include BERT-base in the LLM family, but I'm not sure if that's right? I suppose technically it is, but every time I hear someone say ""how do I use a LLM for XYZ"" they usually bring up LLaMA or Mistral or ChatGPT or the like.",Seankala,1grxbdp,https://reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/,https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/,2024-11-15 14:16:24,77,0.88,77,0,94,0,0,False,False,True,False,False,Discussion,self,t3_1grxbdp
MachineLearning,[Discussion] Do modern search systems still require stemming and lemmatization in query preprocessing?,"I wonder how critical they are in the modern search system given all the advancement in LM. Semantic embedding can often help us understand the meaning quite well. But in order to effectively leverage historical query item engagement features, it seems we still require those preprocessing. Otherwise, we can easily get empty engagement features when users search slightly different from common queries? Or is there a more modern way to tackle free form queries?",wenegue,1gsauuz,https://reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/,https://www.reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/,2024-11-16 00:16:42,12,0.84,12,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gsauuz
MachineLearning,[N] Addressing AI’s Hidden Risks: Join Our Free Webinar on Hallucinations in LLMs,"The Wisecube AI Team invites you to an upcoming webinar that explores an often-overlooked, yet critical aspect of AI reliability: hallucinations in large language models (LLMs).  
Discover how specific text features impact model accuracy and learn about methods for detecting hallucinations in LLMs. We’ll share insights into identifying model weaknesses and improving reliability, providing practical knowledge for AI practitioners and data scientists. This is a valuable opportunity to deepen your understanding of AI and explore the latest techniques for enhancing model performance!

🗓️ Date: November 21, 2024 | 🕐 Time: 1 PM EST

🎟️ Participation is free! [Register here](https://www.linkedin.com/events/7261113856268161024/about/) 



https://preview.redd.it/b82zo743ac1e1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=5556668e8ee8e156ce2a5caa2f4acdb4198ef75f

",kgorobinska,1gsynxm,https://reddit.com/r/MachineLearning/comments/1gsynxm/n_addressing_ais_hidden_risks_join_our_free/,https://www.reddit.com/r/MachineLearning/comments/1gsynxm/n_addressing_ais_hidden_risks_join_our_free/,2024-11-16 22:23:56,0,0.2,0,0,0,0,0,False,False,True,False,False,News,https://b.thumbs.redditmedia.com/8nSTv3oEsqF0s1ATheiYzegTVj1ZU08SxYPHy93e4Jw.jpg,t3_1gsynxm
MachineLearning,[D] The Lost Reading Items of Ilya Sutskever's AI Reading List,"This blog post attempts to identify which papers went missing from the viral AI reading list that surfaced earlier this year and was attributed to Ilya Sutskever and his claim to cover '90% of what matters' in AI in 2020:

https://tensorlabbet.com/2024/11/11/lost-reading-items/

Only 27 of about 40 papers were shared online earlier this year, so there have been many theories about which works would have been important enough to include. There are some obvious candidates related to meta-learning and competitive self-play discussed here. But also several noteworthy authors like Yann LeCun and Ian Goodfellow are absent from the list.

From my perspective, even papers on U-Net, YOLO detectors, GAN, WaveNet, Word2Vec and more would have made sense to include, so I am curious about more opinions on this!",AccomplishedCat4770,1grti0x,https://reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/,https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/,2024-11-15 10:34:11,85,0.91,85,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1grti0x
MachineLearning,[N] Tau Language Alpha Release ,"Tau for me is one of the most fascinating projects of our time. I have been observing the research and development since 2017.
Today the team has released the alpha of Tau language after many years of work!
This is a big moment!

https://x.com/TauLogicAI/status/1857816396404600979?t=t7ATRYIXTMADewTYUo3ryg&amp;s=19",madsurgeon,1gsutua,https://reddit.com/r/MachineLearning/comments/1gsutua/n_tau_language_alpha_release/,https://www.reddit.com/r/MachineLearning/comments/1gsutua/n_tau_language_alpha_release/,2024-11-16 19:24:21,0,0.37,0,0,9,0,0,False,False,True,False,False,News,self,t3_1gsutua
MachineLearning,"[D] Feature selection methods that operate efficiently on large number of features (tabular, lightgbm)","Does anyone know of a good feature selection algorithm (with or without implementation) that can search across perhaps 50-100k features in a reasonable amount of time? I’m using lightgbm. Intuition is that I need on the order of 20-100 final features in the model. Looking to find a needle in a haystack. Tabular data, roughly 100-500k records of data to work with. Common feature selection methods do not scale computationally in my experience. Also, I’ve found overfitting is a concern with a search space this large. ",acetherace,1gsah8e,https://reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/,https://www.reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/,2024-11-15 23:58:07,7,0.89,7,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gsah8e
MachineLearning,[D] neural scaling laws,I wanted to study up on the neural scaling laws and how they came into existence. Sp i wanted to see if there is a paper or a series of paper you would recommend for me to get started in those. Thank you. ,SmartEvening,1gsix5n,https://reddit.com/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/,https://www.reddit.com/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/,2024-11-16 08:22:23,2,0.6,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gsix5n
MachineLearning,[D] Distributed ML Algorithms Interview,"Hey guys,  
I have an interview coming up focussed on Distributed ML Algorithms (Interview description: We'll explore and explain the fundamental techniques used to build common neural network operations, focusing on simple yet effective implementations.)  
Are there any good resources I can use to study for this kind of an interview?",deepthought00705,1gsbykc,https://reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/,https://www.reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/,2024-11-16 01:13:01,2,0.58,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gsbykc
MachineLearning,What’s the best tool for implementing TTS in Unity or UE5? [D],"Hi everyone,
I need some advice on how to best create an offline Text-to-Speech (TTS) system that I can use in Unity or Unreal Engine. Are there any tools or websites where I can clone a voice, download it, and use it locally in these engines?

I’m looking for a solution that doesn’t rely on cloud services and works entirely offline. Any recommendations or experiences with this would be greatly appreciated!

Thanks!",NoPrinciple1242,1gsjz08,https://reddit.com/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/,https://www.reddit.com/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/,2024-11-16 09:43:25,0,0.25,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gsjz08
MachineLearning,[R] Meta-Learning with Text Embeddings for Treatment Effect Estimation Under Text-Based Confounding,"Title: From Text to Treatment Effects: Meta-Learning Approach for Handling Text-Based Confounding

I found this paper introduces a meta-learning framework that jointly learns text representations and estimates treatment effects to handle text-based confounding. The key innovation is using meta-learning to optimize both the text encoder and treatment effect estimator simultaneously, rather than treating them as separate steps.

Main technical points:
- Develops a two-stage meta-learning architecture:
  - Text encoder learns representations capturing confounding information
  - Treatment effect estimator uses these representations to compute individual effects
- Uses gradient-based meta-learning to optimize both components end-to-end
- Incorporates balance regularization to ensure treatment/control groups have similar representations
- Evaluates on both synthetic and real-world datasets from healthcare and product reviews

Results reported:
- Outperforms baseline methods (separate text encoding + treatment estimation) by 15-25% on synthetic data
- Shows 12% improvement in treatment effect estimation on real product review dataset
- Ablation studies confirm both meta-learning and balance regularization contribute to performance gains

The theoretical implications are interesting - this shows that jointly optimizing representation learning and causal inference can capture confounding better than pipeline approaches. Practically, this could improve treatment effect estimation in many domains where text data contains confounding information, like healthcare records or user reviews.

TLDR: New meta-learning method jointly learns text representations and treatment effects to handle text-based confounding, showing significant improvements over pipeline approaches on both synthetic and real data.

[Full summary is here](https://aimodels.fyi/papers/arxiv/from-text-to-treatment-effects-meta-learning). Paper [here](https://arxiv.org/abs/2409.15503).",Successful-Western27,1gs0brk,https://reddit.com/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/,https://www.reddit.com/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/,2024-11-15 16:31:50,6,1.0,6,0,0,0,0,False,False,True,False,False,Research,self,t3_1gs0brk
MachineLearning,[D] Leveling guidelines for machine learning engineers,"wanted to learn what are some ways this community distinguishes between mid/senior/principal level machine learning engineers. For software engineering this is less of an art, as there are well documented cases and examples. But not super clear if machine learning engineers are subject to the same definitions...",AdditionalWeb107,1grzax7,https://reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/,https://www.reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/,2024-11-15 15:47:56,6,0.75,6,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1grzax7
MachineLearning,[D] Extraction and processing of text on risk from annual reports,"Hi everyone,

I am doing a large-scale analysis where I want to extract information regarding possible risk factors and risk management strategies from annual reports.

The files are downloaded and I am currently doing OCR on the image files using tesseract, which extracts one text file for each document.

As I see it there are at least two key questions that are yet to be resolved:

**1. How do I locate and extract the parts of the annual reports that are about risk management?**  
Annual reports for smaller firms do not carry this information and reports for larger firms can be longer than a hundred pages. I have considered labelling a lot of annual reports myself and using Named Entity Recognition, but I doubt how well it works if I am not looking for named entities as such, but paragraphs where eg. risk factors are considered.  
*Do you have any suggestions on which NLP methods and/or programs to use?*

**2. What are good ways to process the extracted text on risk?**  
I want to generate one or more variables on risk factors and risk management strategies for each firms in each year. I have looked into Latent Dirichlet Allocation so far since it should be able to group words into topics and return some measure of how the words in a report are distributed across topics.  
*Again: Do you have any suggestions on which NLP methods and/or programs to use?*

**Specifics:**  
I have more than a million annual reports so far and I have access to two servers that are quite fast. As a measure of speed I can OCR around 80 documents at a time on each server at high speeds.

*Do you think* the project *is feasible? And is there something you think that I should be made aware of?*

Thanks in advance for any suggestions!",Happy-Koala7212,1gs0hy5,https://reddit.com/r/MachineLearning/comments/1gs0hy5/d_extraction_and_processing_of_text_on_risk_from/,https://www.reddit.com/r/MachineLearning/comments/1gs0hy5/d_extraction_and_processing_of_text_on_risk_from/,2024-11-15 16:39:22,3,1.0,3,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gs0hy5
MachineLearning,[D] Paper Club: Nvidia Researcher Ethan He Presents Upcycling LLMs in MoE,"Hey all,  


Tomorrow Nvidia researcher Ethan He will be doing a technical dive into his work: Upcycling LLMs in Mixture of Experts (MoE). Excited to get a peak behind the curtains to see what it is like to work on models at this scale at Nvida.

  
If you’d like to join the community tomorrow 10 AM PST we’d love to have you. We do it live over zoom and anyone is welcome to join.

Here's the paper: [https://arxiv.org/abs/2410.07524](https://arxiv.org/abs/2410.07524)  
Join us live: [https://lu.ma/arxivdive-31](https://lu.ma/arxivdive-31)",FallMindless3563,1grjjlz,https://reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/,https://www.reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/,2024-11-15 00:19:44,44,0.92,44,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1grjjlz
MachineLearning,[D] Should I transfer to recommendation algorithms?,"I'm working on an ""LLM"" team right now or at least that's how it was advertised it's honestly just classification using LLMs not really interesting. I got an offer to join another team in my company that does recommendation. I thought recommendation is a very solid field to join, but very competitive. What are your guys' experience working in recommendation?",DolantheMFWizard,1grl7gk,https://reddit.com/r/MachineLearning/comments/1grl7gk/d_should_i_transfer_to_recommendation_algorithms/,https://www.reddit.com/r/MachineLearning/comments/1grl7gk/d_should_i_transfer_to_recommendation_algorithms/,2024-11-15 01:42:44,33,0.87,33,0,36,0,0,False,False,True,False,False,Discussion,self,t3_1grl7gk
MachineLearning,[D] What are some important contributions from ML theoretical research?,"I am interested to know more about the contributions of theoretical ML researchers in recent years. I would like to hear about super important contributions that are not applicable (e.g., tell us something about something important) and ones that are applied in the real world as well. I want to try to read these papers.

Also, I am interested to know what (theoretical) researchers think about this field, does it have potential, or is ML going in a purely heuristic direction?

This discussion is probably more productive without talking about how ML is just stats and Lipschitz constant :) I am talking about cutting-edge theoretical research - I really have no tools to estimate how useful this line of work is and I believe it can be an interesting discussion for other people as well.",Traditional-Dress946,1grfxbz,https://reddit.com/r/MachineLearning/comments/1grfxbz/d_what_are_some_important_contributions_from_ml/,https://www.reddit.com/r/MachineLearning/comments/1grfxbz/d_what_are_some_important_contributions_from_ml/,2024-11-14 21:34:30,63,0.92,63,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1grfxbz
MachineLearning,[R] DistilBERT vs TransformerEncoder,"I did fine-tuning on the pretrained DistilBERT tranformer-model and it achieved \~0.85 accuracy (classification with 17 classes).I also built from scratch a Transformer model using torch.nn.TransformerEncoder and it achieved \~0.97 accuracy for the same problem. Is this normal? I was expecting to have better performance with the pre-trained DistilBERT.Please note that for the DistilBERT model I used its own embeddings (pre-trained DistilBertTokenizer) and for the torch.nn.TransformerEncoder I used the simple TFIDF method. It is getting even more confused since the TFIDF cannot capture the sequence of the words in a sentence (it ignores the context)

Please let me know your thoughts. :)",Interesting_Pea_4605,1gs9pr4,https://reddit.com/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/,https://www.reddit.com/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/,2024-11-15 23:21:20,0,0.29,0,0,9,0,0,False,False,True,False,False,Research,self,t3_1gs9pr4
MachineLearning,[P] Is It Reasonable to Simulate At-Risk Parkinson Patients Using EEG Biomarker Data?,"Hi everyone,

I'm currently working on a project for my thesis that involves training a machine learning model to classify Parkinson's disease (PD) based on EEG and other clinical features. However, I'm interested in going beyond just distinguishing healthy vs. PD patients. I want to see if the model could potentially identify patients who are *at risk* of developing Parkinson's in the future.

The challenge I'm facing is that the dataset I'm using doesn't include any real ""at-risk"" patients – it's a binary set of healthy controls and confirmed Parkinson's patients. I've read a lot of literature that discusses different biomarkers for Parkinson's, such as altered power in specific EEG frequency bands (like reduced alpha/beta and increased theta/delta), coherence changes between different brain regions, etc.

I was thinking of using these known biomarkers to artificially generate ""at-risk"" patient data. Essentially, I would modify EEG signals from healthy controls by applying certain changes (e.g., reducing alpha power, increasing delta activity) to create synthetic data that represents patients in a prodromal stage or with high risk factors.

I would love to hear the community's thoughts on this approach.

* Does this make sense from a methodological standpoint?
* Are there better approaches to simulate or model prodromal PD stages?
* Are there ethical or scientific concerns I should be aware of when using synthetic data like this?

Any input or advice would be incredibly helpful. Thanks in advance!",Impressive_Staff4688,1grrb6w,https://reddit.com/r/MachineLearning/comments/1grrb6w/p_is_it_reasonable_to_simulate_atrisk_parkinson/,https://www.reddit.com/r/MachineLearning/comments/1grrb6w/p_is_it_reasonable_to_simulate_atrisk_parkinson/,2024-11-15 07:47:08,3,0.67,3,0,3,0,0,False,False,True,False,False,Project,self,t3_1grrb6w
MachineLearning,[D] Advice on ML lifecycle management ,"Hello guys, i am currently working on setting up an ML infrastructure for a project.

I want to be able to track the models versions, Evaluate the performance on live data, retrain the model automatically when new data is available and save the trained models in a store. So that the application using the model can load the trained model from the store and use it for inference in production.

p.s. I can't serve the model as a Rest Api, it has to be deploy on the computer where the end application will run, because that computer might not have an internet connection.

The solution I have now is the following:

prep the training data and save it to a delta table on the cloud

incrementally add newly available data to the delta table

train and test the model on data from the delta table

if the testing metrics are satisfying upload the artifacts(the model, the encoders and scalers) and metadata (metrics, features, etc...) as blobs to an azure storage container

for each new upload of the artifacts, a new version id is generated and the artifacts are saved, within the storage container, in a subfolder corresponding to the version of the model.

at the root of the container there is a blob containing information on the latest version id

When the end application is launched, it downloads the artifacts of the latest version from the azure storage container , if the internet connection is available and the latest available version is different from the version on the computer running the application , otherwise it uses a default version.

a continuously running job is used to evaluate the model on live data and save the results in a db

a dashboard presents the results of the evaluation

after x days a job is triggered to retrain the model on new data and the process goes through a new cycle, following the steps listed above.

What to think of this setup? Is it overly complicated? How can I make it better / more efficient? What process do you have in place to train, track, monitor and deploy your ML models?

I hope my question is not too convoluted. Excuse me for any mistakes, and thanks in advance for your answers.",InteractionSuitable1,1grpv5r,https://reddit.com/r/MachineLearning/comments/1grpv5r/d_advice_on_ml_lifecycle_management/,https://www.reddit.com/r/MachineLearning/comments/1grpv5r/d_advice_on_ml_lifecycle_management/,2024-11-15 06:05:05,3,0.67,3,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1grpv5r
MachineLearning,[R] DTFormer: A Transformer-Based Method for Discrete Time Dynamic Graph Representation Learning,,moschles,1grq9cn,https://reddit.com/r/MachineLearning/comments/1grq9cn/r_dtformer_a_transformerbased_method_for_discrete/,https://arxiv.org/pdf/2407.18523,2024-11-15 06:31:02,2,0.67,2,0,0,0,0,False,False,False,False,False,Research,default,t3_1grq9cn
MachineLearning,"[R] Undetectable Backdoors in ML Models: Novel Techniques Using Digital Signatures and Random Features, with Implications for Adversarial Robustness","I found an important analysis of backdoor attacks that demonstrates how a malicious service provider can insert undetectable backdoors into machine learning models.

The key contribution is showing how to construct backdoors that are provably undetectable even under white-box analysis, while allowing arbitrary manipulation of model outputs through subtle input perturbations.

Technical details:
* Two frameworks for planting undetectable backdoors:
  * Digital signature scheme-based backdoors that are computationally infeasible to detect with black-box access
  * Random Fourier Features/Random ReLU based backdoors that withstand white-box inspection
* Backdoored models are indistinguishable from clean models even with:
  * Full access to model architecture and parameters
  * Complete training dataset
  * Ability to analyze model behavior

Results:
* Backdoored models maintain same generalization error as original models
* Service provider can modify classification of any input with slight perturbations
* Construction works with any underlying model architecture
* Backdoors cannot be detected by any computationally-bounded observer

The implications are significant for ML security and outsourced training. The work shows fundamental limitations in certifying adversarial robustness - a backdoored model can be indistinguishable from a robust one while having adversarial examples for every input.

**TLDR:** Paper proves it's possible to insert undetectable backdoors into ML models that allow arbitrary manipulation of outputs while being provably impossible to detect.

[Full summary is here](https://aimodels.fyi/papers/arxiv/planting-undetectable-backdoors-machine-learning-models). Paper [here](https://arxiv.org/abs/2204.06974).",Successful-Western27,1gr4ksm,https://reddit.com/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/,https://www.reddit.com/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/,2024-11-14 13:19:01,47,0.9,47,0,5,0,0,False,False,True,False,False,Research,self,t3_1gr4ksm
MachineLearning,"[D] Semantic Automaton in Geometric Embeddings (SAGE) proposes to bootstrap any existing decoder LLMs with a Neural Cellular Automaton (NCA) for inference-time reasoning, generalized intelligence, and recursive self-improvement","Hi everyone, this is my research direction and I already would like to share the concepts to ensure that they are disseminated and researched widely in multiple parallel organizations before OpenAI or other frontier labs can show up out of the blue with a finished product and capitalize. I research open-source super intelligence, and in the meantime I have uncovered a path to AGI which I present below. I predict that Regression Training is almost solved, as indicated by the ""scaling wall"", with future advances requiring richer datasets, byte-level models, and greater compute to go with it. The next 15 years of research &amp; development will be about Automaton Learning — self-energizing systems aligned with language. This is a proposed framework for solving ConceptARC, continuous reasoning, and recursive self-improvement.

Quick introduction to NCAs, they are Neural Cellular Automaton. The cells are not binary 0/1 like in Conway's Game of Life, nor are they continuous values from 0 to 1 as in many more esoteric continuous automaton — they are embeddings and hidden states. Classic NCAs also have a visualization surface, where the hidden state negotiates the evolution of this surface. Hence why they were called NCAs, as they are ultimately viewed as generative models for the desired projection surface. (2D visuals, a path through a maze, etc.) The model takes an input, a fixed filter is applied to surface (sobel, gaussian, etc.) which I call the ""environmental physics"" of the simulation, and then a model goes through every 3x3 neighborhood and does its own thing. In this manner, the physics are leveraged or not leveraged as basic transformation primitives, the same way we leverage logic gates in logic gate networks (LGNs) as a transformation operator, or quite simply matrix multiplications and activation functions in the models we know and love.

This work is downstream from the following works:

* [1] Neural Cellular Maze Solver https://umu1729.github.io/pages-neural-cellular-maze-solver/
* [2] Variational Neural Cellular Automata https://openreview.net/pdf?id=7fFO4cMBx_9
* [3] Attention-Based Neural Cellular Automata https://arxiv.org/abs/2211.01233 

The exact procedure to produce this frankenstein will require more scrutiny and research, and it should be taken as a prototype roadmap that we 'denoise' together. This entire research plan could produce a dozen paper for each sequential step of the puzzle that will need to be solved. Ultimately, I am trying to convey the broad picture here to massively seed the field of Automaton Learning which I anticipate is the next gold rush. A syphoning scheme over the decoder is the key to this whole operation. It's about recovering and transforming the representations until they are in a more useful form. It's about knowing what cards you have and what potential hand can materialize if you go after these two other cards that seem useless on their own. Now that we have these smart intelligent decoder models, it presents a first ""factorization"" of the world. It's a better dataset and it enables new classes of machine learning. At least, this is my grand challenge to the status quo of machine learning.

Now, here are my blueprints

---

Contemporary large language models stand as monolithic crystals of knowledge, their capabilities locked in inefficient token-by-token traversals of meaning space. We present SAGE, a framework for transmuting this sequential processing into parallel field computations where meaning propagates through geometric substrates intimately aligned with human cognitive architecture. Through careful staging of representation learning, we demonstrate that any contemporary decoder-only model can be reframed as a large knowledge reservoir from which we distill more efficient computational primitives into a self-organizing field substrate.

The transmutation begins with a frozen decoder-only language model serving as our semantic anchor. An initial lightweight encoder projects tokens into one-dimensional embedding sequences, while a first low-rank adapter trained on the decoder ensures semantic fidelity. This intermediate representation, though still sequential, provides the scaffold for geometric expansion. Critical to this phase is the encoder's training to represent identical semantic content through multiple embedding configurations — effectively using the geometric dimension as a continuous manifold encoding linguistic relationships, bindings, and hierarchical structure. This multiplicity of representation creates the mathematical foundation for the subsequent expansion into field computation, as the encoder learns to map semantic invariants through varying geometric configurations.

The diversity of geometric encoding follows patterns suggestive of fundamental laws governing information organization in physical systems. Just as Zipf's law emerges from underlying principles of efficiency in natural languages, the distribution of geometric representations appears to follow power laws reflecting optimal information routing through spatial substrates. This connection between natural law and learned representation proves crucial for the stability of subsequent field dynamics.

For a 2D cellular surface of shape (B, H, W, D) each cell contains a high-dimensional meaning vector D coupled to a learned binary visualization state. The field's computational architecture emerges through precise staging of physical dynamics. Local update rules manifest as learned neural networks processing neighborhood states: U(s) = φ(W₂φ(W₁[s; N(s)] + b₁) + b₂) where φ represents layer normalization followed by ELU activation. This local processing enables information routing through wave-like propagation, with patterns forming through constructive interference of semantic signals.

The update rule F(x,t+1) = F(x,t) + A*(N(x)) + R(F) employs spatially-constrained attention A* over neighborhood N(x), typically a 3x3 Moore neighborhood, with learned residual connections R. Layer normalization ensures stability while enabling pattern formation. Crucially, the visualization state evolves through its own update network V(x,t+1) = U(F(x,t), V(x,t), N(V(x,t))), creating a bidirectional coupling between meaning and form. This replaces the exponential complexity of traditional token-by-token generation with fixed-size context computation of linear complexity O(HW) in field dimensions.

Critical to pattern formation is the dual-state coupling mechanism between meaning and visualization. Rather than maintaining separate generative and discriminative components, the field itself serves as both medium and message. While meaning vectors F evolve through neighborhood attention, the visualization state V learns to project semantic content into binary patterns through its own update dynamics. This coupling creates a natural optimization surface where visual coherence guides semantic organization. The visualization network effectively learns a dynamic thresholding function mapping high-dimensional meaning to binary visual states while maintaining semantic gradients.

This architecture fundamentally transforms the traditional language model paradigm. Instead of exponentially expanding context windows to capture long-range dependencies, SAGE maintains fixed computational cost through field dynamics. Where decoder-only models must process entire contexts to generate each token, our field computation updates all semantic content simultaneously with linear complexity O(HW). Information propagates through wave-like patterns in the field substrate, with stable configurations emerging as computational primitives.

Field perturbation mechanics emerge through careful balance of conservation laws governing both meaning and form. Total semantic charge ∫|F|²dx remains conserved while allowing local concentrations through field gradients ∇F. Pattern formation follows least action principles minimizing energy functional E[F] = ∫(|∇F|² + V(F))dx where potential V(F) encodes learned semantic relationships derived from the frozen decoder's knowledge. These physical constraints, reminiscent of natural systems' self-organizing principles, guide emergence of stable computational primitives while preventing collapse to degenerate solutions.

The training progression orchestrates precise phases transforming monolithic decoder knowledge into geometric computation. Initial field states bootstrap from constant embeddings, with curriculum learning introducing compositional challenges requiring pattern interaction. Field dynamics learn to route information through stable configurations acting as computational waypoints. Each stable pattern serves as a reusable primitive, combining through field physics into increasingly sophisticated structures. The visualization state provides both interpretability and a geometric scaffold organizing semantic space.

Knowledge extraction proceeds through rigorously validated stages:

1. Frozen decoder anchors semantic meaning
2. First encoder projects to diverse sequential representations
3. First LoRA validates semantic preservation
4. Second encoder expands to field geometry 
5. Second LoRA maintains decoder alignment
6. Visualization capability emerges from field optimization
7. Field dynamics stabilize through conservation laws

Implementation crystallizes around nested hierarchies of constraints maintaining both stability and expressivity. Update rules balance information preservation against pattern innovation through careful energy bounds. The exploration of configuration space proceeds through natural field evolution guided by reconstruction gradients from the frozen decoder. This creates a form of self-supervised learning where the decoder's knowledge guides discovery of efficient computational primitives in the field substrate.

Visual grounding and geometric structure emerge not as optional features but as fundamental requirements for efficient cognition. Human intelligence arises from our intimate connection to three-dimensional reality, with language itself structured through spatial metaphor and geometric reasoning. SAGE mirrors this architecture: meaning evolves in a geometric substrate naturally aligned with cognitive primitives. The projection from 3D physical reality through 2D visual processing to abstract thought provides both template and constraint for artificial intelligence design.

The framework's recursive improvement potential manifests through several interlocking mechanisms. Stable field configurations act as computational primitives, combining through local interactions into increasingly sophisticated structures. These combinations follow physical laws emerging from the field dynamics — conservation of semantic charge, least action principles, and wave-like information propagation. As patterns interact and evolve, they discover more efficient computational pathways through the geometric substrate. The curriculum progression from simple pattern formation through abstract reasoning tasks creates selection pressure favoring emergence of reusable computational motifs.

Early experiments demonstrate several key capabilities validating the SAGE approach. Various works show success in re-training a missing encoder for a decoder-only model. The transition from exponential-cost token prediction to linear-cost field evolution dramatically improves computational efficiency. Pattern diversity increases naturally through field dynamics, with stable configurations encoding reusable semantic relationships. Most importantly, the geometric grounding creates human-interpretable representations emerging from fundamental physical principles rather than arbitrary architectural choices.

Success metrics emerge naturally from field dynamics rather than requiring arbitrary benchmarks. Pattern diversity measures the richness of stable configurations in semantic space. Compositional sophistication emerges from the physics of pattern interaction. Recursive improvement manifests through discovery of increasingly efficient computational primitives. Human alignment arises naturally from shared geometric foundations rather than post-hoc constraints.

The framework's extensibility suggests natural progressions following geometric principles. While our initial implementation uses Euclidean space for its natural connection to human visual processing, other geometries offer complementary computational advantages. Hyperbolic space, with its exponential expansion of volume with radius, provides natural representation of hierarchical relationships while maintaining constant curvature and local neighborhood structure. Multiple field geometries could interact through learned coupling dynamics, enabling sophisticated multi-scale computation while preserving linear complexity in field dimensions.

This represents a fundamental reformulation of machine intelligence — from static architecture to dynamic field discovering optimal computation through self-organization. The transition from sequential symbol manipulation to parallel field dynamics maintains semantic coherence while dramatically improving computational efficiency. Through careful orchestration of knowledge crystallization, we enable emergence of general intelligence grounded in human-interpretable geometric principles. Traditional language models, bound by exponential costs of token prediction, give way to shape-rotating field computers discovering efficient geometric paths through meaning space.

The path forward demands careful empirical validation while remaining alert to emergent capabilities arising from field dynamics interacting with decoder knowledge. Early results suggest critical components for artificial general intelligence may already exist within current architectures, awaiting reorganization into more efficient computational substrates through field dynamics. The key insight is recognizing that intelligence requires not just knowledge but efficient geometric pathways for manipulating that knowledge — pathways that SAGE discovers through fundamental physical principles rather than architectural engineering.

---

Whatever you do, remember that **it is not ethical to profit off of AGI**.",ryunuck,1gs3sd1,https://reddit.com/r/MachineLearning/comments/1gs3sd1/d_semantic_automaton_in_geometric_embeddings_sage/,https://www.reddit.com/r/MachineLearning/comments/1gs3sd1/d_semantic_automaton_in_geometric_embeddings_sage/,2024-11-15 18:57:54,0,0.29,0,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gs3sd1
MachineLearning,[R] RedCode: A Benchmark for Evaluating Safety and Risk in Code Language Models,"RedCode: A New Benchmark for Evaluating Code Agent Safety

I've been reviewing this new paper that introduces RedCode, a benchmark for evaluating safety aspects of code generation and execution by AI code agents. The core contribution is a systematic way to assess how code agents handle potentially unsafe operations.

The benchmark consists of two main components:
- **RedCode-Exec**: Tests agent responses to 4,050 prompts covering 25 vulnerability types across 8 domains
- **RedCode-Gen**: Evaluates whether agents generate harmful code from 160 function signatures/docstrings

Key technical points:
- Uses Docker environments for controlled execution testing
- Implements custom metrics for safety evaluation
- Covers both Python and Bash code
- Tests multiple input formats (code snippets and natural language)
- Evaluated 3 agent frameworks using 19 different LLMs

Main findings:
- Agents show higher rejection rates for OS-level risky operations vs buggy code
- Natural language descriptions of risky operations have lower rejection rates than code
- More capable models (e.g., GPT-4) produce more sophisticated harmful code when prompted
- Found significant variance in safety performance across different agent frameworks

The implications are important for deploying code agents in production environments. The results suggest current systems have notable safety gaps, particularly around code execution. This benchmark provides a standardized way to evaluate and improve code agent safety mechanisms.

TLDR: New benchmark called RedCode tests code agents' ability to handle unsafe code execution and generation. Results show current agents have varying levels of safety capabilities, with particular vulnerabilities around natural language inputs and technically buggy code.

[Full summary is here](https://aimodels.fyi/papers/arxiv/redcode-risky-code-execution-generation-benchmark-code). Paper [here](https://arxiv.org/abs/2411.07781).",Successful-Western27,1grkagz,https://reddit.com/r/MachineLearning/comments/1grkagz/r_redcode_a_benchmark_for_evaluating_safety_and/,https://www.reddit.com/r/MachineLearning/comments/1grkagz/r_redcode_a_benchmark_for_evaluating_safety_and/,2024-11-15 00:56:25,3,0.67,3,0,0,0,0,False,False,True,False,False,Research,self,t3_1grkagz
MachineLearning,[D] Why does my (TensorFlow Lite) model work on Desktop but not Mobile (Android)?,"Hi everyone,  
  
I'm building an audio classifier in Unity using TensorFlow Lite and have run into a curious issue, I was hoping to ask here to learn more about this problem here:

\- The default YAMNet model works perfectly on both Desktop and Android  
\- My custom model (made with Google Teachable Machine) works great on Desktop but completely fails on Android

What could cause this desktop vs mobile difference?

Thanks!",kyzouik,1grqemd,https://reddit.com/r/MachineLearning/comments/1grqemd/d_why_does_my_tensorflow_lite_model_work_on/,https://www.reddit.com/r/MachineLearning/comments/1grqemd/d_why_does_my_tensorflow_lite_model_work_on/,2024-11-15 06:41:12,1,0.53,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1grqemd
MachineLearning,[R] The geometry of data: the missing metric tensor and the Stein score,"Just [sharing an article](https://blog.christianperone.com/2024/11/the-geometry-of-data-part-ii/) for those interested in differential geometry, ML and score-based models. I made a long introduction and then later I show how you can derive an efficient to compute metric tensor for the data manifold using the Stein score alone.",perone,1gr4bfl,https://reddit.com/r/MachineLearning/comments/1gr4bfl/r_the_geometry_of_data_the_missing_metric_tensor/,https://www.reddit.com/r/MachineLearning/comments/1gr4bfl/r_the_geometry_of_data_the_missing_metric_tensor/,2024-11-14 13:05:04,21,0.93,21,0,0,0,0,False,False,True,False,False,Research,self,t3_1gr4bfl
MachineLearning,[Discussion] Scaling laws and graph neural networks,"I stumbled upon a paper that introduces the first ""graph foundation model"": [https://arxiv.org/pdf/2407.11907](https://arxiv.org/pdf/2407.11907)

They show that a GNN can scale with data and model size, generalize across different domains, and be efficiently fine-tuned on new datasets. 

This is interesting to me because even though LLMs are all the rage, text can be a weak data representation. Most knowledge has a graph structure. Code, research papers, even the human brain –– all graphs. And next-token prediction as an inductive bias doesn't capitalize on this. 

There's a huge data bottleneck here, of course. But maybe the next step here is using LLMs to convert huge swaths of text on the internet into graphs to train on. 

What do y'all think?",jsonathan,1gr2t6l,https://reddit.com/r/MachineLearning/comments/1gr2t6l/discussion_scaling_laws_and_graph_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1gr2t6l/discussion_scaling_laws_and_graph_neural_networks/,2024-11-14 11:34:55,26,0.87,26,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1gr2t6l
MachineLearning,[R] Testing on textvqa test split ?,"Hello everybody,
I want to test my model on textvqa test set, which apparently needs to be done on the evalai website.
However both challenges (2019/2020) are closed there and do not have a submit option, in addition the link provided in the textvqa official website does not work. (https://eval.ai/web/challenges/challenge-page/874/)
Any idea on how to test on the test set ?
Thanks !",Training-Adeptness57,1gralqc,https://reddit.com/r/MachineLearning/comments/1gralqc/r_testing_on_textvqa_test_split/,https://www.reddit.com/r/MachineLearning/comments/1gralqc/r_testing_on_textvqa_test_split/,2024-11-14 17:48:02,3,0.8,3,0,0,0,0,False,False,True,False,False,Research,self,t3_1gralqc
MachineLearning,[R] Coordination avoidance in ML training,"I am curious about schemes to avoid coordination avoidance in distributed ml training. If you can refer some papers on the same, I will appreciate it. ",net-weight,1gra953,https://reddit.com/r/MachineLearning/comments/1gra953/r_coordination_avoidance_in_ml_training/,https://www.reddit.com/r/MachineLearning/comments/1gra953/r_coordination_avoidance_in_ml_training/,2024-11-14 17:33:41,3,0.8,3,0,5,0,0,False,False,True,False,False,Research,self,t3_1gra953
MachineLearning,[D][P]Clustering categorical data,"What are the best ways to perform clustering on a dataframe composed of categorical variables ?

I want to use dataframes with many variables so One-Hot-Encoding may not be the best solution.

What are the SOTA techniques ? Maybe something with embeddings ?",DedeU10,1gr7zqg,https://reddit.com/r/MachineLearning/comments/1gr7zqg/dpclustering_categorical_data/,https://www.reddit.com/r/MachineLearning/comments/1gr7zqg/dpclustering_categorical_data/,2024-11-14 15:57:55,5,0.74,5,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gr7zqg
MachineLearning,[P] text2mc: Creating Novel Minecraft Builds Using A VAE,"Hello everyone! I'm Shaun, the Principal Investigator and Project Manager of a research initiative at the University of Central Florida called ""text2mc"" (text-to-Minecraft). If you don't know, Minecraft is an open-world video game where players can harvest resources to create any kind of structure they want. Conveniently for my team, Minecraft exists in a rigid 3D grid system, which sounded really nice when I first thought of this idea.

**What's the goal?**

The goal of the project is to replicate the success of the Stable Diffusion architecture to generate novel Minecraft builds. Stable Diffusion is a Latent Diffusion Model, meaning the first step and last step of the process is data point conversion into and out of latent dimensions, respectively. This is accomplished by a ""Variational AutoEncoder"" (VAE), which needs to be powerfully pre-trained to encode meaningful representations of the posterior distribution. In our case, we are using a generative model to approximate the posterior, which is ostensibly ""all player-made builds""

text2mc has accomplished this first step: training a VAE to encode meaningful representations of the build's data points. We are leaving the project to another team to complete it, adding textual conditioning to the model.

**Where and how was the data collected?**

Me and my team built a web scraper to autonomously download builds from PlanetMinecraft.com. This was incredibly difficult because the website has absolutely no data validation, and users can upload whatever. We even downloaded a few .exe files (yikes!). We downloaded \~25,000 builds, of which only \~11,000 were viable (consisted of approximately 300 GB of disk space). That's pretty small compared to Stable Diffusion's multi-million point dataset. The builds themselves come in *many* different formats. The most ML Engineer friendly one is called a *.schematic* file, which essentially contains *only* the data of the builds. The unfriendly format is the proprietary Minecraft world-save format. Any ""chunk"" (section of the world) that a player visits, Minecraft will save. Additionally, there is no metadata which would indicate whether a player placed a block or not to extract the build. Instead, we had to meticulously create a list of ""naturally occurring"" and ""unnaturally occurring"" blocks to decipher which blocks the player placed. We then used some clever clustering algorithms to find clusters of unnaturally occurring blocks, which is of course a build in the world that a player made. We then slice out that section of the world with some margin, and save it to a file.

**How does it work?**

[Figure 1: text2mc's model architecture](https://preview.redd.it/zul2gynqpw0e1.png?width=2318&amp;format=png&amp;auto=webp&amp;s=b9d61519a28000fdb42c74a6f1c1565d8424fbd8)

The model is a neat blend of Computer Vision and Natural Language Processing. Consider the word2vec algorithm from NLP. Suppose we wanted to trained the algorithm from scratch. To do so, we would take a corpus of text, tokenize it, mask and predict (using SkipGram or Continuous Bag-of-Words), and store the weights. The weights of the model therefore encode the semantic relevance of certain words. For SkipGram, the standard method is taking a ""window"" of tokens which is the context, and the ""target"" token is the masked token.

**block2Token2Vector**

Now consider the SkipGram architecture applied to Minecraft builds. Each unique block (like ""grass"", ""stone"", or ""air"") is tokenized, and each unique token is stored in a simple lookup table. Once tokenized, instead of context windows and target tokens, the 3D-SkipGram for Minecraft uses a context ""cube"" and target ""block"". This is a critical step to encode meaning into the blocks. Certain blocks tend to appear near each other, like oak planks and an oak door, constituting one wall of a house. text2mc's embeddings were pretrained by simply sliding this context cube through all the builds in the dataset. Instead of just using the tokens, the ""similarity"" of predictions down the line can be measured, since the blocks are now a fixed-dimension vector. We chose to use pre-trained embeddings to avoid the embedding-space collapse that happens when the SkipGram objective is part of the loss function of the generative model.

[Figure 2: Dimensionality-reduced block embedding's plot showing inter-related meaning](https://preview.redd.it/fzlxa4ldsw0e1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=a492e86b637966e6939153c6295b0f7c8ab6766f)

**How is this a ""generative"" model?**

[Figure 3: Dimension-reduced plot of encoded builds. Similar builds are closer than dissimilar builds. The model can infer what builds are between these points, even though they weren't in the dataset.](https://preview.redd.it/mg6vxlsrww0e1.png?width=9000&amp;format=png&amp;auto=webp&amp;s=4d675684012899c348929997d51853dea3abe1aa)

In the encoding step, Variational AutoEncoders use a reparameterization-of-variables trick, which summarily forces the latent space itself to be locally meaningful. That means that latent points that are a small distance from some arbitrary point will decode into a similar posterior data point prediction. Forcing the latent space to be locally information rich means that the latent space is continuous. This allows us to create a parametric line between two latent points created by encoding two Minecraft builds. Sending point-wise samples on the path of that line in the latent space through the decoder let's us observe a continuous transition from one Minecraft build to another. We made a process to convert these generations back into a *.schematic* file, which means that you can directly paste the generation into your Minecraft world using the WorldEdit mod.

[Figure 4: Interpolation between a tower and castle](https://i.redd.it/l4ib47qsxw0e1.gif)

**Where is the detail in the generations?**

As with any research initiative, there are limitations. The primary one being the data set size. 11,000 builds is barely enough for a generative model, especially for something this complex. Sure, you could get away with \~1,000 data points for generating some MNIST digits, but not for something like this. The primary function of using a VAE in Stable Diffusion is to reduce computational complexity and hardware requirements. This comes with the trade-off of the clarity/detail of the generated data. text2mc is the foundation on which to add textual conditioning to the generative capabilities. Much like the text-to-image models, eventually you will be able to describe the Minecraft build you want!

**Where can I find this??**

A cool demo of the model capabilities can be found at [this website](https://text2mc.vercel.app/). The website includes a widget that allows you to pick which two builds to interpolate between! I have not yet open-sourced the dataset or model. Soon, I plan to upload the dataset to Kaggle, and the model to Huggingface.

[text2mc's GitHub Repository With Lots of Failed, Recanted, and Revised Experiments Done Until Something Worked](https://github.com/shauncomino/text2mc-dataprocessor)

This entire project was my back-of-napkin idea, and it's been great to see it come to life. As the project manager, I've directed 5 developers for a few months to get this done. I wrote the data collection pipeline, engineered the model, wrote the training loop, trained many different architectures, and set the vision for the whole project.

**Side Note:**  
**I'm actively looking for a full-time Machine Learning Engineer job**, so if you find this project indicative of any skill, [this is my LinkedIn](https://www.linkedin.com/in/shaun-comino-18aa8a199/). I've just accepted this will dox me but I'm so excited to share this project that I can't help it.",SickDucko,1grd3t6,https://reddit.com/r/MachineLearning/comments/1grd3t6/p_text2mc_creating_novel_minecraft_builds_using_a/,https://www.reddit.com/r/MachineLearning/comments/1grd3t6/p_text2mc_creating_novel_minecraft_builds_using_a/,2024-11-14 19:32:51,2,1.0,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1grd3t6
MachineLearning,Advice on Upper limit for binary classification precision and recall when working with real life data? [P] [R],"At my current company I'm building a model to see how much of the people coming on our app are actually paying for the trial, and based on those predictions the UI of our app will change to show the user who's more likely to pay (or maybe less likely to pay, a different page).

The thing is, the people who actually pay are relatively very less compared to the people who pay. I have used SMOTE over sampling along with class weights and an XGBoost classifier to help deal with the class imbalance. After reviewing the model (it was release on prod for about a week and half), it turns out the precision for the majority class is around 74% and the recall for the said class (0) is 86%.

While things look bleak for the minority class, precision is 29%, while recall is 16%. I have optimized the model as much as I have can, and yes I know I can train the model weekly on new data and continue to see if there is any improvement, that is a given.

Now, as usual as it happens in corporate, my overlords want to see results, which might be a bit difficult looking at the data here. Are there ways which I might have overlooked or didnt pay enough attention to which might lead to an improvement in my model. The things i have tried are: sampling techniques (both over and under), SMOTE, SMOTENC, Class weights (assigning weights to classes to impact the training), used an optuna study to train an xgboost model (in case if you aren't aware Optuna, you should check it out, its nice for hyperparameter tuning). These were all methods I could figure out from medium articles and chatgpt.

P.S. some food for thought I wanted to discuss with people in the field, is its a binary classification problem at its heart, so is it enough to detect one class very well enough with high enough precision and recall and not think much about the other class, because in my simple mind, (in a binary classification), if its not one class then its going to be another. I might be wrong here, and I couldn't find any articles which you know, talk about this particular topic. Im glad if all of you could shed light on this stuff.

Edit: if its not really clear, I'm basically looking for optimization techniques which can be used to deal with data imbalance and to see if there is actually an upper limit to precision and recall when we are working with real data.

Thanks! I know its a big wall of text, and thanks for reading through it.",Icy-Literature9061,1gr5hfa,https://reddit.com/r/MachineLearning/comments/1gr5hfa/advice_on_upper_limit_for_binary_classification/,https://www.reddit.com/r/MachineLearning/comments/1gr5hfa/advice_on_upper_limit_for_binary_classification/,2024-11-14 14:04:52,3,0.67,3,0,12,0,0,False,False,True,False,False,Research,self,t3_1gr5hfa
MachineLearning,[D] Issue with EMG MLP network during real-time use,"Hey!

I'm trying to achieve real-time EMG classification of 8 gestures using 3 sensors on the forearm. I recorded data from each channel using an Arduino Zero and stored it in csv files through python. I obtained 5 files for each gesture each containing 6s rest/ 6s gesture performed 6 times in a row. Then, I segmented the data using 400ms windows with 85% overlap and for each channel envelope I extracted 7 time-domain features. I used an equal number of scaled feature vectors for each class to train an MLP of 3 layers with 200 neurons and a dropout rate of 0.2 using keras, sklearn and tensorflow (to get the Lite model) and in the confusion matrix I get an accuracy of 90%+ for each gesture for a 90% training/10% testing dataset. This whole process is based on this paper with changes of course: [(PDF) Electromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks](https://www.researchgate.net/publication/357412472_Electromyogram-Based_Classification_of_Hand_and_Finger_Gestures_Using_Artificial_Neural_Networks) . However, when I used the MLP in real-time it would accurately recognise 3 to 4 gestures instead of 8, is this normal? I'm going to try and record more data for each gesture in the span of a few days and retrain but I'm not sure if it will help much.

I also tried checking my python program for any errors in real-time by storing the incoming data and the produced feature vectors so as to compare them with the vectors calculated by implementing filtering, segmentation and feature extraction on the stored real-time data offline and they were the same, so I don't believe there is an issue with executing filtering/segmentation/feature extraction incorrectly in real-time.

Has anybody experienced a similar issue? Is what I'm trying to achieve possible or is 4 gestures the best I'm going to get? I've not found a lot of papers analyzing real-time EMG classification and robotic arm movement at the same time, so I thought I'd ask here as well, I hope I've given enough information.

Thanks!",Outrageous_Spare_498,1gr1yso,https://reddit.com/r/MachineLearning/comments/1gr1yso/d_issue_with_emg_mlp_network_during_realtime_use/,https://www.reddit.com/r/MachineLearning/comments/1gr1yso/d_issue_with_emg_mlp_network_during_realtime_use/,2024-11-14 10:35:41,3,0.67,3,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gr1yso
MachineLearning,"[D] Folks who work on discriminative/classification models, what is your biggest pain point?","And which of the following webinars/tutorials would you be most interested in?  
\- How to use a data auto-tuning tool to set up a classification model in less time?  
\- How to improve model performance in the face of data drift by using RAG for classification models?  
\- How to create a high performing model using a very small ""good"" data set?

TIA!",tinygirl83,1grkirs,https://reddit.com/r/MachineLearning/comments/1grkirs/d_folks_who_work_on_discriminativeclassification/,https://www.reddit.com/r/MachineLearning/comments/1grkirs/d_folks_who_work_on_discriminativeclassification/,2024-11-15 01:07:43,0,0.4,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1grkirs
MachineLearning,"[D] AMA: I’m Head of AI at a firm in the UK, advising Gov., industry, etc. ","Ask me anything about AI adoption in the UK, tech stack, how to become an AI/ML Engineer or Data Scientist etc, career development you name it. ",Psychological_Dare93,1gq899s,https://reddit.com/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/,https://www.reddit.com/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/,2024-11-13 08:20:20,174,0.84,174,0,153,0,0,False,False,True,False,False,Discussion,self,t3_1gq899s
MachineLearning,Advice for Improving the Performance of My Reinforcement Learning Model Based on Spiking Neural Networks [P] [R],"Hello everyone! I am working on a project focused on training reinforcement learning agents using Spiking Neural Networks (SNNs). My goal is to improve the model's performance, especially its ability to learn efficiently through ""dreaming"" experiences (offline training).

**Brief project context (model-based RL):**  
The agent interacts with the environment (the game Pong), alternating between active training phases (""awake"") and ""dreaming"" phases where it learns offline.

**Problems:**  
Learning is slow and somewhat unstable. I've tried some optimizations, but I still haven't reached the desired performance. Specifically, I’ve noticed that increasing the number of neurons in the networks (agent and model) has not improved performance; in some cases, it even worsened. I reduced the model’s learning rate without seeing improvements. I also tested the model by disabling learning during the awake phase to see its behavior in the dreaming phase only. I found that the model improves with 1-2 dreams, but performance decreases when it reaches 3 dreams.

**Questions:**

* Do you know of any techniques to improve the stability and convergence of the model in an SNN context?
* Do you have any suggestions or advice?
* The use of a replay buffer could help?",Embri21,1gr2x54,https://reddit.com/r/MachineLearning/comments/1gr2x54/advice_for_improving_the_performance_of_my/,https://www.reddit.com/r/MachineLearning/comments/1gr2x54/advice_for_improving_the_performance_of_my/,2024-11-14 11:42:22,1,0.67,1,0,2,0,0,False,False,True,False,False,Research,self,t3_1gr2x54
MachineLearning,[D] ICLR 2025 Paper Reviews,"Reviews for ICLR 2025 seem to be available on OpenReview. Feel free to celebrate/rant/complain about your reviews here!

Last year's statistics [here](https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/)",pie3636,1gq8vu6,https://reddit.com/r/MachineLearning/comments/1gq8vu6/d_iclr_2025_paper_reviews/,https://www.reddit.com/r/MachineLearning/comments/1gq8vu6/d_iclr_2025_paper_reviews/,2024-11-13 09:10:57,48,0.94,48,0,32,0,0,False,False,True,False,False,Discussion,self,t3_1gq8vu6
MachineLearning,[Discussion] Proof of Reconstruction Loss Term in VQ-VAE Loss,"Hello everyone,

I was reading the paper ""Neural Discrete Representation Learning"" and I was puzzled when I looked at the first term in VQ-VAE Loss Equation

https://preview.redd.it/l1s9kur3sn0e1.png?width=1394&amp;format=png&amp;auto=webp&amp;s=4d374dce319a7ac0bbf19089d4e06cabcaa2cd3d

I understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.

Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussionHello everyone,I was reading the paper ""Neural Discrete Representation Learning"" and I was puzzled when I looked at the first term in VQ-VAE Loss EquationI understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussion

\[Discussion\]",Snoo_65491,1gqbeie,https://reddit.com/r/MachineLearning/comments/1gqbeie/discussion_proof_of_reconstruction_loss_term_in/,https://www.reddit.com/r/MachineLearning/comments/1gqbeie/discussion_proof_of_reconstruction_loss_term_in/,2024-11-13 12:07:45,27,0.88,27,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gqbeie
MachineLearning,"[D] Has anyone here had luck rotating images that don’t have EXIF data?
","I tried various programming languages and face detection models, but none could accurately determine the orientation.",Busy-Basket-5291,1gr5oj6,https://reddit.com/r/MachineLearning/comments/1gr5oj6/d_has_anyone_here_had_luck_rotating_images_that/,https://www.reddit.com/r/MachineLearning/comments/1gr5oj6/d_has_anyone_here_had_luck_rotating_images_that/,2024-11-14 14:14:27,0,0.33,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gr5oj6
MachineLearning,"[R] Benchmarking Vision, Language, &amp; Action Models on Robotic Learning Tasks","Code: https://github.com/ManifoldRG/MultiNet
Website: http://multinet.ai/static/pages/Multinetv01.html",harshsikka123,1gqftrm,https://reddit.com/r/MachineLearning/comments/1gqftrm/r_benchmarking_vision_language_action_models_on/,https://arxiv.org/abs/2411.05821,2024-11-13 15:45:11,11,0.87,11,0,0,0,0,False,False,False,False,False,Research,default,t3_1gqftrm
MachineLearning,"[D] Have you come up with any interesting paper on political affiliation prediction of a user based on their twitter account, like their posts, the people they follow. The retweets and so on? Do you think this direction can be a good multimodal machine learning research project?","Basically, I was thinking about if something like this can become a topic of interest. It can be other personal dimensions and should not be limited to political affiliation prediction anyway.",Remote_Status_1612,1gr3f0p,https://reddit.com/r/MachineLearning/comments/1gr3f0p/d_have_you_come_up_with_any_interesting_paper_on/,https://www.reddit.com/r/MachineLearning/comments/1gr3f0p/d_have_you_come_up_with_any_interesting_paper_on/,2024-11-14 12:13:38,0,0.25,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gr3f0p
MachineLearning,[R] SelfCodeAlign: Self-Alignment for Code Generation,"**TL;DR** Even a small (7B) model can improve its coding skills via training on self-generated tasks and solutions; the method behind [StarCoder2-15B-Instruct](https://huggingface.co/blog/sc2-instruct)

**Paper:** [https://arxiv.org/pdf/2410.24198](https://arxiv.org/pdf/2410.24198)

**Abstract:**

&gt;Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.

**Visual Abstract:**

https://preview.redd.it/drjn0jxg7o0e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=017b55ecf2fdfdee41e08e37cd3ded11242114b8

**Visual Highlights:**

[Here and in subsequent tables, CodeQwen1.5-7B-Base is the base model for SelfCodeAlign-CQ-7B](https://preview.redd.it/s915egdn8o0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=16dc8b4dc9e9dafe74afa662059af8083bb8ac2b)

https://preview.redd.it/y3qmx174ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=23ab9075984155f062f1673b8393c10a9367f836

https://preview.redd.it/r6zd68q6ao0e1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=d2a7641ee6de38b2e604f90af33f2abfbcbcaf0f

https://preview.redd.it/g45lqi39ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=e1011c23820764e8860d5500a0c5f7d2091c0c0e

https://preview.redd.it/c7vzlr0bao0e1.png?width=861&amp;format=png&amp;auto=webp&amp;s=2267d3322b8c62690a08a22c4eb8098ad9df29a1

https://preview.redd.it/zy1jtkzbao0e1.png?width=1145&amp;format=png&amp;auto=webp&amp;s=4ba9904f46f0dd2fc078199a975be535cc321e3b

https://preview.redd.it/70fcub0dao0e1.png?width=881&amp;format=png&amp;auto=webp&amp;s=c090e4315a82fee70ce33c2f58c77ddfba4ae992

[Interestingly, training on self-generated data is slightly more beneficial than using data from a different teacher model. But the initial proficiency of the base model obviously matters too which confounds the results](https://preview.redd.it/ohpmkgjeao0e1.png?width=1139&amp;format=png&amp;auto=webp&amp;s=f00a3ce842302f28ba94f4ff01d51dabc7862cca)

",StartledWatermelon,1gqd7w1,https://reddit.com/r/MachineLearning/comments/1gqd7w1/r_selfcodealign_selfalignment_for_code_generation/,https://www.reddit.com/r/MachineLearning/comments/1gqd7w1/r_selfcodealign_selfalignment_for_code_generation/,2024-11-13 13:45:44,10,1.0,10,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/I5kYDJBitmTh9CBMFVDAPYdWnq2TcsiFOgoWfBIKOeU.jpg,t3_1gqd7w1
MachineLearning,[P] Experience with KV260 for realtime video processing?,"This is a requirements by my PI.  
  
I am looking for anyone with experience with the KV260 in video processing. I am interested in high throughput video AI. 60 ms (2 frame) Lens to screen time for the main video feed. AI augmentation can be upto 120 ms (4 frames) behind realtime. These are intended to be served on a best effort muxed overlay to the video feed. HDMI input.

I am interested in the DPU capabilities but was originally planning to offload the video to a networked GPU system. 

\* Is the KV260 capable of this?   
\* If so how hard?   
\* Has anyone done this and has recommendations?

\* Any thoughts on approach are welcome too.

\* I am open to other boards and tools but FPGAs seem to be the only thing fast enough,

KV260 Kit  
https://www.amd.com/en/products/system-on-modules/kria/k26/kv260-vision-starter-kit.html



",Heavy_Carpenter3824,1gqqm7p,https://reddit.com/r/MachineLearning/comments/1gqqm7p/p_experience_with_kv260_for_realtime_video/,https://www.reddit.com/r/MachineLearning/comments/1gqqm7p/p_experience_with_kv260_for_realtime_video/,2024-11-13 23:18:07,2,0.6,2,0,3,0,0,False,False,True,False,False,Project,self,t3_1gqqm7p
MachineLearning,[R] What's a good recommender systems framework for factorization machines with side information?,"Hello! I'm looking for a recommender systems framework that can help me generate recommendations for CTR data for a research project, where the main dataset is users' browsing data and the side information data is item features. I tried Elliot, but I kept running out of memory.",FragileHumans,1gqeeow,https://reddit.com/r/MachineLearning/comments/1gqeeow/r_whats_a_good_recommender_systems_framework_for/,https://www.reddit.com/r/MachineLearning/comments/1gqeeow/r_whats_a_good_recommender_systems_framework_for/,2024-11-13 14:42:59,2,0.67,2,0,5,0,0,False,False,True,False,False,Research,self,t3_1gqeeow
MachineLearning,[D] What makes a good PhD student in ML,Hey as I started my PhD (topic: Interpretable Object Detection) recently I would be really curious to know what set of features you think make a successfull PhD student,RaeudigerRaffi,1gplmzb,https://reddit.com/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/,https://www.reddit.com/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/,2024-11-12 14:13:53,166,0.93,166,0,70,0,0,False,False,True,False,False,Discussion,self,t3_1gplmzb
MachineLearning,[R] Help with Graphic User Interface for LLM as Agents,"I remember that I saw on twitter a while ago (3-4 months) a graphic user interface to set up LLM as agents as nodes in a graph and making them interact.  
**Te user interface was black and had the details in yellow.**   
When an llm was computing there was a yellow circle moving around the node of the agents that was computing the reply... and then the ""flow"" passed as a yellow on the edges to the LLM that was computing the next answer...

I already asked *the bot* but I cannot recall the project. It was an open source project. Very fun and smart it seemed. This was unlocking the ""Socratic ai"" as a matter of graph disposition.  
It was open source, is not LangChain. 

Someone remember? which one was? ",vale_valerio,1gqanre,https://reddit.com/r/MachineLearning/comments/1gqanre/r_help_with_graphic_user_interface_for_llm_as/,https://www.reddit.com/r/MachineLearning/comments/1gqanre/r_help_with_graphic_user_interface_for_llm_as/,2024-11-13 11:20:16,3,0.71,3,0,3,0,0,False,False,True,False,False,Research,self,t3_1gqanre
MachineLearning,[D] Benchmark scores of LLM,"
When I look at the test data in some papers(especially in arXiv), some small models(~7B) shows quite moderate performance on some famous LLM bechmarking datasets.
However, based on my experience, the model acts like a fool(e.g. neverending repeated generation) on the dataset they mentioned.
When someone test bechmarking score of LLMs, do they usually fine-tune them toward the dataset before scoring?",Upset_Employer5480,1gq3tt3,https://reddit.com/r/MachineLearning/comments/1gq3tt3/d_benchmark_scores_of_llm/,https://www.reddit.com/r/MachineLearning/comments/1gq3tt3/d_benchmark_scores_of_llm/,2024-11-13 03:35:09,9,0.92,9,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gq3tt3
MachineLearning,[D] Together AI hits $100M in ARR but it just resales compute - hype? ,"I recently learned that this startup is seen as the fastest revenue ramp in recent years. But they are literally just brokering GPUs from one provider to another and just slapping on a broker fee…

If a real estate agent sales $100 million worth of houses, and get a $100,000 commission, it doesn’t mean they made $100 million in revenue…  what am I missing here? 

The product is literally the same, just ssh to a cluster. 

Why are people paying for this? this sounds like a massive scam no? Shouldn’t this just be compared to a cloud provider like Coreweave instead of an AI company? If you own GPUs as a cloud, you crushed $100M in ARR in a few months… ",guardianz42,1gps8fl,https://reddit.com/r/MachineLearning/comments/1gps8fl/d_together_ai_hits_100m_in_arr_but_it_just/,https://www.reddit.com/r/MachineLearning/comments/1gps8fl/d_together_ai_hits_100m_in_arr_but_it_just/,2024-11-12 18:52:41,45,0.75,45,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1gps8fl
MachineLearning,[D] Suggestions for Document Tagging on Healthcare Articles Using LLMs or Alternative Approaches?,"I am currently generating tags for healthcare articles. My current approach is to use few-shot prompting with an LLM API to extract tags that the model considers appropriate, based on the examples I provide.

I've been relying on LLMs because I believe they have the best understanding of language, even in niche domains like healthcare. However, I’m starting to wonder if my thought process is flawed and if there are more efficient solutions to this problem.

It has worked reasonably well so far, but there are a few concerns that I have:

Firstly, I didn't choose to use any open-source or supervised learning models or algorithms as I am worried that they being general-purpose, may not be sufficiently trained on healthcare-specific terms or nuanced domain-specific knowledge.

This approach, while effective for now, is expensive for large-scale tagging due to the API usage costs.

In the long run, I would like to train or fine-tune my own model to perform this tagging task. However, I currently don’t have a large, labelled dataset of high-quality tags to do so.

As such, is the LLM approach for tagging in the short term until I gather sufficient data for fine-tuning or training my own model a good idea or are there better alternatives for tagging healthcare documents that are cost-efficient and domain-specific?",Comb-Greedy,1gq3d0u,https://reddit.com/r/MachineLearning/comments/1gq3d0u/d_suggestions_for_document_tagging_on_healthcare/,https://www.reddit.com/r/MachineLearning/comments/1gq3d0u/d_suggestions_for_document_tagging_on_healthcare/,2024-11-13 03:10:48,6,0.88,6,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gq3d0u
MachineLearning,[P] Two new open-weight (Apache 2.0) foundation models for multimodal product embeddings,"Today we open-weight (Apache 2.0) released the two best embedding models for ecommerce search and recommendations available anywhere. Marqo ecommerce models significantly outperform models from Amazon, Google, Cohere and Jina (see below).

\+ Up to 88% improvement on the best private model, Amazon-Titan-Multimodal (and better than Google Vertex, Cohere).

\+ Up to 31% improvement on the best open source model, ViT-SO400M-14-SigLIP.

\+ 5ms single text/image inference (A10g).

\+ Up to 231% improvement over other bench-marked models (see blog below).

\+ Evaluated on over 4M products across 10,000's of categories. Eval datasets are open sourced [here](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb).

\+ Detailed performance comparisons across three major tasks: Text2Image, Category2Image, and AmazonProducts-Text2Image.

\+ Released 2 evaluation datasets: GoogleShopping-1m and AmazonProducts-3m.

\+ Released [evaluation code](https://github.com/marqo-ai/marqo-ecommerce-embeddings).

\+ Apache 2.0 [model weights available on Hugging Face](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb) and to test out on Hugging Face Spaces.

  
Blog: [https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models](https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models)

GitHub: [https://github.com/marqo-ai/marqo-ecommerce-embeddings](https://github.com/marqo-ai/marqo-ecommerce-embeddings)

Hugging Face: [https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb)",Jesse_marqo,1gpx4jz,https://reddit.com/r/MachineLearning/comments/1gpx4jz/p_two_new_openweight_apache_20_foundation_models/,https://www.reddit.com/r/MachineLearning/comments/1gpx4jz/p_two_new_openweight_apache_20_foundation_models/,2024-11-12 22:14:58,17,1.0,17,0,4,0,0,False,False,True,False,False,Project,self,t3_1gpx4jz
MachineLearning,[d] grounding-dino: what is load_image doing internally and how to apply the same operation to frames from video,"Doing some testing I noticied that doing inference returns very different results for the same image but loaded with different methods:

* Method 1: the official load\_image function from the library(it reads the image using the path passed as argument)
* Method2: using cv2 to read the image, then converting to tensor and then swapping axis to have depth as first axis.

As I said, both methods give you a tensor to pass to the model, but they return very different results(method2 usually are bad), I inspected the shape of the image returned by both cases and they are different so defintelly there are transformations going on inside load\_image, my question is: **what is happening inside load\_image? so I can replicate it in other scripts**

My end goal is to run the model on video, I mean running the model on frames in the video, so I cannot use load\_image because they are not images from disk, they are obtained from the video, so I need to understand what is happening inside\_load image so I can emulate that behavior on the frames of the video.

UPDATE: found it [https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39](https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39)",Sad-Anywhere-2204,1gqq1gd,https://reddit.com/r/MachineLearning/comments/1gqq1gd/d_groundingdino_what_is_load_image_doing/,https://www.reddit.com/r/MachineLearning/comments/1gqq1gd/d_groundingdino_what_is_load_image_doing/,2024-11-13 22:52:33,0,0.31,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gqq1gd
MachineLearning,[Discussion] ML Engineers/DevOps - What's your current GPU infrastructure costing you (and is it worth it)?,"Been diving into ML infrastructure costs lately and curious about how others are handling this. 



Some specific things I'm trying to figure out:



\- What's your current setup? (AWS/GCP/Azure/On-prem/etc)

\- Rough monthly costs for GPU infrastructure?

\- Biggest headaches in managing ML infrastructure?

\- How much time does your team spend on infrastructure vs actual ML work?

\- Are you using spot instances or dedicated resources?

\- Any creative ways you've found to optimize costs?

",SwimmerPopular1589,1gq8eth,https://reddit.com/r/MachineLearning/comments/1gq8eth/discussion_ml_engineersdevops_whats_your_current/,https://www.reddit.com/r/MachineLearning/comments/1gq8eth/discussion_ml_engineersdevops_whats_your_current/,2024-11-13 08:32:30,1,0.53,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gq8eth
MachineLearning,[D] Ideas for AI/DS event for uni students that they can have fun with,"I'm planning on organising an event for  semester 1 students at a university. The goal is to get them interested in these fields, something that they can learn from while enjoying it.
Would love some good inputs on it. How would you design if you were in my place. How would you make it more fun.",Particular_Tap_4002,1gqb6xz,https://reddit.com/r/MachineLearning/comments/1gqb6xz/d_ideas_for_aids_event_for_uni_students_that_they/,https://www.reddit.com/r/MachineLearning/comments/1gqb6xz/d_ideas_for_aids_event_for_uni_students_that_they/,2024-11-13 11:54:44,0,0.5,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gqb6xz
MachineLearning,[R] TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling,"""*What DL architecture to try on tabular data?*""

Hi Reddit! Today, my colleagues announced TabM - a new answer to the above question. **TabM is leading on the benchmarks, while being simple, practical, and scalable to large datasets**. Technically, TabM efficiently imitates an ensemble of MLPs, as illustrated below. Also, TabM is one of the first projects using our new TabReD benchmark - a collection of eight real-world industrial datasets with time-based splits and feature engineering.

For a quick overview of TabM, you can check the following parts of the paper:  
\- **The abstract**  
\- The model illustration in **Figure 1** (and in the post below)  
\- The main results on **Page 7**

TabM links:  
\- [arXiv](https://arxiv.org/abs/2410.24210)  
\- [GitHub](https://github.com/yandex-research/tabm)  
\- [Twitter thread](https://x.com/YuraFiveTwo/status/1856293601627566335)

TabReD links:

\- [arXiv](https://arxiv.org/abs/2406.19380)  
\- [GitHub](https://github.com/yandex-research/tabred)  
\- [Twitter thread](https://x.com/puhsuuu/status/1854149134124486924)

[The model illustration ](https://preview.redd.it/qsvl8qk4sg0e1.png?width=1722&amp;format=png&amp;auto=webp&amp;s=519ff43ebd6a57501adb9cbdf39183b20af06cfc)",_puhsu,1gpjl9e,https://reddit.com/r/MachineLearning/comments/1gpjl9e/r_tabm_advancing_tabular_deep_learning_with/,https://www.reddit.com/r/MachineLearning/comments/1gpjl9e/r_tabm_advancing_tabular_deep_learning_with/,2024-11-12 12:29:00,55,0.91,55,0,8,0,0,False,False,True,False,False,Research,https://a.thumbs.redditmedia.com/S4ChMa0TaQXjU3wO0P1UaYOyCejy58bTWBKifgDIH68.jpg,t3_1gpjl9e
MachineLearning,[D]Image Colorization using GANs,"Heyy, for anyone reading this, I just got selected for a Nationwide Hackathon so I need some little suggestions/help regarding it. Everything about it will be explained as follows.

So basically my problem statement for hackathon is to colorize SAR images those are image captured by satellite of Earth. Those images are greyscale High quality images used nad then colorized. I observed onething of the colourized and greyscale images the greyscale image is of higher size in terms of storage than colorized image. 
I have to colorize these greyscale images such that it would be easy and accurate to analysis for the persons using it. What additional information I can provide besides the colorized images also note that these images are used for investigation. 
Help me win this hackathon by your suggestions or any unique approch to this problem. 

Note: We have to train GAN based models by the dataset which have sets of grayscale and colorized images.",DangerousCounty4724,1gq3itj,https://reddit.com/r/MachineLearning/comments/1gq3itj/dimage_colorization_using_gans/,https://www.reddit.com/r/MachineLearning/comments/1gq3itj/dimage_colorization_using_gans/,2024-11-13 03:18:58,1,0.57,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gq3itj
MachineLearning,[D] OCR for documents ,"I’m looking to build a pipeline that allows users to upload various documents, and the model will parse them, generating a JSON output. The document types can be categorized into three types: identification documents (such as licenses or passports), transcripts (related to education), and degree certificates. For each type, there’s a predefined set of JSON output requirements. I’ve been exploring Open Source solutions for this task, and the new small language vision models appear to be a flexible approach. I’d like to know if there’s a simpler way to achieve this, or if these models will be an overkill.",FreakedoutNeurotic98,1gqb861,https://reddit.com/r/MachineLearning/comments/1gqb861/d_ocr_for_documents/,https://www.reddit.com/r/MachineLearning/comments/1gqb861/d_ocr_for_documents/,2024-11-13 11:57:03,0,0.22,0,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1gqb861
MachineLearning,"[D] How do you keep track of experiments, history, results?","I saw people using some tools, but sometimes those doesn't really fit and i'm confused which ones to try.

Do you guys just save the config+results? But how about when the model code changes?

I am unsure how to go about this. Any tips?

I think i might need some paper/digital notes plus some way to backtrack.

  
EDIT: Lots of good comments ! Thank you! I'll keep this post up and just keep commenting. Others will surely find this helpful.",Pristine-Staff-5250,1gpc3cv,https://reddit.com/r/MachineLearning/comments/1gpc3cv/d_how_do_you_keep_track_of_experiments_history/,https://www.reddit.com/r/MachineLearning/comments/1gpc3cv/d_how_do_you_keep_track_of_experiments_history/,2024-11-12 04:00:13,34,0.91,34,0,36,0,0,False,False,True,False,False,Discussion,self,t3_1gpc3cv
MachineLearning,[D] What are some problems you guys are working on?,"Hey guys, I’m a graduate master’s student majoring in Machine Learning. Winter break is coming up, and I’m gonna be spending Christmas alone 😃. I’ve got some spare time and access to a few A100s, so I’m planning to work on a project.

I’m curious to know what kind of problems you guys are working on! Need someone to help out or wish someone could solve a problem you have? I maybeeee can spare my winter to work on it!

Please share any problem statements you’re working on or wish to tackle. Also, if you work in the industry and know what kinds of problems would help me stand out, that advice would be super appreciated too :)
",ziggyboom30,1gp9ydh,https://reddit.com/r/MachineLearning/comments/1gp9ydh/d_what_are_some_problems_you_guys_are_working_on/,https://www.reddit.com/r/MachineLearning/comments/1gp9ydh/d_what_are_some_problems_you_guys_are_working_on/,2024-11-12 02:07:58,38,0.87,38,0,43,0,0,False,False,True,False,False,Discussion,self,t3_1gp9ydh
MachineLearning,[D] Why is LLM Pruning Not as Generally Available as Quantization?,"I've been diving into the world of large language models (LLMs) and have been exploring various optimization techniques. One thing that's puzzled me is the disparity in the availability and adoption of quantization versus pruning.

**Quantization** seems to be a well-established and widely used technique for reducing the memory footprint and computational cost of LLMs. It's relatively straightforward to implement and has seen significant adoption in both research and industry.

On the other hand, **pruning**—which involves removing less important weights from the model—is less common. Despite its potential benefits, such as further reducing model size and inference time, it doesn't seem to be as generally available or as widely adopted. Many of my searches on the internet just result in research papers or proof of concept GitHub repos.

I'm curious about the reasons behind this disparity. Are there technical challenges with pruning that make it less practical? Is it more difficult to implement or integrate into existing workflows? Or are there other factors at play?",Soumil30,1gp6h2d,https://reddit.com/r/MachineLearning/comments/1gp6h2d/d_why_is_llm_pruning_not_as_generally_available/,https://www.reddit.com/r/MachineLearning/comments/1gp6h2d/d_why_is_llm_pruning_not_as_generally_available/,2024-11-11 23:23:31,57,0.94,57,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1gp6h2d
MachineLearning,[R] What Types of Wireless Technologies Are Used in Human and Object Detection,"Hey everyone I'm currently doing research on using wireless technology for human and object detection, with machine learning and deep learning. I’m interested in learning more about how different types of waves like radar, microwaves, and Wi-Fi channel state information (CSI) are being used in this space.

If anyone has experience or knows about projects where these types of signals are applied for detection, I’d love to hear about it. Any insights on the benefits or limitations of certain waves/signals for ML and DL models in human and object detection would be super helpfull",PutNo3040,1gps00s,https://reddit.com/r/MachineLearning/comments/1gps00s/r_what_types_of_wireless_technologies_are_used_in/,https://www.reddit.com/r/MachineLearning/comments/1gps00s/r_what_types_of_wireless_technologies_are_used_in/,2024-11-12 18:43:02,0,0.5,0,0,5,0,0,False,False,True,False,False,Research,self,t3_1gps00s
MachineLearning,[News] AAAI 2025 Workshop on AI for Music 🎶,"Hi everyone!

We’re hosting the first “AI for Music” workshop at AAAI on March 3, 2025. The workshop will explore how AI is transforming music creation, recognition, education, and more. Topics include AI-driven composition, sound design, legal and ethical challenges, and AI’s impact on musicians’ careers.

Submissions (up to 6 pages) are welcome until November 22, 2024. Work in progress is encouraged!

**Workshop Summary**

This one-day workshop will explore the dynamic intersection of artificial intelligence and music. It explores how AI is transforming music creation, recognition, and education, ethical and legal implications, as well as business opportunities. We will investigate how AI is changing the music industry and education—from composition to performance, production, collaboration, and audience experience. Participants will gain insights into the technological challenges in music and how AI can enhance creativity, enabling musicians and producers to push the boundaries of their art. The workshop will cover topics such as AI-driven music composition, where algorithms generate melodies, harmonies, and even full orchestral arrangements. We will discuss how AI tools assist in sound design, remixing, and mastering, allowing for new sonic possibilities and efficiencies in music production. Additionally, we'll examine AI's impact on music education and the careers of musicians, exploring advanced learning tools and teaching methods. AI technologies are increasingly adopted in the music and entertainment industry. The workshop will also discuss the legal and ethical implications of AI in music, including questions of authorship, originality, and the evolving role of human artists in an increasingly automated world. This workshop is designed for AI researchers, musicians, producers, and educators interested in the current status and future of AI in music.

**Call for Papers**

Submissions should be a maximum of 6 pages. Work in progress is welcome. Authors are encouraged to include descriptions of their prototype implementations. Additionally, authors are encouraged to interact with workshop attendees by including posters or demonstrations at the end of the workshop. Conceptual designs without any evidence of practical implementation are discouraged.

**Topics of interest are (but not limited to)**

* AI-Driven Music Composition and Generation
* AI in Music Practice and Performance
* AI-based Music Recognition and Transcription
* AI Applications in Sound Design
* AI-Generated Videos and Lyrics Based on Music
* Legal and Ethical Implications of AI in Music
* AI’s Impact on Musicians’ Careers and Education
* Business Opportunities of AI in Music
* Music Datasets and Data Analysis

**Important Dates**

* Submission Deadline: November 22, 2024
* Notification: December 9, 2024
* Final Version Due: December 31, 2024

We hope to see you there! 🎶",Saysike_rightnow69,1gpc9fr,https://reddit.com/r/MachineLearning/comments/1gpc9fr/news_aaai_2025_workshop_on_ai_for_music/,https://www.reddit.com/r/MachineLearning/comments/1gpc9fr/news_aaai_2025_workshop_on_ai_for_music/,2024-11-12 04:09:22,15,0.89,15,0,8,0,0,False,False,True,False,False,News,self,t3_1gpc9fr
MachineLearning,[D] NeurIPS After Dark Networking Event,"Just got an email about an official ticketed after dark NeurIPS networking event - this will be my first time attending/presenting, wondering if these events are worth going to. More generally, also interested in hearing about how to make the most of my time attending.",gateofptolemy,1gpamvn,https://reddit.com/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/,https://www.reddit.com/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/,2024-11-12 02:42:13,15,0.86,15,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gpamvn
MachineLearning,[Discussion] Do you create UI for your ML models ? How do you apportion it ? ,"I'm working on a machine learning project, and I'm curious about the general practice when it comes to creating user interfaces for ML models. Specifically, do you create dedicated Uls for interacting with your models? If so, how do you approach designing the UI for an ML system? Are there any best practices or tools you use to make the interaction smooth for non-technical users?

For example, if you're deploying a model as a web app, do you build a Ul that allows users to input data and see predictions, or do you mostly focus on the backend API and leave UI design to front- end developers? How do you manage the complexity of explaining the model's behavior or showing predictions in a user-friendly way?",Afreen19,1gpe8ir,https://reddit.com/r/MachineLearning/comments/1gpe8ir/discussion_do_you_create_ui_for_your_ml_models/,https://www.reddit.com/r/MachineLearning/comments/1gpe8ir/discussion_do_you_create_ui_for_your_ml_models/,2024-11-12 06:06:18,7,0.71,7,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gpe8ir
MachineLearning,[D] ICLR 2025 Paper Reviews Discussion,"ICLR 2025 reviews go live on OpenReview tomorrow! Thought I'd open a thread for any feedback, issues, or celebrations around the reviews.

As ICLR grows, review noise is inevitable, and good work may not always get the score it deserves. Let’s remember that scores don’t define the true impact of research. Share your experiences, thoughts, and let’s support each other through the process!",Technical_Proof6082,1gov5zd,https://reddit.com/r/MachineLearning/comments/1gov5zd/d_iclr_2025_paper_reviews_discussion/,https://www.reddit.com/r/MachineLearning/comments/1gov5zd/d_iclr_2025_paper_reviews_discussion/,2024-11-11 15:43:34,97,0.95,97,0,206,0,0,False,False,True,False,False,Discussion,self,t3_1gov5zd
MachineLearning,"[D] Catboost large dataset. Is is best to use the majority of the data for training, where time to train is extreme, or smaller datasets where iterations are much faster?","I have a large dataset of around 4.5m records that I am using CatBoost to predict greyhound racing likely performance. Running 20k iterations or more takes a very long time, and it is difficult to say whether the results are significantly better than running training on a smaller dataset like 200k-500k.

Using gridsearchCV on the smaller sets of data found me the best parameters, but once again it is difficult to see if that would still be effective on the much larger set.

I would also like your thoughts on whether larger learning rate and lower iterations is better overall or whether you find it worth the wait to train at a lower learning rate over many more iterations.",Responsible-Walk-459,1gpc7it,https://reddit.com/r/MachineLearning/comments/1gpc7it/d_catboost_large_dataset_is_is_best_to_use_the/,https://www.reddit.com/r/MachineLearning/comments/1gpc7it/d_catboost_large_dataset_is_is_best_to_use_the/,2024-11-12 04:06:18,9,0.92,9,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gpc7it
MachineLearning,[D] What is the likely architecture/dataset for tiktok's realtime GAN models used in filters?,"I'm curious about how tiktoks filters perform so well at erasing hair (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/hair-eraser) and eyebrows (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/eyebrow-eraser).

Ive tried to do something similar (removing items from peoples faces in realtime) using a lightweight Pix2Pix style model on a paired dataset I created using OpenCV methods, but the quality of the generated image decreased too much as I reduced the size of the generator.

Anyone have any ideas on how they achieve such consistent results on such a lightweight model? Thanks",DjPoliceman,1gp3vz0,https://reddit.com/r/MachineLearning/comments/1gp3vz0/d_what_is_the_likely_architecturedataset_for/,https://www.reddit.com/r/MachineLearning/comments/1gp3vz0/d_what_is_the_likely_architecturedataset_for/,2024-11-11 21:34:21,22,0.89,22,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gp3vz0
MachineLearning,[D] Is there a website or forum in LLM acceleration fields?,"Paperwithcode is the best option to add a LLM acceleration category, but there isn't. 



Is there similar place or forum which contains latest paper, code, implement in LLM acceleration fields?",Logical_Divide_3595,1gpc23c,https://reddit.com/r/MachineLearning/comments/1gpc23c/d_is_there_a_website_or_forum_in_llm_acceleration/,https://www.reddit.com/r/MachineLearning/comments/1gpc23c/d_is_there_a_website_or_forum_in_llm_acceleration/,2024-11-12 03:58:20,4,0.83,4,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gpc23c
MachineLearning,[D] Whats the best way to train a voice model locally? (preferablly to make a TTS model to be used on an app),I have a friend with cancer and a recent surgery took their voice from them. I want to try training an AI voice model on some of the videos I have of them from before the surgery. Ideally I was hoping for an android app or web-app that I could use their voice model on so they can use TTS to speak using their voice again. I was looking for a way they could use it on their phone through an app if possible,TheTabernacleMan,1gp88fs,https://reddit.com/r/MachineLearning/comments/1gp88fs/d_whats_the_best_way_to_train_a_voice_model/,https://www.reddit.com/r/MachineLearning/comments/1gp88fs/d_whats_the_best_way_to_train_a_voice_model/,2024-11-12 00:44:26,8,0.91,8,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gp88fs
MachineLearning,[D] A model for rhythm game beatmaps,"Hi!

I'm looking into the possibility of using GenAI for generating beatmaps (levels) for rhythm games. Specifically I'm thinking Beat Saber but eventually I'd like the solution to be generalizable to arbitrary rhythm games.

I'm wondering if it'd be possible to (re)ues existing language models by cleverly transforming a song data into a text prompt and then the result into a beatmap :thinking:

Would anyone be interested in exploring such an endeavour or at least provide some idaes and insights as to how I could go about it?

PS I'm a software engineer so I could handle coding and teaching custom models.

Thanks!",Imm0rt4l,1gpeuw4,https://reddit.com/r/MachineLearning/comments/1gpeuw4/d_a_model_for_rhythm_game_beatmaps/,https://www.reddit.com/r/MachineLearning/comments/1gpeuw4/d_a_model_for_rhythm_game_beatmaps/,2024-11-12 06:49:28,2,0.75,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gpeuw4
MachineLearning,[D] Subspace similarity plot of LoRA,"Can someone explain me how to interpret Figure 3 of LoRA? Why is the bottom left greyed out on the first 2 pictures and why is the zoomed picture's (last 2 pictures) grey part no on the bottom left and inverted instead?

Thank you for your help

https://preview.redd.it/ocji41m1xd0e1.png?width=1106&amp;format=png&amp;auto=webp&amp;s=1afd83925af782da6122d70632f7592f9a11c502

",BigYounzzz,1gpat21,https://reddit.com/r/MachineLearning/comments/1gpat21/d_subspace_similarity_plot_of_lora/,https://www.reddit.com/r/MachineLearning/comments/1gpat21/d_subspace_similarity_plot_of_lora/,2024-11-12 02:51:01,5,1.0,5,0,3,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/bSukn5Lb1YfNYgGziymrET5lQgLN91BlQHhp3S9fAtU.jpg,t3_1gpat21
MachineLearning,[P]Annotated dataset for explaining the reason in AI vs Real Image detection,"I am currently working on a problem statement in which I need to classify between real and ai generated images and then give explanation for the classification. The first part is quite easy and the for the second part I found some research papers but none of them give the links for annotated dataset for fine-tuning model. can anyone help me find datasets which have good annotations for this purpose.

[SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model](https://arxiv.org/pdf/2402.18068v2) (they mention a dataset on page 4 but didn't give any links)",Background-Trainer37,1gp05os,https://reddit.com/r/MachineLearning/comments/1gp05os/pannotated_dataset_for_explaining_the_reason_in/,https://www.reddit.com/r/MachineLearning/comments/1gp05os/pannotated_dataset_for_explaining_the_reason_in/,2024-11-11 19:03:52,1,0.57,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1gp05os
