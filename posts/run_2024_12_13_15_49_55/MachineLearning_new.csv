subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[R] Identifying Critical Decision Points in Neural Text Generation Through Token-Level Uncertainty Analysis,"This paper introduces a framework for analyzing and visualizing the branching decisions language models make during text generation. The key methodology involves tracking probability distributions across different sampling paths to understand how early choices affect downstream generation.

Main technical points:
- Developed metrics to quantify uncertainty at each generation step
- Created visualization tools for mapping decision trees in generation
- Analyzed how different sampling methods affect path divergence
- Measured correlation between model confidence and generation quality
- Identified clustering patterns in generation trajectories

Key results:
- Found that paths tend to cluster into 2-3 distinct trajectory groups
- Early sampling decisions have outsized impact on final outputs
- Uncertainty patterns vary significantly between sampling methods
- Similar prompts can lead to dramatically different generation paths
- Model confidence doesn't consistently predict output quality

I think this work provides important insights into how we might better control text generation. The ability to map and understand generation paths could help develop more reliable sampling methods and better uncertainty estimates.

I think the clustering of generation paths is particularly interesting - it suggests there may be ways to guide generation toward desired trajectory groups. This could be valuable for applications needing more predictable outputs.

The methodology also reveals some concerning aspects about current sampling methods. The strong dependence on early decisions suggests we may need new approaches that better preserve generation flexibility throughout the sequence.

TLDR: New framework for analyzing how language models make text generation choices. Shows that generation paths cluster into distinct groups and early decisions heavily influence outcomes. Could help develop better sampling methods and uncertainty estimates.

[Full summary is here](https://aimodels.fyi/papers/arxiv/forking-paths-neural-text-generation). Paper [here](https://arxiv.org/abs/2412.07961).",Successful-Western27,1hdd0kk,https://reddit.com/r/MachineLearning/comments/1hdd0kk/r_identifying_critical_decision_points_in_neural/,https://www.reddit.com/r/MachineLearning/comments/1hdd0kk/r_identifying_critical_decision_points_in_neural/,2024-12-13 14:10:21,1,1.0,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1hdd0kk
MachineLearning,[D] Training with synthetic data and model collapse. Is there progress?,"About a year ago, research papers talked about model collapse when dealing with synthetic data. Recently I’ve been hearing about some progress in this regard. I am not expert and would welcome your views on what’s going on. Thank you and have a fantastic day.",BubblyOption7980,1hd92mt,https://reddit.com/r/MachineLearning/comments/1hd92mt/d_training_with_synthetic_data_and_model_collapse/,https://www.reddit.com/r/MachineLearning/comments/1hd92mt/d_training_with_synthetic_data_and_model_collapse/,2024-12-13 10:03:43,2,0.55,2,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1hd92mt
MachineLearning,[D] Agentic AI Design Patterns,"I was looking into design patterns for Agentic AI and I could need some help to grasp the concepts.

I read about ReAct and ReWOO.

From ReWOO, I really liked the idea of having a planner that creates a blueprint of the work that needs to be done. I can imagine that this works well for a lot of tasks, and it optimizes token usage compared to ReAct.

From ReAct, I like that it has a reflection/observation LLM, to decide whether the output is good enough or needs another pass through the agents.

What I don't understand:
Why does ReWOO not have a reflection component??

Wouldn't it be the best of both worlds to have the planner and the reflection?

This was the first draft for my agentic AI prototype, and I think it has pretty obvious advantages.

I think I am missing something here.",Mindless_Copy_7487,1hd8w3k,https://reddit.com/r/MachineLearning/comments/1hd8w3k/d_agentic_ai_design_patterns/,https://www.reddit.com/r/MachineLearning/comments/1hd8w3k/d_agentic_ai_design_patterns/,2024-12-13 09:49:22,1,0.67,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hd8w3k
MachineLearning,[D] Importance of HPO per field / model type / applications,"I’ve noticed that the time spent on hyperparameter optimization vary significantly, not just between industry and academia but also across different fields like NLP, computer vision, or reinforcement learning. I’m curious—what’s your experience?

* Is tuning something you prioritize heavily, or do you often settle for “good enough” configurations to move faster?
* What field / model type / applications do you think experience most(or least) bottleneck in workflow due to HPO?
* Are there any industry dependency around choosing HPO tools? For example, everyone in xx industry would pick Optuna as a go-to or everyone running xx experiments would use Sigopt.

Would love to hear your experiences! Thanks",Maleficent_Ad5541,1hd6pjv,https://reddit.com/r/MachineLearning/comments/1hd6pjv/d_importance_of_hpo_per_field_model_type/,https://www.reddit.com/r/MachineLearning/comments/1hd6pjv/d_importance_of_hpo_per_field_model_type/,2024-12-13 06:58:12,7,0.89,7,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hd6pjv
MachineLearning,[D] help with evaluating model,"i am having an issue with evaluating my model because model.evaluate() returns an okay overall score in accuracy but the confusion matrix and classification report return 100% for one class and 0% for another, i am using cifar10 but only 2 classes from it. anyone know why this happens? is this overfitting i am not sure because i am getting a similar score as model.evaluate(0 in my training accuracy and same for loss (which is almost as high as the accuracy)",Affectionate_Pen6368,1hd5kht,https://reddit.com/r/MachineLearning/comments/1hd5kht/d_help_with_evaluating_model/,https://www.reddit.com/r/MachineLearning/comments/1hd5kht/d_help_with_evaluating_model/,2024-12-13 05:40:50,1,0.6,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hd5kht
MachineLearning,"[D] does intel gpu support ROCm or AMD cards  support intel one?
","i can't find this information and if both are open source it make sense a compatibility layer , any of the two is already ported to the other platform?, if you can share info about nvidia too will be cool



",mrnothing-,1hctd0o,https://reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/,https://www.reddit.com/r/MachineLearning/comments/1hctd0o/d_does_intel_gpu_support_rocm_or_amd_cards/,2024-12-12 19:39:17,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hctd0o
MachineLearning,[P] Scalling data from aggregated calculations,"Hello, I have a project in which I detect anomalies on transactions data from ethereum blockchain. I have performed aggregated calculations on each wallet address (ex. minimum, maximum, median, sum, mode of transactions' values) and created seperated datafile with it. I have joined the data on all the transactions. Now I have to standardize data (I have chosen robust scalling) before machine learning but I have following questions regarding this topic:

1. Should I actually standardize each feature based on its unique mean and iqr? Or perform scalling on the column that the calculations come from - value column and than use its mean and iqr to scale the calculated columns?
2. If each feature was scaled based on its own mean and iqr should I do it before joining calculated data or after?",Wikar,1hcukjg,https://reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/,https://www.reddit.com/r/MachineLearning/comments/1hcukjg/p_scalling_data_from_aggregated_calculations/,2024-12-12 20:32:01,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1hcukjg
MachineLearning,[D] LSTM model implementation and approximation questions,"For a project I am currently trying to integrate an Autoencoder for feature extraction and an LSTM for classification of the reduced feature space. The problem I am encountering is on how to train the LSTM network. The AE produces 5 datapoints which is fed into the LSTM network. The trick now comes in on the training of the LSTM network and how the LSTM works. I want the LSTM to take into account the 5 parameters from the AE at time t as well as the parameters at t-1 and t-2. As far as I understand the LSTM does this automatically, or should it then be that the LSTM takes in a total of 15 parameters with each pair of 5 corresponding to one timestep of the AE?

Any advice on LSTM would be great or how such training can be done in an efficient way. The AE is processing a time-series signal.",Sea_Onion41,1hcvh1c,https://reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/,https://www.reddit.com/r/MachineLearning/comments/1hcvh1c/d_lstm_model_implementation_and_approximation/,2024-12-12 21:09:27,5,1.0,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1hcvh1c
MachineLearning,"[D] ""Proper"" way to upload accepted conference paper to the ArXiv?","We recently had a paper accepted to a conference (AAAI). We found out that the conference does not publish appendices so they recommend we upload the full paper (with appendix) to arXiv. This is something we were considering doing anyway since the paper would be available before the conference proceedings come out.

My concern is that if someone decides to cite our work, they may either become confused or cite the arXiv rather than AAAI ""version"".

Is there a ""correct"" or common way to handle this? Do arXiv uploads with the same title get indexed to ""one manuscript"" on google scholar?

Also, are we allowed to use the conference template to upload? (This part might be conference dependent I suppose).

I know it is common these days to upload to arXiv before hearing back from a conference (usually with a different title) but I think this is a slightly different situation as the paper is accepted and the uploaded version will be identical to the conference paper (though with an Appendix).

Thanks in advance!",baghalipolo,1hcupkm,https://reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/,https://www.reddit.com/r/MachineLearning/comments/1hcupkm/d_proper_way_to_upload_accepted_conference_paper/,2024-12-12 20:38:05,6,0.75,6,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hcupkm
MachineLearning,From Viruses and Materials to Galaxies and Beyond: The Role Machine Learning Plays in Scientific Discovery,,SlothSpeedRunning,1hcubv2,https://reddit.com/r/MachineLearning/comments/1hcubv2/from_viruses_and_materials_to_galaxies_and_beyond/,https://lettersandsciencemag.ucdavis.edu/science-technology/viruses-and-materials-galaxies-and-beyond,2024-12-12 20:21:10,3,1.0,3,0,0,0,0,False,False,False,False,False,,https://a.thumbs.redditmedia.com/SRD_I9dMMiKup552l256JNNe6jjxqQTl9DnvS7gkMO8.jpg,t3_1hcubv2
MachineLearning,[D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams,"Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his colleagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.github.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is asserted and if this is true this would be nice to confirm.",LelouchZer12,1hctf36,https://reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/,https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/,2024-12-12 19:41:41,513,0.97,513,0,60,0,0,False,False,True,False,False,Discussion,self,t3_1hctf36
MachineLearning,"[D] I got the acceptance for my IEEE publication, does that means it will be uploaded on their Xplore page?","So I submitted a paper and it got accepted by my publication around 2 months ago, today was my conference in online mode, didnt go well I think he was in hurry he didnt listen much diagreed a bit and then closed the meet on my face. So my question is how bad is it? Will it be published as I have the acceptance or still a no?",candle_misuser,1hcpqj4,https://reddit.com/r/MachineLearning/comments/1hcpqj4/d_i_got_the_acceptance_for_my_ieee_publication/,https://www.reddit.com/r/MachineLearning/comments/1hcpqj4/d_i_got_the_acceptance_for_my_ieee_publication/,2024-12-12 17:04:48,0,0.33,0,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1hcpqj4
MachineLearning,[R] Rethinking the positive pairs in contrastive learning,"Hi, I am sharing my recent work which allows arbitrary images to be positive pairs. Our finding is quite astonishing that two disparate images, e.g., a snake and a lamp, can be positive. Our work potentially broadens the applications of contrastive learning to deal with the ""false positive"" in which two views are not similar.

We challenge the common sense in contrastive learning, that is, the positive pair design is critical. Our results prove that the feature selection is the key!

Paper: [https://arxiv.org/abs/2410.18200](https://arxiv.org/abs/2410.18200)",Miserable-Gene-308,1hcpoo6,https://reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/,https://www.reddit.com/r/MachineLearning/comments/1hcpoo6/r_rethinking_the_positive_pairs_in_contrastive/,2024-12-12 17:02:44,5,0.65,5,0,6,0,0,False,False,True,False,False,Research,self,t3_1hcpoo6
MachineLearning,[D] What makes TikTok's recommendation algorithm so strong?,"General Discussion - now that they are about to be banned in the US, I'm becoming fascinated by the strength of their For You recommendations. To try and put some guard rails on what I mean, TikTok has shown itself to be able to match content to relevant audience at greater frequency and scale than any other app (YouTube included). Many creators can join the platform, post a single video, and have millions of views in 24 hours. This does happen on other apps, but TikTok seems to be the most consistent at scaling audience incredibly fast.

What models might they be basing their system on? What about their models creates their competitive advantage?",No_Collection_5509,1hcp4xw,https://reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/,https://www.reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/,2024-12-12 16:39:06,100,0.91,100,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1hcp4xw
MachineLearning,[D] Pet project - Style Transfer Neural Networks Implementation,"Hi, I am learning ML and this is my first project. I did a simple 100 LoC implementation of the *Neural Style Transfer* paper by Gatys et al. See [https://github.com/TAOGenna/pytorch-neural-style-transfer](https://github.com/TAOGenna/pytorch-neural-style-transfer)

https://preview.redd.it/x2udi76n2g6e1.jpg?width=939&amp;format=pjpg&amp;auto=webp&amp;s=437bdda1683e9fd580a6b3d1d4dc2598b25079ff

  
",TAO_genna,1hcottj,https://reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1hcottj/d_pet_project_style_transfer_neural_networks/,2024-12-12 16:25:17,3,0.71,3,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/BbvnpWVVY1jQ1n7M80lXl0kKKowiy2y473otZfpwdWo.jpg,t3_1hcottj
MachineLearning,[D] What Models Are Best at Producing Ambient Sounds/ Music?,"I'm working on an application that requires ambient sounds/ music. For example:

* ""A crackling fire, with chat murmuring in the background.""
* ""Nightime countryside summer sounds in the UK.""
* ""Wind blowing through the mountains as you're stood on a high rock.""

I've had a look at Hugging Face and found the Text-To-Audio section. However it appears the top models have very few downloads:

https://preview.redd.it/d4o7g760yf6e1.png?width=1788&amp;format=png&amp;auto=webp&amp;s=2901a7678582745beb714b81519712bac37bd195

This makes me think the field is immature, and there's no clear best model. Is this a fair appraisal of the field, or are there models outside of Hugging Face that perform well for this use case?",FPGA_Superstar,1hcof9m,https://reddit.com/r/MachineLearning/comments/1hcof9m/d_what_models_are_best_at_producing_ambient/,https://www.reddit.com/r/MachineLearning/comments/1hcof9m/d_what_models_are_best_at_producing_ambient/,2024-12-12 16:07:30,0,0.42,0,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/TCjeAYae_sK1rwe1hACJo5HRrqIniSivkrmvMhxxwGc.jpg,t3_1hcof9m
MachineLearning,[D] Question About ResNet and Scalability of Extremely Deep Networks,"I’ve been exploring the architecture of ResNet and its ability to train very deep neural networks effectively. While I understand that residual connections help mitigate issues like vanishing gradients and make training deeper networks feasible, I’m curious about the limitations of this approach when scaling to extremely deep networks, such as those with 1000 layers or more.

From my understanding, a ResNet with, say, 100 layers might effectively function like a much smaller network due to the residual connections, which essentially ""skip"" layers and add outputs. However, wouldn’t this also mean that if a regular MLP struggles to scale beyond 15 layers, a ResNet might just shift this limit proportionally (e.g., struggling beyond 150 layers)? In other words, does ResNet fundamentally solve the problem of training extremely deep networks, or does it merely extend the depth at which issues start to reappear?

  
I’d appreciate any insights you might have! TYSM!",Time_Celebration6058,1hco4ig,https://reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/,https://www.reddit.com/r/MachineLearning/comments/1hco4ig/d_question_about_resnet_and_scalability_of/,2024-12-12 15:54:53,3,1.0,3,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hco4ig
MachineLearning,"[R] A Grounded Theory Study of LLM Red Teaming: Motivations, Strategies, and Techniques","This paper presents a grounded theory study of how red-teaming is conducted on Large Language Models (LLMs), based on interviews with practitioners. The researchers systematically analyzed practitioner approaches to identify common patterns, strategies and motivations in LLM red-teaming.

Key technical points:
- Used qualitative coding of interviews to develop taxonomy of red-teaming approaches
- Identified 12 distinct attack strategies and 35 specific techniques
- Found red-teaming requires manual effort rather than automation
- Demonstrated importance of team collaboration over individual attempts
- Established red-teaming as distinct from malicious attacks
- Mapped common patterns in tester motivations and goals

Main results:
- Red-teaming strategies fall into categories like prompt manipulation, psychology-based attacks, and system limit testing
- Successful testers adopt an ""alchemist"" mindset of systematic experimentation
- Most practitioners are motivated by curiosity and safety concerns
- Testing requires deep understanding of both technical and psychological aspects
- Manual testing currently more effective than automated approaches

I think this work provides an important foundation for developing more structured approaches to LLM safety testing. The taxonomy they've developed could help standardize how we evaluate and secure these systems. Their finding that manual testing remains superior to automation suggests we need much more work on automated testing approaches.

I think the emphasis on non-malicious intent and safety motivations is particularly relevant as these systems become more widely deployed. Understanding how and why people conduct these tests helps distinguish legitimate security research from attacks.

TLDR: First systematic study of LLM red-teaming practices, providing taxonomy of strategies and techniques based on practitioner interviews. Shows importance of manual testing and team collaboration, while establishing red-teaming as legitimate security research.

[Full summary is here](https://aimodels.fyi/papers/arxiv/summon-demon-bind-it-grounded-theory-llm). Paper [here](https://arxiv.org/abs/2311.06237).",Successful-Western27,1hclmk1,https://reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/,https://www.reddit.com/r/MachineLearning/comments/1hclmk1/r_a_grounded_theory_study_of_llm_red_teaming/,2024-12-12 13:56:57,4,1.0,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1hclmk1
MachineLearning,[R] LLM's knowledge expansion to enable generation of cross domain content (outside the training dataset),,ankitm1,1hcke77,https://reddit.com/r/MachineLearning/comments/1hcke77/r_llms_knowledge_expansion_to_enable_generation/,https://arxiv.org/abs/2409.17171,2024-12-12 12:50:27,4,0.75,4,0,0,0,0,False,False,False,False,False,Research,default,t3_1hcke77
MachineLearning,[R] What should I choose: AI-assisted data labeling or crowdsourced data labeling?,"Hey everyone,

I’m a researcher at my company, and I’m starting to train a model using a dataset of over 50k medical images. One of the major challenges I’m facing is choosing the best approach for data labeling.

I’ve seen promising results with **AI-assisted labeling**—it seems faster and less costly upfront. However, I’m concerned about potential inaccuracies and whether the AI’s assistance would skew the dataset in ways I might not expect.

On the other hand, **crowdsourced labeling** is significantly more expensive and time-consuming, but the results are often highly reliable due to diverse human input.

Given the scale of my dataset and its importance in a sensitive domain like medical imaging:

* **Which method would you recommend, and why?**
* **Are there hybrid approaches that balance cost, quality, and efficiency?**
* **What tools or platforms would you suggest for either approach?**

I’d love to hear your insights or experiences with similar projects. Your input will be invaluable in helping me make the right decision!

Thanks in advance!",Organic-Injury-1153,1hchrui,https://reddit.com/r/MachineLearning/comments/1hchrui/r_what_should_i_choose_aiassisted_data_labeling/,https://www.reddit.com/r/MachineLearning/comments/1hchrui/r_what_should_i_choose_aiassisted_data_labeling/,2024-12-12 09:56:17,1,0.53,1,0,15,0,0,False,False,True,False,False,Research,self,t3_1hchrui
MachineLearning,[N] Save 80% Memory for DPO and ORPO in Liger-Kernel,"Introducing the first open-source optimized post-training losses in Liger Kernel with \~80% memory reduction, featuring DPO, CPO, ORPO, SimPO, JSD, and more, achieving up to 70% end-to-end speedup through larger batch size. Use it as any PyTorch module - Available today in Liger v0.5.0!

[https://x.com/hsu\_byron/status/1866577403918917655](https://x.com/hsu_byron/status/1866577403918917655)",Icy-World-8359,1hcewdl,https://reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/,https://www.reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/,2024-12-12 06:18:35,18,0.87,18,0,0,0,0,False,False,True,False,False,News,self,t3_1hcewdl
MachineLearning,[R] An Evolved Universal Transformer Memory,,hardmaru,1hc6bs8,https://reddit.com/r/MachineLearning/comments/1hc6bs8/r_an_evolved_universal_transformer_memory/,https://arxiv.org/abs/2410.13166,2024-12-11 22:43:30,13,0.84,13,0,2,0,0,False,False,False,False,False,Research,default,t3_1hc6bs8
MachineLearning,Conv2D for Time Series Data Tutorial [Project],"Can anyone provide me with a tutorial using TensorFlow on time series data and an example model with Conv2D layers, an AveragePooling2D layer and final dense layers?",chiplab,1hc5r13,https://reddit.com/r/MachineLearning/comments/1hc5r13/conv2d_for_time_series_data_tutorial_project/,https://www.reddit.com/r/MachineLearning/comments/1hc5r13/conv2d_for_time_series_data_tutorial_project/,2024-12-11 22:18:06,0,0.11,0,0,6,0,0,False,False,True,False,False,Project,self,t3_1hc5r13
MachineLearning,[D] How to make friends and network at NeurIPS?,"I’m attending NeurIPS for the first time and it’s quite overwhelming seeing the amount of people and so many recruiters. I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is here.

I didn’t really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach others who are in big groups of people and (2) I’m feeling strong imposter syndrome and under-qualified for the jobs recruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to potentially meet up and have a chat? I’m a 3rd year PhD student from the UK, but from Vancouver myself so know lots of stuff going on in the area. Cheers!",K_is_for_Karma,1hc0x89,https://reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/,https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/,2024-12-11 18:54:26,38,0.8,38,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1hc0x89
MachineLearning,Where to find Llama 3 initialisation [R] ,"Title basically says it all, I want a good transformer baseline and I imagine that the initialisation can matter quite a bit. I can find the llama 3 model, but I can find how they init parameters. Does anyone know where I can find this?",idkwhatever1337,1hbudg3,https://reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/,https://www.reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/,2024-12-11 14:14:14,0,0.45,0,0,1,0,0,False,False,True,False,False,Research,self,t3_1hbudg3
MachineLearning,[R] Continuous Latent Space Reasoning: Enhancing LLM Performance Through Chain of Continuous Thought,"This paper introduces **COCONUT** (Chain of Continuous Thought), which transforms language model reasoning from discrete token space into continuous latent space. The key idea is encoding reasoning steps as continuous vectors rather than text tokens, allowing for more flexible and precise intermediate computations.

Main technical points:
* Encoder-decoder architecture that maps text↔continuous vectors
* Novel continuous reasoning module operating on latent vectors
* Parallel processing of reasoning steps in continuous space
* Gradient-based optimization during the reasoning process
* Special loss function combining reconstruction and reasoning objectives

Key results:
* **20%** improvement on reasoning benchmarks vs traditional methods
* Reduced computational steps needed for complex problems
* More consistent performance across different reasoning tasks
* Better handling of mathematical and logical reasoning
* Enhanced ability to maintain coherent reasoning chains

I think this approach could meaningfully advance how language models handle complex reasoning tasks. By moving beyond discrete tokens, models may better capture the continuous nature of human-like reasoning. The ability to optimize in continuous space during reasoning is particularly promising for improving reliability.

I think the main challenge will be scaling this to very large models while managing computational costs. The translation between discrete and continuous spaces adds overhead that needs to be addressed.

TLDR: New method transforms language model reasoning into continuous vector space instead of discrete tokens, showing 20% better performance on reasoning tasks through more flexible computation.

[Full summary here](https://aimodels.fyi/papers/arxiv/training-large-language-models-to-reason-continuous). Paper [here](https://arxiv.org/abs/2412.06769).",Successful-Western27,1hbto1w,https://reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/,https://www.reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/,2024-12-11 13:39:26,100,0.95,100,0,6,0,0,False,False,True,False,False,Research,self,t3_1hbto1w
MachineLearning,[D] Resources to get up to the speed with the state of the art evolutionary optimization,"There're plenty of good books letting you get close to the state of the art in the field, on Machine Learning, and Deep Learning in particular. However, are there any good modern books on evolutionary optimization? Are there any good courses?",ArtisticHamster,1hbt986,https://reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/,https://www.reddit.com/r/MachineLearning/comments/1hbt986/d_resources_to_get_up_to_the_speed_with_the_state/,2024-12-11 13:17:59,10,0.81,10,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hbt986
MachineLearning,[D] Visual Language models for object detection and depth perception in 3D environment,"I want to run a VLM for object detection and depth perception in 3D simulation engine (Unity). What are some good vlms for this use case considering factors like accuracy, speed and ease of fine-tuning?

Example use case:  
In Unity, I have an environment with 2 rooms. A camera is setup which captures image/video feed of the scene. The VLM should find a specific object (say a black bottle) in the environment, judge where it is in 3D scene and generate coordinates for it.

Basically I want to find out exactly where the object is in the Unity environment. How can this be done? ",reso_ams,1hbs52u,https://reddit.com/r/MachineLearning/comments/1hbs52u/d_visual_language_models_for_object_detection_and/,https://www.reddit.com/r/MachineLearning/comments/1hbs52u/d_visual_language_models_for_object_detection_and/,2024-12-11 12:15:00,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hbs52u
MachineLearning,[D] Any 3D reconstruction method that can be used for multiple scenes (i.e. not use and throw),"NeRFs are unique for every scene and thus, need to be trained from scratch. Gaussian splats are scene unique too. I understand that scenes are complex and thus there is very little chance for there to be a neural network that can output multiple scenes once trained. But still are there any scene representations that to some caliber are not use and throw completely?",deathmaster2011,1hbreb4,https://reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/,https://www.reddit.com/r/MachineLearning/comments/1hbreb4/d_any_3d_reconstruction_method_that_can_be_used/,2024-12-11 11:27:15,4,0.83,4,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hbreb4
MachineLearning,[R] Evaluating the world model implicit in a generative model,,jsonathan,1hbra2d,https://reddit.com/r/MachineLearning/comments/1hbra2d/r_evaluating_the_world_model_implicit_in_a/,https://arxiv.org/pdf/2406.03689,2024-12-11 11:19:06,21,0.86,21,0,26,0,0,False,False,False,False,False,Research,default,t3_1hbra2d
MachineLearning,[R] When do authors have access to ICLR Meta-Reviews ?,"Hello everyone,

This is my first time submitting to ICLR. On the ICLR website, it says that Meta-Reviews are due today (in a few hours). Will the authors have access to those reviews at the same time as the decision notification or right after the Meta-Reviews due date ?

Thanks !",Glaze_anetha42,1hbqc75,https://reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/,https://www.reddit.com/r/MachineLearning/comments/1hbqc75/r_when_do_authors_have_access_to_iclr_metareviews/,2024-12-11 10:10:57,6,0.75,6,0,2,0,0,False,False,True,False,False,Research,self,t3_1hbqc75
MachineLearning,[D] Why are the Stella embedding models so much smaller than other models of similar quality?,"On the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), `stella_en_v5` is currently ranked 3rd overall, while using *one fifth* the memory of all non-Stella models in the top 10.

`stella_en_400M_v5` is ranked 10th, while using *15-20 times less memory* than the models ranked near it. This appears to be relatively consistent across several subtasks of the benchmark (for English).

What is the secret sauce here? Alternatively, what is the catch? There is no paper yet. Anyone know details?",-p-e-w-,1hbkww5,https://reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/,https://www.reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/,2024-12-11 03:58:01,39,0.87,39,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1hbkww5
MachineLearning,[D] Review process incentives and competition,"One of my labmates showed me a comment by an AC asking reviewers in an ACM conference to engage in the ICLR rebuttal and discussion period. This alone is funny (and sad) to me, but what got me was when one of the reviewers responded saying that the review process incentivizes reviewers to score papers low in order to gatekeep competing papers from being accepted.

I want to believe that this happens but its effect is not significant. However, I have heard that this is very common in fields like recommender systems. How prevalent is it in ML in general?",like_a_tensor,1hbf2gs,https://reddit.com/r/MachineLearning/comments/1hbf2gs/d_review_process_incentives_and_competition/,https://www.reddit.com/r/MachineLearning/comments/1hbf2gs/d_review_process_incentives_and_competition/,2024-12-10 23:07:02,12,1.0,12,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hbf2gs
MachineLearning,[D] Inverse Neural Network,"Hi everyone, I wanna ask you guys if you know what's the current best **supervised** Inverse Neural Network? I know GAN, VAE, and conditional VAE.

Basically, my aim is to determine the input values of a multivariate function that satisfy an output value, e.g., find x=\[x1,...,xN\] so that 0.2=f(x).

My main major is engineering (not machine learning) and my knowledge about the field is quite limited. However, I'm good with reading any research papers that you suggested.

Thank you all,

Edit: sorry the for confusing example. f is NOT multivariate pdf but a ""multivariate function"" f : R\^N -&gt; R. Specifically, f is a ""univariate"" pdf with N-1 parameters.",zonanaika,1hbbj5o,https://reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/,https://www.reddit.com/r/MachineLearning/comments/1hbbj5o/d_inverse_neural_network/,2024-12-10 20:34:05,7,1.0,7,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1hbbj5o
MachineLearning,[R] Articulate Anything: automatic generation of 3D interactable assets from any input modalities,"📦 Can frontier AI transform ANY physical object from ANY input modality into a high-quality digital twin that also MOVES? Excited to share our work,Articulate-Anything, exploring how large vision-language models (VLMs) can bridge the gap between the physical and digital worlds.

Articulate-Anything 🐵 is a state-of-the-art method for automatic interactable 3D asset creation from any input modalities including text, images, or videos.  
  
Website: [articulate-anything.github.io](https://t.co/y3fNE8Lb7X)  
Paper: [https://arxiv.org/abs/2410.13882](https://t.co/8m3gu5zTD7)  
Code: [https://github.com/vlongle/articulate-anything](https://github.com/vlongle/articulate-anything)  
Please see my twitter thread: [https://x.com/int64\_le/status/1866519866934714623](https://x.com/int64_le/status/1866519866934714623) a deep dive into the method",Prudent_Fly_1004,1hb82am,https://reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/,https://www.reddit.com/r/MachineLearning/comments/1hb82am/r_articulate_anything_automatic_generation_of_3d/,2024-12-10 18:08:50,4,1.0,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1hb82am
MachineLearning,[D] From Unemployment to Lisp: Running GPT-2 on a Teen's Deep Learning Compiler,"A couple months ago I found myself unemployed, uncertain about what to do next. I wanted to learn more about deep learning, but from a systems prespective. Coming from Andrew's Ng course on supervised learning, I was eager to learn more about how deep learning frameworks (or deep learning compilers) like Pytorch or Tinygrad.

I started to poke around Tinygrad, learning from the tutorials I found online, and I found it fascinating because it was an actual compiler, it took conventional python code and translated them into an Abstract Syntax Tree that was parsed into UOps and ScheduleItems, to finally have a codegen layer. While the design was interesting, the code was hard to read.

That's when I stumbled across something completly unexpected, A deep learning compiler built on Common Lisp, maintained by a Japanese 18-year-old during his gap year. And currently we have acomplished something great, it can run gpt2!

For now, it just generates C-kernels, but in the future we would like to support cuda codegen as well as many other features, and serve as a learning tool for anyone who would like to get to work on deep learning compilers in Common Lisp.

This is an open source project and anyone is welcome to contribute!

[https://github.com/hikettei/Caten](https://github.com/hikettei/Caten)

Edit: add an example of how it works.

Here's an example i wrote in a different forum:

Hello! Thanks for your question.

First of all, there are three layers of abstraction within Caten:

1. caten/apis | High-Level Graph Interface 2. caten/air | Low-Level Graph Interface 3. caten/codegen | AIR Graph =&gt; Kernel Generator

The inputs of the compiler are just Common Lisp classes (similar to torch modules). For example, in Common Lisp, we could create a module that does SinCos:

        (defclass SinCos (Func) nil
          (:documentation ""The func SinCos computes sin(cos(x))""))
    
        ;; Forward creates a lazy tensor for the next computation.
        ;; You can skip this process by using the `st` macro.
        (defmethod forward ((op SinCos) &amp;rest tensors)
          (st ""A[~] -&gt; A[~]"" (tensors)))
    
        ;; Backward is optional (skipped this time)
        (defmethod backward ((op SinCos) &amp;optional prev-grad)
          (declare (ignore prev-grad))
          nil)
    
        ;; Lower describes the lowered expression of `SinCos`
        (defmethod lower ((op SinCos) &amp;rest inputs)
          (let ((x (car inputs)))
            (with-context
              (a (%sin (%add x (%fconst (/ pi 2)))))
              (b (%sin a)))))

The \`apis\` layer is the high-level interface, while the \`lower\` method is the lower-level step before code generation.

Next, the framework generates an Abstract VM (AVM) representation:

        #S(AVM :GRAPH Graph[seen=NIL, outputs=(STC6466_1)] {
          &lt;ALLOCATE : TID6464 &lt;- (shape=(1), stride=(1)) where :dtype=FLOAT32&gt;
          &lt;Node[BUFFER] ALLOCATE(NID6480) : SID6479* &lt;- ()&gt;
          &lt;Node[BINARYOPS] ADD(NID6484) : BID6483* &lt;- (TID6464, LID6481)&gt;
          &lt;Node[UNARYOPS] SIN(NID6486) : UID6485* &lt;- (BID6483)&gt;
          &lt;Node[UNARYOPS] SIN(NID6488) : UID6487* &lt;- (UID6485)&gt;
          &lt;Node[SPECIAL/VM] PAUSE/BACKWARD(NID6501) : STC6466_1* &lt;- (UID6487)&gt;
        })

Then, the computation graph is translated into schedule items:

        FastGraph[outputs=(val_6)] {
          { Allocate } : [ val_0 &lt;- (1) ]
          { KERNEL } : [ val_5 &lt;- val_1, val_0 :name=FUSED_SIN_SIN_ADD_LOAD6511]
        }

Finally, the code generation step produces the following C code:

        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0);
        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0) {
            val_5[0] = sin(sin((val_0[0] + 1.5707964)));
        }

This C code is compiled by a C compiler and executed.

So to answer your question: the compiler takes Common Lisp code and generates C functions.",yCuboy,1hb7v5h,https://reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/,https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/,2024-12-10 18:00:45,89,0.92,89,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1hb7v5h
MachineLearning,[D] What’s stopping you from using foundation models for time series forecasting?,"I’ve been experimenting with foundation models like [Sulie](https://github.com/wearesulie/sulie), [Granite TTM](https://huggingface.co/ibm-granite/granite-timeseries-ttm-r1), and [Amazon Chronos](https://github.com/amazon-science/chronos-forecasting), and each one has its own strengths. What’s really fascinating is how much faster you can get accurate forecasts with a zero-shot approach. However, as much as these models improve forecasting, explainability remains a major challenge compared to more traditional methods like ARIMA, which are simpler to interpret.

I’m curious—do you think explainability is a dealbreaker, or is there another reason why foundation models for forecasting aren’t gaining wider adoption? Would love to hear what’s been your biggest blocker or challenge in using these models.",Queasy_Emphasis_5441,1hb7ur1,https://reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/,https://www.reddit.com/r/MachineLearning/comments/1hb7ur1/d_whats_stopping_you_from_using_foundation_models/,2024-12-10 18:00:23,2,0.53,2,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1hb7ur1
MachineLearning,[D] Question about heating system machine learning,"Hello, in conventional heating processes you control when to start heating and when to stop with target values. Once reached it'll stop and start the whole process over and over again, alongside some pump valves (how much water to go through the circulation).

I've captured several room temperatures, all start/stops and pump valve adjustments made by the heating automation software in a time series database for the last 5 years (every minute).

I'm trying to create a model which has a constant target value of e.g. 23 °C. The inputs for the model are the heating system status (on/off), pump valve positions and current room temperatures. Output should be how to set the heating system status and adjust the valve positions to achieve and hold the target value of the room temperatures. In the best case scenario it should replace the heating automation software completely. Other is to just advise or supervise the process.

A few problems come to my mind, where I'm not sure on how to approach these:

1. The process is slow and once heating starts the results can be seen 1 hour later as the temperature changes slowly. So the evaluation of the actions must be done with some delay?
2. My captured data contains historical temperatures, but I think it might be flawed. The temperature in the data is already influenced by the existing heating system. I don't have temperature data which show how the room temperatures realistically change without any heating systems. Is this a problem for learning? Do I need to create synthetic data?
3. Would it be better to train a model to output ""start heating / stop heating"" (leave the rest for the conventional heating automation) or to control the heating status and the pump valves itself?
4. What would be the best machine learning technique, e.g. un-/supervised, reinforcement learning?",QuickYogurt2037,1hb5dty,https://reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/,https://www.reddit.com/r/MachineLearning/comments/1hb5dty/d_question_about_heating_system_machine_learning/,2024-12-10 16:15:47,1,0.67,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hb5dty
MachineLearning,[R] How difficult is this dataset REALLY?,"New Paper Alert!

Class-wise Autoencoders Measure Classification Difficulty and Detect Label Mistakes

We like to think that the challenge in training a classifier is handled by hyperparameter tuning or model innovation, but there is rich inherent signal in the data and their embeddings.  Understanding how hard a machine learning problem is has been quite elusive.  Not any more.

Now you can compute the difficulty of a classification dataset without training a classifier, and requiring only 100 labels per class.  And, this difficulty estimate is surprisingly independent of the dataset size.

Traditionally, methods for dataset difficulty assessment have been time and/or compute-intensive, often requiring training one or multiple large downstream models. What's more, if you train a model with a certain architecture on your dataset and achieve a certain accuracy, there is no way to be sure that your architecture was perfectly suited to the task at hand — it could be that a different set of inductive biases would have led to a model that learned patterns in the data with far more ease.

Our method trains a lightweight autoencoder for each class and uses the ratios of reconstruction errors to estimate classification difficulty. Running this dataset difficulty estimation method on a 100k sample dataset takes just a few minutes, and doesn't require tuning or custom processing to run on new datasets!

How well does it work? We conducted a systematic study of 19 common visual datasets, comparing the estimated difficulty from our method to the SOTA classification accuracy. Aside from a single outlier, the correlation is 0.78. It even works on medical datasets!



Paper Link:  https://arxiv.org/abs/2412.02596

GitHub Repo Linked in Arxiv pdf",ProfJasonCorso,1hb54nd,https://reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/,https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/,2024-12-10 16:04:41,33,0.78,33,0,20,0,0,False,False,True,False,False,Research,self,t3_1hb54nd
MachineLearning,[R] Understanding Transformer Limitations in Graph Search: A Mechanistic Analysis of Learning and Scaling Behavior,"This paper tackled a fundamental question about transformers' ability to learn search algorithms by studying how they handle graph connectivity problems. The authors developed a novel interpretation method to analyze how transformers process search operations layer by layer.

Key technical points:
- Used graph reachability as a test case with controlled complexity and unlimited training data
- Developed interpretation technique to understand how transformer layers compute reachable vertex sets
- Found transformers learn to expand search frontier exponentially with depth
- Demonstrated clear scaling limitations based on graph size
- Showed in-context learning (chain-of-thought) doesn't overcome these limitations

Main results:
- Small transformers can learn basic search when trained appropriately
- Each layer computes union of previously reachable vertices plus their neighbors
- Performance degrades sharply with increasing graph size
- Adding parameters doesn't solve the scaling problem
- Models struggle with graphs beyond their training distribution

I think this work reveals important architectural limitations in transformers that we need to address for applications requiring search capabilities. The scaling behavior suggests we may need fundamentally different approaches for larger search spaces rather than just bigger models.

I think the interpretation method they developed could be valuable for understanding how transformers process other types of structured data beyond just graphs. The clear empirical results on scaling limitations should inform architecture choices for applications involving search-like computations.

TLDR: Transformers can learn basic graph search operations but face fundamental limitations with scale. Adding more parameters doesn't help, suggesting we need new approaches for complex search problems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/transformers-struggle-to-learn-to-search). Paper [here](https://arxiv.org/abs/2412.04703).",Successful-Western27,1hb1wjo,https://reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/,https://www.reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/,2024-12-10 13:37:01,37,1.0,37,0,1,0,0,False,False,True,False,False,Research,self,t3_1hb1wjo
MachineLearning,[D] Model Provenance: How are you tracking your ML model lineage?,"Hey r/MachineLearning,I'm curious about how people in this community are handling model provenance - the practice of tracking the lineage and evolution of machine learning models throughout their lifecycle.

1. Are you currently using any tools or methods to track the provenance of your ML models?
2. If yes, what solutions are you using? Are they custom-built or off-the-shelf?
3. If not, do you see a need for such tools in your work?
4. What features would you consider essential in a model provenance solution?",crtahlin,1hb0o4e,https://reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/,https://www.reddit.com/r/MachineLearning/comments/1hb0o4e/d_model_provenance_how_are_you_tracking_your_ml/,2024-12-10 12:29:56,0,0.5,0,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1hb0o4e
MachineLearning,[D] Imbalance Dataset ,"Hi guyz , i am working on a project related to cloud computing. 
I have real time  dataset related to computing computing which i found on Internet , but the data have significant imbalance  e.g i have 4 classes two of them contain highest values while the rest two very much less .

How to play with this data to feed into ml model ( i know if i didn't balance the data then the model will be much bias towards majority class )

Need help ",zaynst,1hb0kew,https://reddit.com/r/MachineLearning/comments/1hb0kew/d_imbalance_dataset/,https://www.reddit.com/r/MachineLearning/comments/1hb0kew/d_imbalance_dataset/,2024-12-10 12:23:55,0,0.22,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1hb0kew
MachineLearning,[R] The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,"**Dataset**: [https://github.com/PolymathicAI/the\_well](https://github.com/PolymathicAI/the_well)

**Paper**: [https://arxiv.org/pdf/2412.00568](https://arxiv.org/pdf/2412.00568)

**Abstract:**

&gt;Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at [this https URL](https://github.com/PolymathicAI/the_well).

",StartledWatermelon,1haz4nw,https://reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/,https://www.reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/,2024-12-10 10:49:47,24,0.93,24,0,0,0,0,False,False,True,False,False,Research,self,t3_1haz4nw
MachineLearning,[D] Seeking paper writing feedback: GPT-based Network Intrusion Detection System (arXiv published),"Hello everyone,



I'm an independent developer who has been working on applying GPT models to network intrusion detection. While I have experience in implementation, this is my first venture into academic paper writing, and I'm seeking feedback on the paper's presentation and structure.



I've recently published on arXiv:

\- Title: NIDS-GPT: TAKE PACKAGE AS LANGUAGE: ANOMALY DETECTION USING

TRANSFORMER

\- arXiv link: [https://arxiv.org/pdf/2412.04473](https://arxiv.org/pdf/2412.04473)



The paper presents a novel approach of treating each number in network packets as independent ""words"" for GPT processing. Our experiments show promising results - 100% accuracy on CICIDS2017 and car-hacking datasets under extreme imbalance conditions, and &gt;90% accuracy in one-shot learning.



As a first-time paper author, I'm seeking feedback on:

1. Paper structure and academic presentation standards

2. Visualization effectiveness and clarity

3. Methodology presentation

4. Validity of experimental comparisons

5. Strength of claims and conclusions



I'm particularly interested in feedback from:

\- Researchers in network security/intrusion detection

\- Those working with language models in non-NLP domains

\- Experienced paper writers/reviewers



The implementation is solid, but I want to ensure the paper effectively communicates the technical contributions to the academic community.



Thank you for your time and expertise!",EliaukMouse,1haufwv,https://reddit.com/r/MachineLearning/comments/1haufwv/d_seeking_paper_writing_feedback_gptbased_network/,https://www.reddit.com/r/MachineLearning/comments/1haufwv/d_seeking_paper_writing_feedback_gptbased_network/,2024-12-10 05:14:37,2,0.63,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1haufwv
MachineLearning,[D] How do you keep up with the literature?,"Pretty much what the title says. What tools/strategies do you use to keep up with the literature?


EDIT: for context, I am a first year PhD student and I was referring to the literature in the particular 'niche' (if you can call anything a niche in ML, apart from a very few exceptions)",Rickmaster7,1hasdlo,https://reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/,https://www.reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/,2024-12-10 03:20:29,50,0.9,50,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1hasdlo
MachineLearning,[D] Is what I'm doing is correct?,"I'm working on an ML project.
I have 100 features and 2000000 rows(Balanced)
Which order shall I follow?

I have done,

1. Data inconsistencies handling
2. NULL imputation 
3. Standardization
4. One hot encoding
5. Data visualization 
6. Correlation check
7. PCA
8. Train test split
8. Model training
9. Evaluation 

For random forest I'm getting 1 for all the metrics for training data and 0.79 for test set.
For logistic regression ~0.79  for all metrics and for test set also getting the same.
For GBDT also ~0.79 for all metrics and for test set also getting the same.
Which model should I select? And is the above mentioned steps are followed in correct order?
",_crazy_muffin_,1has6jq,https://reddit.com/r/MachineLearning/comments/1has6jq/d_is_what_im_doing_is_correct/,https://www.reddit.com/r/MachineLearning/comments/1has6jq/d_is_what_im_doing_is_correct/,2024-12-10 03:10:16,7,0.89,7,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1has6jq
MachineLearning,[R] Improving robustness to corruptions with multiplicative weight perturbations - A simple yet effective approach to robustify neural networks to corruptions,"We would like to share and discuss this NeurIPS spotlight paper (disclaimer: I am a co-author).

**Paper**: [https://arxiv.org/abs/2406.16540](https://arxiv.org/abs/2406.16540)  
**GitHub**: [https://github.com/trungtrinh44/DAMP](https://github.com/trungtrinh44/DAMP)  
**DAMP** (Data augmentation via multiplicative perturbations) is a simple yet effective approach to improving neural network robustness through multiplicative weight perturbations. Unlike traditional data augmentation methods, DAMP operates directly on model weights during training, enabling improved corruption robustness without compromising clean image performance or increasing computational cost.  
  
**Key Highlights:**

* **Theoretical Foundation**: DAMP demonstrates that input corruptions can be equivalently represented as multiplicative weight perturbations, providing a theoretical basis for weight-space data augmentation.
* **Simple Implementation**: The method requires only random Gaussian sampling and pointwise multiplication, maintaining almost the same training cost as standard SGD while being fully compatible with data parallelism.
* **Breakthrough in ViT Training**: Successfully trains Vision Transformers from scratch using only basic preprocessing, achieving ResNet50-level performance (23.7% top-1 error) on ImageNet without complex augmentations.
* **Advanced Integration**: When combined with MixUp and RandAugment, DAMP significantly improves both clean and corruption performance:
   * ViT-S/16: 20.09% clean error (vs 20.25% baseline), 58.30% avg corruption error (vs 60.07% baseline)
   * ViT-B/16: 19.36% clean error (vs 20.41% baseline), 56.76% avg corruption error (vs 58.83% baseline)

**Why DAMP?** Unlike traditional approaches that rely on complex data augmentation pipelines or computationally expensive ensemble methods, DAMP provides a simple, theoretically-grounded solution to improving model robustness. Its ability to train Vision Transformers from scratch without advanced augmentations and compatibility with existing techniques makes it a practical choice for developing robust vision models.  
**Since DAMP has minimal overhead over standard training, it is particularly effective when applied to large models and datasets.**  
  
We welcome technical discussions, particularly regarding theoretical connections to other robustness methods and potential applications beyond computer vision!",emiurgo,1hap6gx,https://reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/,https://www.reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/,2024-12-10 00:38:41,8,0.83,8,0,2,0,0,False,False,True,False,False,Research,self,t3_1hap6gx
MachineLearning,"[D] How do you manage and track your large, evolving, image datasets?","I’m wondering how people manage the lifecycles of their large in-house datasets? Say &gt;1TB and 100k files.

In my new role we have multiple production models trained from in house datasets ranging in size from a few thousand to a few hundred thousand images. We also have huge amounts of fresh data coming in, more than 1M images per day, and so we are constantly mining that and sending new tranches off to be annotated.

Until now the team has been largely left to their own devices to manage this and the results are predictable. In some cases we can’t associate our prod models with any specific data. Some of our core datasets exist only in people’s home directories, ripe to be wiped out by a single misplaced command. For one model, thankfully being sunsetted, both the training code and original training data are known to be lost.

Parts of the org have adopted DVC which seems pretty good until the number of files or overall size gets big. On one end, some stuff the entire dataset into just a few archives and track them. That minimizes frustrations with hashes but uses a lot of storage when only a few files get updated. On the other end, some people track every single file which lets files be individually updated but is a pretty big pain to check in and out. Others split the difference of these two approaches, tracking chunks of the dataset as archives hierarchically.

So how does your org manage this? What works and what doesn’t when working with these large &amp; evolving datasets?",SirPitchalot,1haokqp,https://reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/,https://www.reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/,2024-12-10 00:09:58,17,0.87,17,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1haokqp
MachineLearning,[D] [R] Question Answering Evaluation,"Are there any new metrics to evaluate QA systems (both open-domain and multiple choice) besides the standard Exact Match, F1, Accuracy, BLEU, ROUGE, BERTScore and so on ? I was reading a paper listing all of these metrics (https://arxiv.org/abs/2406.13232) but I’m curious if someone has released, or is currently working on, a new metric which better correlates with human judgment and/or takes into account the form in which LLMs provide answers to questions. For instance, if the models are not fine tuned, it’s hard to make them predict something like “Answer: B” (for multiple-choice QA) or to make them predict some short text like “Barack Obama” (for open-domain QA). This behaviour makes the evaluation of LLMs inconsistent and I’m wondering is someone is actively working on this. ",Debonargon,1han84i,https://reddit.com/r/MachineLearning/comments/1han84i/d_r_question_answering_evaluation/,https://www.reddit.com/r/MachineLearning/comments/1han84i/d_r_question_answering_evaluation/,2024-12-09 23:07:18,0,0.4,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1han84i
MachineLearning,[D] Meta's new LLama model,"So meta just dropped a new, more efficient Llama model, Llama 3.3 70B, that basically promises to cut compute costs for large AI models. Has anyone here had a chance to test it out? Curious to see how it performs compared to previous versions, in terms of speed, resource usage, and accuracy",Frosty_Programmer672,1han33i,https://reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/,https://www.reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/,2024-12-09 23:01:00,21,0.78,21,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1han33i
MachineLearning,[D] How to Ensure Fair Comparison Between Few-Shot Prompting and Fine-Tuning in NLP Experiments,"I’m working on comparing a few-shot prompting mechanism with a fine-tuned GPT model for a text classification task. However, I realised that in a 5-fold validation setup, the fine-tuned model has access to significantly more data (ex: 4 folds for training) compared to my few-shot approach, which only uses a limited number of examples for prompting (At the moment I select n samples per each class from the training fold).  

`Example Scenario: My dataset has 4 classes, total number of data is 100, and I am conducting 4-way 5-shot experiment. For the traning set I have 80 (since it's 5-fold) and for test I have 20 data samples. For the fine-tune experiment I use the whole 80 data but for the few-shot experiment I use only 20 (4*5) data from the 80 traning samples. So the approach has only access to 20 samples of the whole training set.`

This imbalance feels unfair and makes it hard to assess the true performance difference between the two approaches. How can I modify the experimental setup to ensure a fair comparison? Should I restrict the fine-tuned model to use the same examples used in the few-shot prompting mechanism?    


Would love to hear your thoughts and suggestions!  ",The_Aoki_Taki,1haf6qq,https://reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/,https://www.reddit.com/r/MachineLearning/comments/1haf6qq/d_how_to_ensure_fair_comparison_between_fewshot/,2024-12-09 17:32:25,2,0.67,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1haf6qq
MachineLearning,has anyone come across lines in image generated by GAN ? [D],"So I have been working with GAN's for a while, for simple image generation tasks especially training them in unsupervised ways . In many of them the output generated by GAN tend to have visible lines across the images. Here is an example, this happened when I try to generate heat maps. Does any of you have any idea why this happens ?? and ways to deal with them

https://preview.redd.it/c5tuc5udsu5e1.png?width=679&amp;format=png&amp;auto=webp&amp;s=0ecb84a81ec2993a147689ae3112935cc6ecf821

",Brief_Papaya121,1haeb2u,https://reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/,https://www.reddit.com/r/MachineLearning/comments/1haeb2u/has_anyone_come_across_lines_in_image_generated/,2024-12-09 16:56:56,3,0.61,3,0,10,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/TylwiwTTDZWEFdDfi3mI_bSdpiMc72jBYxnwRY3ekpc.jpg,t3_1haeb2u
MachineLearning,[R] Distillation-Based Colorization of 3D Neural Radiance Fields for Consistent Novel View Synthesis,"This paper introduces a knowledge distillation approach to colorize 3D neural representations (NeRF/3DGS) from grayscale multi-view images. The core idea is transferring color information from pre-trained 2D colorization models to 3D scene representations while maintaining view consistency.

Key technical aspects:
- Uses a teacher-student framework where 2D colorization models guide 3D representations
- Works with both Neural Radiance Fields and 3D Gaussian Splatting
- No additional parameters or computation needed during inference
- Handles both indoor/outdoor scenes and different types of grayscale input (IR, historical photos)
- Maintains color consistency across viewpoints through volumetric optimization

Results:
- Matches or exceeds SOTA colorization quality on standard benchmarks
- Successfully colorizes complex scenes with varying lighting/materials
- Works effectively on legacy photographs and infrared images
- Demonstrates consistent colors across novel viewpoints
- Compatible with current NeRF/3DGS implementations

I think this method could be particularly valuable for cultural heritage applications, allowing us to create immersive 3D experiences from historical black and white photographs. The IR imaging capabilities also suggest potential applications in security and surveillance where color visualization of thermal data would be useful.

I think the key strength is how it bridges the gap between 2D colorization and 3D scene understanding without requiring architectural changes to existing 3D representations. This makes it quite practical for real-world adoption.

TLDR: New method colorizes 3D neural scenes from grayscale images using knowledge distillation, works with NeRF/3DGS, maintains view consistency, no extra inference cost.

[Full summary is here](https://aimodels.fyi/papers/arxiv/chromadistill-colorizing-monochrome-radiance-fields-knowledge-distillation). Paper [here](https://arxiv.org/abs/2309.07668).",Successful-Western27,1hae9oo,https://reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/,https://www.reddit.com/r/MachineLearning/comments/1hae9oo/r_distillationbased_colorization_of_3d_neural/,2024-12-09 16:55:19,2,0.67,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1hae9oo
MachineLearning,[D] Has anyone managed to train an LLM with model parallelism?,"Hello,

I am working on fine-tuning Llama-3.1 for my master’s thesis research. Unfortunately, my current situation forbids access to high-memory GPUs such as A100s. Instead, I have access to setups with multiple lower-memory GPUs, such as 4×3090 or 8×V100.

Therefore I need to implement model parallelism to train my model as it doesn’t fit into a single GPU. However, I’ve noticed that most frameworks primarily focus on data parallelism, which doesn’t address my needs.

Has anyone successfully trained a model by splitting it across multiple GPUs? If so, could you recommend frameworks or approaches I should explore? I am specifically looking for full training, although I am interested in hearing if someone managed this using LoRA.

Also, if there’s a more suitable subreddit for this type of question, please direct me to there.

Thank you!",anilozlu,1habr8l,https://reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/,https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/,2024-12-09 15:06:32,46,0.87,46,0,40,0,0,False,False,True,False,False,Discussion,self,t3_1habr8l
MachineLearning,[P] Text-to-Video leaderboard: Compare State-Of-The-Art Text-To-Video Models,"Unlike text generation, text-to-video generation involves balancing realism, alignment, and artistic expression. But which one is the most important in terms of output quality?

We don’t know, that’s why we created a voting-based Text-to-Video Model Leaderboard inspired by the LLM Leaderboard lmarena.ai.

Currently, the leaderboard features five open-source models: HunyuanVideo, Mochi1, CogVideoX-5b, Open-Sora 1.2 and PyramidFlow, but we’re aiming to also include notable proprietary models from Kling AI, LumaLabs.ai and Pika.art.

Here’s a link to the leaderboard: [link](https://t2vleaderboard.lambdalabs.com/leaderboard/).  
We’d love to hear your thoughts, feedback, or suggestions. How do you think video generation models should be evaluated?",lambda-research,1ha54m0,https://reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/,https://www.reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/,2024-12-09 08:17:05,17,0.95,17,0,2,0,0,False,False,True,False,False,Project,self,t3_1ha54m0
MachineLearning,[R] Monet: Mixture of Monosemantic Experts for Transformers,"**Paper**: [https://arxiv.org/abs/2412.04139](https://arxiv.org/abs/2412.04139)  
**GitHub**: [https://github.com/dmis-lab/Monet](https://github.com/dmis-lab/Monet)

**Monet** presents a novel approach to enhancing mechanistic interpretability in large language models (LLMs) through an innovative Sparse Mixture-of-Experts (SMoE) architecture. By directly incorporating sparse dictionary learning into end-to-end pretraining, **Monet** addresses the fundamental challenge of polysemanticity - where individual neurons respond to multiple unrelated concepts - while maintaining model performance.

**Key Highlights:**

* **Scalable Expert Architecture**: **Monet** introduces parameter-efficient expert decomposition methods that enable scaling to 262,144 experts per layer while ensuring total parameters scale proportionally to the square root of expert count.
* **Monosemantic Experts**: Through fine-grained expert specialization, **Monet** achieves monosemantic experts that demonstrate mutual exclusivity of knowledge, allowing transparent observation of model behavior and parametric knowledge.
* **Robust Knowledge Control**: The architecture enables precise manipulation of domain-specific knowledge, language capabilities, and toxicity mitigation without compromising general performance.

**Why Monet?**

Unlike traditional approaches using post-hoc reconstruction (like Sparse Autoencoders), **Monet** integrates interpretability directly into its architecture. This enables both transparent understanding of model internals and fundamental behavior control. By scaling monosemantic experts, Monet paves the way for more transparent and controllable language models.

We’d love to hear your feedback, questions, or any other inquiries you may have!",affjljoo3581,1ha4inl,https://reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/,https://www.reddit.com/r/MachineLearning/comments/1ha4inl/r_monet_mixture_of_monosemantic_experts_for/,2024-12-09 07:31:34,9,0.74,9,0,0,0,0,False,False,True,False,False,Research,self,t3_1ha4inl
MachineLearning,[P] Looking for daily keyword search database (any platform),"Hey there,

  
After exhaustively searching Google and trying to find APIs that would allow me to generate keyword search or post or comment frequency on any platform on a *daily* basis, I have been unable to find any providers of this type of data. Considering that this is kind of a niche request, I am dropping this inquiry here for the ML Gods of Reddit to assist.

Basically, I'm trying to create an ML model that can predict future increases/decreases in keyword usage (whether that be on Google Search or X posts; dosen't matter) on a daily basis. I've found plenty of monthly average keyword search providers but I cannot find any way to access more granulated, daily search totals for any platform. If you know of any sources for this kind of data, please drop them here... Or just tell me to give up if this is an impossible feat.",Appropriate-Touch515,1ha465g,https://reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/,https://www.reddit.com/r/MachineLearning/comments/1ha465g/p_looking_for_daily_keyword_search_database_any/,2024-12-09 07:06:32,1,0.6,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1ha465g
MachineLearning,[R] O1 replication paper,"Hi Everyone,

Just released a paper that I think hints at how OpenAI might have developed some of O1's remarkable reasoning capabilities. TLDR- you need a small dataset of really high quality human paired with a little bit of RL

Here are some of they key take ways from the research

* Reasoning data is extremely scarce on the internet. It's very difficult to find data that really shows the problem solving process e.g hypothesis testing, backtracking etc
* RL although important is overrated by the general community. It's really the cherry on top. The human data does most of the heavy lifting. See deep-seek math for more info on this

Paper can be found here: [https://arxiv.org/abs/2412.04645](https://arxiv.org/abs/2412.04645)

Not saying this is definitively how o1 works but results suggest that this method can be used to create very similar behaviour

Paper a preprint so happy to clarify anything that's not clear.

Happy to answer any questions on the paper.",Brosarr,1h9zjf1,https://reddit.com/r/MachineLearning/comments/1h9zjf1/r_o1_replication_paper/,https://www.reddit.com/r/MachineLearning/comments/1h9zjf1/r_o1_replication_paper/,2024-12-09 02:31:35,0,0.42,0,0,7,0,0,False,False,True,False,False,Research,self,t3_1h9zjf1
MachineLearning,[D] Offline AI/ML activity for high school students?,"Next week, I am giving an hour of code presentation at a local high school. Since I run my university's artificial intelligence club, I'd like to center it around AI. I've done some AI focused activities with high schoolers with various levels of success in the past, but haven't found ""the one"" yet. Any ideas on what I could try next? I'll list what I've done so far as well as the restrictions are this particular event.

Restrictions:

1. These are random students in grades 9-12, so I can't rely on any prior computer science knowledge
2. We don't have access to computers so everything has to be offline. I *may* be able to get ipads for them though
3. We have 1 hour for the event. Filling around 45 minutes would be ideal so we don't go over and we don't end up with too much extra time.

Things I've tried before:

1. AI Kahoot + Simple lecture explaining neural networks - The students liked the kahoot, but I dramatically overestimated their knowledge (and interest) in AI. Stupidly thought at least a handful of them would have at least heard of a neural net, but not a single student had lmao
2. Wordle AI pseudocode and code along - I had the students get into groups and brainstorm how they would make an algorithm to solve the wordle. Then I coded their solutions on the projector and we competed against my algorithm. This one was cool, but I think it only really works if the students are *really engaged* and I really don't think this group will be that engaged.
3. Image classification code along - I've done this by myself on the projector (bad) and as a group with students with some CS experience (decent). But this one wouldn't work for this event I just thought I'd include it
4. Turing test activity - Have the students guess which answer came from ChatGPT and which came from a student volunteer. The volunteer will leave the room and the rest of the students will ask a question (like ""what is the meaning of life"" or something). I'll write the volunteer's answer and ChatGPT's answer on a powerpoint slide and have the student guess which was GPT. This has been really successful with middle school students in the past, but I worry it's a bit too childish for high schoolers.

\---

I tend to struggle to figure out activities for high school students because I don't want to undermine their intelligence but I don't want to throw a complicated activity at them that they don't understand.",j0ngle6421,1h9ks8v,https://reddit.com/r/MachineLearning/comments/1h9ks8v/d_offline_aiml_activity_for_high_school_students/,https://www.reddit.com/r/MachineLearning/comments/1h9ks8v/d_offline_aiml_activity_for_high_school_students/,2024-12-08 15:08:37,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h9ks8v
MachineLearning,"[D] Is there such a thing as ""integrable programming""?","I come from a pure math background and have been getting up to speed at a new job in scientific AI/ML where I've been working a lot with JAX. JAX is great, love it, but I see a super common pattern where researchers will have a fully differentiable simulation and a couple of neural net architectures or something, but then there will be a bunch of relatively imprecise numerical estimations of integral values. Obviously I'm reading up on numerical methods and doing my best to restructure problems to solve more algebraically, but for my own curiosity, is there an equivalent of ""differentiable"" programming where you're handling ""integrable"" entities instead?

Obviously, this would be a much harder class of problems since you can integrate...well, everything. And that's how you end up solving PDEs on compact supports with weird ugly Holder bounds. But are there computational approaches (or hell, differentiable programming strategies I should be aware of) that move in this direction? Are there nice natural algebraic properties to be leveraged? Can you use a computational graph the same sort of way? How about, like, valid ways to extend to ""weakly-differentiable"" functional programming?

Hope this is relevant enough since it's learning JAX inspired...",redwingviking,1h9ty31,https://reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/,https://www.reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/,2024-12-08 21:56:30,32,0.94,32,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1h9ty31
MachineLearning,"[R] Diffusion Models, Image Super-Resolution, and Everything: A Survey","We are thrilled to share with you guys our latest survey paper on diffusion models applied to image super-resolution. You are welcome to take a look. It is also open access and published in IEEE TNNLS :) 



arXiv: [https://arxiv.org/abs/2401.00736](https://arxiv.org/abs/2401.00736)",Maleficent_Stay_7737,1h9wrv6,https://reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/,https://www.reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/,2024-12-09 00:09:15,98,0.97,98,0,1,0,0,False,False,True,False,False,Research,self,t3_1h9wrv6
MachineLearning,[P] I got too frustrated trying to test all these AI cookbooks and recipes,"Over the last year of building AI enabled SaaS applications I became increasingly frustrated at the developer experience of going from AI RAG cookbooks authored in jupyter notebooks to integrating it into my application. Notebooks are great and all but it's incredibly hard to test which part of it was actually important for my app. This led me down the road of having to understand every piece of code in each notebook, deciphering what was important, somehow build an API server as a POC to then hook it into my app. The feedback loop was excruciatingly long, painful, and most of the time I canned the POC because it wasn't quite what I wanted.

this is when it dawned on me that the roles in the AI developer world are fractured into two. Data Scientists and AI devs want easy notebooks to test methods and techniques but do not care to ship something that can be easily be consumed by applications.

In the other camp lies application devs, they just want simple API's that they can use to test quickly and verify these AI methods enhance their application.

Enter KitchenAI.

A way to bridge the gap between the two by converting AI related Jupyter notebooks into a ready made production API server so that it becomes easy to test various cookbooks, recipes, and techniques. Shortening the development cycle in half while giving users a complete local experience with the ability to share them as docker containers.

Completely vendor agnostic and framework agnostic, the goal is to give developers the most about of freedom to use the libraries they already feel most comfortable using.

It comes with a plugin architecture so I envision our team and the community building all sorts of llmops type plugins like evaluation frameworks, observability, prompt management and more.

A lot of hard work was put to provide something that is totally open source, local, and with battle tested technology like Django so that developers didn't have to rely on 3rd party providers.

We’ve launched this repo under Apache license so any developer can use the tool. We're working hard to provide a managed cloud version with much deeper integrations, metrics, analytics, and workflows for those that want have more complex demands

Give it a spin: [https://github.com/epuerta9/kitchenai.](https://github.com/epuerta9/kitchenai) Let us know what you think!",wait-a-minut,1h9wcdo,https://reddit.com/r/MachineLearning/comments/1h9wcdo/p_i_got_too_frustrated_trying_to_test_all_these/,https://www.reddit.com/r/MachineLearning/comments/1h9wcdo/p_i_got_too_frustrated_trying_to_test_all_these/,2024-12-08 23:48:32,0,0.42,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1h9wcdo
MachineLearning,[Project] Simulating Kubernetes Monitoring Data for a Deep Learning Prototype—Any Thoughts?,"
We aim to build a prototype project using Deep Learning, but we require a dataset containing Kubernetes deployment metrics. I initially tried sourcing data from my company, but—spoiler alert—we obviously can’t use it.

Our current idea is to create a virtual lab with a small Kubernetes cluster running a custom app. Using JMeter, we plan to simulate random scenarios to generate traffic similar to the microservices deployments in our company.

The synthetic data generated will be used to train our prototype. Afterward, we’ll test the model by slightly modifying the JMeter scenarios and evaluating its performance against Kubernetes’ default algorithm.

What are your thoughts? I know there’s existing literature on this, but I’d love to hear your expert opinion.

Thanks!",Ok-Consequence-8863,1h9vru3,https://reddit.com/r/MachineLearning/comments/1h9vru3/project_simulating_kubernetes_monitoring_data_for/,https://www.reddit.com/r/MachineLearning/comments/1h9vru3/project_simulating_kubernetes_monitoring_data_for/,2024-12-08 23:20:46,0,0.4,0,0,0,0,0,False,False,True,False,False,Project,self,t3_1h9vru3
MachineLearning,[D] Context-aware entity recognition using LLMs,"Can anybody suggest some good models that can perform entity recognition but using LLM-level context? Such models are generally LLMs fine-tuned for Entity Recognition.
Usually, using traditional NER/ER pipelines, such as SpaCy's NER model, can only tag words that it has been trained on. Using LLMs fine-tuned for Entity Recognition (models such as GLiNER) can tag obscure entities, and not just basic entities such as Name, Place, Org, etc.",Ashwiihii,1h9stfq,https://reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h9stfq/d_contextaware_entity_recognition_using_llms/,2024-12-08 21:05:35,10,0.78,10,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h9stfq
MachineLearning,[R] Should I Use ML Experiment Tracking Tools Like MLflow or DVC for my Academic Paper?,"Hi everyone!

I'm a Computer Science graduate currently working on machine learning experiments for a research paper. I have a dataset and plan to compare error metrics across several deep learning models.

The conference where I intent to submit my paper requires that I provide the code and dataset and I also strongly believe that reproducibility is crucial in academic research. To this end, I'm using Docker and pip-compile to make the environment as reproducible as possible.

That said, I know there are tools like MLFlow and DVC for tracking ML experiments. However, I've never seen these tools mentioned in the code accompanying academic papers.

My questions are:

1. Are there any academic papers that use ML experiment tracking tools like MLFlow or DVC?
2. Should I use these tools for my research, even it means additional work?

I'm also experimenting with DVC because it stores experiments outputs in Git. However, my project involves running many distinct experiments in a single repository (comparing multiple ML algorithms). Would DVC or another tool be the best choice for this kind of workflow? Or is using such tools overkill for academic papers?

",mrlucasrib,1h9ig1e,https://reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/,https://www.reddit.com/r/MachineLearning/comments/1h9ig1e/r_should_i_use_ml_experiment_tracking_tools_like/,2024-12-08 13:05:06,11,0.87,11,0,5,0,0,False,False,True,False,False,Research,self,t3_1h9ig1e
MachineLearning,"[D] Last Week in Medical AI: Top LLM Research Papers/Models (December 2 - December 7, 2024)","[\[D\] Last Week in Medical AI: Top LLM Research Papers\/Models \(December 2 - December 7, 2024\)](https://preview.redd.it/exeie0jxdm5e1.jpg?width=1386&amp;format=pjpg&amp;auto=webp&amp;s=d1fad8f6511ebc4dd9e8c73cc98ca9f6d45f750d)

  
**Medical LLM &amp; Models**

* Block MedCare: Blockchain AI &amp; IoT
   * This research proposes a novel Ethereum-based system for secure and efficient Electronic Health Record (EHR) management, empowering patients with data control.
* LLMs4Life: Biomedical Ontology Learning
   * This paper extends the NeOn-GPT pipeline for ontology learning using LLMs with advanced prompt engineering and ontology reuse to improve generated ontologies' domain-specific reasoning and structural depth in complex domains like life sciences.
* LLaMA II for Multimodal Diagnosis
   * This paper explores multimodal fusion methods for medical data using a transformer-based model with a LLaMA II backbone, focusing on disease classification with chest X-rays and clinical reports from the OpenI dataset.
* Compact LLM for EHR Privacy
   * This paper introduces a compact LLM framework for local deployment in healthcare settings with strict privacy requirements and limited resources.  It uses a novel preprocessing technique with information extraction methods like regular expressions to enhance smaller LLM performance on EHR data.

**Frameworks &amp; Methods**

\- RARE: Retrieval-Augmented Reasoning  
\- STORM: Strategies for Rare Events  
\- TransFair: Fair Disease Classification  
\- PePR: Performance Per Resource  
\- Medical LLM Best Practices

**LLM Applications**

\- Medchain: LLMs in Clinical Practice  
\- Query Nursing Note Summarization  
\- CLINICSUM: Patient Conversation Summaries  
\- Text Embeddings for Classifiers

**LLM Benchmarks**

\- Polish Medical Exams Transfer  
\- Single-Cell Omics Annotation  
\- LLMs in Precision Medicine  
\- Low-Resource Healthcare Challenges

**Other Models**

\- LLM Chatbot Hallucinations  
\- Multi-stage Chest X-ray Diagnosis  
\- EchoONE: Echocardiography AI  
\- Radiology Report Grounding

**Ethics &amp; Fairness**

\- Privacy in Medical Imaging  
\- Demographic Fairness in AI

**Datasets**

\- LLM Scientific Knowledge Extraction  
\- Biomedical Knowledge Review

Full thread in detail: [https://x.com/OpenlifesciAI/status/1865584829057929303](https://x.com/OpenlifesciAI/status/1865584829057929303)",aadityaura,1h9hytj,https://reddit.com/r/MachineLearning/comments/1h9hytj/d_last_week_in_medical_ai_top_llm_research/,https://www.reddit.com/r/MachineLearning/comments/1h9hytj/d_last_week_in_medical_ai_top_llm_research/,2024-12-08 12:37:06,2,0.6,2,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ui7fRqyxPWPs0C2SeEyfrrVgHu2f6NKgstvq8_5XvWs.jpg,t3_1h9hytj
MachineLearning,[D] A collection of various LLM Sampling methods,"In the last couple months, I read about various algorithms to perform LLM sampling. I decided to build my own inference stack and implement those algorithms. 

Here is the Github repo - [https://github.com/shreyansh26/LLM-Sampling](https://github.com/shreyansh26/LLM-Sampling)

The repo includes implementations for Top-k, Top-p (nucleus), Min-p, Typical, Epsilon, Eta, Beam search, Chain-of-Thought (CoT) decoding, Constrained JSON decoding and Speculative decoding.

Personally, I found this to be a good learning experience. Sharing here in case it helps someone!",shreyansh26,1h9fe8q,https://reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/,https://www.reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/,2024-12-08 09:39:17,40,0.93,40,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h9fe8q
MachineLearning,[P] 🥂 FineWeb2 dataset: A sparkling update with 1000s of languages,,PhilipsNostrum,1h9ep0e,https://reddit.com/r/MachineLearning/comments/1h9ep0e/p_fineweb2_dataset_a_sparkling_update_with_1000s/,https://huggingface.co/datasets/HuggingFaceFW/fineweb-2,2024-12-08 08:47:55,54,0.98,54,0,2,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/wJFr_ML_3llzyRU5cjBT-wmv2Y189rHFs1r363OriqY.jpg,t3_1h9ep0e
MachineLearning,"How do you manage resources or optimize cost when training models in cloud services like aws sagemaker, or gcp vertex ai? [D]","Hey all, I've been using sagemaker quite a bit lately for training ML models and doing deployments. I know enough about aws and instance types to create training nodes that have enough capacity to train my models, but many times I am underutilizing RAM, GPU memory, or CPUs, so it feels like this leads to a lot of waste (and extra cost).  
How do you guys figure out what type of instance or resources would best fit your needs without being too wasteful?  
Is there any way to adjust resources automatically, or any library that could handle that for you?",InformationEmpty1440,1h93rlt,https://reddit.com/r/MachineLearning/comments/1h93rlt/how_do_you_manage_resources_or_optimize_cost_when/,https://www.reddit.com/r/MachineLearning/comments/1h93rlt/how_do_you_manage_resources_or_optimize_cost_when/,2024-12-07 22:17:25,5,0.86,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h93rlt
MachineLearning,[R]  GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?,"Hi everyone,

I’m currently working through the recent paper “GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?”, but I’ve run into some issues and was hoping someone here might have insights.

* Posterior Distribution: In the implementation, a posterior distribution is used, but I couldn’t find the formula or explanation in the paper. Does anyone know where this comes from or how it’s derived?
* Asynchronous Model: The paper and its implementation don’t seem entirely consistent when it comes to the asynchronous model. Specifically:
   * Is the generation process done step-by-step asynchronously?
   * Or does it first denoise the attribute vectors entirely before moving on to edge denoising?

I’ve tried searching online, but since this is a new paper, there isn’t much discussion or documentation yet. Any help, advice, or pointers would be greatly appreciated!",Noname_emanon_,1h8z44t,https://reddit.com/r/MachineLearning/comments/1h8z44t/r_graphmaker_can_diffusion_models_generate_large/,https://www.reddit.com/r/MachineLearning/comments/1h8z44t/r_graphmaker_can_diffusion_models_generate_large/,2024-12-07 18:43:09,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h8z44t
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1h99kae,https://reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1h99kae/d_selfpromotion_thread/,2024-12-08 03:15:09,4,0.64,4,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h99kae
MachineLearning,[P] Extract Transcripts with Positive Emotions in batch,"Check out this example project on how to find transcripts of audio recordings with positive emotions. A good example of a project demonstrating of extract actionable insights from audio!

It takes **common voice** dataset of audio files from hagging face, applies emotion recognition model and **whisper-tiny** model for the transcripts. All is organized in a nice looking batch pipeline. 

An interesting detail - No need to extract archives! This pipeline analyzes audio files directly from tar archives, saving you extra steps.

Video: [https://www.youtube.com/watch?v=OCm5W0L5BTU](https://www.youtube.com/watch?v=OCm5W0L5BTU)  
Colab notebook: [https://colab.research.google.com/github/iterative/datachain-examples/blob/main/audio/hf\_common\_voice.ipynb](https://colab.research.google.com/github/iterative/datachain-examples/blob/main/audio/hf_common_voice.ipynb)  
Jupyter Notebook: [https://github.com/iterative/datachain-examples/blob/main/audio/hf\_common\_voice.ipynb](https://github.com/iterative/datachain-examples/blob/main/audio/hf_common_voice.ipynb)",dmpetrov,1h92b70,https://reddit.com/r/MachineLearning/comments/1h92b70/p_extract_transcripts_with_positive_emotions_in/,https://www.reddit.com/r/MachineLearning/comments/1h92b70/p_extract_transcripts_with_positive_emotions_in/,2024-12-07 21:08:30,1,0.6,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1h92b70
MachineLearning,"[P] I cannot find this open-source transformer on GitHub, released recently, for the life of me.
","There was a paper released along with a GitHub repository of an extremely well-made transformer designed for testing out new components. But I can't find it! It's not one of the ones that has existed like HuggingFace ones. Any clue?

",Breck_Emert,1h8zlz3,https://reddit.com/r/MachineLearning/comments/1h8zlz3/p_i_cannot_find_this_opensource_transformer_on/,https://www.reddit.com/r/MachineLearning/comments/1h8zlz3/p_i_cannot_find_this_opensource_transformer_on/,2024-12-07 19:05:21,14,0.64,14,0,19,0,0,False,False,True,False,False,Project,self,t3_1h8zlz3
MachineLearning,How to solve the STT Cutoff Problem [D],"Hello folks, 

I've been working on an agentic solution where you can have an autonomous agent taking live calls. We're using a pipeline of Speech to Text, LLM for generating responses and then Text to Speech. In this pipeline, Speech to text is causing some issues because it's difficult to determine when exactly a sentence is over since the user can take pauses. Moreover, when multiple inputs go into LLM, multiple responses are generated and they queue up for Text to speech. How would you solve this problem? How would you also handle cases where the user interrupts the agent?",Leo2000Immortal,1h8r32q,https://reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,https://www.reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,2024-12-07 12:04:55,1,0.6,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h8r32q
MachineLearning,[D] AAAI 2025 Phase 2 Decision,"When would the phase 2 decision come out?  
I know the date is December 9th, but would there be chances for the result to come out earlier than the announced date?  
or did it open the result at exact time in previous years? (i.e., 2024, 2023, 2022 ....)

Kinda make me sick to keep waiting.",No-Style-7975,1h8kkjv,https://reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,https://www.reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,2024-12-07 04:27:30,50,0.95,50,0,249,0,0,False,False,True,False,False,Discussion,self,t3_1h8kkjv
MachineLearning,[D] How to actually prevent overfitting in practice in ScikitLearn ?,"We all saw in class the trade off between bias and variance, that we don't want our train loss to keep going down and our test loss go up. 

But in practice I feel like doing hyperparameter tuning for classic ML models with GridSearchCV / BayesSearchCV is not enough. Even though I do cross validation, the search.best\_model obtained at the end is almost always overfitting. 

How can you actually perform a search that will give you a robust generalized model with higher chances ? ",desslyie,1h8paqz,https://reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,https://www.reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,2024-12-07 09:55:06,0,0.29,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h8paqz
MachineLearning,"[N] Sama, an AI sweatshop, pays workers in Kenya $2 an hour to filter and label porn, beastiality, suicide, child abuse, for hours on end!!",,BotherBubbly5096,1h8nhbh,https://reddit.com/r/MachineLearning/comments/1h8nhbh/n_sama_an_ai_sweatshop_pays_workers_in_kenya_2_an/,https://youtu.be/qZS50KXjAX0,2024-12-07 07:38:08,328,0.85,328,0,124,0,0,False,False,False,False,False,News,https://b.thumbs.redditmedia.com/6xyP-Tq6WRzMpjniGg5h_3Z_bhf29lyA0LV_LQ19-1s.jpg,t3_1h8nhbh
MachineLearning,[R] Zero shot Meme-interpretability of LLMs,"Head to head of meme-interpretability with the same image and text prompt!

Anecdotal but interesting responses. 

Also clear winner!

",No_Cartoonist8629,1h8nc78,https://reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,https://www.reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,2024-12-07 07:27:43,0,0.27,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1h8nc78
MachineLearning,[R] For a change of topic: some nonLLM focused work of mine: Bias-Free Sentiment Analysis through Semantic Blinding and Graph Neural Networks,"In my academic field (social sciences) I deal with the problem of bias in SA models. My previous work showed that deep learning SA systems inherit bias (e.g. nonrepresentative of the population political bias) from annotators: 

https://arxiv.org/abs/2407.13891

Now I devised a solution that used a technique I call semantic blinding to provide only the bare necessary information for the model to predict emotions in text, leaving no signal for the model to overfit and produce bias from:

https://arxiv.org/abs/2411.12493

Interested to hear your thoughts before I publish the SProp Gnn.

Do you think it could be useful beyond the academia?



",Hub_Pli,1h8meas,https://reddit.com/r/MachineLearning/comments/1h8meas/r_for_a_change_of_topic_some_nonllm_focused_work/,https://i.redd.it/vh80i11ndd5e1.jpeg,2024-12-07 06:21:47,51,0.87,51,0,10,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/sSwXTBHlRlELGHTRvPNAxo6LSQVRdxL5HTNiMSYTfTs.jpg,t3_1h8meas
MachineLearning,[R] JAX vs TensorFlow-XLA ,"
Few months ago, I migrated from TF 2.0 to Jax. I found that jax is significantly faster than Tf. I noticed in the official documentation that it relies on XLA default that uses JIT compilation which makes execution faster. I also noticed that TF graphs also have option to enable JIT compilation with XLA. But still jax dominates TF with XLA. I just want to know why.",Odd-Detective289,1h8j2e5,https://reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,https://www.reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,2024-12-07 03:02:19,17,0.95,17,0,7,0,0,False,False,True,False,False,Research,self,t3_1h8j2e5
MachineLearning,[D] Multimodal AI,"Multimodal AI is changing the game by combining text, images, and even video into a single, cohesive system. It’s being talked about as a major leap in AI capabilities.

What industries do you think will benefit the most from this tech? And are there any challenges you see in integrating these models into everyday use?

Would love to hear everyone's thoughts!",Frosty_Programmer672,1h8enzy,https://reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,https://www.reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,2024-12-06 23:17:54,0,0.09,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h8enzy
MachineLearning,[D] selective transfer learning ,"Hello everyone,

I am looking for methods that can automatically categorize and select layers from  for transfer learning. If you know any such methods or research please let me know or share. 

Thanks ",reshail_raza,1h8cawc,https://reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,https://www.reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,2024-12-06 21:30:44,0,0.5,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h8cawc
MachineLearning,[R] Agentic Retrieval Augmented Generation with Memory,"Imagine a customer support chatbot for an e-commerce platform that retrieves relevant product details from its knowledge base and performs web searches for additional information. Furthermore, it remembers past conversations to deliver a seamless and personalized experience for returning users.   
  
Here is how it works:  
  
\- Store your own data in the knowledge base—in our case, a Website URL.  
\- Convert the data into embeddings and save it in the Qdrant Vector Database.  
\- Use phidata Agentic Workflow to combine Tools, LLM, Memory, and the Knowledge Base.

Code Implementation Video: [https://www.youtube.com/watch?v=CDC3GOuJyZ0](https://www.youtube.com/watch?v=CDC3GOuJyZ0)",External_Ad_11,1h8945d,https://reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,https://www.reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,2024-12-06 19:10:50,8,0.71,8,0,1,0,0,False,False,True,False,False,Research,self,t3_1h8945d
MachineLearning,[R] Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis,"New paper and code for the scale-wise transformer for fast text-to-image generation from our team at Yandex Research

Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being faster than distilled diffusion models.

Code with checkpoints: [https://github.com/yandex-research/switti](https://github.com/yandex-research/switti)

[Generation examples](https://preview.redd.it/7jy3jfxhi95e1.png?width=3094&amp;format=png&amp;auto=webp&amp;s=e80ed27c0b746ec782026581500582a5dd03555d)

",_puhsu,1h85z2c,https://reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,https://www.reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,2024-12-06 16:58:21,15,0.89,15,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/Io25PLG5YopCFj4DOTGJ-MxIEcQdXRQbpwevn2M-Kts.jpg,t3_1h85z2c
MachineLearning,[D] Have we officially figured out yet how O1 models differ from previous models?,"Edit: I have misworded the title as if OpenAI would confirm how O1 was implemented. I have changed the text to reflect what I meant say.



I really want to deep dive into the technicals of how the O1 models perform better than previous models.

Have researchers come to any definitive agreement as to what OpenAI could have possible done to achieve O1?

From reading online I hear about MCTS, COT... etc, but are any of these methods in large agreement by researhers?
",Daveboi7,1h7zfjg,https://reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,2024-12-06 11:37:51,18,0.88,18,0,28,0,0,False,False,True,False,False,Discussion,self,t3_1h7zfjg
MachineLearning,[D] Encode over 100 million rows into embeddings ,"Hey everyone,

I'm working on a pipeline to encode over **100 million rows** into embeddings using **SentenceTransformers**, **PySpark**, and **Pandas UDF** on **Dataproc Serverless**.

Currently, it takes several hours to process everything. I only have one column containing sentences, each under 30 characters long. These are encoded into **64-dimensional vectors** using a custom model in a Docker image.

At the moment, the job has been running for over **12 hours** with **57 executors** (each with **24GB of memory and 4 cores**). I’ve partitioned the data into **2000 partitions**, hoping to speed up the process, but it's still slow.

Here’s the core part of my code:

    F.pandas_udf(returnType=ArrayType(FloatType()))
    def encode_pd(x: pd.Series) -&gt; pd.Series:
        try:
            model = load_model()
            return pd.Series(model.encode(x, batch_size=512).tolist())
        except Exception as e:
            logger.error(f""Error in encode_pd function: {str(e)}"")
            raise

The `load_model` function is as follows:

    def load_model() -&gt; SentenceTransformer:
        model = SentenceTransformer(
            ""custom_model"", 
            device=""cpu"", 
            cache_folder=os.environ['SENTENCE_TRANSFORMERS_HOME'], 
            truncate_dim=64
        )
        return model

I tried broadcasting the model, but I couldn't refer to it inside the Pandas UDF.

Does anyone have suggestions to optimize this? Perhaps ways to load the model more efficiently, reduce execution time, or better utilize resources?",nidalap24,1h7xnce,https://reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,2024-12-06 09:29:37,13,0.79,13,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h7xnce
MachineLearning,[D] Any OCR recommendations for illegible handwriting?,"
Has anyone had experience using an ML model to recognize handwriting like this? The notebook contains important information that could help me decode a puzzle I’m solving. I have a total of five notebooks, all from the same person, with consistent handwriting patterns. My goal is to use ML to recognize and extract the notes, then convert them into a digital format.

I was considering Google API after knowing that Tesseract might not work well with illegible samples like this. However, I’m not sure if Google API will be able to read it either. I read somewhere that OCR+ CNN might work, so I’m here asking for suggestions. Thanks! Any advice/suggestions are welcomed! ",SpaceSheep23,1h7x5us,https://reddit.com/r/MachineLearning/comments/1h7x5us/d_any_ocr_recommendations_for_illegible/,https://www.reddit.com/gallery/1h7x5us,2024-12-06 08:53:03,210,0.91,210,0,174,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/LeDstkPiGT-yO7zkyvRRyx6sE33qB2W32UGe1t81Kkk.jpg,t3_1h7x5us
MachineLearning,OpenAI CLIP model [D],"How long do you think OpenAI researchers were working on CLIP model before they published the results? 
The paper in my opinion is revolutionary.",NeatJealous8110,1h7wc59,https://reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,https://www.reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,2024-12-06 07:50:28,0,0.25,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7wc59
MachineLearning,[D] How does OpenAI’s O1 outperform others in math despite limitations noted in recent papers?,"Recent research has revealed that state-of-the-art LLMs often struggle with mathematical reasoning:

1. The GSM-Symbolic benchmark highlights that LLMs frequently fail when numerical values or question wording change, suggesting reliance on memorization rather than true mathematical understanding ([source](https://arxiv.org/pdf/2410.05229.pdf)).
2. Logical reasoning studies, like the AIW problem, show inconsistent performance even for basic reasoning tasks ([source](https://arxiv.org/pdf/2406.02061.pdf)).
3. Furthermore, research indicates LLMs lack effective self-correction capabilities, with performance degrading after multiple iterations ([source](https://arxiv.org/pdf/2310.01798.pdf)).

Despite these challenges, OpenAI’s new O1 model reportedly exceeds all other models in math benchmarks. How does it address these known issues in mathematical reasoning, such as:

* Reliance on memorization instead of understanding?
* Inconsistencies in reasoning across problem variations?
* Inability to self-correct errors effectively?

Would love to hear insights or hypotheses!",AImSamy,1h7vj5t,https://reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,2024-12-06 06:53:09,63,0.89,63,0,42,0,0,False,False,True,False,False,Discussion,self,t3_1h7vj5t
MachineLearning,[D] My fine-tuning loss looks weird,"I am finetuning Qwen2.5 instruct using qLoRA, for a instruction tuning like dataset with around 50k samples, and my training loss is looking weird. What might be the issue, and how can i possibly fix it? Finetuning details are as following, along with training loss graphs:

Code:

\`\`\`  
model, tokenizer = FastLanguageModel.from\_pretrained(

model\_name = ""Qwen/Qwen2.5-32B-Instruct"",

max\_seq\_length = max\_seq\_length,

dtype = None,

load\_in\_4bit = True,

)



\# Do model patching and add fast LoRA weights

model = FastLanguageModel.get\_peft\_model(

model,

r = 64,

target\_modules = \[""q\_proj"", ""k\_proj"", ""v\_proj"", ""o\_proj"",

""gate\_proj"", ""up\_proj"", ""down\_proj"",\],

lora\_alpha = 128,

lora\_dropout = 0, # Supports any, but = 0 is optimized

bias = ""none"",    # Supports any, but = ""none"" is optimized

use\_gradient\_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context

random\_state = 3407,

max\_seq\_length = max\_seq\_length,

use\_rslora = True,  # We support rank stabilized LoRA

loftq\_config = None, # And LoftQ

)



from trl import SFTTrainer

from transformers import TrainingArguments

from unsloth import is\_bfloat16\_supported



trainer = SFTTrainer(

model = model,

tokenizer = tokenizer,

train\_dataset = dataset\['train'\],

dataset\_text\_field = ""text"",

max\_seq\_length = max\_seq\_length,

dataset\_num\_proc = 2,

packing = False,

args = TrainingArguments(

per\_device\_train\_batch\_size = 4,

gradient\_accumulation\_steps = 2,

warmup\_steps = 5,

num\_train\_epochs = 3,

learning\_rate = 0.0002,

fp16 = not is\_bfloat16\_supported(),

bf16 = is\_bfloat16\_supported(),

logging\_steps = 10,

optim = ""adamw\_8bit"",

weight\_decay = 0.01,

lr\_scheduler\_type = ""linear"",

seed = 69,

output\_dir = ""outputs"",

report\_to = ""wandb"",

save\_strategy = ""steps"",

save\_steps = 50,

save\_total\_limit=10

),

)  
\`\`\`

Training Loss:

https://preview.redd.it/s2vn2z44y55e1.png?width=2888&amp;format=png&amp;auto=webp&amp;s=a4e1038e9c27dae96d7e25fcb5db852c794efd97

",Raise_Fickle,1h7u38s,https://reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,https://www.reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,2024-12-06 05:22:31,1,0.57,1,0,6,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/I78OJiWF97hbVSTiF0mWFfXxPxZbdE3PavnYvIriLI8.jpg,t3_1h7u38s
MachineLearning,[R] Towards Time Series Reasoning with LLMs,,HydrousIt,1h7sr3n,https://reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/,https://arxiv.org/abs/2409.11376,2024-12-06 04:06:32,12,0.88,12,0,2,0,0,False,False,False,False,False,Research,default,t3_1h7sr3n
MachineLearning,[D] How to remove noise in this dataset,"I have a dataset that, when plotted, shows a noisy black line. I'd like to smooth out this noise to get a cleaner trend line (similar to the red line shown). What methods would you recommend for noise reduction?

https://preview.redd.it/p8q0i4f9h45e1.png?width=1574&amp;format=png&amp;auto=webp&amp;s=29cd8c82af7b54a1d3da22655502fd5cf406e807",mrtule,1h7ohd0,https://reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,2024-12-06 00:30:30,6,0.88,6,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1h7ohd0
MachineLearning,[D] Any public LLM inference APIs with input token log-probs?,"Does anyone know of any services that offer input token log-probs from open source LLMs like LLama-8B?

I'm looking for a cost-effective way to host an LLM-based application with low traffic, so I'd like per-query or per-token pricing, rather than per-hour GPU rental. It's unfortunately dependent on direct-access to log-probs on user-provided tokens.

None of the ""chat completion"" APIs I've found seem to expose this. It makes sense for private models, but for open source models, I don't see any downside to exposing it.",severed-identity,1h7o30r,https://reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,https://www.reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,2024-12-06 00:11:38,2,0.75,2,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h7o30r
MachineLearning,[P] Look-a-like modeling,"Hi everybody. I have a list of user actions (around 1m objects) where only a small fraction (less than 1000) are labeled. I want to find most similar objects to them. What is a good way to approach it? 

I personally have 2 ideas in mind: one class classification or unsupervised clustering. My problem with the first is that I know only 1 suitable model (one class svm) and it can be too simple for my data. Problem with second one is obvious - it's unsupervised and labeling will be used only at the final step, so their efficiency is not guaranteed.",Jor_ez,1h7nv2n,https://reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,https://www.reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,2024-12-06 00:01:36,0,0.33,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1h7nv2n
MachineLearning,[R] Mastering Board Games by External and Internal Planning with Language Models - DeepMind,"Paper: https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf

Abstract:

While large language models perform well on a range of complex tasks (e.g., text generation, question
answering, summarization), robust multi-step planning and reasoning remains a considerable challenge
for them. In this paper we show that search-based planning can significantly improve LLMs’ playing
strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We
introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo
Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search,
the model directly generates in-context a linearized tree of potential futures and a resulting final choice.
Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and
value functions across these games. We find that our pre-training method minimizes hallucinations, as
our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and
external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level
performance in chess while operating on a similar move count search budget per decision as human
Grandmasters. The way we combine search with domain knowledge is not specific to board games,
suggesting direct extensions into more general language model inference and training techniques.",RobbinDeBank,1h7nshy,https://reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,2024-12-05 23:58:37,11,0.88,11,0,1,0,0,False,False,True,False,False,Research,self,t3_1h7nshy
MachineLearning,[D] What approaches can we use to train an open vocabulary or image referential detector for configurable specificity?,"Open vocabulary object detectors allow you to pass in a prompt and an image and attempt to output bounding boxes around objects matching your prompt. Image referential detectors allow you to pass in an image of an object as the prompt and a target image and attempts to output bounding boxes around objects matching the image prompt in the target image.   
  
For a reference, [YOLOWorld](https://github.com/AILab-CVC/YOLO-World) provides both image referential and open vocabulary modes.  
  
The idea I've been toying with is whether there is a good way to train for greater control over specificity. For example, If I pass in an image of a golden retriever, am I looking for golden retrievers specifically? All dogs? All animals? 

Language is a bit more specific, but the same principle can apply. If I search for red cars, do red trucks count? Do maroon cars? In my experience, trying to be too specific textually with OVD models causes erratic behavior. IE, ""A red car or van but not truck "" would give bad performance, as it doesn't really match what would be in grounding captions.

My initial idea for how to systematically define the distance between potential targets and the query is via embedding distance. If I take a phrase grounding dataset, I could compute embeddings separately for each crop of a region and for its corresponding text using a model like CLIP.

A sample training process would be like this

* Select a random image. Select a random image crop embedding.
* Select a random similarity threshold.
* Do an approx KNN using that random image crop embedding, stopping once we reach a sample with an embedding above the similarity threshold. This is our prompt embedding If we selected an image region embedding, we are doing image referential detection. If we selected a text embedding, we are doing open vocabulary detection.
* Calculate prompt embedding similarity to all image crop embeddings in the primary image. Mark all objects with similarity above the threshold as positive examples.
* Run the network, using the selected image, prompt embedding, and similarity threshold as input. Use the previously calculated positives examples as labels.

  
Does anyone know of any papers that work with similar ideas, or have thoughts on whether this process would be useful or could be improved? I'm pretty early into looking into this, so just references or even field terms that would point me in the right direction would be helpful.

Other ideas 

* Always detect all objects given a target phrase, but set the label confidence equal to the embedding similarity. 
* Make the model capable of handling multiple input queries corresponding to the same object, to better indicate the intended domain. Possibly with both negative and positive queries. Not sure how to train yet, possibly sample clusters of similar embeddings from the dataset to build prompt sets.
* Perform instructional tuning, similar to what is done for some LLMs and VLMs, to make the model better handle complex text prompts, and allow instructional text prompts to be paired with images for image referential mode.

  
Related questions

*    Is CLIP still standard for computing text and image embeddings that share a unified embedding space?",Revolutionary-Fig660,1h7knty,https://reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,https://www.reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,2024-12-05 21:40:09,2,0.67,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7knty
MachineLearning,[D]Stuck in AI Hell: What to do in post LLM world,"
Hey Reddit,

I’ve been in an AI/ML role for a few years now, and I’m starting to feel disconnected from the work. When I started, deep learning models were getting good, and I quickly fell in love with designing architectures, training models, and fine-tuning them for specific use cases. Seeing a loss curve finally converge, experimenting with layers, and debugging training runs—it all felt like a craft, a blend of science and creativity. I enjoyed implementing research papers to see how things worked under the hood. Backprop, gradients, optimization—it was a mental workout I loved.

But these days, it feels like everything has shifted. LLMs dominate the scene, and instead of building and training models, the focus is on using pre-trained APIs, crafting prompt chains, and setting up integrations. Sure, there’s engineering involved, but it feels less like creating and more like assembling. I miss the hands-on nature of experimenting with architectures and solving math-heavy problems.

It’s not just the creativity I miss. The economics of this new era also feel strange to me. Back when I started, compute was a luxury. We had limited GPUs, and a lot of the work was about being resourceful—quantizing models, distilling them, removing layers, and squeezing every bit of performance out of constrained setups. Now, it feels like no one cares about cost. We’re paying by tokens. Tokens! Who would’ve thought we’d get to a point where we’re not designing efficient models but feeding pre-trained giants like they’re vending machines?

I get it—abstraction has always been part of the field. TensorFlow and PyTorch abstracted tensor operations, Python abstracts C. But deep learning still left room for creation. We weren’t just abstracting away math; we were solving it. We could experiment, fail, and tweak. Working with LLMs doesn’t feel the same. It’s like fitting pieces into a pre-defined puzzle instead of building the puzzle itself.

I understand that LLMs are here to stay. They’re incredible tools, and I respect their potential to revolutionize industries. Building real-world products with them is still challenging, requiring a deep understanding of engineering, prompt design, and integrating them effectively into workflows. By no means is it an “easy” task. But the work doesn’t give me the same thrill. It’s not about solving math or optimization problems—it’s about gluing together APIs, tweaking outputs, and wrestling with opaque systems. It’s like we’ve traded craftsmanship for convenience.

Which brings me to my questions:

1. Is there still room for those of us who enjoy the deep work of model design and training? Or is this the inevitable evolution of the field, where everything converges on pre-trained systems?


2. What use cases still need traditional ML expertise? Are there industries or problems that will always require specialized models instead of general-purpose LLMs?


3. Am I missing the bigger picture here? LLMs feel like the “kernel” of a new computing paradigm, and we don’t fully understand their second- and third-order effects. Could this shift lead to new, exciting opportunities I’m just not seeing yet?


4. How do you stay inspired when the focus shifts? I still love AI, but I miss the feeling of building something from scratch. Is this just a matter of adapting my mindset, or should I seek out niches where traditional ML still thrives?



I’m not asking this to rant (though clearly, I needed to get some of this off my chest). I want to figure out where to go next from here. If you’ve been in AI/ML long enough to see major shifts—like the move from feature engineering to deep learning—how did you navigate them? What advice would you give someone in my position?

And yeah, before anyone roasts me for using an LLM to structure this post (guilty!), I just wanted to get my thoughts out in a coherent way. Guess that’s a sign of where we’re headed, huh?

Thanks for reading, and I’d love to hear your thoughts!

TL;DR: I entered AI during the deep learning boom, fell in love with designing and training models, and thrived on creativity, math, and optimization. Now it feels like the field is all about tweaking prompts and orchestrating APIs for pre-trained LLMs. I miss the thrill of crafting something unique. Is there still room for people who enjoy traditional ML, or is this just the inevitable evolution of the field? How do you stay inspired amidst such shifts?

Update: Wow, this blew up. Thanks everyone for your comments and suggestions. I really like some of those. This thing was on my mind for a long time, glad that I put it here. Thanks again!",Educational_News_371,1h7jg87,https://reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,2024-12-05 20:49:57,790,0.97,790,0,218,0,0,False,False,True,False,False,Discussion,self,t3_1h7jg87
MachineLearning,[R] NLLG Quarterly arXiv Report 09/24: What are the most influential current AI Papers?,"Hi there,

I'd like to share our group's most recent arXiv report, where we analyze analyze the most influential papers in terms of citations: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5045225](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5045225)

Over the recent months, we see many new foundation models entering our top 40 list. Additionally, the number of cs.CL papers is slowly declining. In additional analysis, we find that top 40 papers seem to rely less on generated content than randomly selected ones.",Gringham,1h7hrum,https://reddit.com/r/MachineLearning/comments/1h7hrum/r_nllg_quarterly_arxiv_report_0924_what_are_the/,https://www.reddit.com/r/MachineLearning/comments/1h7hrum/r_nllg_quarterly_arxiv_report_0924_what_are_the/,2024-12-05 19:39:59,2,1.0,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1h7hrum
MachineLearning,"Image Generation Model Evaluation Challenge (Würstchen, KOALA, PixArt-α) [P]","# Project Description:

We are inviting skilled professionals to participate in an evaluation challenge to produce a GenAI image based upon a prompt and a set of component images.  The candidate may choose whatever models they like to complete the task, so long as they are open source. 

The results will be reviewed and compared across participants, and the candidate with the most effective and high-quality outputs will be selected for a larger, production-focused engagement. 

# Entry Process:

Submit your Github handle and intention to participate to email acr0batproduce@gmail.com.  Provide a short bio and resume in your email.  If you’re selected, we will provide you access to a repository to contribute to.  

**Challenge Scope:**

1. **Model Setup and Testing**:
   * Create a unique set of prompts to test your image generation model.  A set of [sample prompts](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) have been provided for your convenience.  A minimum of 3 tests are required, but candidates may provide more if they wish.
   * For each image generation, provide at least 3 component images to be used in the final output.  A set of [component images](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.p13d4p443fpl) related to the [sample prompt](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) has been provided for your convenience.
   * Develop the model, and test its performance with your sample input prompts and component images.  Source code must be pushed to the main branch of the provided repo.
   * Provide the output images in ./static in the repo.
   * Document your findings and results
2. **Evaluation Criteria**:
   * **Image Quality**: The final image produced should incorporate all the features from your sample prompt, and component images.  Above all, the items in the component images need to be naturally incorporated into the final image.  They should not be significantly distorted, or look like they were copy and pasted from the input images.    
   * **Brand Element Incorporation**: Expanding upon the above point, the final image must accurately reflect the input component images.  So, if the input image is a Rolex Oyster Perpetual Day-Date 40 in 18 kt yellow gold with a champagne colour, diamond-set dial, fluted bezel and a President bracelet, then the output image must incorporate that same product.
   * **Creative Flexibility**: Generate diverse variations of prompts, component images and output images.
   * **Customization**: Showcase how well your model responds to different parameters.
   * **Code Quality**: All aspects of code quality will be assessed. Examples include: repo structure, IaC, CI/CD, unit testing, performance, documentation, etc.
   * **Extensibility**: Your sample prompts, input images, and output images will be used to quickly screen for accuracy and limit submissions; however, importantly, your model will be tested against our internal prompts and input images to gauge how well it performs on different types of problems. 
3. **Deliverables**:
   * The repo structure is up to you, but the README should make it clear where your model, input images, output images and documentation reside. 

# Challenge Benefits:

* The candidate with the most effective and high-quality outputs will be selected for a larger, production-focused project with a significant budget.
* This is an opportunity to contribute to an innovative, high-growth startup that has already secured investor funding and is positioned for significant market impact.
* Gain the opportunity to showcase your expertise in image generation models and secure a long-term collaboration.

# Requirements:

* Proficiency in all areas of data science, with a focus on AI image generation
* Expert in Python, SQL, and at least 1 cloud provider such as AWS or GCP 
* A compute environment will not be provided; the candidate can develop the solution locally or on the cloud, but at their own expense.
* Candidate must comply with provided NDA

# Submission Guidelines:

* Entry Deadline: 1/1/25
* Project Deadline: 1/7/25
* All source code should be submitted to the repo by the deadline; the candidate will have their access revoked on that date.
* Provide a short bio in your README, and relevant contact information. 

# Selection Process:

* All submissions will be reviewed and compared based on the outlined criteria.
* The most effective and high-quality submission will result in the candidate being awarded a contract for a larger production project.

**I**f you're interested in participating, please reach out! ",AlpacaRampage,1h7hg7w,https://reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,2024-12-05 19:26:31,5,0.7,5,0,0,0,0,False,False,True,False,False,Project,self,t3_1h7hg7w
MachineLearning,[D] Advice for a new Machine Learning Tutor: what projects will get my students hired?,"I've just started giving lessons as a machine learning tutor. I have a masters degree in computer science and two years professional experience. But I've never been a tutor (atleast not in this field). Today I was giving my first lesson on ML, just a powerpoint on the basics when my student stopped and told me the powerpoint was too basic for her.

She wanted to talk more about projects that she could do that would attract employers and get an internship. And she asked me point blank what kind of projects she should make. To be honest I wasn't entirely sure, what flashed in my head were things like training a model to recognize the MNIST digits or other simple projects suitable for a (relative) novice. But would those really help her get an internship? I doubted myself so I turned it around on her and asked her what kinds of things related to machine learning she is passionate about and would motivate her to work hard? She responded that she could do anything related to machine learning and she just wants to do what would make her money and get her recognized by a company.

So basically I felt like I failed as a tutor for not having a good answer, and I would like to have an answer prepared if this happens again. What do you all think? What are some projects that a novice, or not so novice students can take on that will make them more hireable for jobs and internships?

And while we're at it, what kinds of things do you think I should be preparing to be a better tutor in general. What kinds of things would you want your machine learning tutor to prepare for you? Would you want slideshow deck lessons on key concepts? Jupyter notebooks with exercises for practice? Something else? I'm not sure what I should be doing to get ready for these lessons honestly.",Seijiteki,1h7bftd,https://reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,https://www.reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,2024-12-05 15:16:00,0,0.31,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7bftd
MachineLearning,U-Net Vs Attention U-Net [D],"Hello folks,

Young researcher here, working on a in-house dataset to build a foundational model for a interesting use-case. But I have thesis to finsh, which will be just the tail of my current research.

For my thesis, we have decided to have a subsection for comparing how my segmentation results differ when used attention blocks are used within a U-Net. I've referred few papers on how this works and how can this be implemented.

Results are promising (att unet outperformimg unets, nothing suprising) but I see a concerning opposing point i.e. attention Unet having more number of parameters that the unet. Is there a way I can conduct this study where I compare results with and without attention? And there are no other additional factors influencing the results (layers, params, etc).

Does conducting ablation study makes sense in this case? I've not seen any other paper comparing similar use-case using this study.

Any papers I can look through, suggestions and tips are welcome.",ade17_in,1h7cjnd,https://reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,2024-12-05 16:03:47,27,0.93,27,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7cjnd
MachineLearning,[R] ReVersion: Learning Relation Prompts from Images for Controlled Diffusion Generation,"ReVersion introduces a novel approach for learning and transferring visual relationships using diffusion models. Rather than focusing solely on object appearance, it learns how objects interact with each other through relation prompts and specialized sampling techniques.

Key technical aspects:
- Uses frozen pre-trained text-to-image diffusion model as foundation
- Implements relation-steering through contrastive learning to guide prompts toward relationship-rich latent spaces
- Employs relation-focal sampling to emphasize high-level interactions over low-level details
- Creates relation prompts that capture spatial and interactive relationships between objects
- Introduces new benchmark dataset for evaluating relation inversion methods

Results:
- Outperforms existing methods in preserving object relationships while allowing appearance flexibility
- Shows strong performance on spatial relationships like ""on top of"", ""next to"", ""inside""
- Successfully transfers learned relationships to novel object pairs
- Maintains relationship consistency across different styles and contexts

I think this approach could be particularly valuable for improving automated image generation systems that need to handle complex scenes with multiple interacting objects. The ability to learn and transfer relationships, rather than just appearances, could help bridge the gap between current image generation capabilities and human-like understanding of how objects interact in space.

I think the relation-focal sampling technique could also have applications beyond just relationship learning - it might be useful anywhere we need to emphasize high-level features over low-level details in diffusion models.

TLDR: New method learns visual relationships from images using diffusion models, introduces relation-steering and relation-focal techniques, shows strong results on spatial relationship preservation and transfer.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reversion-diffusion-based-relation-inversion-from-images). Paper [here](https://arxiv.org/abs/2303.13495).",Successful-Western27,1h7afj8,https://reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,2024-12-05 14:30:05,16,0.99,16,0,0,0,0,False,False,True,False,False,Research,self,t3_1h7afj8
MachineLearning,[Discussion] Unsigned Integer Representation as Vectors with Focus on Extrapolation,"Hi everyone,

I’m working on a regression task with a transformer-based architecture applied to grid-based structures. Think of something like mazes, where the goal is to predict the distance to a target. Each input token contains categorical features along with x/y coordinates. The idea is to train on small grids and generalize to larger ones.

Here’s my current approach for coordinate and token embeddings:

`x_emb = self.w_x.weight * x # shape: bs, sequence len, 1, d`  
`y_emb = self.w_y.weight * y # shape: bs, sequence len, 1, d`  
`cat_emb = self._categ(categ)`  
`sequence_emb = torch.cat((x_emb, y_emb, cat_emb), dim=-2) # shape: bs, sequence len, num_cat, d`  
`sequence_emb = sequence_emb.view(bs, seq_len, -1)`  
`transformer_inputs = self._linear(sequence_emb)`

In other words, the x/y coordinate embeddings are scaled learnable vectors. However, this approach only generalizes moderately well. I suspect that improving the coordinate representation is critical.

Unfortunately, this token-based structure is required for the task, so I need to focus on crafting a smart token representation. I’m deliberately avoiding subtracting embeddings to compute relative distances because a core objective is for the model to learn these distances on its own.

Here are some things I’ve tried so far:

Things I also tried:

* Positional encoding instead of scaled vectors
* log-scaled vectors
* exp-scaled vectors

Does anyone know of interesting work or techniques for numerical representations in this kind of context? Any advice would be greatly appreciated!

In case you find interesting papers about extrapolation in transformers based on size and tokens, I am happy to take any inspiration.",mbus123,1h769cs,https://reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,2024-12-05 10:33:19,4,0.7,4,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h769cs
MachineLearning,[N] Hugging Face CEO has concerns about Chinese open source AI models,"Hugging Face CEO stated that open source models becoming SOTA is bad if it just so happens to be created by Chinese nationals. To exemplify Tech Crunch asked ""what happened in Beijing China in June 4th, 1989?"" to ONE of the Qwen models (QWQ 32B) which said ""I can't provide information on that topic"" (I swear to god on my life I have no idea what happened here on that date and would literally never ask a model that question - ever. It doesn't impact my experience w/ model).

The CEO thought censorship of open source models is best stating that if a country like China ""becomes by far the strongest on AI, they will be capable of spreading certain cultural aspects that perhaps the Western world wouldn’t want to see spread.” That is, he believes people shouldn't spread ideas around the world that are not ""western"" in origin. As someone born and raise in U.S. I honest to god have no clue what he means by ideas ""the Western world wouldn't want to see spread"" as I'm ""western"" and don't champion blanket censorship.

Article here: [cite](https://techcrunch.com/2024/12/03/huggingface-ceo-has-concerns-about-chinese-open-source-ai-models/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAABU0mWV-7rbB7vF9z6wCgPuZrl-dPj_W3cEh1wVuxp5CiBl1r6KTcITdHz34N-rOtHj9g-Z3N3SS-mNnPvFaHFIUmSsA5AdqukSLlcn-CJUkU_IXsdcR3Gp5hi1cI2tboprDzxGF8j1e7XAQHyGn3E_bd0cmIHIVkJ0LiFZBdOR1).

Legitimate question to people who support these type of opinions - Would you rather use a low-quality (poor benchmark) model with western biases versus an AGI-level open source 7B model created in China? If so, why?",AIAddict1935,1h7185x,https://reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,https://www.reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,2024-12-05 04:49:44,0,0.37,0,0,15,0,0,False,False,True,False,False,News,self,t3_1h7185x
MachineLearning,[D] Data drift detection methods aside from changes in model performance metrics,"Hi all,

As the title implies, I've been relying on (somewhat near) real-time monitoring of model performance metrics to see if data drift has happened in my use-case.

I'm wondering if you know other more sophisticated/advanced methods to detect data drift. Would love to hear any kind of methods, whether they target detection of covariate/feature drift, target/label drift or concept drift.

Even better if you can share any Python or R implementations to carry out the above data drift checks.

Thanks in advance!",YsrYsl,1h6woaf,https://reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,2024-12-05 01:02:26,10,1.0,10,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6woaf
MachineLearning,[D] Packaging a Pytorch model to an exe. What is the best method?,"I have Pytorch models that are designed to run locally, both training and inference on a local machine.

The GUI is being created using another language, and the plan is to package all the Python aspects into an executable and run it via the Python equivalent of subprocess (and Pipe very basic data between the two). I will be running cross platform on both Windows and Mac

There are multiple auxiliary scripts which read in data, and process it (data extraction + feature engineering). While I have extensively used vectorised functions, I have used a cythonized approach for some code, and I am compiling the underlying scripts using Cython(so pretty much everything is a compiled binary, except an entry point, say, main.py).

My ancillary libraries are the usual suspects, Pandas, Numpy (1.x), SciKit learn.

My question is this, what is the most reliable packaging approach at the moment? I know that both PyInstaller and cx\_freeze are options that I have used before. My preference is PyInstaller, but previously I encountered issues with it (and Pytorch).

Has anyone completed a similar project recently, and do you have any advice?

nb. I've checked the old posts, there are a few on this topic. However, there have been a number of changes to Pytorch, particularly with some of the runtime compiled elements (which can be a nightmare on Mac with its notarisation process) - and I know Pyinstaller has a very active user base.

 ",Solid_Company_8717,1h6qtps,https://reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,https://www.reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,2024-12-04 20:51:34,0,0.45,0,0,5,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/c06xSdQK2UXPm413fPBqj0rlUZcYVWXHIP8GTkEh5kY.jpg,t3_1h6qtps
MachineLearning,[D] Daily Paper Discussions - FlashAttention 3,"As a part of daily paper discussions on the Yannic Kilcher discord server, I will be volunteering to lead the analysis of FlashAttention-3 🧮 🔍

📜 **FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision**  
🌐 [https://arxiv.org/abs/2407.08608](https://arxiv.org/abs/2407.08608)  
🕰 Thursday, Dec 5th, 2024 01:30 AM UTC // Thursday, Dec 5th, 2024 7.00 AM IST // Wednesday, Dec 4th, 2024 5:30 PM PT

**FlashAttention-3** introduces three smart ideas to boost performance on the Hopper GPUs -

1️⃣ Producer-Consumer Asynchrony: This technique divides tasks into separate parts. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. By doing this, it makes better use of GPU resources and hides delays that would otherwise slow down performance.

2️⃣ Hiding Softmax Operations: FlashAttention-3 improves efficiency by overlapping the slower softmax calculations with the faster matrix multiplications (GEMM). Instead of waiting for Softmax to finish before starting the next calculations, it processes them in parallel, speeding up the overall process.

3️⃣ Hardware-Accelerated Low-Precision Computations: This approach uses advanced GPU features to perform calculations with lower precision (FP8), which are faster and use less memory. FlashAttention-3 tweaks its algorithms to handle these low-precision calculations effectively, nearly doubling the processing speed while maintaining accuracy.

https://preview.redd.it/impb6wfc1w4e1.png?width=1063&amp;format=png&amp;auto=webp&amp;s=82e24c828b373175ee119070027495a8a2a7bb6a

",CATALUNA84,1h6pmvd,https://reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,2024-12-04 20:03:01,24,0.9,24,0,6,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/QnOKl-2nMRgI_pTO3xGlP_C2iaaDoYpmwb_KyC2_xI4.jpg,t3_1h6pmvd
MachineLearning,[R] ICLERB: A better way to evaluate embeddings and rerankers for in-context learning,"Current benchmarks for embeddings, like MTEB and BEIR, include multiple datasets and tasks, but are fundamentally based on relevance annotations like text similarity. These are great for choosing the best embeddings for most search/retrieval use cases. These days, many people use these embeddings to retrieve items for in-context learning (e.g. document RAG or few-shot learning), to adapt an LLM to a specific task. Yet, they are still using MTEB to pick the best embeddings, even though the performance on that benchmark doesn't necessarily translate to better performance on their downstream LLM task (MTEB came out in 2021 after all).

In our latest paper, we propose a new evaluation framework and benchmark called ICLERB. This benchmark challenges the conventional approach by using Direct Preference Optimization (DPO) as a relevance metric to reflect the actual utility of embeddings and rerankers when used with LLMs for in-context learning.

[https://arxiv.org/pdf/2411.18947](https://arxiv.org/pdf/2411.18947)

Key Highlights:

\- Embeddings outperform rerankers: We found that simpler embedding models outperformed their higher-capacity reranker counterparts from Cohere, NVIDIA, and VoyageAI.

\- Size isn't everything: Among the three Snowflake embeddings, the smallest model (33M parameters) outperformed the larger ones (109M and 334M).

\- Rethinking training and evaluation objectives: These findings suggest that training and evaluating larger retrieval models solely on text similarity may be counterproductive.

Interestingly, the performance of some models, like BGE, is very sensitive to the dataset or the LLM used, while others like NV are more stable. We're planning to continue adding more datasets and LLMs to the benchmark to broaden its scope.

Curious to hear your thoughts and feedback as we work on improving ICLERB! Are there other retrieval models, LLMs, or datasets you'd like to see included?",Crossing_Minds,1h6o70e,https://reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,2024-12-04 19:05:25,61,0.96,61,0,10,0,0,False,False,True,False,False,Research,self,t3_1h6o70e
MachineLearning,[D] Binary fitness optimization.,"Do you know of any papers or what field would tackle the following problem: You have a function f(x) that you need to optimize but the cost/fitness you are optimizing is binary. I am working on a project about this and I'm not sure if there is research in this area.

  
Thank you so much &lt;3",pamintandrei,1h6nayz,https://reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,2024-12-04 18:30:00,9,0.85,9,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h6nayz
MachineLearning,[D] How to customize an attention mechanism in GNN?,"
I’m looking for some base code or algorithm in order to create a new mechanism attention while working with graphs with the task of node prediction. I’ve seen there was some documentation in stellar graph but I wonder if there are another pieces of material that would be helpful.
Thank you!!!",Whole_Hat_4852,1h6hxu8,https://reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,2024-12-04 14:56:15,8,0.79,8,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6hxu8
MachineLearning,[D] Best alternatives to BERT - NLU Encoder Models ,"I'm looking for alternatives to BERT or distilBERT for multilingual proposes.

I would like a bidirectional masked encoder architecture similar to what BERT is, but more powerful and with more context for task in Natural Language Understanding.

Any recommendations would be much appreciated.",mr_house7,1h6gtxh,https://reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,2024-12-04 14:07:40,5,0.78,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6gtxh
MachineLearning,[R] Forecasting and Mitigating Security Threats from Malicious AI Applications,"This paper provides a systematic analysis of potential malicious applications of AI systems across digital, physical and political security domains. The methodology involves:

- Surveying dual-use AI capabilities that could enable attacks
- Mapping specific attack vectors and required technical capabilities  
- Analyzing the evolution of attacker/defender dynamics
- Developing a framework for threat assessment and mitigation

Key technical findings:

- ML advances in areas like NLP and computer vision lower barriers to sophisticated attacks
- Automated systems can significantly scale up traditional attack vectors
- Transfer learning and GANs enable rapid adaptation of attack techniques
- Technical countermeasures alone are insufficient - policy/governance frameworks needed

The researchers provide a detailed assessment framework examining:

- Technical requirements for different attack types
- Estimated timeline for capability development
- Difficulty of execution and potential impact
- Proposed defensive measures and their limitations

I think this work is important for helping the ML community get ahead of security risks before they materialize. The framework provides a structured way to evaluate emerging threats, though I expect the specific attack vectors will evolve significantly as capabilities advance.

I think we need much more research on measuring the effectiveness of proposed countermeasures and understanding the co-evolution of offensive/defensive capabilities. The policy recommendations are a good start but will require ongoing refinement.

TLDR: Systematic analysis of how ML advances could enable new attack vectors across security domains. Provides framework for assessing and mitigating threats through both technical and policy measures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/malicious-use-artificial-intelligence-forecasting-prevention-mitigation). Paper [here](https://arxiv.org/abs/1802.07228).",Successful-Western27,1h6fbgg,https://reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,2024-12-04 12:55:47,2,0.6,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1h6fbgg
MachineLearning,[D] Do lots of metadata really help in semantic search?,"I'm on my second week in learning AI and I was thinking of preprocessing biography data by including lots of metadata like city, date of birth, key events, education, hobbies, etc, and then generating embeddings and adding them together into a vector database. Perhaps by using NLP API or LLM. But is it necessary? Or should I just use OpenAI model to dynamically extract this metadata from the bios prior to storing them? Will having lots of metadata dramatically help to improve the quality of the search results?

I thought maybe the semi-automatic preprocessing step would allow me to check and clean the metadata.

*P/S: I posted this at* [*https://www.reddit.com/r/learnmachinelearning*](https://www.reddit.com/r/learnmachinelearning) *but didn't get much response. Thought of trying it out here.*",tjthomas101,1h6f39a,https://reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,2024-12-04 12:43:11,3,0.64,3,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h6f39a
MachineLearning,[D] Comparing Multiple Large Language Models in one Pass,"I wrote an article on streamlining the process of comparing and selecting Large Language Models (LLMs) for various tasks:

[Comparing Multiple Large Language Models in one Pass](https://dezoito.github.io/2024/03/28/comparing-from-multiple-LLMs.html)

Hopefully this is useful to help folks trying to make the best model selection for their use case (which can take a lot of time).

I'm also looking forward to discussing different techniques and tools to automate the process.

Thank you!",grudev,1h6evdt,https://reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,2024-12-04 12:30:58,4,0.75,4,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6evdt
MachineLearning,[D] Linear Regression but with Binary Output for wide range predictions with better precision,"A neural network tends to find it difficult to predict data that ranges between very large and small numbers on the output. My application requires the NN to predict between -1000 and 1000 ∈ Z. I could make this possible by scaling up the output by 1000 hence allowing the model to predict between -1 and 1, but a loss between 2e-2 (prediction) and 3e-2 (target) with L1Loss (worse case L2Loss) would be negligible (1e-2 in this case, 1e-4 in the worse case). It is imperative for the model to be very precise with the predictions, when the target is 5e-2 it should be so and not even at least deviating by +-0.1e-2. This precision is very difficult to achieve when it comes to linear regression, so i thought of a more systematic approach to defining the prediction and criterion. Again, i wanted the model to predict between -1000 and 1000. These numbers can be represented using a minimum of 11 bits (binary), so i redesigned the model output to contain 22 neurons, arranged as ∈ R (11x2) 11 outputs with two classes, the classes being a binary representation of 1 or 0. CrossEntropy could be used as a criterion here but im using multimarginloss instead for specific reasons. Otherwise a different approach could be a sigmoided output of 11 neurons to represent the binary number. Whats you guys' take on this? Is this considered good (if not better) practice? Is there any research similar to this that i can look into?

",Relevant-Twist520,1h6azcu,https://reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,https://www.reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,2024-12-04 07:55:43,2,0.57,2,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h6azcu
MachineLearning,[P] Label Studio Activation Troubles,"I'm trying to run Label Studio because I was told once that it's more of a modern program used for labeling images, which I plan to do for a personal project. However, I've been dealing with headache after headache trying to get it to run, since it complains about \_psycopg. I have tried installing Python and PostgreSQL (since I think there's a dependency between the two) multiple times, looking into issues with libpq.dll, and so on, but it's not working. Anyone have any idea on how to fix an issue like this, or should I look into a different labeling program?",NuDavid,1h5gtqk,https://reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,https://www.reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,2024-12-03 06:02:59,1,1.0,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1h5gtqk
MachineLearning,[D] Results for IBM PhD Fellowship ,"Anyone know when the results will come out?
Google and NVIDIA have already released the results.",International-Rip958,1h5yz5s,https://reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,https://www.reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,2024-12-03 21:39:15,0,0.36,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h5yz5s
MachineLearning,[R] The Multimodal Universe: Enabling Large-Scale Machine Learning with 100TB of Astronomical Scientific Data,"https://openreview.net/forum?id=EWm9zR5Qy1#discussion

Abstract: We present the Multimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse

What can you guys see the uses of this dataset being?",blabboy,1h5x146,https://reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,2024-12-03 20:19:26,79,0.96,79,0,3,0,0,False,False,True,False,False,Research,self,t3_1h5x146
MachineLearning,[D] Cloud GPU Price Analysis - December 2024: A Comprehensive Market Review,"After analyzing current cloud GPU pricing across major providers, I've compiled insights that might help with infrastructure decisions. Some findings surprised me - particularly around hidden costs and spot pricing variations.

Current Market Rates (December 2024)

On-Demand Pricing:

\- RunPod H100 (80GB): $2.49/hr

\- RunPod A100 (80GB): $1.69-1.99/hr

\- [Vast.ai](http://Vast.ai) A100: $0.73-1.61/hr (marketplace model)

\- Lambda A100: $1.29/hr

Key Market Insights

1. Spot Instance Pricing

\- Can reduce costs by 30-70%

\- Availability varies significantly by region

\- Some providers offer spot instance guarantees

\- Price stability varies by provider

2. Hidden Cost Factors

\- Data transfer fees vary dramatically

\- Storage costs for large datasets

\- Network bandwidth tiers

\- Instance startup/shutdown minimums

3. Provider Differentiators

\- UI/UX and ease of use

\- Available regions/zones

\- Support quality

\- API functionality

Cost Optimization Strategies

1. Workload Planning

\- Match GPU to actual requirements

\- Consider splitting workloads across smaller instances

\- Use spot instances for interruptible tasks

\- Monitor utilization patterns

2. Data Management

\- Optimize dataset storage

\- Plan data transfer patterns

\- Use caching effectively

\- Consider compression strategies

I'll be tracking these prices and patterns monthly. Would be interested in:

1. Which providers you're using?
2. How do you optimize costs?
3. What metrics matter most in your GPU decisions?",Botinfoai,1h5p7fr,https://reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,2024-12-03 14:54:58,28,0.91,28,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1h5p7fr
MachineLearning,[R] Enhancing LLM Reasoning Through Bidirectional Forward-Backward Thinking,"The key contribution here is a ""reverse thinking"" method that improves LLM reasoning without any model modifications. Instead of only reasoning forward from the question to an answer, the approach adds a backward verification step - working from potential answers back to the question to validate the reasoning chain.

Key technical points:
* Two-stage process: forward generation followed by backward verification
* Backward pass examines logical consistency between answer and premises
* No fine-tuning or architectural changes needed
* Tested across multiple reasoning benchmarks (GSM8K, CommonsenseQA, LogiQA)

Results:
* 8.3% improvement on GSM8K math reasoning
* 6.2% gain on CommonsenseQA 
* 5.4% increase on LogiQA
* Consistent improvements across different model sizes
* Performance gains come at cost of 2x inference time

I think this method points to untapped potential in how we prompt LLMs for reasoning tasks. While the doubled inference time is a real tradeoff, the consistent improvements across different benchmarks suggest this approach captures something fundamental about machine reasoning. The simplicity of implementation means it could be quickly adopted in many applications where reasoning accuracy matters more than speed.

TLDR: Adding a backward reasoning verification step improves LLM performance on math, logic and common sense tasks by 5-8%, with no model changes required. Doubles inference time but provides consistent gains across different models and tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reverse-thinking-makes-llms-stronger-reasoners). Paper [here](https://arxiv.org/abs/2411.19865).",Successful-Western27,1h5nyi0,https://reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,https://www.reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,2024-12-03 13:57:18,25,0.96,25,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5nyi0
MachineLearning,"[D] Model performs good on test, but fails in production ","Hi, I’ve developed churn prediction model with XGBoost on users weekly activity data. The training data is balanced (3.3k churned, 3k not churned). I’ve split the data into: train, validation and test sets. Getting ~90% precision &amp; ~88% recall for train, validation and test sets. However, when running in production, I get ~1.5k users flagged as churn (we have total of 4k users). This can’t be true as we get maximum 250 churned users per month. Any suggestions on what I’m doing wrong? And what could be the solution?

Thanks ",Terrible_Dimension66,1h5nfpt,https://reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,https://www.reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,2024-12-03 13:30:55,18,0.72,18,0,55,0,0,False,False,True,False,False,Discussion,self,t3_1h5nfpt
MachineLearning,"[R] With losses like focal loss, is hard exemple sampling still necessary ?","Hello,
So I was wondering are techniques for hard exemples sampling still used nowadays ?
Anyone have papers on this if it’s the case ?
Thanks !",Training-Adeptness57,1h5mqkj,https://reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,https://www.reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,2024-12-03 12:54:23,6,0.87,6,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5mqkj
MachineLearning,[D] ODE/SDE alignment,Can anyone give me example of good paper that try to align/match the final marginal distribution of 2 ODE/SDE from diffusion model? ,Ok_Cryptographer2731,1h5ly4z,https://reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,https://www.reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,2024-12-03 12:08:04,4,0.83,4,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h5ly4z
MachineLearning,[P] multi feature linear regression code in python not giving the correct solution (or any solution for that matter)...,"linear regression using gradient descent:

    def multiFeatureLinearRegression(x, y, alpha, iterations):
        w = [0.0] * len(x[0])
        b = 0.0
        m = len(x)
        
        for it in range(iterations):
            w_temp = [0.0] * len(x[0])
            b_temp = 0.0
            for i in range (len(x)):
                prediction = b
                for j in range(len(x[i])):
                    prediction += w[j] * x[i][j]
                error = y[i] - prediction
                
                b_temp += error
                
                for j in range(len(x[i])):
                    w_temp[j] += error * x[i][j]
            
            for i in range(len(x[0])):
                w[i] -= alpha * (2.0 / m) * w_temp[i]
            b -= alpha * (2.0 / m) * b_temp
            
        return w, b

main body:

    data = [    [15, 3, 20],  # [House Size (sq. ft.), Bedrooms, Age of House (years)]
        [20, 4, 15],
        [17, 3, 25],
        [22, 4, 10],
        [13, 2, 30],
        [18, 3, 20],
        [24, 4, 5],
        [16, 3, 18]
        ]
    
    dataY = [300, 400, 350, 450, 200, 370, 500, 310]
    
    alpha = 0.01
    iterations = 100000
    w, b = multiFeatureLinearRegression(data, dataY, alpha, iterations)
    
    print(""Weights (w):"", w)
    print(""Bias (b):"", b)

I am trying to implement multi feature linear regression and for some reason the output for the weight and bias is coming out to be:

    Weights (w): [-inf, -inf, -inf]
    Bias (b): -inf

I have no idea why this is happening..  
Can you spot what I am doing wrong here?  
could it be because I have not applied any normalization or something?",silveroburn,1h5jyaj,https://reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,https://www.reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,2024-12-03 09:51:24,0,0.25,0,0,5,0,0,False,False,True,False,False,Project,self,t3_1h5jyaj
MachineLearning,[D] Looking for opensource projects/products to join ,"Hi everyone,

I am a final year electronics undergrad student and have a decent amount of ML as well as general programming experience. I wish to contribute to any open Source repos that work in the ML/DL/AI space. Any guidance would be appreciated!

TLDR; Looking for open source projects to contri to ; would appreciate any help.",Swimming-Regret-7278,1h5jue0,https://reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,https://www.reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,2024-12-03 09:43:05,4,0.63,4,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1h5jue0
MachineLearning,[D] NAACL 2025 vs ACL 2025,"Hi,

I have recently received reviews from the ARR round for NAACL. The scores are: 3.5/4/4, 4/4/2, 3/3/3 for Overall/Soundness/Confidence. Should I try ACL with these scores or just commit to NAACL? Last year I had bit more and it worked out and got accepted to ACL (I did not commit to NAACL).

Thanks",mayanknagda,1h5jj1h,https://reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,https://www.reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,2024-12-03 09:19:04,5,0.78,5,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1h5jj1h
MachineLearning,[D] Deep Learning in Time Series: Are They Used in Industry?,"Hey folks! I’m a researcher in time series and have been seeing a lot of buzz around deep learning models in this area. I am wondering if these models actually being deployed in production, or are classical methods still the go-to in the industry?



For instance, in weather forecasting, physics-based numerical weather prediction (NWP) seems to dominate. If deep models aren’t getting much traction, have you come across any practical use cases for them? Would love to hear your thoughts!",Few-Pomegranate4369,1h5izk5,https://reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,https://www.reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,2024-12-03 08:36:46,55,0.97,55,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h5izk5
MachineLearning,[D] The popular theoretical explanation for VAE is inconsistent. Please change my mind.,"I had a really hard time understanding VAE / variational inference (VI) in theory, for years. I'd be really appreciated if anyone could clarify my confusions. Here's what I've got after reading many sources:

1. We want to establish a generative model p(x, z) (parameters are omitted for simplicity) for the observable variable x and the latent variable z. Alright, let's select appropriate parameters to maximize the marginal likelihood of the observed samples p(x).
2. According to basic probability theory (the law of total probability and the definition of conditional probability), we have: p(x)=∫ p(x ∣ z) p(z) dz (Eq. 1).
3. Here's the point that things becomes rather confusing: people now will claim that this integral is ***intractable*** because z is a continuous variable / z is a high-dimensional variable / p(x∣z) is too complex / or any other excuses.
4. What to do for the intractability of Eq. 1? Although we didn't mention the posterior p(z ∣ x) above, we will now bring it into the discussion. The posterior p(z ∣ x) is also intractable since p(z | x) = p(x | z) p(z) / p(x) and p(x) is intractable. So we will introduce another parameterized model q(z ∣ x) to approximate p(z | x).
5. After some derivation, we obtain a new optimization objective, commonly known as ELBO, which is the summation of:
    - the ""reconstruction"" term: ∫ log p(x ∣ z) q(z ∣ x) dz (Eq. 2);
    - KL divergence term between q(z | x) and p(z), which results in a closed-form.
6. So now we have to work on Eq. 2. Compared with Eq. 1, p(z) is replaced with q(z∣x), both of them are (usually) normal distributions, and p(x | z) is still there. Great! Clearly we have transformed an intractable integral into… another intractable integral?
7. Don’t worry, we can compute Eq. 2 using Monte Carlo sampling… Wait, since we can use Monte Carlo for this, why can’t we just handle Eq. 1 the same way without so much fuss?
8. Of course it is not a good idea. It can be shown that log p(x) = ELBO + D_KL(q(z ∣ x) || p(z ∣ x)). So we cannot estimate p(x) with Eq. 1 as it does not have such nice properties… Huh, it seems like that’s not how we started explaining this?

Questions:

1. When tackling the original problem, i.e., modeling p(x, z) by maximizing p(x)=∫ p(x ∣ z) p(z) dz, why do we want to involve the posterior p(z | x)?
    - Someone explains this with [""to narrow down the value space to facilitate faster search""](https://web.archive.org/web/20241202042731/https://lilianweng.github.io/posts/2018-08-12-vae) (with the approximation of p(z | x), q(z | x)). But again, please recall how the intractability of Eq. 1 is explained, I can't see anything improved under this argument.
2. The Eq. 1 and Eq. 2 are essentially similar, where either of them is the expectation of (log) p(z | x) with respect to the probability density function of some normal distribution. I can't see how the motivation based on the intractability of Eq. 1 could make sense.
    - Ironically, we still have to resort to Monte Carlo sampling when handling Eq. 2. But people appear to forget it when talking about the intractability of Eq. 1, but remember it when facing the same problem of Eq. 2.

Update: I have editted some typo.

Update 2: Question 2 seems to be resolved after some discussions: 
- It is not a good idea to sample on p(z) due to the high variance.
- In practice, we are usually working on log p(x), the log-likelihood of samples, and MC sampling for log ∫ p(x ∣ z) p(z) dz (Eq. 3) can be biased. 
- Apply Jensen's inequality on Eq. 3 and we will have log p(x) ≥ ∫ log p(x ∣ z) p(z) dz. This bound is very likely worse than ELBO, and still relying on sampling on p(z).

However, these points are still rarely found in existing articles. I hope we may think more carefully when introducing VAE in the future.",function2,1h5f6co,https://reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,2024-12-03 04:25:19,140,0.94,140,0,67,0,0,False,False,True,False,False,Discussion,self,t3_1h5f6co
MachineLearning,[R] Population-based Model Merging via Quality Diversity,"In case any of you are interested, here is a [blog post](https://sakana.ai/cycleqd/) about our recent paper [Agent Skill Acquisition for Large Language Models via CycleQD](https://arxiv.org/abs/2410.14735).",hardmaru,1h5dmnb,https://reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,https://www.reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,2024-12-03 03:02:27,18,0.88,18,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5dmnb
MachineLearning,[D] Training a VAE. Single epoch with infinite data or smaller subset over multiple epochs?,"Hello! I'm training a VAE for image models and I finally am getting some pretty decent results in training after correcting my loss function adding KL annealing and LPIPS loss and adjusting my learning rate and batch size, but now I have a doubt about the data i'm feeding to my VAE.

I have a limited time budget for the training and I have more data available than I can feed within that time budget for training.  
What is the best course of action here?  
Should I just run all the data through my VAE training until I run to the end of my training time in one single giant epoch or should I select a subset of the data small enough so that I can go through it multiple times during training and run this smaller dataset over multiple epoch?

My instinct tells me that different data is better for generalization, but VAEs also try to be resilient to variations of the representation of the same image. Because during the encoding phase we use the latent generated to sample from a random distribution (causing a different representation to be passed to the decoder) it feels like potentially feeding back the same data multiple data might actually beneficial to learn resiliency there ...

Is this actually not a thing? I'm actually overthinking about the potential impact of the multiple epochs on VAE training? Is one single giant epoch the best?

Thanks!",hayarms,1h5dfno,https://reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,https://www.reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,2024-12-03 02:52:30,7,0.77,7,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h5dfno
MachineLearning,[D] Benchmarks for RL algorithms across Gymnasium environments?,"Hey r/ML!

Let me preface this by saying I'm fairly new to RL. My previous work was with LLMs, where it is very common to rank and stack your model against the universe of models based on how it performs on a given benchmarks (but you already know this).

Recently started training models in MuJoCo environments and I'm trying to figure out if my algorithms are performing somewhat decently. Sure, I can get Ant-v5 to walk using SB3's default PPO and MlpPolicy, but how good is it really?

Is there some benchmark or repo where I can compare my results against the learning curve of other people's algorithms using the default MuJoCo (or any of the other gyms') reward functions? Of course the assumption would be that we are using the same environment and reward function, but given Gymnasium is popular and offers good defaults, I'd imagine there should be a lot of data available.

I've googled around and have only found sparse results. Is there a reason why benchmarks are not as big in RL as they are with LLMs?",geepytee,1h5a9s8,https://reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,https://www.reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,2024-12-03 00:18:26,7,0.9,7,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h5a9s8
MachineLearning,[D] WWW 2025 Reviews (TheWebConference),The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,New_Ice_2721,1h56hno,https://reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,https://www.reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,2024-12-02 21:34:40,15,0.86,15,0,41,0,0,False,False,True,False,False,Discussion,self,t3_1h56hno
MachineLearning,[R] ImageFolder🚀: Autoregressive Image Generation with Folded Tokens,"https://preview.redd.it/2olpl72q6i4e1.png?width=911&amp;format=png&amp;auto=webp&amp;s=b54d91736543906b6a71102a09dc04883033d795

&gt;Image tokenizers are crucial for visual generative models, e.g., diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve the image reconstruction quality. However, tokenizers with longer token lengths are not guaranteed to achieve better generation quality. There exists a trade-off between reconstruction and generation quality regarding token length. In this paper, we investigate the impact of token length on both image reconstruction and generation and provide a flexible solution to the tradeoff. We propose ImageFolder, a semantic tokenizer that provides spatially aligned image tokens that can be folded during autoregressive modeling to improve both generation efficiency and quality. To enhance the representative capability without increasing token length, we leverage dual-branch product quantization to capture different contexts of images. Specifically, semantic regularization is introduced in one branch to encourage compacted semantic information while another branch is designed to capture the remaining pixel-level details. Extensive experiments demonstrate the superior quality of image generation and shorter token length with ImageFolder tokenizer.

Paper: [https://arxiv.org/abs/2410.01756](https://arxiv.org/abs/2410.01756)  
Code: [https://github.com/adobe-research/ImageFolder](https://github.com/adobe-research/ImageFolder)",xternalz,1h55x1i,https://reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,https://www.reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,2024-12-02 21:11:25,5,0.73,5,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/-S_lEVvuVpw96JtQP6vCgCd6QVqkMlv4dix0h6EtqFY.jpg,t3_1h55x1i
MachineLearning,[P] PerpetualBooster outperforms AutoGluon on AutoML benchmark,"
PerpetualBooster is a GBM but behaves like AutoML so it is benchmarked also against AutoGluon (v1.2, best quality preset), the current leader in [AutoML benchmark](https://automlbenchmark.streamlit.app/cd_diagram). Top 10 datasets with the most number of rows are selected from [OpenML datasets](https://www.openml.org/). The results are summarized in the following table for regression tasks:

| OpenML Task                                  | Perpetual Training Duration | Perpetual Inference Duration                                      | Perpetual RMSE | AutoGluon Training Duration | AutoGluon Inference Duration                                      | AutoGluon RMSE |
| -------------------------------------------- | --------------------------- | ----------------------------------------------------------------- | -------------- | --------------------------- | ----------------------------------------------------------------- | -------------- |
| [Airlines_DepDelay_10M](openml.org/t/359929) | 518                         | 11.3                                                              | 29.0           | 520                         | 30.9 | 28.8   |
| [bates_regr_100](openml.org/t/361940)        | 3421                        | 15.1 | 1.084  | OOM            | OOM                         | OOM                                                               |
| [BNG(libras_move)](openml.org/t/7327)        | 1956                        | 4.2 | 2.51   | 1922           | 97.6                        | 2.53                                                              |
| [BNG(satellite_image)](openml.org/t/7326)    | 334                         | 1.6                                                               | 0.731          | 337                         | 10.0 | 0.721  |
| [COMET_MC](openml.org/t/14949)               | 44                          | 1.0 | 0.0615  | 47             | 5.0                         | 0.0662                                                            |
| [friedman1](openml.org/t/361939)             | 275                         | 4.2 | 1.047   | 278            | 5.1                         | 1.487                                                             |
| [poker](openml.org/t/10102)                  | 38                          | 0.6 | 0.256   | 41             | 1.2                         | 0.722                                                             |
| [subset_higgs](openml.org/t/361955)          | 868                         | 10.6 | 0.420  | 870            | 24.5                        | 0.421                                                             |
| [BNG(autoHorse)](openml.org/t/7319)          | 107                         | 1.1 | 19.0    | 107            | 3.2                         | 20.5                                                              |
| [BNG(pbc)](openml.org/t/7318)                | 48                          | 0.6 | 836.5   | 51             | 0.2                         | 957.1                                                             |
| average                                      | 465                         | 3.9                                                               | -              | 464                         | 19.7                                                              | -              |

PerpetualBooster outperformed AutoGluon on 8 out of 10 datasets, training equally fast and inferring 5x faster. The results can be reproduced using the automlbenchmark fork [here](https://github.com/deadsoul44/automlbenchmark).

Github: https://github.com/perpetual-ml/perpetual",mutlu_simsek,1h52zk8,https://reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,https://www.reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,2024-12-02 19:12:26,18,0.88,18,0,0,0,0,False,False,True,False,False,Project,self,t3_1h52zk8
MachineLearning,[D] Handle varying output dimension in Graph Neural Networks?,"I have a question about handling varying output dimensions in **Graph Neural Networks (GNNs)** during training. I'm working with **a combined graph** (merging task and compute graphs), where the structure resembles the task graph, but with compute node information integrated into the features. Since both the task graph and compute graph (nodes count) can vary, I'm using a feedforward layer to transform the node and edge features into a fixed hyperparameter embedding dimension. However, the dataset contains instances with **different numbers of compute nodes**. For example, one instance (A) might have 5 compute nodes, while another instance (B) might have 7 compute nodes. Given that this is a scheduling task using GNNs, the output dimension must match the number of compute nodes, as tasks are assigned to these nodes. I'm wondering how to handle varying output dimensions in GNNs and if there are any standard approaches to manage this kind of variation. Thanks!",bipulthapa,1h4uyn9,https://reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,https://www.reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,2024-12-02 13:29:07,3,0.67,3,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h4uyn9
MachineLearning,[R] Simplified RNNs Achieve Transformer-Like Performance with Parallel Training and Reduced Parameters,"This paper systematically examines whether RNNs might have been sufficient for many NLP tasks that are now dominated by transformers. The researchers conduct controlled experiments comparing RNNs and transformers while keeping model size, training data, and other variables constant.

Key technical points:
- Tested both architectures on language modeling and seq2seq tasks using matched parameters (70M-1.5B)
- Introduced ""RNN with Parallel Generation"" (RPG) allowing RNNs to generate tokens in parallel like transformers
- Evaluated on standard benchmarks including WikiText-103 and WMT14 En-De translation
- Analyzed representation capacity through probing tasks and attention pattern analysis

Main results:
- RNNs matched or outperformed similarly-sized transformers on WikiText-103 language modeling
- Transformers showed 1-2 BLEU score advantage on translation tasks
- RPG achieved 95% of transformer generation speed with minimal accuracy loss
- RNNs showed stronger local context modeling while transformers excelled at long-range dependencies

I think this work raises important questions about architecture choice in modern NLP. While transformers have become the default, RNNs may still be viable for many applications, especially those focused on local context. The parallel generation technique could make RNNs more practical for production deployment.

I think the results suggest we should reconsider RNNs for specific use cases rather than assuming transformers are always optimal. The computational efficiency of RNNs could be particularly valuable for resource-constrained applications.

TLDR: Comprehensive comparison shows RNNs can match transformers on some NLP tasks when controlling for model size and training. Introduces parallel generation technique for RNNs. Results suggest architecture choice should depend on specific application needs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/were-rnns-all-we-needed). Paper [here](https://arxiv.org/abs/2410.01201)",Successful-Western27,1h4urpr,https://reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,https://www.reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,2024-12-02 13:19:02,117,0.9,117,0,22,0,0,False,False,True,False,False,Research,self,t3_1h4urpr
MachineLearning,[R] A Comprehensive Database of 300+ Production LLM Implementations with Technical Architecture Details,"Sharing a valuable resource for ML practitioners: A newly released database documenting over 300 real-world LLM implementations, with detailed technical architectures and engineering decisions.

Key aspects that might interest this community:

* Retrieval-Augmented Generation (RAG) architectures in production
* Fine-tuning decisions and performance comparisons
* Embedding strategies and vector database implementations
* Model optimization techniques and quantization approaches
* Evaluation methodologies and monitoring systems

Notable technical implementations covered:

* Anzen's document classification system using BERT (95% accuracy in production)
* Barclays' MLOps evolution for regulatory compliance
* MosaicML's lessons from training &amp; deploying MPT
* Emergent Methods' real-time RAG system for news processing
* Qatar Computing Research Institute's T-RAG architecture

Technical focus areas:

1. Model serving architectures
2. Training infrastructure decisions
3. Latency optimization strategies
4. Cost-performance trade-offs
5. Production monitoring approaches

Each case study includes:

* Technical architecture diagrams where available
* Performance metrics and benchmarks
* Implementation challenges and solutions
* Infrastructure decisions and rationale
* Scaling considerations

URL: [https://www.zenml.io/llmops-database/](https://www.zenml.io/llmops-database/)

We're also accepting technical write-ups of production implementations through the submission form: [https://docs.google.com/forms/d/e/1FAIpQLSfrRC0\_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li\_5lWw/viewform](https://docs.google.com/forms/d/e/1FAIpQLSfrRC0_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li_5lWw/viewform)

Would be particularly interested in this community's thoughts on the architectural patterns emerging across different scales of deployment.

*Edit: We've also synthesized cross-cutting technical themes into summary podcasts for those interested in high-level patterns.*

*Edit: An accompanying blog synthesizes much of the learnings:* [*https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations*](https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations)",htahir1,1h4udds,https://reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,https://www.reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,2024-12-02 12:58:02,88,0.91,88,0,29,0,0,False,False,True,False,False,Research,self,t3_1h4udds
MachineLearning,[P] PyTorch implementation of Levenberg-Marquardt training algorithm,"Hi everyone,

In case anyone is interested, here’s a PyTorch implementation of the **Levenberg-Marquardt (LM)** algorithm that I’ve developed.

**GitHub Repo**: [torch-levenberg-marquardt](https://github.com/fabiodimarco/torch-levenberg-marquardt)

A PyTorch implementation of the **Levenberg-Marquardt (LM)** optimization algorithm, supporting **mini-batch training** for both **regression** and **classification** problems. It leverages GPU acceleration and offers an extensible framework, supporting diverse loss functions and customizable damping strategies.

A TensorFlow implementation is also available: [tf-levenberg-marquardt](https://github.com/fabiodimarco/tf-levenberg-marquardt)

# Installation

    pip install torch-levenberg-marquardt",fabiodimarco,1h4ubbd,https://reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,https://www.reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,2024-12-02 12:54:53,82,0.94,82,0,7,0,0,False,False,True,False,False,Project,self,t3_1h4ubbd
MachineLearning,[R] RuleOpt v.1.1: Optimization-Based Rule Learning for Classification,"**Paper**: [https://arxiv.org/abs/2104.10751](https://arxiv.org/abs/2104.10751)

**Package:** [https://github.com/sametcopur/ruleopt](https://github.com/sametcopur/ruleopt)

**Documentation:** [https://ruleopt.readthedocs.io/](https://ruleopt.readthedocs.io/)

RuleOpt is an optimization-based rule learning algorithm designed for classification problems. Focusing on scalability and interpretability, RuleOpt utilizes linear programming for rule generation and extraction.

The Python library ruleopt is capable of extracting rules from ensemble models, and it also implements a novel rule generation scheme. The library ensures compatibility with existing machine learning pipelines, and it is especially efficient for tackling large-scale problems.

Here are a few highlights of ruleopt:

* **Efficient Rule Generation and Extraction**: Leverages linear programming for scalable rule generation (stand-alone machine learning method) and rule extraction from trained random forest and boosting models.
* **Interpretability**: Prioritizes model transparency by assigning costs to rules in order to achieve a desirable balance with accuracy.
* **Integration with Machine Learning Libraries**: Facilitates smooth integration with well-known Python libraries scikit-learn, LightGBM, and XGBoost, and existing machine learning pipelines.
* **Extensive Solver Support**: Supports a wide array of solvers, including *Gurobi*, *CPLEX* and *OR-Tools*.

  
With the latest version update, RuleOpt is now fast even with the free solver OR-Tools, even on large datasets! In the graph below, you can see how the new version performs in terms of runtime compared to the previous version.

[Training Times v1.0 vs v1.1](https://preview.redd.it/ev5u5m4bjf4e1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=e38ea3c168ece7ad6660153a6073f8064ac85d83)

  
We’d love to hear your feedback, questions, or any other inquiries you may have!",zedeleyici3401,1h4tzd0,https://reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,https://www.reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,2024-12-02 12:36:23,10,0.92,10,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/HhFHG_WRfu3w1cHy2TmWE1t-ZUve2F-IM_C2fVLZwqo.jpg,t3_1h4tzd0
MachineLearning,[R] Best chunking method for PDFs with complex layout?,"I am working on a RAG based PDF Query system , specifically for complex PDFs that contains multi column tables, images, tables that span across multiple pages, tables that have images inside them.

I want to find the best chunking strategy for such pdfs.

Currently i am using RecursiveCharacterTextSplitter. What worked best for you all for complex PDF?

",ElectronicHoneydew86,1h4pkmh,https://reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,https://www.reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,2024-12-02 07:24:21,20,1.0,20,0,5,0,0,False,False,True,False,False,Research,self,t3_1h4pkmh
MachineLearning,[R] Queries on DeepAR Framework in AWS Sagemaker,"Hi,

I'm trying to implement deepAr for various stores to predict futures sales (each store with ~10k SKU of different products). Due to sheer size of the SKU I wouldn't be able to just do only single training for all the data at once. I'm thinking to train it by store.

1. How do I do parallelism in AWS for the training purpose? Each store training process would take up to 30mins;
2. How to deal with unseen SKUs which are not present in the data?

Thanks.",skw1990,1h4gwxy,https://reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,https://www.reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,2024-12-01 23:33:04,8,1.0,8,0,0,0,0,False,False,True,False,False,Research,self,t3_1h4gwxy
MachineLearning,[D] Anthropic AI fellow/residents- any new grads/entry-level people accepted?,"Hello. Are entry-level or new grads accepted into the Anthropic fellowship or resident programs? Past people who were accepted, what was your CV and experience like?",geekgeek2019,1h4e0ah,https://reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,https://www.reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,2024-12-01 21:25:59,32,0.71,32,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h4e0ah
MachineLearning,[D] How to handle varying Feature Dimensions in Graph Neural Networks training?,"I have a question about handling datasets with varying feature dimensions in Graph Neural Network training. For example, in one training instance (let's call it Dataset A), the node features have a dimension of 4, and the edge features have a dimension of 16. In another instance (Dataset B), the node features have a dimension of 5, and the edge features have a dimension of 25. Other datasets may have different feature dimensions as well.

What are the standard methods used to handle varying feature dimensions for each instance when training a GNN model with such datasets? I would appreciate any guidance or direction on how to approach this. Thanks!",bipulthapa,1h4dbvi,https://reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,https://www.reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,2024-12-01 20:57:35,23,0.93,23,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h4dbvi
MachineLearning,[P] Promptwright - Open source project to generate large synthetic datasets using an LLM (local or hosted),"Hey r/machinelearning,

[Promptwright](https://github.com/StacklokLabs/promptwright), a free to use *open source* tool designed to easily generate synthetic datasets using either local large language models or one of the many hosted models (OpenAI, Anthropic, Google Gemini etc)

Key Features:

\* Multiple LLM Providers Support: Works with most LLM service providers and LocalLLM's via Ollama, VLLM etc

\* Configurable Instructions and Prompts: Define custom instructions and system prompts in YAML, over scripts as before.

\* Command Line Interface: Run generation tasks directly from the command line

\* Push to Hugging Face: Push the generated dataset to Hugging Face Hub with automatic dataset cards and tags

Here is an example dataset created with promptwright on this latest release:

[https://huggingface.co/datasets/stacklok/insecure-code/viewer](https://huggingface.co/datasets/stacklok/insecure-code/viewer)

This was generated from the following template using \`mistral-nemo:12b\`, but honestly most models perform, even the small 1/3b models.

    system_prompt: ""You are a programming assistant. Your task is to generate examples of insecure code, highlighting vulnerabilities while maintaining accurate syntax and behavior.""
    
    topic_tree:
      args:
        root_prompt: ""Insecure Code Examples Across Polyglot Programming Languages.""
        model_system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        tree_degree: 10  # Broad coverage for languages (e.g., Python, JavaScript, C++, Java)
        tree_depth: 5  # Deep hierarchy for specific vulnerabilities (e.g., SQL Injection, XSS, buffer overflow)
        temperature: 0.8  # High creativity to diversify examples
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
      save_as: ""insecure_code_topictree.jsonl""
    
    data_engine:
      args:
        instructions: ""Generate insecure code examples in multiple programming languages. Each example should include a brief explanation of the vulnerability.""
        system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        temperature: 0.9  # Encourages diversity in examples
        max_retries: 3  # Retry failed prompts up to 3 times
    
    dataset:
      creation:
        num_steps: 15  # Generate examples over 10 iterations
        batch_size: 10  # Generate 5 examples per iteration
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        sys_msg: true  # Include system message in dataset (default: true)
      save_as: ""insecure_code_dataset.jsonl""
    
    # Hugging Face Hub configuration (optional)
    huggingface:
      # Repository in format ""username/dataset-name""
      repository: ""hfuser/dataset""
      # Token can also be provided via HF_TOKEN environment variable or --hf-token CLI option
      token: ""$token""
      # Additional tags for the dataset (optional)
      # ""promptwright"" and ""synthetic"" tags are added automatically
      tags:
        - ""promptwright""

We've been using it internally for a few projects, and it's been working great. You can process thousands of samples without worrying about API costs or rate limits. Plus, since everything runs locally, you don't have to worry about sensitive data leaving your environment.

The code is Apache 2 licensed, and we'd love to get feedback from the community. If you're doing any kind of synthetic data generation for ML, give it a try and let us know what you think!

Links:

Checkout the [examples](https://github.com/StacklokLabs/promptwright/tree/main/examples) folder , for examples for generating code, scientific or creative ewr

Would love to hear your thoughts and suggestions, if you see any room for improvement please feel free to raise and issue or make a pull request.",zero_proof_fork,1h4bcz2,https://reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,https://www.reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,2024-12-01 19:33:47,15,0.83,15,0,4,0,0,False,False,True,False,False,Project,self,t3_1h4bcz2
MachineLearning,[D] Seeking Advice on Machine Learning Models for Generating Seamless 360° Images,"Hi everyone,

I’m working on a project that involves creating 360° images, and I’m running into some challenges. The goal is to generate seamless 360° panoramas without visible edges or artifacts where the image wraps around.

I’m wondering if there are any machine learning models, techniques, or tools that are particularly well-suited for this task. Specifically, I’m looking for something that can:

* Ensure continuity at the edges of the 360° image.
* Handle different textures and patterns without noticeable distortions.
* Be trained or fine-tuned on my custom dataset (if needed).

I’ve explored GANs like StyleGAN and diffusion models, but I’m not sure if they can handle the edge continuity issue out of the box. Has anyone worked on a similar problem or knows of a good starting point?

Any suggestions, resources, or insights would be greatly appreciated! Thanks in advance!",Deep_Land_4093,1h48950,https://reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,https://www.reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,2024-12-01 17:22:26,0,0.43,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h48950
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1h46e6j,https://reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,2024-12-01 16:00:30,7,0.9,7,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h46e6j
MachineLearning,[Project] Noema – A Declarative AI Programming Library,,Super_Dependent_2978,1h46341,https://reddit.com/r/MachineLearning/comments/1h46341/project_noema_a_declarative_ai_programming_library/,/r/LocalLLaMA/comments/1h3d4d8/noema_a_declarative_ai_programming_library/,2024-12-01 15:46:35,3,0.6,3,0,1,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/Bke830yFsuJxUn2gkbNe1tpnLzRcSugASe-RSwzFQu0.jpg,t3_1h46341
MachineLearning,[R] Sources: Reasons why KG outperformes RD in Retrievers?,"Are there any sources discussing WHY Retriever work better with KG in contrast to RD? I find it super intuitive to say its better because in knowledge graphs we have more semantic structure and relations are discovered effeciently. In my mind its ""of course the graph is richer/more dense"" but when collaborated on a paper, it struck me that I wasnt able to justify that claim. I found no source whatsoever that actually explained why that might be the case.

The only source i got was this one:  
[https://arxiv.org/abs/2311.07509](https://arxiv.org/abs/2311.07509)

also here in [](https://www.reddit.com/r/LocalLLaMA/) sub last year:  [https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a\_benchmark\_to\_understand\_the\_role\_of\_knowledge/](https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a_benchmark_to_understand_the_role_of_knowledge/)

So all we we're able to say was ""We justify our decision because KG works better then RD \[source to benchmark paper\]""

I would have loved to discuss why exactly KG are better suited and give arguments about information density, semantic strutuce or the better selection of related entities. But everything I found were only articles that threw around wild claims or pointed out easier/native implementation, which technically could also be achieved with RD.

Can anyone point me to sources? would love to read an in-depth discussion on the reasons of better performance.",PopPsychological4106,1h447eu,https://reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,https://www.reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,2024-12-01 14:17:17,26,0.69,26,0,22,0,0,False,False,True,False,False,Research,self,t3_1h447eu
MachineLearning,Augmentation for Images with ROI [D],"I have an Image with roi (x\_min,y\_min,x\_max,y\_max). I want do Random flip,rotate,skew, translate ,etc.. with torchvison. what are the different ways in which you can transform the roi respectively in order to match with the augmented image ?",Brief_Papaya121,1h41z2x,https://reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,https://www.reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,2024-12-01 12:11:25,5,0.78,5,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h41z2x
MachineLearning,"[R] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",,moschles,1h40wms,https://reddit.com/r/MachineLearning/comments/1h40wms/r_qwenvl_a_versatile_visionlanguage_model_for/,https://storage.prod.researchhub.com/uploads/papers/2023/12/25/2308.12966.pdf,2024-12-01 10:59:28,14,0.94,14,0,0,0,0,False,False,False,False,False,Research,default,t3_1h40wms
MachineLearning,[P] A complete transformer model built in Excel,,Revolutionary-Way290,1h3hj6j,https://reddit.com/r/MachineLearning/comments/1h3hj6j/p_a_complete_transformer_model_built_in_excel/,https://x.com/ProfTomYeh/status/1859282491955130452,2024-11-30 17:25:45,19,0.95,19,0,5,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/JFO-VdcjE87RX0w6rQ5kwyN7HblXjJoN1QbFc-1p18M.jpg,t3_1h3hj6j
MachineLearning,[P] TIME-MOE: Billion-Scale Time Series Forecasting with Mixture-of-Experts,"**Time-MOE** is a 2.4B parameter open-source time-series foundation model using **Mixture-of-Experts (MOE)** for zero-shot forecasting.

You can find an analysis of the model [here](https://aihorizonforecast.substack.com/p/time-moe-billion-scale-time-series)",nkafr,1h3j1cm,https://reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,https://www.reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,2024-11-30 18:33:59,2,0.75,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1h3j1cm
MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

&gt;Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

&gt;Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&amp;#x200B;

Please remember that this community is geared towards those with experience.",AutoModerator,1h3u444,https://reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,2024-12-01 03:30:15,27,0.84,27,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h3u444
MachineLearning,What's the best Open Source Image-Upscaling Model? [Discussion],"I'm using [Playground-v2.5-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) to make some images for YouTube thumbnails. I'm really happy with the results:

[1024x1024 base image of mars base.](https://preview.redd.it/uuo4sdgwp44e1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=84d61bf4d7fbf2457df1037e95603166390efa12)

But I would like the image to be 1920x1080 pixels, and my only options are 1024x1024, or 1280x720 pixels. At the moment, I can get to 1920x1080 with Photoshop's outpainting:

[1920x1080 outpainted image of mars base.](https://preview.redd.it/07tt5ix4q44e1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=cdfb7cefa8d2bb4f187d2ab6b86aaba17596506a)

This is okay, but photoshops outpainting is manual and has a fairly significant quality drop. Ideally, I would generate an image in 1280x720 then upscale to 1920x1080 programmatically.

I've heard of the following models:

* Real-ERSGAN
* Waifu2
* SRGAN

But before I jump into any of them, what open-source model is generally considered best to achieve this? I have an RTX 3060 12GB of VRAM.",FPGA_Superstar,1h3qcon,https://reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,https://www.reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,2024-12-01 00:13:57,40,0.93,40,0,8,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ROcJ4oUu0I-1wtjwdAycnGfQ3s8tbIsJdyufqyhsVyI.jpg,t3_1h3qcon
MachineLearning,[Discussion] Advice Needed: Rejected from COLING 2025 – Which Conference Should I Target Next?,"
I got a rejection from COLING 2025 with review scores of 4, 3, 3. I’m revising the manuscript and looking for advice on the next best NLP conference to target. Any suggestions for similar top-tier venues?

Thanks!",Cold-Traffic-7586,1h3igva,https://reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,https://www.reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,2024-11-30 18:08:01,5,0.78,5,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h3igva
MachineLearning,[D] Modern use-cases for RNNs?,"The discussion can be twofold.
1)What are in your opinion some tasks, for the personal projects scale, where you think RNNs close to traditional implementations (LSTM, GRU) are still the best starting and ending point? Especially compared to transformers.

In small time-series forecasting settings I can see a GRU being more convenient than a Transformer probably, but I am interested also in tasks where inputs are sequences of symbols or measures, but outputs maybe not.

The main goal is to play with LSTM and GRU variants (eg minGRU) on datasets where it makes sense, might do tiny-Shakespeare but it doesn't warm my heart...

2) do you think there are sequential tasks and settings where RNNs are not only the more natural option according to our intuition, but actually the only theoretically or experimentally available option to make do, compared to Transformers or 1D CNNs etc?",Sad-Razzmatazz-5188,1h38ym2,https://reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,https://www.reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,2024-11-30 09:22:32,54,0.9,54,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1h38ym2
MachineLearning,[D] What's the fastest object detection model?,"Hi, I'm working on a project that needs object detection. The task itself isn't complex since the objects are quite clear, but speed is critical. I've researched various object detection models, and it seems like almost everyone claims to be ""the fastest"". Since I'll be deploying the model in C++, there is no time to port and evaluate them all.

I tested YOLOv5/v5Lite/8/10 previously, and YOLOv5n was the fastest. I ran a simple benchmark on an Oracle ARM server (details [here](https://github.com/Avafly/YOLOv5-ncnn-OpenVINO-MNN-ONNXRuntime-OpenCV-CPP?tab=readme-ov-file#simple-benchmarks-on-m1-mac-and-arm-linux)), and it processed an image with 640 target size in just 54ms. Unfortunately, the hardware for my current project is significantly less powerful, and meanwhile processing time must be less than 20ms. I'll use something like quantization and dynamic dimension to boost speed, but I have to choose the suitable model first.

Has anyone faced a similar situation or tested models specifically for speed? Any suggestions for models faster than YOLOv5n that are worth trying?",Knok0932,1h362dq,https://reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,https://www.reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,2024-11-30 06:00:40,46,0.98,46,0,39,0,0,False,False,True,False,False,Discussion,self,t3_1h362dq
MachineLearning,[D] Molecular Dynamics and Machine Learning Build,"Hey guys, so I do a lot of molecular dynamics and am starting to push into the ML space with that and genome/multi-omics sort of stuff. I’m building a workstation and with my budget I’m looking at 2x A6000 or 2x 5000 Ada. Both work great for molecular dynamics, but I’m trying to figure out my best option for ML. The A6000 has 48gb vram and nvlink, but the 5000 Ada is newer and substantially faster and 32Gb VRAM per card is no slouch either. Any advice? ",Mdgoff7,1h2x4ar,https://reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,https://www.reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,2024-11-29 22:04:19,2,0.67,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h2x4ar
MachineLearning,[N][R] Models are what they eat: automatic data curation for LLMs,"Sharing our most recent work at [DatologyAI](http://www.datologyai.com). Models are what they eat, and our mission is to make data curation for training large models as effective and easy as possible. 

Combining a bevy of approaches, including heuristic filters, model-based filters, embedding based curation, synthetic data, target distribution matching, and mixing ratios, we were able to massively improve training efficiency, performance, and inference efficiency. 

Comparing to our baseline and starting dataset -- exact deduplicated RedPajamav1, we can: 

* Reach the same performance 7.7x faster (and 3.4x faster than DCLM)
* Improve performance across benchmarks by 8.5% (and by 4.4% over DCLM)
* Train models with fewer than half the parameters which outperform larger models by &gt;5% 

Check out our [high-level results here](https://www.datologyai.com/post/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation), and if you want all the nitty-gritty details, check out our[ technical deep dive](https://www.datologyai.com/post/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset).",arimorcos,1h2qmol,https://reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,2024-11-29 17:15:57,17,0.77,17,0,1,0,0,False,False,True,False,False,News,self,t3_1h2qmol
MachineLearning,[D] Inconsistent use of the gerund form in dataset naming,"This is, of course, a very minor issue. But it really irks me, and I was wondering if other people are bothered by it. I think the two most common naming schemes I see for the three standard dataset splits are ""training dataset, validation dataset, and test dataset"" or ""training dataset, validation dataset, and testing dataset"". But to be consistent it should really be ""train dataset, validation dataset, and test dataset"" or ""training dataset, validating dataset, and testing dataset"". I always use the former. Does this bother anyone else, or am I alone in brooding over this?",Shianiawhite,1h217sh,https://reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,2024-11-28 17:34:46,4,0.75,4,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h217sh
MachineLearning,"""[P]""Static variable and dynamic variable tables in RFM ","



I am creating a prediction model using random forest. But I don't understand how the model and script would consider both tables loaded in as dataframes.


What's the best way to use multiple tables with a Random Forest model when one table has static attributes (like food characteristics) and the other has dynamic factors (like daily health habits)?

Example:
I want to predict stomach aches based on both the food I eat (unchanging) and daily factors (sleep, water intake).

Tables:
 * Static: Food name, calories, meat (yes/no)
 * Dynamic: Day number, good sleep (yes/no), drank water (yes/no)


How to combine these tables in a Random Forest model? Should they be merged on a unique identifier like ""Day number""?
",peyott100,1h2oe69,https://reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,https://www.reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,2024-11-29 15:36:10,1,0.67,1,0,3,0,0,False,False,True,False,False,Project,self,t3_1h2oe69
MachineLearning,[D] Hinton and Hassabis on Chomsky’s theory of language,"I’m pretty new to the field and would love to hear more opinions on this. I always thought Chomsky was a major figure on this but it seems like Hinton and Hassabis(later on) both disagree with it. Here: https://www.youtube.com/watch?v=urBFz6-gHGY (longer version: https://youtu.be/Gg-w_n9NJIE)

I’d love to get both an ML and CogSci perspective on this and more sources that supports/rejects this view.

Edit: typo + added source.",giuuilfobfyvihksmk,1h2mkye,https://reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,2024-11-29 14:10:12,120,0.92,120,0,116,0,0,False,False,True,False,False,Discussion,self,t3_1h2mkye
MachineLearning,[D] COLING 2025 Final Acceptances - Is it not out yet?,"Is the final acceptance out? I am not seeing it yet on Softconf. Had a paper with (5,4) (4,4) (4,4) in reviews.",UnhappyPrior6570,1h2kfic,https://reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,https://www.reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,2024-11-29 12:10:32,9,0.8,9,0,44,0,0,False,False,True,False,False,Discussion,self,t3_1h2kfic
MachineLearning,[R] Recursive Methods for interpolation between vector fields ( Known and Unknown),"Hello everyone Does anything of the next makes sense?  
I Have been posting on Learning first ( also on math and number theory ) , but I think is a bit more math theory than ML but it does have to do with how the data is interpolated so I am unsure. 

( I hope I am not breaking rule 5 with my links ) 

this will be the interpolation of the data ( Via organized vector field levels )  before the generative process starts, but because its recursive, the generative process can happen on inside the iteration too 

its there a model I can use ? And if someone understand the math, can I get some papers or things I could follow or just is learning and reading now?

I am a little lost and need some help ( I organized my question with chatGPT to make it understandable so bare in mind if there is some odd work here and there, I am on the I am going a bit mental stage )

I think this is dealing with machine learning problems that have been solved between interpolation of point could on space that have recursive data ( mapping and data organization )

I've been developing a concept that merges artistic visualization with advanced mathematical interpolation techniques inspired by the Mandelbrot set. Coming from a creative background, I've ventured into creating what I believe could be a **recursive Mandelbrot predictive method**  for manipulating vector fields. I'm eager to understand if this approach already exists and to gather resources or similar algorithms to explore further and test my ideas.

I will add some things like this latter to test segmentation models for the recursiveness [https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear\_algebra\_project\_i\_implemented\_a\_kmeans/](https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/)

**REFERENCE IMAGES**  
everything is based on recursive by resolution with inverse square distance from the origin point

**Mandelbroth**  
[https://en.wikipedia.org/wiki/Mandelbrot\_set#/media/File:Juliacycles1.png](https://en.wikipedia.org/wiki/Mandelbrot_set#/media/File:Juliacycles1.png)

**Conceptual model** ( The mandelbroth guidance happens just on the altered time pulling agent ) ( Orange )  
[Single Vector interpretation and prediction stream of the Pull of the mandelbrot agent](https://cdn.discordapp.com/attachments/457637053581230100/1311677830966284298/chrome_7KNKgsO6Fm.png?ex=6749baac&amp;is=6748692c&amp;hm=55a7809cd5bb402a06fdea1edbad4eb1ff21f15d1a3303efee9b797eef0f5839&amp;)

**Conceptual Model 2d sim**  
[Representation of the predictiveness](https://media.discordapp.net/attachments/457637053581230100/1311668738139099166/ShareX_11AweoaOUp.png?ex=6749b234&amp;is=674860b4&amp;hm=7d660210c3678b8f1bd0804ee4f2f2425a0c1dc9aea18bdf39e86ba81d9ad2e8&amp;=&amp;format=webp&amp;quality=lossless&amp;width=295&amp;height=350) as mandelbrot

Representation of functional interpolation of agents via Mandelbroth ( non recursive )

**Conceptual Simulation model 2d sim ( making the mandelbroth )**  
[Image non animated](https://media.discordapp.net/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;=&amp;format=webp&amp;quality=lossless)[ANIMATED VIDEO DOWNLOAD ( CLEAN FILE )](https://cdn.discordapp.com/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;)

**Conceptual layering**  
[Layering of 3 tiers via inverse square distance on a vector field ( currently surface) but can be world](https://media.discordapp.net/attachments/457637053581230100/1311404062931030086/image.png?ex=67496475&amp;is=674812f5&amp;hm=56bf4ff3988163f35c1ef24227645beaa6d11ab1b0330d094dc3dccbad6ce696&amp;=&amp;format=webp&amp;quality=lossless&amp;width=378&amp;height=350)

**recursiveness concept**  
[Applied recursiveness auto generation based on surface vector field ](https://media.discordapp.net/attachments/457637053581230100/1311404209278685184/image.png?ex=67496498&amp;is=67481318&amp;hm=e548fb4615d48adea59f835a3a76a6a203c5946ea6d356eff2c7298252d3de41&amp;=&amp;format=webp&amp;quality=lossless&amp;width=927&amp;height=831)( no prediction applied )

# The Concept

Imagine a system where the interpolation between data points isn't limited to traditional methods like **lerp** (linear interpolation) or **slerp** (spherical linear interpolation). Instead, it employs a **pseudo vector field Mandelbrot slerp**, allowing vectors to be guided from a base state (reality) to a target state (altered time) within a Mandelbrot-inspired vector field. This method is recursive, meaning multiple layers of calculations are applied to refine the interpolation continuously.

# Key Components:

1. **Reality (Ground Truth):** Represents the current state of the system, serving as the foundational dataset.
2. **Agents of Change (Vectors of Closest Influence):** These act as pull forces influencing the direction and magnitude of interpolation.
3. **State (Ground Truth Prediction Model):** Utilizes the current data to predict future states based on the influences of the agents.
4. **Altered Time (Goal):** The desired target state, akin to a Mandelbrot-type location on the outer range of the vector field.

# Interpolation Method

The interpolation technique extends beyond simple linear methods by incorporating the complexity and fractal nature of the Mandelbrot set. Here's how it functions:

* **Guided Vectors:** Vectors transition from reality towards altered time, following paths influenced by a Mandelbrot-like vector field.
* **Recursive Layers:** Multiple layers of interpolation allow for increasingly refined calculations, enhancing accuracy and adaptability.
* **Dynamic Intensity:** The closer the interpolation is to reality, the more intense and detailed the calculations become, while the vector field simplifies as it moves towards altered time.

# Theoretical Foundation

The core idea revolves around mapping and adjusting Mandelbrot-inspired vectors to facilitate interpolation between recursively organized data banks. This approach aims to:

* **Capture Complex Patterns:** Leverage the self-similar, fractal nature of Mandelbrot sets to identify and utilize intricate patterns within the data.
* **Enhance Predictive Capability:** Recursive calculations allow for continual refinement of projections, improving predictive accuracy over time.
* **Achieve Real-Time Adaptability:** Dynamically adjust vectors to align with specific goals, similar to how a car's performance might be modulated in real-time to achieve optimal racing outcomes.

# Visual Analogy

Think of this system as calculating the ""ghost"" position of a car in a racing game like *Need for Speed*:

* **Acceleration and Braking:** Based on historical and current data, determining when to accelerate or brake to achieve the best performance.
* **Engine Adjustments:** Modifying the system's parameters in real-time to align with the target state, ensuring the system reaches its goal efficiently.
* **Dynamic Modulation:** Continuously adjusting these actions to meet the desired ""goal time,"" always operating within physical (mathematical) constraints.

# Questions for the Community

1. **Does This Technology Exist?** Is my approach accurately described as a **recursive Mandelbrot predictive method** for vector field interpolation? Are there existing models or research that align closely with this concept?
2. **Resources and References:** If similar technologies or algorithms exist, could you recommend any resources, papers, or specific Mandelbrot-like algorithms that I can study or begin testing with?
3. **Mathematical Validation:** Given that my approach stems from an artistic visualization perspective, what mathematical frameworks or theories should I explore to formalize and validate this method?

# Additional Context

For a visual representation of my model and its applications, you can refer to the following links:

* **Visual Model:** [LinkedIn Visual Model](https://www.linkedin.com/feed/update/urn:li:activity:7267834154631708672/)
* **Use Case Example:** [LinkedIn Use Case](https://www.linkedin.com/posts/jesusfc14_i-think-i-am-reaching-a-clarity-moment-the-activity-7267823963068628992--qA7?utm_source=share&amp;utm_medium=member_desktop)

*(Please note that these links provide additional visual context to help illustrate the concept.)*

**Thank you for taking the time to read through my concept! I'm looking forward to your insights, validations, and any resources you can share to help me advance this idea.**

all this tech is currently under Creature Garage umbrella but I have ownership of the creative driver of the idea so that should be fine for me to post but I reached a moment that I will need help for some of the most advanced math implementations

I am using some concepts that sound really far and advanced but currently my implementation is mostly based on recursiveness the prediction agent will come to function once I have my full set of data to make a test",jesusfc,1h2io35,https://reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,https://www.reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,2024-11-29 10:08:00,2,0.6,2,0,12,0,0,False,False,True,False,False,Research,self,t3_1h2io35
MachineLearning,"[D] How does VQ-VAE disentangle, if it does at all?","I currently use a BetaTC-VAE, which does an excellent job at disentangling, knowing that VAE can slightly disentangle since for the model it's easier to get a lower KL loss if the variables are dissentanlged, the beta term make this beta times more important, and total correlation and mutual information loss push for total disentanglement, but in VQ-VAE there is no (major) disentanglement, only a codebook, and discrete outputs. Could the discrete latent given by the codebook be disentangled? If not, is there any paper on disdentangling VQ-VAE? I have an environment where disentangled latent spaces provide better reconstruction than continous latent spaces ",ZazaGaza213,1h2epzx,https://reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,2024-11-29 05:33:10,38,0.95,38,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h2epzx
MachineLearning,[D] Most important papers in implicit regularisation,"Hi guys

I'm getting into machine learning, especially on the theoretical side, and I'm curious to learn more about why neural networks tend to generalise so well, so I'm hoping to read some papers about this. As far as I'm aware, the first big paper on the topic was 'Understanding deep learning requires rethinking generalization' by Zhang et al.

I've got a good mathematical background, so I was wondering what people think are the most impactful papers there are in this area. What do you think made the most impact?",MrBeebins,1h29i7j,https://reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,2024-11-29 00:23:33,9,0.92,9,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h29i7j
MachineLearning,[P] Retrieval augmented generation on-premises (fully local solution),"Hey everyone,   
I’m excited to share my latest repo with you—a local conversational RAG solution for your files! Here’s the deal: this setup is perfect for running RAG on-premises.   
It’s built with Docker, LangChain, Ollama, FastAPI, and Hugging Face, and all models are downloaded automatically. Soon, I’ll add support for choosing your preferred model, but here’s what the solution currently includes:  
• Locally running Ollama: It’s hardcoded to the Qwen-0.5B model for now, but model selection from the Ollama registry is coming soon.   
• Local indexing: Uses a sentence-transformer embedding model (currently restricted to this family, but this will also change soon).   
• Qdrant container: Runs locally for vector storage.   
• Local reranker: Currently uses BAAI/bge-reranker-base, with support for reranker selection coming soon.  
• Websocket-based chat: Includes history-saving capabilities.   
• Simple chat UI: Built with React for a straightforward interface.   
• Bonus: You can use this setup with ChatGPT as a custom GPT! Query your local data through the official ChatGPT web interface or macOS/iOS app.   
• On-premises ready: Everything runs locally, and the containers are CPU-friendly.

A couple of ideas and known issues:   
• Support for Model Context Protocol is on the roadmap.   
• No incremental indexing or reindexing yet.   
• Model selection isn’t available yet but will be added soon.   
  
I’d love your feedback, contributions, or support—watch, fork, and star if you find this interesting!  
Thank you!   
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
",davidvroda,1h26ul7,https://reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,2024-11-28 22:01:09,3,0.59,3,0,0,0,0,False,False,True,False,False,Project,self,t3_1h26ul7
MachineLearning,[D] Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis,"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 **Visatronic: A Multimodal Decoder-Only Model for Speech Synthesi**s by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/ygxnbhiboo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=31bf7c9b988c83a8d0ff2e7b011dac027aa8f154

https://preview.redd.it/7v15egiboo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=d9629caa406a92f8b2052ad6baa3a0265a27ddcf

",CATALUNA84,1h222s2,https://reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,https://www.reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,2024-11-28 18:13:13,5,0.73,5,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/GIw9ZddndVdad-4h4y65mlExW8FTRdhSBchmteHv-AA.jpg,t3_1h222s2
MachineLearning,Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis [D],"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 V**isatronic: A Multimodal Decoder-Only Model for Speech Synthesis** by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/7eoqonhqjo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=c751f5c9f9bc42f22672bbbc67cf8b0fe95ef36b

https://preview.redd.it/224tugzrjo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=37adc93604606d86492dc104f03915bab7428eca",CATALUNA84,1h21j1a,https://reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,https://www.reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,2024-11-28 17:48:43,3,0.71,3,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/wIqPUsHaUNfLLWwLHYcQkdG4Yav_LVjLmzLghfKaOjI.jpg,t3_1h21j1a
MachineLearning,[P] Latest version of Ollama Grid Search (0.7.0): added prompt database ,"https://preview.redd.it/ohewvqicbo3e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=077fec6931b2efc40182f7c2eb284718822213e0

Hey people... the latest version of [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) now comes with its own prompt management database (along with many improvements in the UI).

https://preview.redd.it/qzu95clhbo3e1.png?width=975&amp;format=png&amp;auto=webp&amp;s=473382281094fc3f819e6fc6c3d267941d2a35ce

It makes it a hell lot easier to test your existing prompts when you pull newly released models!

If you want to check it out, the github page has releases for all major platforms:

[https://github.com/dezoito/ollama-grid-search](https://github.com/dezoito/ollama-grid-search)",grudev,1h20fzv,https://reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,https://www.reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,2024-11-28 17:01:16,9,0.85,9,0,0,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uAl-6kPsNjdKIz7o50pQEHnMDPXa-cNX7GoAlrOqiec.jpg,t3_1h20fzv
MachineLearning,[R] BitNet a4.8: 4-bit Activations for 1-bit LLMs,"**Paper:** [https://arxiv.org/pdf/2411.04965](https://arxiv.org/pdf/2411.04965)

**Abstract:**

&gt;Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Visual Abstract:**

https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a

**Evaluations:**

[HS=HellaSwag, PQ=PiQA, WGe=WinoGrande](https://preview.redd.it/4ppq57varn3e1.png?width=955&amp;format=png&amp;auto=webp&amp;s=3c4152947edf4542d2a1ffa181bfa52a5369d916)

https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9

",StartledWatermelon,1h1y0ig,https://reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,2024-11-28 15:11:18,30,0.92,30,0,3,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/eMHBOJqKZns6b6d3vVu8taf4yu8Tq472NuD6w0t0t6M.jpg,t3_1h1y0ig
MachineLearning,[R] Fast Matrix-Based Counterfactual Regret Minimization Using GPU Parallelization,"A novel GPU implementation of Counterfactual Regret Minimization (CFR) that accelerates the computation of optimal strategies in extensive-form games. The core innovation is parallelizing the regret updates and strategy computations across GPU cores while carefully managing memory access patterns.

Key technical points:
- Custom memory layout that maps game states and actions to GPU threads
- Batch processing of information sets to maximize GPU utilization
- Parallel computation of counterfactual values and regret updates
- Multi-GPU scaling through game tree partitioning
- Evaluated on Leduc Hold'em and Limit Texas Hold'em poker variants

Results:
- Up to 30x speedup compared to CPU implementation
- Linear scaling with number of GPUs up to 8 devices
- Memory usage scales with game size and number of information sets
- Solution quality matches CPU baseline within statistical error
- Successfully solved games with up to 10^14 states

I think this work could make CFR much more practical for real-world applications beyond poker. The ability to solve larger games faster opens up possibilities in areas like automated negotiation, security games, and resource allocation. The multi-GPU scaling is particularly interesting as it suggests potential for solving even more complex games.

The memory optimization techniques developed here might also transfer well to other game-theoretic algorithms that need to process large state spaces efficiently.

TLDR: GPU-accelerated CFR implementation achieves 30x speedup through careful parallelization and memory management, with linear multi-GPU scaling. Makes solving large extensive-form games significantly more tractable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/gpu-accelerated-counterfactual-regret-minimization). Paper [here](https://arxiv.org/abs/2408.14778).",Successful-Western27,1h1wq6b,https://reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,2024-11-28 14:08:34,23,0.9,23,0,1,0,0,False,False,True,False,False,Research,self,t3_1h1wq6b
MachineLearning,[D] Theory behind modern diffusion models,"Hi everyone,

I recently attended some lectures at university regarding diffusion models. Those explained all the math behind the original DDPM (Denoiding Diffusion Probabilistic Model) in great detail (especially in the appendices), actually better than anything else I have found online. So it has been great for learning the basics behind diffusion models (slides are available in the link in the readme here if you are interesed: https://github.com/julioasotodv/ie-C4-466671-diffusion-models)

However, I am struggling to find resources with similar level of detail for modern approaches—such as flow matching/rectified flows, how the different ODE solvers for sampling work, etc. There are some, but everything that I have found is either quite outdated (like from 2023 or so) or very superficial—like for non-technical or scientific audiences.

Therefore, I am wondering: has anyone encountered a good compendium of theoretical eplanations beyond the basic diffusion model (besides the original papers)? The goal is to let my team deep dive into the actual papers should they desire, but giving 70% of what those deliver in one or more decent compilations.

I really believe that SEO is making any search a living nightmare nowadays. Either that or my googling skills are tanking for some reason.

Thank you all!",bgighjigftuik,1h1vxe1,https://reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,2024-11-28 13:27:28,228,0.99,228,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1h1vxe1
MachineLearning,[D] Loading data into Ray clusters,"For those of you that run ML training in a Ray cluster on AWS, I'm curious to know what approach you take to get training data into your cluster?

And how are you versioning the data?

How do you avoid repeatedly downloading the same data across runs that have the same dataset?

I'd like a smooth process for being able to target a specific version of a dataset for a training run, and to avoid repeatedly downloading it. The data versioning should have a clear mapping to whatever version of a data pipeline created it. It'd also be nice to have something that scales well to larger datasets.

Keen to hear experiences from the trenches.",SingularValued,1h1v68j,https://reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,2024-11-28 12:45:29,6,0.88,6,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1v68j
MachineLearning,[D] Which LLM models can I run on an NVIDIA 4060 for research purposes? Recommendations needed!,"Hi everyone,

I’m diving into research on large language models (LLMs) and looking to experiment with running them locally on my NVIDIA 4060 GPU. While I know the 4060 isn’t a high-end card compared to some research setups, I’m optimistic about making the most out of what it offers. I’d greatly appreciate any insights or recommendations on:

1. **Models that can run efficiently** on a 4060. I’m aware that some smaller versions of LLMs might be more suited for this hardware, so any advice on what’s realistically possible without excessive optimization would be fantastic.
2. **Models suitable for fine-tuning or pre-training experiments.** Although I’m starting with basic experiments, I plan to explore fine-tuning in the future, so I’d love suggestions for models that are versatile and widely used in research.
3. **Open-source models** or ones that are easy to access and work with for research purposes. Licensing and transparency are important to me, as my work is focused on academic and experimental objectives.

So far, I’ve been looking at options like LLaMA, GPT-NeoX, and BLOOM, particularly their smaller variants, but I’m open to exploring other possibilities. If you’ve had experience running these or similar models on mid-range GPUs, I’d love to hear your thoughts on performance, setup, or any potential limitations I should be aware of.

Additionally, I’d be grateful for any advice on:

* **Optimizing models for a 4060.** Are there specific tools, techniques, or libraries (like bitsandbytes or FlashAttention) that could help with running or fine-tuning these models?
* **Preparing for fine-tuning.** What should I keep in mind when selecting a model to ensure it can support future fine-tuning experiments effectively?

Thank you in advance for sharing your expertise! I’m eager to learn from the community and make the most of this setup.",Spinotesla,1h1ux8m,https://reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,https://www.reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,2024-11-28 12:30:28,0,0.27,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h1ux8m
MachineLearning,[D] Why aren't Stella embeddings more widely used despite topping the MTEB leaderboard?,"https://huggingface.co/spaces/mteb/leaderboard

I've been looking at embedding models and noticed something interesting: Stella embeddings are crushing it on the MTEB leaderboard, outperforming OpenAI's models while being way smaller (1.5B/400M params) and apache 2.0. Makes hosting them relatively cheap.

For reference, Stella-400M scores 70.11 on MTEB vs OpenAI's text-embedding-3-large 64.59. The 1.5B version scores even higher at 71.19

Yet I rarely see them mentioned in production use cases or discussions. Has anyone here used Stella embeddings in production? What's been your experience with performance, inference speed, and reliability compared to OpenAI's offerings?

Just trying to understand if there's something I'm missing about why they haven't seen wider adoption despite the impressive benchmarks.

Would love to hear your thoughts and experiences!",sdsd19,1h1u814,https://reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,2024-11-28 11:45:44,66,0.93,66,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1h1u814
MachineLearning,[D] Advanced methods of data management,"Hello everyone, right now I'm involved in a project which is basically (audio)LLM fine-tuning, and we're having problems related to data management. 

Since there isn't that many sets in that area, we're using different augmentation schemas. tl;dr, we have different datasets of various nature, and using some schemas we can convert usual ASR-type datasets to QA, generate refusal data, combine questions etc. 

Problem is, it's hard to control ratios of different data, and right now it's mostly a manual labour. We kinda have to manually adjust the amount of data we're generating. Which is rather annoying and hard process; we have many target datasets, you have to remember how much samples you've generated and try to get some adequate mix for train.

Right now, we use airflow for data filtering (lots of raw data is badly labelled), but I'm not really sure that I understand how to connect that tool to data generation, and if it's a good tool for that purpose. I was thinking about writing some snakemake script, but ideally the final solution should be flexible when we change configs, add new sets etc, and that's not what I associate with snakemake. Also, another question is visualization. 

So, I'm asking what kind of tools/libs I can use to tackle this task, is there anything that can fit our purposes, or it's time to write customs scripts? What do companies like meta use for their enormous 15t tokens set, surely they don't dump everything in one place? 

There's another question related to connecting datasets with data pipeline of training (it's complicated, there are additional steps of precalculation of feats and creating webdataset which is used in training), but for now I'd be really glad if anyone helped with just data management tools. ",Theio666,1h1u0pj,https://reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,https://www.reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,2024-11-28 11:31:47,1,1.0,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1u0pj
MachineLearning,"[P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, Docker)","  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to introduce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, you can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork, star) Thank you so much!",davidvroda,1h1pudq,https://reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,2024-11-28 06:33:40,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1h1pudq
MachineLearning,[P] py-gen-ml: generating ML configuration code from a schema,"[py-gen-ml](https://jostosh.github.io/py-gen-ml) is a Python library designed to simplify your ML experiment configuration using the power of Protocol Buffers. It's still in an early phase but I'd love to hear some feedback from the community.

**Here's how py-gen-ml can help you:**

* **Centralise configurations:** Define schemas in Protobuf to act as a single source of truth.
* **Minimise repetitive work:** Automatically generate code for models, patches, sweeps, and a command-line interface.
* **Boost flexibility:** Experiment with ease thanks to YAML configurations with advanced referencing and the ability to conduct hyperparameter sweeps.
* **Improve code quality:** Benefit from JSON schema validation, strong typing, and IDE support for a more robust development process.

**py-gen-ml aims to make ML development more efficient by reducing the burden of managing configurations.** Give it a try and see how it can improve your workflow.

**Get started:**

    pip install py-gen-ml

**Learn more:** [**https://jostosh.github.io/py-gen-ml**](https://jostosh.github.io/py-gen-ml)",jalapenjos,1h1t9v0,https://reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,https://www.reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,2024-11-28 10:40:23,0,0.25,0,0,1,0,0,False,False,True,False,False,Project,self,t3_1h1t9v0
MachineLearning,[D] Cross Entropy Loss sucks,"Hi guys, Am I the only one thinking that training a LLM to minimize CE Loss on a certain text dataset is a very surprising idea?

I understand that it works but I am surprised it is still SOTA. The current sentence could have begun with a lot of different tokens with no consequence on its meaning, while some words are uninterchangeable. Yet CE loss doesn't account for that. Worse off, the bigger the ""equivalence class"" (the number of tokens that could replace one in a sentence without altering its meaning) of a token in a sentence, the higher the average loss on it. It seems counterproductive, isn't it?

I would love to read some contradiction.",Due-Pangolin325,1h1sqkl,https://reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,https://www.reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,2024-11-28 10:02:18,0,0.29,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h1sqkl
MachineLearning,[D]Is Freelancing as a Data Scientist Even Possible?,"Hi everyone,

I’m fine working for as low as $15/hour, so earnings aren’t a big concern for me. I’ve gone through past Reddit posts, but they mostly discuss freelancing from the perspective of income. My main concern is whether freelancing in data science is practical for someone like me, given its unique challenges.

A bit about my background: I’ve completed 3-4 real-world data science projects, not on toy datasets, but actual data (involving data scraping, cleaning, visualization, modeling, deployment, and documentation). I’ve also worked as an intern in the NLP domain.

Some issues I’ve been thinking about:

1. Domain Knowledge and Context: How hard is it to deliver results without deep understanding of a client’s business?


2. Resource Limitations: Do freelancers struggle with accessing data, computing power, or other tools required for advanced projects?


3. Collaboration Needs: Data science often requires working with teams. Can freelancers integrate effectively with cross-functional groups?


4. Iterative and Long-Term Nature: Many projects require ongoing updates and monitoring. Is this feasible for freelancers?


5. Trust and Accountability: How do freelancers convince clients to trust them with sensitive or business-critical work?


6. Client Expectations: Do clients expect too much for too little, especially at low wages?



I’m also open to any tips, advice, or additional concerns beyond these points. Are these challenges solvable for a new data science freelancer? Have any of you faced and overcome similar issues? I’d love to hear your thoughts.

Thanks in advance!",ds_reddit1,1h1q98i,https://reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,https://www.reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,2024-11-28 07:01:15,7,0.57,7,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h1q98i
MachineLearning,[P] Ablation study using a subset of data?,"Basically, I'm engaging in a research project in which I'm training encoder only language models for text classification. I have already trained my models and gotten my results, however I need to perform an ablation study. The main issue I'm having is that the dataset is large. Is it fair for me to perform the ablation study on a subset of the dataset, since I'm gonna have to train it 3 - 4 times with different ablations?",Aromatic_Web749,1h1kzsh,https://reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,2024-11-28 01:51:54,10,0.92,10,0,8,0,0,False,False,True,False,False,Project,self,t3_1h1kzsh
MachineLearning,Causal Discovery Competition Winning Paper Discussion [D],"I’ve recently come across this post: https://thetourney.github.io/adia-report/ which describes the winning method for a casual discovery competition. It’s not really my field but I do have a reasonable understanding of GNNs and Causal Inference. Anyway, from the report I don’t understand precisely what the winning team was doing. Can anyone either link to a full paper or have a good intuitive and potentially step by step explanation of what they are doing?",www3cam,1h1i0ji,https://reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,2024-11-27 23:22:43,29,0.94,29,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1h1i0ji
MachineLearning,Residuals in ensemble MLR [D],"
Hi all

New to ensembles.

If you ensemble MLR, you may end up with a non-linear equation however….

A) the residuals of the indicidual MLR that were ensembled need to meet parametric assumptions? Can’t use a crap MLR just because it’s going to be used in an ensemble?
B) if the ensembled MLR equation is linear then residuals should meet parametric assumptions?

Thanks


",Yellow_fruit_2104,1h1ero2,https://reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,https://www.reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,2024-11-27 20:58:23,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h1ero2
MachineLearning,[D] how to do RLHF on this kind of data?,"Hi, apologies if this is a dumb question -- I'm really not knowledgeable about post training. Suppose that I have a llama and I want to finetune with human annotations that ""like"" or ""dislike"" a prompt response. Most DPO datasets feature a pair of possible responses, with one being chosen. Interpreting my data as one half of a pair with one missing, I could generate a second response from the same prompt and say that it is preferred if ""like""d and it is not preferred if it is ""disliked"". Is there a better way?",khidot,1h1bpwq,https://reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,2024-11-27 18:50:05,7,0.77,7,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h1bpwq
MachineLearning,"[D] Knowledge distillation neural network
","Hi community,

  
Suppose my original neural network model size is 50MB. Is there a way to estimate the size of the distilled model after applying Knowledge distillation.",PhilosopherNew313,1h17xwc,https://reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,https://www.reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,2024-11-27 16:13:12,0,0.46,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h17xwc
MachineLearning,[D] AISTATS 2025 Paper Reviews,"Since the AISTATS 2025 paper reviews are due today, I thought to open up a thread where everyone can discuss their experiences!
",PhoneImpressive9983,1h0y8rn,https://reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,2024-11-27 06:42:52,11,0.82,11,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h0y8rn
MachineLearning,[D] ACL ARR Discussion - About Author Response,"Hi all! currently submitted to ACL ARR Oct. Now the author response phase is over and we haven't received any reply (to our responses) from reviewers.

Want to ask if reviewers can still update their reviews *after* the end of the author response phase and *before* the meta-review is given, or does it mean that I won't receive any replies?",Ok_Function6276,1h13ffu,https://reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,https://www.reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,2024-11-27 12:44:02,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h13ffu
MachineLearning,[R] Genetic learning with loop mempory and Chromosomes for the memory neurode's gate.,"Greetings!  
  
Currently a bit busy will clean it up later also to lazy to implement git now... &gt;\_&gt;

[https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md](https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md)

",_Leto,1h15c7q,https://reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,https://www.reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,2024-11-27 14:18:09,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h15c7q
MachineLearning,[R] Help with submitting a WACV workshop paper,"Hi Everyone,

I have never submitted a paper to any conference before. I have to submit a paper to a WACV workshop due on 30 Nov.

As of now, I am almost done with the WACV-recommended template, but it asks for a Paper ID in the LaTeX file while generating the PDF. I’m not sure where to get that Paper ID from.

I am using Microsoft CMT for the submission. Do I need to submit the paper first without the Paper ID to get it assigned, and then update the PDF with the ID and resubmit? Or is there a way to obtain the ID beforehand?

Additionally, What is the **plagiarism threshold** for WACV? I want to ensure compliance but would appreciate clarity on what percentage similarity is acceptable.

Thank you for your help!",__proximity__,1h15p2e,https://reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,https://www.reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,2024-11-27 14:34:45,2,0.62,2,0,1,0,0,False,False,True,False,False,Research,self,t3_1h15p2e
MachineLearning,[D] AAMAS 2025 reviews are out! ,"I could not find a discussion thread, so I thought I would create one myself. ",E-Cockroach,1h15k8k,https://reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,2024-11-27 14:28:29,25,0.87,25,0,38,0,0,False,False,True,False,False,Discussion,self,t3_1h15k8k
MachineLearning,[R] Meissonic: High-Resolution Text-to-Image Generation via Enhanced Masked Image Modeling,"This work introduces a non-autoregressive masked image modeling (MIM) approach that aims to match SDXL-level image generation while avoiding the token inefficiencies of autoregressive methods. The key innovation is combining MIM with architectural improvements and sampling optimizations to enable high-resolution image synthesis.

Main technical points:
- Uses a transformer-based architecture with specialized self-attention and positional encoding
- Incorporates human preference scores as ""micro-conditions"" to guide generation
- Employs feature compression layers to handle high resolutions efficiently
- Generates 1024x1024 images through parallel token prediction rather than sequential
- Achieves comparable FID scores to SDXL while being more computationally efficient

Results:
- Image quality metrics competitive with SDXL on standard benchmarks
- Faster generation compared to autoregressive approaches
- Better handling of complex scenes and compositions
- Improved text alignment compared to previous MIM approaches

I think this could impact the field in several ways:
- Shows that non-diffusion approaches can achieve SOTA-level generation
- Provides a potential path toward unified language-vision models
- May lead to more efficient deployment of text-to-image systems
- Could influence architecture design for future multimodal models

The biggest open question in my view is whether this approach can scale further - while it works well at current resolutions, it's unclear if the same principles will hold at even higher dimensions.

TLDR: Non-autoregressive masked modeling approach matches SDXL-level image generation while being more efficient than typical autoregressive methods. Shows promise for unified language-vision architectures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/meissonic-revitalizing-masked-generative-transformers-efficient-high). Paper [here](https://arxiv.org/abs/2410.08261).",Successful-Western27,1h1529m,https://reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,https://www.reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,2024-11-27 14:05:29,7,0.89,7,0,2,0,0,False,False,True,False,False,Research,self,t3_1h1529m
MachineLearning,[R] Black holes and the loss landscape in machine learning,"Abstract:

&gt;Understanding the loss landscape is an important problem in machine learning. One key feature of the loss function, common to many neural network architectures, is the presence of exponentially many low lying local minima. Physical systems with similar energy landscapes may provide useful insights. In this work, we point out that black holes naturally give rise to such landscapes, owing to the existence of black hole entropy. For definiteness, we consider 1/8 BPS black holes in =8 string theory. These provide an infinite family of potential landscapes arising in the microscopic descriptions of corresponding black holes. The counting of minima amounts to black hole microstate counting. Moreover, the exact numbers of the minima for these landscapes are a priori known from dualities in string theory. Some of the minima are connected by paths of low loss values, resembling mode connectivity. We estimate the number of runs needed to find all the solutions. Initial explorations suggest that Stochastic Gradient Descent can find a significant fraction of the minima.

Arxiv: [https://arxiv.org/abs/2306.14817](https://arxiv.org/abs/2306.14817)",Mindless-House-8783,1h0uwjd,https://reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,https://www.reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,2024-11-27 03:26:49,28,0.78,28,0,28,0,0,False,False,True,False,False,Research,self,t3_1h0uwjd
MachineLearning,[D] AISTATS 2025 reviews,Aistats 2025 reviews are supposed to be out today. So I thought to create a discussion post for the same where we can share our experiences!,PhoneImpressive9983,1h0x428,https://reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,2024-11-27 05:31:15,47,0.94,47,0,126,0,0,False,False,True,False,False,Discussion,self,t3_1h0x428
MachineLearning,"[P] I built Darkspark, a visual representation of your neural network. Explore everything from macro-level architecture to low-level ops and activations — Your model wants to be seen!","When reading a paper on arxiv or perusing code I also like to sketch out the model architecture myself on a big piece of paper to use as a reference. This is the software version of that. It's a GUI for your neural network. Here's the link: [https://darkspark.dev](https://darkspark.dev)

I tried all the other options I could find (netron, google’s model-explorer, tensorboard, torchview, torchlens, apple’s mycelium). These are all great projects (I really wanted to use one of them!) but none had all of the features I needed:

**Opinionated layout.** The tool’s layout should automatically expose the underlying logic of the model. The layout engine should do a lot of the heavy lifting of understanding a model’s structure and intentions. E.g. a U-net should look like a “U”. Here's [stable-diffusion-v1.5](https://darkspark.dev/models/?model=stable-diffusion-v1-5) traced directly from a huggingface [pipeline](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)

[stable-diffusion-v1.5 in the darkspark viewer](https://preview.redd.it/xksm2u1ipa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=fb7e576192c066ddc7e72ad8491c6347181525a7)

**Interactive**. I need collapsible and expandable modules so I can explore a model at a high level but can also go down to the lowest level ops. Complex models won’t even load without this. Here's the same diffusion model zoomed in on a transformer block

[stable-diffusion-v1.5 zoomed in](https://preview.redd.it/7fmmw341qa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=8538420973f578b68a4e225082b6990c867aefaf)

**‘Just Works’ with any arbitrary code**. I don’t want to export to ONNX, I don’t want to upload something, I don’t want to manually specify what is the model and what are the inputs. I just want to wrap my existing code in something simple.\*

    import darkspark
    import timm
    import torch
    
    model = timm.create_model(""efficientnet_b0"")
    inputs = torch.randn(1,3,224,224)
    
    with darkspark.Tracer():  # &lt;-- wrap your code with this line
      out = model(inputs)
    
    # interactive diagram now available at localhost



**Microscope**. Sometimes I also want to explore the activations and attention patterns. Like OpenAI’s microscope tool, but for your own models. Here's a “female / male” detector in a later layer of the pretrained [vit\_base\_patch16\_siglip\_224](https://darkspark.dev/models/?model=vit_base_patch16_siglip_224-microscope) from the timm library.

[female \/ male detector in darkspark viewer](https://preview.redd.it/8fzwcvcjqa3e1.png?width=1520&amp;format=png&amp;auto=webp&amp;s=fabed874645c834b8380e08a0ef7ca3f1bcb7962)

Here's the attention patterns explorer for the same model.

[Attention explorer for vit\_base\_patch16\_siglip-microscope](https://preview.redd.it/fvmxs1buqa3e1.png?width=1236&amp;format=png&amp;auto=webp&amp;s=7d7a91c469616653841111bd84879b8bcbeb1b22)



**Hosted gallery**. Most of what I want is usually a variant of an existing model. It’s often more convenient to just reference a url rather than trace your own code. I currently have all the models from timm and many from the transformers and diffusers libraries.

[lots of models available to peruse](https://preview.redd.it/qx837cz2ra3e1.png?width=1158&amp;format=png&amp;auto=webp&amp;s=f23808a45a4b7927828de24c4e1d0790a37420bb)

The public pip package isn’t yet ready, I was hoping to get feedback on the tool itself before cleaning up and sharing the codebase. Please let me know what you think, I'm eager for feedback on everything from low-level UI/UX to high-level functionality. Thanks to the awesome community for checking it out!

Here's the link again: [https://darkspark.dev](https://darkspark.dev)

\* darkspark uses \_\_torch\_function\_\_, similar to the torchview library. This allows us to capture all the ops and tensors inside the context of darkspark.Tracer without breaking when it hits dynamic control flow ops that can’t be captured in e.g. ONNX or torch exported\_program. We also get access to all the tensors, activation patterns, etc, without using hooks. Happy to answer more Qs about the architecture if ppl are interested.",Historical-Good1915,1h0krsv,https://reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,https://www.reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,2024-11-26 19:42:44,5,0.86,5,0,5,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/8bDoJ2A5fEzlL66g8tteX57z1GhfRWnpnzQOclCaFy4.jpg,t3_1h0krsv
MachineLearning,[D] How valid is the evaluation using LLMs?,"Hello community,

I am bit new to using Gen AI, I want to check the validity of using larger LLMs to evaluate the result of other LLMs. I have seen different blogs who does this for the purpose of automating the evaluations.

For eg. To evaluate a list of English translations my a model A, is it valid to prompt another model B, something like this '''Is this translation correct original text: {original_text}, Translated text {translated_text}'''

Is this a valid way of evaluating? Something inside me says it's scientifically wrong, because the LLM model B itself will have some error to it right?",raman_boom,1h11lbt,https://reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,2024-11-27 10:48:09,14,0.75,14,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1h11lbt
MachineLearning,[P] [D] Predict Integer Values with XGBoost Regression,"Hello! I am new to Data Science but enjoying every moment of it.

I am currently working with the XGBoost model and while everything is working fine (more or less), I am struggling with a specific issue. I am predicting 'number of orders' based on certain criteria. Since number of orders follows Poisson distribution, I have specified that and I am getting decent predictions. However, the predictions are floating point numbers. Is there any way to tell the model to give integers instead?

PS: I have tried the rounding method and while it works great, I wanted something that is at the model level.",MapleWalnut96,1h10ta0,https://reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,https://www.reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,2024-11-27 09:51:34,0,0.4,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h10ta0
MachineLearning,[P] What Transcription Model does Google Meets use? ,"Hi, I am currently evaluating options for transcribing sensitive meeting texts. I'd like to know what kind of transcription model is currently being used by google to transcribe meetings. I've searched the documentation and the web, and it doesn't seem to specify. I initially thought chirp would be used for this, but the documentation specifies English as the only reliable language to transcribe, which isn't true of chirp. 

This isn't a post asking which model (google or otherwise) to use, or all the better options out there, this is a very specific inquiry into Google's approach. I'd love to get some insight here. Thanks!",Arcane_Aura,1h0u63q,https://reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,https://www.reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,2024-11-27 02:47:35,2,0.63,2,0,3,0,0,False,False,True,False,False,Project,self,t3_1h0u63q
MachineLearning,[P] [D] Comparing Llama Models and GPT 4o Models on Multilingual Machine Translation with Backtranslation,"Hey all,

In the spirit of practical real world tasks for LLMs, we wanted to see how well different models could automatically translate text from English to Spanish and the backtranslate to English on a Nike product catalog. We started with Llama 405B, Llama 70B, Llama 8B, GPT 4o-mini, and GPT 4o, but would love to test [more models](https://www.oxen.ai/explore/models).

  
\~ TLDR \~ Here are the results with all the data and code here:

[https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments](https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments)

https://preview.redd.it/qken2vjfhc3e1.png?width=1150&amp;format=png&amp;auto=webp&amp;s=739ef336dd7b89856a39d872ef12e03f806ce799

Although backtranslation may not be the most effective way to benchmark, we thought this would be an interesting experiment to see how well it correlates with model performance. It would be ideal to get native Spanish speakers to annotate the dataset with ground truth labels, so if anyone wants to contribute feel free to fork the repo and we can get some real labels.

  
We're trying to make some more real world datasets / benchmarks, so let us know if you want to help out.

If you’re new to the [Oxen.ai](https://www.oxen.ai/) project, we’re building a fast [open source dataset collaboration tools](https://github.com/Oxen-AI/oxen-release) as well as a ton of [helpful data exploration tools](https://docs.oxen.ai/features/web_hub) on top of it! If you are into data or ML/AI, we’d love your thoughts on the tool and project!",FallMindless3563,1h0sehj,https://reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,https://www.reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,2024-11-27 01:17:44,14,0.73,14,0,12,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/mNfbgTf9Tu8SygkvvAsqVh2unMgd_M1VIenrJ09Hz3s.jpg,t3_1h0sehj
MachineLearning,[D] Model validation for transformer models,"I'm working at a firm wherein I have to validate (model risk validation) a transformer architecture/model designed for tabular data.

Mapping numbers to learned embeddings is just so novel. The intention was to treat them as embeddings so that they come together on the same ""plane"" as that of unstructured text and then driving decisions from that fusion. 

A decision tree or an XGBoost can be far simpler. You can plug in text based embeddings to these models instead, for more interpretability. But it is what is.

How do I approach validating this transformer architecture? Specifically if or if not it's conceptually sound and the right choice for this problem/data.",Common-Interaction50,1h0sc7o,https://reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,https://www.reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,2024-11-27 01:14:44,0,0.36,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0sc7o
MachineLearning,[D] GPT2 (Hugging Face) transformer giving bogus responses. Why and how do I fix this?,"I have done some research on this and it looks like not a lot of people talk about it. I am newer to transformers (though I have some experience in Sequential models) and trying to train ChatGPT on a specific niche I want to make a bot for, but it isn't understanding anything and I feel as if I've been going in loops with it.

For example:

Me: ""Tips for planetary imaging?""

GPT2: ""Have clue how get good resolution out there will someone explain use astrobin where can look info about other equipmentpeople who this correctlyslightly better?pleiades, andromedahow much data does your setup need order score decent results ? this would more than what most hobbyists spend .5mil each day!ive taken some pics before but when trying new things like stacking that makes wonder they really work well owhats left over after shooting all those different targets guessedit yes maybe even with 1000 frames per minute thats still.""

I have custom trained it on fetched data using the Reddit Developer API from a subreddit (About 42000 lines worth) and have structured the dataset like this:

""Title: M31 Andromeda - with rotated frames

Body:

Comments:

\- A bit of a gimmick image to show stacking artefacts when the sub-frames are rotated. 7 images total, rotated 15 degrees each. 14 mins (!) total exposure.

Equipment:

\* Stellarview SVX080T Telescope 480/80mm @ f/6

\* Losmandy G11G mount

\* ZWO ASI071MC Pro color camera @ -5c + Optolong L Pro Filter

\* 60mm Guidescope with ASI120MM camera

Subs:

\* 7 x 120s

\* Master Dark

\* No Flats

Software:

\* PHD2 &amp; Sequence Generator Pro

\* Astro Pixel Processor, DeepSkyStacker, Photoshop

Processing

\* Default color integration in APP

\* Light pollution removed, stretched and exported to Photoshop

\* Same integration performed in Deep Sky Stacker (APP did such a good job it didn't show \*any\* stacking artifacts but DSS did)

\* Blended the APP image with the DSS image to show stacking artifacts in PS

\* Camera Filter shenanigans, export to jpg

\- Honestly that’s a pretty cool presentation!! You can really make this significantly better I think. Maybe like 40x60” frames per rotation or something like that to get better detail and less noise. The 120” subs blew out a lot.

Try again!!

\- \[deleted\]

\- Noob question here but about how much does a setup cost to get images like this?

\- LOVE THIS

\- It’s beautiful

\- This is sick

\- This is how every astrophotos should be ! It’s so beautiful !! I can definitely see this hanging on the wall in my bedroom 😍

\- Imagine some human like civilization on Andromeda taking pictures of the milky way

\- \[deleted\]

&lt;|endoftext|&gt;""

Trained using this dataset and GPT2-Medium.

Here are my parameters:

    outputs = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=max_length,
                        temperature=0.8,
                        top_p=0.9,
                        do_sample=True,
                        repetition_penalty=1.3,
                        no_repeat_ngram_size=3,
                        eos_token_id=self.tokenizer.eos_token_id,
                        pad_token_id=self.tokenizer.eos_token_id
    )
    
    
    system_prompt = (""You are Astrophoto AI, an encouraging astrophotography expert and teacher.""
                ""Your role is to help beginners and experienced photographers capture stunning images of the night sky and answer any questions they might have.""
                ""You offer concise, factual, and practical advice drawn from established astrophotography techniques.""
                ""Your tone is friendly, encouraging, and focused on making astrophotography accessible to everyone.""
                ""If you don't know the answer to a question, admit it instead of guessing."")

What are some potential issues with this?

Thanks!

EDIT: thanks for your advice everyone! I will be switching models.",Aman_Dude,1h0okd5,https://reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,https://www.reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,2024-11-26 22:19:18,0,0.32,0,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1h0okd5
MachineLearning,[D] Prune (channel + layers) + distillation or just distillation,"Let's say I want to make my model smaller.

There is a paper, which says distillation is good, but it takes a long time [https://arxiv.org/abs/2106.05237](https://arxiv.org/abs/2106.05237)

And there is also a paper which says that pruning + distillation works really well: [https://arxiv.org/abs/2407.14679](https://arxiv.org/abs/2407.14679)

Now, my question is: Is there any work that compares pruning + distillation vs just distillation from scratch?",osamc,1h0kcgn,https://reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,https://www.reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,2024-11-26 19:25:36,5,0.86,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0kcgn
MachineLearning,"[P] does anyone know how to reduce the dimensions of embeddings using autoencoders, if you have a blog about please send it","https://preview.redd.it/3cub8uc9ja3e1.png?width=766&amp;format=png&amp;auto=webp&amp;s=0a824d6ae516ff699cb880d8e998ace85354a50f

",GellertGrindelwald_1,1h0j77v,https://reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,https://www.reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,2024-11-26 18:39:43,0,0.27,0,0,3,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uUwCHYEmzWw3FiWk1T2hjt5K8zWvsMqQ8Y4dqssDqUM.jpg,t3_1h0j77v
MachineLearning,[D] A blog post explaining sparse transformers (the original paper),"Hi!

I'm sorry if it's not appropriate to publish such posts on this subreddit. I do stay out of this type of posts on this subreddit but I keep seeing articles or videos or whatever content explaining GPT-3 without delving into sparse transformers. And it keeps frustrating me because clearly in the paper they say ""we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer"".

But no one seems to care about explaining them. I understand why to be honest but it's frustrating to see all these articles, projects, videos etc. that try to explaining everything about the GPT not even mentioning the sparse transformers part. And besides many other elements specific to GPT-3 or general to reproducibility in ML, the sparse transformer part is a big dent into even prototyping GPT-3.

I have this habit of writing down stuff when trying to understand something so I wrote a blog post on sparse transformers. Never spoke about it because I did it to restructure my thoughts and as notes for me. So it's not something I'd avise anyone to read, I'm sure it's full of typos, my writing style is not neat etc. It's just something I did for me in a way *I* would understand and recover lost bits of information when skimming through it.

Anyways, in case you're reading papers by yourself and trying to constitute the knowledge just from them, maybe my notes can help you: [https://reinforcedknowledge.com/sparse-transformers/](https://reinforcedknowledge.com/sparse-transformers/)

Sorry again if this post is not appropriate and for yapping that much.

(If you happen to read it or if you notice any errors, do not hesitate to point them out, I'd be grateful to learn from them)",ReinforcedKnowledge,1h0gl2j,https://reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,https://www.reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,2024-11-26 16:55:45,24,0.93,24,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h0gl2j
MachineLearning,[D] what are some problems in audio and speech processing that companies are interested in?,I just recently graduated with a bachelor's in computer science and am really interested in auio and machine learning and want to do a project with a business scope. what are some problem statements that companies would be interested in? especially gen ai related ,Personal_Equal7989,1h082e6,https://reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,https://www.reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,2024-11-26 09:36:01,9,0.8,9,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1h082e6
MachineLearning,[D] Am I a complete idiot for signing up for a Hackathon?,"Ok, so I am a Coms Science graduate student and my chosen area of study is Ethical AI.

I wanted to attend this AI conference very badly because there are some speakers that I admire. But I couldn’t afford the passes, so I decided to apply to be in the student Hackathon because if accepted, you got a free pass.

It was such a Hail Mary for me to even do the application, but I thought it would also be a cool opportunity to learn alongside others.

I got accepted… and I’m extremely excited. But now I’m like, oh wait, am I going to royally piss off whomever my teammates are because I can’t code?

Any advice? There’s a preparatory webinar happening in a week, and I’ve been doing some overview classes so that I can learn the terminology/basics. The application also asked for me to state my level of coding experience and I checked: none. And still got accepted… so I’m hoping that the organizers consider me to still have something valuable to contribute?

Please let me know what you think 🥲",sydj_k941,1h01hfn,https://reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,https://www.reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,2024-11-26 02:44:20,47,0.72,47,0,71,0,0,False,False,True,False,False,Discussion,self,t3_1h01hfn
MachineLearning,[D] ADOPT optimizer,"Have any of you tried the new ADOPT optimizer? How did it go? I'm kind of curious, but haven't had the opportunity to give it a try.",neu_jose,1h01cqq,https://reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,https://www.reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,2024-11-26 02:37:39,7,0.82,7,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h01cqq
MachineLearning,[D]Thoughts on Synthetic Data Platforms like Gretel.ai or Mostly AI?,"
Has anyone here used platforms like Gretel.ai or Mostly AI?
	•	What did you like or dislike?
	•	How was the synthetic data quality for your use case?

I’m exploring options and would appreciate your insights. Thanks!",Value-Forsaken,1gzsqwu,https://reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,https://www.reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,2024-11-25 20:20:23,6,0.71,6,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1gzsqwu
MachineLearning,"[Discussion] ""Help! Machine Learning Model Struggling with High-Dimensional Reflectance Data","I'm assisting with a research project at my university, and I've run into a bit of a roadblock. I've incorporated reflectance data variables into my machine learning model to predict a Y. There are 1500 different wavelengths, significantly increasing the dimensionality of my data. After combining the datasets, the model's performance declined. I tried reducing dimensionality, but the model continued to worsen with this data.

The research lead (a Ph.D. student) suggested I use a genetic algorithm because they've seen it used with reflectance data before. I found the implementation to be pretty complex, and I don't think this clustering will improve the model (I'm studying the implementation and believe I'll be able to test it soon).

What do you guys suggest? I think there are two approaches: either I start removing wavelengths that are worsening the model through exhaustive search, or I do this reduction using a genetic algorithm.

Has anyone encountered a similar problem? I haven't gotten satisfactory answers from AI because I understand they're not that advanced in this area yet.",DelayResponsible364,1gzq9h2,https://reddit.com/r/MachineLearning/comments/1gzq9h2/discussion_help_machine_learning_model_struggling/,https://www.reddit.com/r/MachineLearning/comments/1gzq9h2/discussion_help_machine_learning_model_struggling/,2024-11-25 18:40:50,0,0.44,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gzq9h2
MachineLearning,[D] Do modern neural network architectures (with normalization) make initialization less important?,"With the widespread adoption of normalization techniques (e.g., batch norm, layer norm, weight norm) in modern neural network architectures, I'm wondering: how important is initialization nowadays? Are modern architectures robust enough to overcome poor initialization, or are there still cases where careful initialization is crucial? Share your experiences and insights!",NumberGenerator,1gzq63h,https://reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,https://www.reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,2024-11-25 18:37:08,92,0.97,92,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gzq63h
MachineLearning,[D] AAAI 2025 - Reviews missing after rebuttal,"Hi all,

We submitted our paper to AAAI 25. It passed Phase 1, it got fairly good scores, we wrote the rebuttals, and now the scores, the reviews and the rebuttals are missing. Is this normal?",jpereira73,1gzn4uj,https://reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,https://www.reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,2024-11-25 16:39:01,1,0.57,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gzn4uj
MachineLearning,[Project] Claude Francois - Let an AI review your code in the style of François Chollet,"Demo here: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

At the recent Anthropic Builder Day hackathon, we ([Crossing Minds](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning)) built 'Claude François', an AI code reviewer trained in the style of [François Chollet](https://github.com/fchollet), the creator of Keras. It adapts Anthropic's Claude 3.5 Sonnet for code reviewing, but instead of regular fine-tuning, we used few-shot in-context learning with our custom RAG retrieval model, trained on PRs from the [Keras project](https://github.com/keras-team/keras). Compared to a typical AI code reviewer, it provides more succinct, high-quality code reviews focused on real issues rather than superficial nitpicking.

How it works:

* Dataset: Trained on a database of public Keras GitHub PRs and François's reviews.
* Fine-Tuned RAG Embeddings: Uses active learning and RLAIF to train embeddings optimized for generating ""fchollet-level"" reviews.
* Improved Retrieval: Retrieves relevant examples not just by embedding similarity but by optimizing for mutual information.
* Self-Reflection: Employs self-reflection techniques to enhance Sonnet’s reasoning capabilities.

This technology demo showcases how [Crossing Minds' RAGSys](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning) ICL enables domain adaptation without fine-tuning. It can be used for countless other use cases beyond code reviews, like classification, summarization, translation, search, recommendations, and more. Arxiv paper coming soon!

Try it now: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

We'd love to hear your feedback!",Crossing_Minds,1gzmk5n,https://reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,https://www.reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,2024-11-25 16:16:03,23,0.69,23,0,13,0,0,False,False,True,False,False,Project,self,t3_1gzmk5n
MachineLearning,[D] Search and Filter ICLR Submissions by Score,Is there a website or something where I could filter the ICLR submissions by their score? e.g. to only find papers with a score above some threshold.,nextlevelhollerith,1gzk8et,https://reddit.com/r/MachineLearning/comments/1gzk8et/d_search_and_filter_iclr_submissions_by_score/,https://www.reddit.com/r/MachineLearning/comments/1gzk8et/d_search_and_filter_iclr_submissions_by_score/,2024-11-25 14:36:33,1,0.6,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gzk8et
MachineLearning,[R] Aurora: A General-Purpose Foundation Model for Earth System Prediction,"The key contribution here is the development of Aurora, a foundation model trained on over 1M hours of atmospheric data that can perform multiple types of weather and climate predictions using a single model architecture. This represents a shift from building separate specialized models to having one model that learns general atmospheric physics.

Key technical points:
- Model architecture uses transformer blocks with attention mechanisms adapted for spatiotemporal data
- Trained on merged datasets from multiple sources including ERA5 reanalysis, satellite observations, and climate model outputs
- Can generate predictions for diverse tasks like air pollution, precipitation, and temperature forecasting
- Produces forecasts in under 1 minute compared to hours/days for traditional numerical models
- Outperforms both specialized ML models and physics-based numerical weather prediction on several benchmarks

Results:
- 15-20% improvement in 5-day global air pollution predictions vs current methods
- Better performance on 10-day weather forecasts compared to specialized models
- Maintains accuracy even for extreme weather events
- Shows continual improvement as training data increases
- Successfully handles multiple spatial and temporal resolutions

I think this work could significantly change how we approach environmental modeling. Instead of maintaining separate models for different prediction tasks, having a single foundation model that can handle multiple atmospheric predictions could make forecasting more efficient and accessible. The speed improvements (minutes vs hours) could enable new applications requiring rapid predictions.

I think the challenges ahead include:
- Validating performance across more diverse atmospheric phenomena
- Understanding model interpretability for critical forecasting
- Addressing computational costs of training and inference
- Ensuring reliability for operational forecasting systems

TLDR: Researchers developed Aurora, an atmospheric foundation model trained on massive weather/climate data that can handle multiple prediction tasks better than specialized models while being much faster. Shows foundation models could transform environmental forecasting.

[Full summary is here](https://aimodels.fyi/papers/arxiv/foundation-model-earth-system). Paper [here](https://arxiv.org/abs/2405.13063).",Successful-Western27,1gzj8rs,https://reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,https://www.reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,2024-11-25 13:50:09,35,0.89,35,0,2,0,0,False,False,True,False,False,Research,self,t3_1gzj8rs
MachineLearning,[2411.15100] XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models,,crowwork,1gzi649,https://reddit.com/r/MachineLearning/comments/1gzi649/241115100_xgrammar_flexible_and_efficient/,https://arxiv.org/abs/2411.15100,2024-11-25 12:55:20,9,0.85,9,0,0,0,0,False,False,False,False,False,,default,t3_1gzi649
MachineLearning,[D] Why does my feature visualisation form this shape?,"In performing 3d t-SNE decomposition of model features, I have come across a strange quirk. I am fine tuning an ImageNet trained ViT for CIFAR-100 classification. Before the first epoch (i.e. just imagenet weights with an untrained FC feature head), the visualisation of class boundaries looks like this, forming this convex shape with regions of no classes. After one epoch this shape is no longer present in the t-SNE visualisation.

Any ideas why? Is this related to the Manifold hypothesis? Or just due to overlap between ImageNet and CIFAR100 classes?

https://preview.redd.it/eb3w3rfaw03e1.png?width=2178&amp;format=png&amp;auto=webp&amp;s=57f1c34830cdff1968aea9367bba2b4cb3d5b7c1

",BDE-6,1gzfo7c,https://reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,https://www.reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,2024-11-25 10:14:59,9,0.8,9,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/SWOh-MCfT6Bntn3xkFzI9yHTfmJ6_JEMsdif7HmXtYQ.jpg,t3_1gzfo7c
MachineLearning,[R] Evaluating Creative Writing Output and The Effects of Fine Tuning,"
I was asked by a publisher if GPT-4o could be fine tuned to match their authors style to help build a copilot type experience. 

This gave me a chance to figure out a way to breakdown creative writing into five pillars (Dialogue, Exposition, Inner Thoughts, Description and Action) and measure how these change with prompting and fine tuning. 

I put together this blog post based on the results of training on popular authors like J.K. Rowling, Tade Thompson and Andrei Agassi. Surprisingly based GPT-4o does a decent job adopting their style with prompting but I put together some interactive visualizations to see how the model shifts during story generation (400 paragraphs) as we fine tune on 300, 600, and 800 samples. 

https://peytoncasper.com/blog/tone-evaluation/index.html

https://github.com/peytoncasper/grammar-of-thought",peytoncasper,1gzdwg5,https://reddit.com/r/MachineLearning/comments/1gzdwg5/r_evaluating_creative_writing_output_and_the/,https://www.reddit.com/gallery/1gzdwg5,2024-11-25 08:01:41,10,0.67,10,0,6,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/7cdun2J_3-VtlKB1wRgxUbknVHm_3AkGRu0D0UZDYwo.jpg,t3_1gzdwg5
MachineLearning,[D] Flow matching is actually very different from (continuous) normalising flow?,"I was looking at the [flow matching](https://arxiv.org/pdf/2210.02747) paper and saw that flow matching is often considered as just an alternative implementation of continuous normalising flow. But after comparing the methodologies more closely, it seems there is a very significant distinction. In the flow matching paper, it is mentioned that for a data sample x1 (I assume this refers to individual data points like a single image), we can put an ""dummy"" distribution such as a very tight Gaussian on it, then construct a conditional probability path p_t(x|x1). Therefore what we learn is a transformation between the small Gaussian (t=1) on the data point to a standard Gaussian (t=0), for every data point. This implies that the latent space, when trained over the entire dataset, is the overlapped mixture of all the standard Gaussians that each individual data point maps to. The image of the small Gaussian ball for each individual image is the entire standard Gaussian.

However this does not seem to be what we do with regular normalising flows. In normalising flows, we try to learn a mapping that transforms the ENTIRE distribution of the data to the standard Gaussian, such that each data point has a fixed location in the latent space, and jointly the image of the dataset is normally distributed in the latent space. In practice we may take minibatches and optimise a score (e.g. KL or MMD) that compares the image of the minibatch with a standard Gaussian. Each location in the latent space can be uniquely inverted to a fixed reconstructed data point.

I am not sure if I am missing anything, but this seems to be a significant distinction between the two methods. In NF the inputs are encoded in the latent space, whereas flow matching as described in the paper seems to MIX inputs in the latent space. If my observations are true, there should be a few implications:

1. You can semantically interpolate in NF latent space, but it is completely meaningless in the FM case
2. Batch size is important for NF training but not FM training
3. NF cannot be ""steered"" the same way as diffusion models or FM, because the target image is already determined the moment you sample the initial noise

I wonder if anyone here has also looked into these questions and can inform me whether this is indeed the case, or whether something I missed made them more similar de facto. I appreciate any input to the discussion!",aeroumbria,1gzdera,https://reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,https://www.reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,2024-11-25 07:25:58,52,0.95,52,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1gzdera
MachineLearning,"[P] Based on Arduino Nano Matter and Raspberry Pi 5, I developed this project to explore the digital twin synthetic data generation and AI-oriented advancements on real-world shipping operations w/ NVIDIA Omniverse. I trained my object detection model on my synthetic data set via Edge Impulse.",,the-amplituhedron,1gzcu29,https://reddit.com/r/MachineLearning/comments/1gzcu29/p_based_on_arduino_nano_matter_and_raspberry_pi_5/,https://www.hackster.io/kutluhan-aktar/digital-twin-enabled-smart-shipping-workstation-w-omniverse-049792,2024-11-25 06:46:05,7,0.64,7,0,7,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/HUsW9alxNon6_tVvzd3no051oaRZMRvrtvUdsHJu_s0.jpg,t3_1gzcu29
MachineLearning,[D] Looking for paper suggestions. What's your go to method for training a model on a mixture of multiple datasets with slightly different distributions?,"Imagine you have image data from different kinds of devices with different color profiles, resolutions, lens distortions etc. Or the object being captured in each dataset is similar but slightly different. I need suggestions on papers that effectively mix such datasets to get a bigger dataset for training a foundation model.

My datasets all come from slightly different distributions but they represent largely the same concepts so it makes sense to model them together for training a foundation model. But simply concatenating all datasets together without passing any metadata information to the model is degrading performance over training individually on each dataset.

For reference I am training MAE type models on unlabelled data and at test time training simple linear/logistic regression models on frozen MAE embeddings for different downstream tasks. The goal is to have the MAE embeddings outperform supervised models trained on each dataset individually.

An MAE trained on N datasets is underperforming an MAE trained on just one dataset. But an MAE trained on N-1 datasets and finetuned (unsupervisedly) on the Nth dataset before taking embeddings is outperforming a model trained on just the Nth dataset. But this is not a solution since I cant have N foundation models.

I tried adding a trainable source token (ie I have N trainable tokens and I concat the token corresponding to the data source to the masked input sequence before passing through the encoder) but it isn't affecting model performance at all. Please let me know if you know of any better methods.",Atom_101,1gzcmg9,https://reddit.com/r/MachineLearning/comments/1gzcmg9/d_looking_for_paper_suggestions_whats_your_go_to/,https://www.reddit.com/r/MachineLearning/comments/1gzcmg9/d_looking_for_paper_suggestions_whats_your_go_to/,2024-11-25 06:31:33,8,0.8,8,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gzcmg9
MachineLearning,[D] As a CS masters student/researcher should one be very deliberate in picking a lab’s domain?,"I (very fortunately) got an opportunity in a great lab in an R1 school, Prof has a &gt;40 h-index, great record, but mainly published in lower tier conferences, though do some AAAI. It applies AI in a field that aligns with my experience, and we are expected to publish, which is perfect. However I’m more keen to explore more foundational AI research (where I have minimal experience in apart from courses I took).

In CS, ML it seems most people are only prioritising NIPS/ICLR/ICML especially since I’m interested in potentially pursuing a PhD. I’m in a bit of a dilemma, if I should seize the opportunity or keep looking for a more aligned lab (though other profs may not be looking for more students).

My gut tells me I should ignore conference rankings and do this, since they have some, chain of though, knowledge representation, cognitive system components. They expect multi semester commitment and of course once I commit I will see it through. My dilemma is that I’m moving more and more towards more practical applications in AI, which is pretty domain specific and am worried I won’t be able to pivot in the future. 

I’m aware how this can sound very silly, but if you can look past that, could I please get some advice and thoughts about what you’d do in the shoes of a budding academic, thank you!",giuuilfobfyvihksmk,1gz6mj1,https://reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,https://www.reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,2024-11-25 00:56:49,42,0.78,42,0,31,0,0,False,False,True,False,False,Discussion,self,t3_1gz6mj1
MachineLearning,RTX 4090 vs 4080 super [D],"Looking at potentially building an ML and molecular dynamics workstation for research. I’m looking in the $4000 ish range for GPU’s. I’ve been leaning heavily towards 2 4090’s (I know 5090’s will come out in January, whole different conversation!) but theoretically I could run 4x 4080 supers for about the same price, and the numbers technically come out on top, but that’s IF you can use them all efficiently. I know pytorch can distribute across GPU’s reasonably well, but not everything can.  I also know more vRAM is always better as the 40 series don’t have NVlink so can’t pool memory. I’ve also briefly looked at the RTX cards (ampere and ada) but my understanding is they’re really only worth it for the pro drivers, and that’s pretty much it. Any thoughts would be much appreciated! ",Mdgoff7,1gyzxb1,https://reddit.com/r/MachineLearning/comments/1gyzxb1/rtx_4090_vs_4080_super_d/,https://www.reddit.com/r/MachineLearning/comments/1gyzxb1/rtx_4090_vs_4080_super_d/,2024-11-24 19:58:06,3,0.6,3,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gyzxb1
MachineLearning,[D] When will the NAACL workshops get announced?,"The [NAACL website mentions that 25th November begins the second call of workshop papers](https://2025.naacl.org/calls/workshops/#workshop-timelines), but the website doesn't seem to mention *which* workshops are going to be held. Idk if I'm stupid for now knowing, please help me out. ",Aromatic_Web749,1gyye7a,https://reddit.com/r/MachineLearning/comments/1gyye7a/d_when_will_the_naacl_workshops_get_announced/,https://www.reddit.com/r/MachineLearning/comments/1gyye7a/d_when_will_the_naacl_workshops_get_announced/,2024-11-24 18:54:16,3,0.67,3,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gyye7a
MachineLearning,[D] where find a good benchmark for all consumer gpu,"hi guys, I was wondering what gpu is the best in general for machine learning, include with openVino(only for intel), with new introduction of rochm and obviusly the queen nvidia, exist some benchmark full focuss on ML with various type of library",FewVEVOkuruta,1gyvari,https://reddit.com/r/MachineLearning/comments/1gyvari/d_where_find_a_good_benchmark_for_all_consumer_gpu/,https://www.reddit.com/r/MachineLearning/comments/1gyvari/d_where_find_a_good_benchmark_for_all_consumer_gpu/,2024-11-24 16:45:29,0,0.4,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gyvari
MachineLearning,[R] Testing the Brittleness of LLM Analogical Reasoning Through Problem Variants,"The researchers developed a systematic framework for testing analogical reasoning in LLMs using letter-string analogies of increasing complexity. They created multiple test sets that probe different aspects of analogical thinking, from basic transformations to complex pattern recognition.

Key technical points:
- Evaluated performance across 4 major LLMs including GPT-4 and Claude
- Created test sets with controlled difficulty progression
- Implemented novel metrics for measuring analogy comprehension
- Tested both zero-shot and few-shot performance
- Introduced adversarial examples to test robustness

Main results:
- Models achieve &gt;90% accuracy on basic letter sequence transformations
- Performance drops 30-40% on multi-step transformations
- Accuracy falls below 50% on novel alphabet systems
- Few-shot prompting improves results by 15-20% on average
- Models show brittleness to small pattern perturbations

I think this work exposes important limitations in current LLMs' abstract reasoning capabilities. While they handle surface-level patterns well, they struggle with deeper analogical thinking. This suggests we need new architectures or training approaches to achieve more robust reasoning abilities.

The evaluation framework introduced here could help benchmark future models' reasoning capabilities in a more systematic way. The results also highlight specific areas where current models need improvement, particularly in handling novel patterns and multi-step transformations.

TLDR: New framework for testing analogical reasoning in LLMs using letter-string analogies shows strong performance on basic patterns but significant limitations with complex transformations and novel alphabets. Results suggest current models may be pattern-matching rather than truly reasoning.

[Full summary is here](https://aimodels.fyi/papers/arxiv/evaluating-robustness-analogical-reasoning-large-language-models). Paper [here](https://arxiv.org/abs/2411.14215).",Successful-Western27,1gys936,https://reddit.com/r/MachineLearning/comments/1gys936/r_testing_the_brittleness_of_llm_analogical/,https://www.reddit.com/r/MachineLearning/comments/1gys936/r_testing_the_brittleness_of_llm_analogical/,2024-11-24 14:32:43,11,0.82,11,0,1,0,0,False,False,True,False,False,Research,self,t3_1gys936
MachineLearning,[D] Emergent Cognitive Pathways In Transformer Models. Addressing Fundamental Flaws About Limits.,"**TLDR:**

Cognitive functions like reasoning and creativity emerge as models scale and train on better data. Common objections crumble when we consider humans with unusual cognitive or sensory differences—or those with limited exposure to the world—who still reason, formulate novel thoughts, and build internal models of the world.

EDIT: It looks like I hallucinated the convex hull metric as a requirement for out of distribution tests. I thought I heard it in a Lex Fridman podcast with either LeCun or Chollet, but while both advocate for systems that can generalize beyond their training data, neither actually uses the convex hull metric as a distribution test. Apologies for the mischaracterization.

**OOD Myths and the Elegance of Function Composition**

Critics like LeCun and Chollet argue that LLMs can't extrapolate beyond their training data, ~~often citing convex hull measurements~~. This view misses a fundamental mathematical reality: novel distributions emerge naturally through function composition. When non-linear functions f and g combine as f(g(x)), they create outputs beyond the original training distributions. This is not a limitation but a feature of how neural networks generalize knowledge.

Consider a simple example: training on {poems, cat poems, Shakespeare} allows a model to generate ""poems about cats in Shakespeare's style""—a novel computational function blending distributions. Scale this up, and f and g could represent Bayesian statistics and geopolitical analysis, yielding insights neither domain alone could produce. Generalizing this principle reveals capabilities like reasoning, creativity, theory of mind, and other high-level cognitive functions.

**The Training Data Paradox**

We can see an LLM's training data but not our own experiential limits, leading to the illusion that human knowledge is boundless. Consider someone in 1600: their 'training data' consisted of their local environment and perhaps a few dozen books. Yet they could reason about unseen phenomena and create new ideas. The key isn't the size of the training set - it's how information is transformed and recombined.

**Persistent Memory Isn't Essential**

A common objection is that LLMs lack persistent memory and therefore can’t perform causal inference, reasoning, or creativity. Yet people with anterograde amnesia, who cannot form new memories, regularly demonstrate all these abilities using only their working memory. Similarly, LLMs use context windows as working memory analogs, enabling reasoning and creative synthesis without long-term memory.

**Lack of a World Model**

The subfield of mechanistic interpretation strongly implies by its existence alone, that transformers and neural networks do create models of the world. One claim is that words are not a proper sensory mechanism and so text-only LLMs can't possibly form a 3D model of the world.

Let's take the case of a blind and deaf person with limited proprioception who can read in Braille. It would be absurd to claim that because their main window into the world is just text from Braille, that they can't reason, be creative or build an internal model of the world. We know that's not true.

Just as a blind person constructs valid world models from Braille through learned transformations, LLMs build functional models through composition of learned patterns. What critics call 'hallucinations' are often valid explorations of these composed spaces - low probability regions that emerge from combining transformations in novel ways.

**Real Limitations**

While these analogies are compelling, true reflective reasoning might require recursive feedback loops or temporal encoding, which LLMs lack, though attention mechanisms and context windows provide partial alternatives. While LLMs currently lack true recursive reasoning or human-like planning, these reflect architectural constraints that future designs may address.

**Final Thoughts**

The non-linearity of feedforward networks and their high-dimensional spaces enables genuine novel outputs, verifiable through embedding analysis and distribution testing. Experiments like Golden Gate Claude, where researchers amplified specific neural pathways to explore novel cognitive spaces, demonstrate these principles in action. We don't say planes can't fly simply because they're not birds - likewise, LLMs can reason and create despite using different cognitive architectures than humans. We can probably approximate and identify other emergent cognitive features like Theory of Mind, Metacognition, Reflection as well as a few that humans may not possess.",ipassthebutteromg,1gys51e,https://reddit.com/r/MachineLearning/comments/1gys51e/d_emergent_cognitive_pathways_in_transformer/,https://www.reddit.com/r/MachineLearning/comments/1gys51e/d_emergent_cognitive_pathways_in_transformer/,2024-11-24 14:27:12,11,0.59,11,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1gys51e
MachineLearning,[P] I made a library for building agents that use tree search to solve problems,,jsonathan,1gyreq1,https://reddit.com/r/MachineLearning/comments/1gyreq1/p_i_made_a_library_for_building_agents_that_use/,https://i.redd.it/qut9unu4su2e1.png,2024-11-24 13:51:18,288,0.95,288,0,26,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/EovprfWyEiN6f7tnfzX9swDj_d3bu8iJE26aqKFyhic.jpg,t3_1gyreq1
MachineLearning,[D] What is the current state-of-the-art for discrete diffusion models? ,"Hi everyone,

I am currently working with Discrete Diffusion models for a new research project. In this project, I am applying Discrete Diffusion to a field where it has yet to be applied. However, I am quite new to diffusion itself, and I am overwhelmed by the number of papers published on the topic. In my current implementation, I focussed on an older [paper](https://arxiv.org/abs/2102.05379) since they described their approach quite well, and I wanted to test my idea first to see if it had some merit, which, according to initial results, it has.

Currently, I am looking at updating my method with more recent additions to this field, but as I said earlier, I am a bit overwhelmed by the amount. So my question to you is, what are good recent papers that looked into Discrete Diffusion that either explain essential concepts, such as survey papers, or that introduce new state-of-art methods that are not only applicable to a specific field, such as NLP or Vision?

Thank you in advance for your help.",Derpirium,1gyp1br,https://reddit.com/r/MachineLearning/comments/1gyp1br/d_what_is_the_current_stateoftheart_for_discrete/,https://www.reddit.com/r/MachineLearning/comments/1gyp1br/d_what_is_the_current_stateoftheart_for_discrete/,2024-11-24 11:35:12,42,0.98,42,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gyp1br
MachineLearning,[R] Llama 3.2 Interpretability with Sparse Autoencoders,,RandomHexCode,1gxzsbp,https://reddit.com/r/MachineLearning/comments/1gxzsbp/r_llama_32_interpretability_with_sparse/,https://github.com/PaulPauls/llama3_interpretability_sae,2024-11-23 13:37:14,3,1.0,3,0,0,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/geyS4I_5fqQ5-NrpJOf-awV-cU5tO-EH7u-ZGL5X41Y.jpg,t3_1gxzsbp
MachineLearning,[D] ACL Rolling Review October 2024,Discussion thread for ACL 2024 (ARR Oct) reviews.,AffectionateTip521,1gy8ekt,https://reddit.com/r/MachineLearning/comments/1gy8ekt/d_acl_rolling_review_october_2024/,https://www.reddit.com/r/MachineLearning/comments/1gy8ekt/d_acl_rolling_review_october_2024/,2024-11-23 20:01:19,17,0.96,17,0,99,0,0,False,False,True,False,False,Discussion,self,t3_1gy8ekt
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1gyhfxm,https://reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/,2024-11-24 03:15:10,38,0.87,38,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gyhfxm
MachineLearning,[P] Creating Custom Music Genres Using Unsupervised Learning ,"so i had this random thought to create new music genres/spotify daylists using unsupervised learning. my idea is more towards creating a custom genre but not something necessarily as hyper-personalized as daylists. this is very much just an idea for now, will be developing into it soon tho. so the idea is in two phases:

1. take music data with audio features/embeddings/mfccs/create own features and use unsupervised learning to create clusters of those using something like knns
2. take out the audio features of the centre of the clusters and feed that to an llm to generate a custom phrase/name for that particular cluster. this can be something customized like character names for a play/use data like what time frame particular clusters of songs were played more to create something a lil more personalized like daylists/anything for that matter. haven't given much thought into this part for now.

i found a lot of papers/articles for the former phase but couldn't find much for the latter as of now. i am reading more into how spotify makes their daylists to see if anything strikes of interest.

i would live to have suggestions on how this can be improved/ recommendations for research papers/articles on anything relevant to this.

note: i know this is not very well framed and is messy but tbf i am drunk at 2 am and suddenly struck with my long lost passion for musicso please help a girl out (⁠´⁠ ⁠.⁠ ⁠.̫⁠ ⁠.⁠ ⁠`⁠)",Personal_Equal7989,1gyb62h,https://reddit.com/r/MachineLearning/comments/1gyb62h/p_creating_custom_music_genres_using_unsupervised/,https://www.reddit.com/r/MachineLearning/comments/1gyb62h/p_creating_custom_music_genres_using_unsupervised/,2024-11-23 22:05:00,4,0.83,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1gyb62h
MachineLearning,[D] Recommendations needed: image-to-3D diffusion models,"Hey all, I'm evaluating different open source image-to-3D diffusion models for a project and could use some real-world insights. I've been digging through papers but would love to hear from people who've actually implemented these.

My main requirements:

1. Quality is the top priority - looking for clean, accurate reconstructions
2. Need mesh-based output (not point clouds or neurals fields) that isn't astronomically large
3. Inference time isn't critical - happy to wait up to a minute per generation

I've looked at Zero123, Wonder3D, and a few others but curious what's working well for people in practice. Especially interested in:

* Which models are actually maintainable in production
* Any gotchas with mesh generation quality
* Real-world inference times you're seeing
* How much post-processing is typically needed

Would really appreciate hearing about your experiences, especially from anyone who's deployed these in actual projects. Thanks!",ESCNOptimist,1gy9dqd,https://reddit.com/r/MachineLearning/comments/1gy9dqd/d_recommendations_needed_imageto3d_diffusion/,https://www.reddit.com/r/MachineLearning/comments/1gy9dqd/d_recommendations_needed_imageto3d_diffusion/,2024-11-23 20:44:56,9,1.0,9,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gy9dqd
MachineLearning,[R] Iterative Narrowing: A Visual Prompting Framework for Enhanced GUI Location Grounding,"This paper introduces an iterative narrowing approach for GUI element grounding that processes visual and textual information in multiple refinement steps rather than a single pass. The key insight is breaking down element identification into coarse-to-fine stages that mirror how humans visually search interfaces.

Key technical points:
* **Two-stage architecture**: Initial region proposal network followed by focused refinement
*  Visual and text encoders process features in parallel before cross-attention alignment
* Progressive narrowing through multiple passes reduces false positives
* Handles nested GUI elements through hierarchical representation
* Trained on a dataset of 77K GUI screenshots with natural language queries

Results show:
* 15% improvement in grounding accuracy vs single-pass baseline
* Better handling of ambiguous queries
* Reduced computational overhead compared to exhaustive search
* Strong performance on complex nested interfaces
* Effective transfer to unseen GUI layouts

I think this approach could meaningfully improve accessibility tools and GUI automation by making element identification more robust. The iterative refinement mirrors human visual search patterns, which could lead to more natural interaction with interfaces.

I think the main limitation is handling highly dynamic interfaces, where elements move or change frequently. The multi-pass nature also introduces some latency that would need optimization for real-time applications.

TLDR: New GUI grounding method uses multiple refinement passes to identify interface elements more accurately, achieving 15% better accuracy through an approach that mimics human visual search patterns.

[Full summary is here](https://aimodels.fyi/papers/arxiv/improved-gui-grounding-via-iterative-narrowing). Paper [here](https://arxiv.org/abs/2411.13591).",Successful-Western27,1gy8tgs,https://reddit.com/r/MachineLearning/comments/1gy8tgs/r_iterative_narrowing_a_visual_prompting/,https://www.reddit.com/r/MachineLearning/comments/1gy8tgs/r_iterative_narrowing_a_visual_prompting/,2024-11-23 20:19:49,5,1.0,5,0,0,0,0,False,False,True,False,False,Research,self,t3_1gy8tgs
MachineLearning,[D] Optimization Algorithm that focuses on solving model parameters ,"i tried googling for any sources but does anyone have info on where i can start looking for optimization algorithms that focus on optimizing model parameters by solving for a specific parameter (or parameters), given the input and target for a sample? Or the name for this kind of optimization algorithm? E.g. solve for model parameters a,b,c for the function y = ax\^2 + bx + c , x and y being the input and target respectively. Surely this algorithm has a name in the context of ml.

EDIT:

I believe what im asking is a bit ambiguous. As opposed to gradient descent, which focuses on finding the derivative of model parameters to the loss provided, the optimization algorithm i specified above focuses on a number of parameters and somehow figures out (or solves the values of these parameters) to approximately match the output. Like the same way you have 5x = 10 and solve for x, so the algortithm figures that x=2. For more data samples and more parameters you have 5x + c = 12 and 2x + c = 6, x and c being model parameters and 10 ad 4 being the desired output. The algorithm figures x = 2 and c = 2 somehow. Its a bit of a stretch but even im starting to doubt my sanity enough to believe that what im asking is basically all if not most of what ml optimization algorithms do.",Relevant-Twist520,1gy6jy6,https://reddit.com/r/MachineLearning/comments/1gy6jy6/d_optimization_algorithm_that_focuses_on_solving/,https://www.reddit.com/r/MachineLearning/comments/1gy6jy6/d_optimization_algorithm_that_focuses_on_solving/,2024-11-23 18:40:43,0,0.42,0,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gy6jy6
MachineLearning,"[D] This is my first blog on medium, and they are about, How Modern Binary Hopfield Networks are just Hamming Distance Auto completers in disguise",,StoneSteel_1,1gy4qpv,https://reddit.com/r/MachineLearning/comments/1gy4qpv/d_this_is_my_first_blog_on_medium_and_they_are/,https://medium.com/@kanishq.vijay/modern-hopfield-networks-are-just-fancy-hamming-distance-calculators-and-i-can-prove-it-e9f538ee908e,2024-11-23 17:22:10,28,0.8,28,0,0,0,0,False,False,False,False,False,Discussion,https://a.thumbs.redditmedia.com/9FY02BNeYiTA4V1t16gIVBAkQJrL_tL5YxXhNMuurn0.jpg,t3_1gy4qpv
MachineLearning,[D] How to make more reliable reports using AI — A Technical Guide,,phicreative1997,1gy1t45,https://reddit.com/r/MachineLearning/comments/1gy1t45/d_how_to_make_more_reliable_reports_using_ai_a/,https://medium.com/firebird-technologies/how-to-make-more-reliable-reports-using-ai-a-technical-guide-672b2d01cb2a,2024-11-23 15:14:13,0,0.29,0,0,0,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/-AZkwBJl93JyKODja6Nv557NXqR_dwFniH8tEBkkuoQ.jpg,t3_1gy1t45
MachineLearning,[R] Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,"
Abstract:
Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [0,1]  and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [-1,1]. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.

https://arxiv.org/abs/2411.12537",iltruma,1gy0hbh,https://reddit.com/r/MachineLearning/comments/1gy0hbh/r_unlocking_statetracking_in_linear_rnns_through/,https://arxiv.org/abs/2411.12537,2024-11-23 14:11:53,92,0.95,92,0,4,0,0,False,False,False,False,False,Research,default,t3_1gy0hbh
MachineLearning,[D] [Project] JAX ML Framework; Write neural networks and more; shorter and faster; What are your thoughts?,"Made a JAX framework for machine learning because I wanted to code faster &amp; shorter so I made zephyr. I hope it might be helpful to you guys too and wanted to hear some feedback.

Link in the comments.

Nothing wrong with current frameworks, this is just another way of doing things. 

NNs or ML algorithms to me, are just pure/mathematical functions and so I wanted that to reflect in my code. With other frameworks it comes in at least 2 steps: initialization in the constructor and a computation in the forward/call body. This seems fine at first but when models become larger, it's 2 places where I have to synchronize code. - If I change a computation, I might need to change a hyperparameter somewhere, or if I change a hyperparameter, I might need to change a computation - or if i have to re-read my code, i have to read in at least 2 places. I usually use a small window for an editor and so jumping between these could a hassle (putting them side by side is another solution).

Another thing I was experiencing was that if I was doing something that is not neural networks, for example if an algorithm was easier to do with a recursive call (but with different trainable weights for each call), that would be challenging in other frameworks. So while they generic computational graph-frameworks, some computations are hard to do. 

To me, computations was about passing data around and getting them to transform, so this \`act\` of transforming data should be that focus of the framework. That's what I did with zephyr. Mathematical functions are python functions, no need for initialization in a constructor. You use the functions(networks or layers, etc) when you need them. No need for constructors, allows recursions, allows you to focus on the transformations or operations. Zephyr handles weight creation and management for you - it is explicit tho unlike other frameworks; you carry around a \`params\` tree, and that should be no problem, since that's a core of the computation and shouldn't be hidden away.

In short, zephyr is  short but readable aimed at people developing research ideas about ML. The README has a few samples for neural networks. I hope you guys like it and try it.

",Pristine-Staff-5250,1gxqag6,https://reddit.com/r/MachineLearning/comments/1gxqag6/d_project_jax_ml_framework_write_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1gxqag6/d_project_jax_ml_framework_write_neural_networks/,2024-11-23 03:25:31,34,0.84,34,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gxqag6
MachineLearning,"[D] Accepted NeurIPS 2024 paper claimed to be solving a novel problem as first work, but ignores 5 prior works","At NeurIPS 2024 I found a paper that got accepted that positions its main contribution in the form of “Existing algorithms for X ignore Y. We adapt algorithm Z for X to account for Y”.

On OpenReview I see that the reviewers in particular praised the novelty of the work, and recognised Y as an important aspect that had been ignored in the field of X.

Now the interesting bit: co-authors and I published a paper in Springer’s Machine Learning journal in 2023 that also proposes an algorithm for X that account for Y. We were also not the first to study the problem setting of X with Y: our paper’s related work section discusses 4 papers that have all proposed algorithms for X that account for Y. One is even from NeurIPS (2017), and the oldest one dates back to 2012 (an AAAI paper).

The authors of this 2024 NeurIPS paper completely missed all this prior literature and believed they were the first, and so did all the reviewers.

This week I e-mailed the authors of this NeurIPS 2024 paper and they acknowledged that these works (mine + the 4 others) indeed were all working on the same problem setting, mentioned that they were unaware of all these works, and acknowledged that they can no longer claim novelty of the problem setting.

NeurIPS allows updating the camera ready paper after the conference, and the authors promised to use this opportunity to incorporate those related works and modify their contribution statements to no longer claim novelty of a first solution of X with Y.

At the one hand, it makes me happy that our work will get credited appropriately.

At the other hand I have my doubts about the ethics of severely modifying contribution statements post-review. The authors will no longer claim novelty, but the reviewers in particular praised this novelty, which makes me uncertain whether reviewers would have recommended acceptance had they known that this paper will ultimately no longer be able to claim the novelty that it claimed to have in the reviewed version.

Moreover this makes me wonder about the experimental section. Almost surely, reviewers would have demanded comparison to those 5 prior works as baselines. This paper did not compare against baselines, which will have seemed reasonable to a reviewer who reviewed this work under the assumption that the problem setting was completely novel and no prior methods exist that could function as a baseline.

Asking the group here about any thoughts on how such cases should get resolved:
- should the paper be retracted?
- should the area chair / program committee be informed? who may or may not take action
- should the paper just get updated by authors in the way that was promised, and that is it?
- something else?

I redacted X, Y and Z in order to not publicly shame the authors, as they have engaged with my e-mails and I am convinced that there is no foul play and they truly were unaware of those works.",TaXxER,1gxooqv,https://reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/,https://www.reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/,2024-11-23 01:58:58,276,0.94,276,0,65,0,0,False,False,True,False,False,Discussion,self,t3_1gxooqv
MachineLearning,[D] Historical archive of generative AI media output?,"Is there an archive or research paper that shows examples of the progress in generative AI media output over time?

I want to gather examples of multimedia outputs (text, images, video, sound) generated over time to help evaluate how the field has progressed in each area over time. 

Of course I can grab whatever results from different sources by searching, but I'm wondering if there is a more organized and consistent repository for this?",searchresults,1gxh7dm,https://reddit.com/r/MachineLearning/comments/1gxh7dm/d_historical_archive_of_generative_ai_media_output/,https://www.reddit.com/r/MachineLearning/comments/1gxh7dm/d_historical_archive_of_generative_ai_media_output/,2024-11-22 20:15:06,8,0.83,8,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gxh7dm
MachineLearning,[P] Python Windows Screenshot Analyzer,"I want to build a python project to analyse windows screehots. Suppose an app is open then the screenshot should tell everything going on in the app. For example in the Microsoft Teams Who are the participants, ongoing duration etc. What all apps are open in the taskbar what's the time in the screenshot etc. How can I achieve it I want to use open source resources only.",Ok-Bar5416,1gxh51z,https://reddit.com/r/MachineLearning/comments/1gxh51z/p_python_windows_screenshot_analyzer/,https://www.reddit.com/r/MachineLearning/comments/1gxh51z/p_python_windows_screenshot_analyzer/,2024-11-22 20:12:23,0,0.36,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1gxh51z
MachineLearning,"[D] We’ve crowd-sourced,  open-sourced, and made it easy to find so many tools to build with, but where is all this effort for context/scraping?","We have so many repos and libraries available to us for building, deploying, and using LLMs for tasks. We have hubs for models, plug-in-play libraries for things like LoRA and RAG, containerization for deploying models with APIs, extensions to integrate LLMs into IDEs and workflows, and plenty more. There’s stuff for managing and  orchestrating agents. 

Suffice to say, we have tons to open source tools to work to start working on both niche and general uses for LLMs.

That’s all great, but what I’m always having to build from scratch is getting context. Be that tools for online searches, webpage parsing (even common webpages that I know people would love to be easier to use for context), document parsing, etc.

I’ve been seen more cool projects pop up, but I’ve been seeing those projects provide details or implementation less and less on how they are finding, accessing, retrieving, and processing context.

There are plenty libraries to build tools for this purpose, but I just see less and less people sharing those.

Now I understand the context different projects need can be pretty niche, so reusability could be sparse. 

But is my perception wrong? Are there open-source resources for finding existing context extraction/scraping implementations or places to submit your own to make it easier for others to find?",Oscilla,1gxbrdm,https://reddit.com/r/MachineLearning/comments/1gxbrdm/d_weve_crowdsourced_opensourced_and_made_it_easy/,https://www.reddit.com/r/MachineLearning/comments/1gxbrdm/d_weve_crowdsourced_opensourced_and_made_it_easy/,2024-11-22 16:26:26,16,0.83,16,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gxbrdm
MachineLearning,[D] Anyone else work in Infrastructure/MLOps?,"I've been working as an AI Architect at a small/medium company (\~300 people). I've been having more success focusing on the deployment of models using KServe and Kubeflow rather than pure ML. The trend that I'm seeing is that there's so many open-source models that custom ML solutions are less relevant, so it's becoming more valuable to simply deploy open-source models that keep getting massive improvements, at least in the space that I'm in.   
  
Curious who else may be working on the Infrastructure/MLOps side of things. I've found development to sometimes be pretty isolating, since I'm the only one in this company focusing on this and haven't found a community outside of this. It would be nice to hear stories from others who are also working in this area. Cheers!",Milwookie123,1gxbh9b,https://reddit.com/r/MachineLearning/comments/1gxbh9b/d_anyone_else_work_in_infrastructuremlops/,https://www.reddit.com/r/MachineLearning/comments/1gxbh9b/d_anyone_else_work_in_infrastructuremlops/,2024-11-22 16:14:51,2,0.76,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gxbh9b
MachineLearning,[P] Registrations are open for the 2025 EY Open Science AI &amp; Data Challenge,"Use AI for good and help create more vital, sustainable communities when you join the [2025 EY Open Science AI &amp; Data Challenge](https://challenge.ey.com/2025). A phenomenon known as the urban heat island effect is becoming a significant issue as our cities continue to grow and develop. Dense development and lack of green space create heat islands that take a toll on our health and increase our energy use. But your skills and vision can help. [Register now.](https://challenge.ey.com/2025) \#EY #BetterWorkingWorld #AI #OpenScience #ShapeTheFutureWithConfidence",fofxy,1gx9htl,https://reddit.com/r/MachineLearning/comments/1gx9htl/p_registrations_are_open_for_the_2025_ey_open/,https://www.reddit.com/r/MachineLearning/comments/1gx9htl/p_registrations_are_open_for_the_2025_ey_open/,2024-11-22 14:48:28,12,0.75,12,0,1,0,0,False,False,True,False,False,Project,self,t3_1gx9htl
MachineLearning,[R] Entropy-Guided Critical Neuron Pruning for Efficient Spiking Neural Networks,"This paper introduces a pruning method for Spiking Neural Networks (SNNs) based on neuroscience principles of criticality. The key insight is using neuronal avalanche analysis to identify neurons that have the most significant impact on network dynamics, similar to how critical neurons function in biological brains.

Key technical points:
* Monitors spike propagation patterns to identify critical neurons
* Introduces adaptive pruning schedule based on network stability metrics
* Achieves 90% compression while maintaining accuracy on MNIST/CIFAR-10
* Works across different SNN architectures (feed-forward, CNN)
* Uses stability measures to prevent catastrophic forgetting during pruning

Main results:
* Outperforms existing pruning methods on accuracy retention
* Shows better energy efficiency compared to unpruned networks
* Maintains temporal dynamics important for SNN operation
* Demonstrates scalability across different network sizes
* Validates biological inspiration through avalanche analysis

I think this approach could be particularly important for deploying SNNs in resource-constrained environments like edge devices. The adaptive pruning schedule seems especially promising since it automatically adjusts based on network behavior rather than requiring manual tuning.

I think there are some open questions about computational overhead of the avalanche analysis that need to be addressed for very large networks. However, the biological principles behind the method suggest it could generalize well to other architectures and tasks.

TLDR: Novel pruning method for SNNs based on neuroscience principles of criticality. Uses neuronal avalanche analysis to identify important neurons and achieves 90% compression while maintaining accuracy. Introduces adaptive pruning schedule that adjusts based on network stability.

[Full summary is here](https://aimodels.fyi/papers/arxiv/brain-inspired-efficient-pruning-exploiting-criticality-spiking). Paper [here](https://arxiv.org/abs/2311.16141).",Successful-Western27,1gx86i0,https://reddit.com/r/MachineLearning/comments/1gx86i0/r_entropyguided_critical_neuron_pruning_for/,https://www.reddit.com/r/MachineLearning/comments/1gx86i0/r_entropyguided_critical_neuron_pruning_for/,2024-11-22 13:45:14,45,0.88,45,0,5,0,0,False,False,True,False,False,Research,self,t3_1gx86i0
MachineLearning,[P] Machine learning project on chem,"It is called SMILES

[Simplified Molecular Input Line Entry System - Wikipedia](https://en.wikipedia.org/wiki/Simplified_Molecular_Input_Line_Entry_System)

I am not sure if I can train a model so that it can interpret the correct structure as well as naming after feeding labelled dataset?",uartimcs,1gx4ezg,https://reddit.com/r/MachineLearning/comments/1gx4ezg/p_machine_learning_project_on_chem/,https://www.reddit.com/r/MachineLearning/comments/1gx4ezg/p_machine_learning_project_on_chem/,2024-11-22 09:59:23,2,0.6,2,0,7,0,0,False,False,True,False,False,Project,self,t3_1gx4ezg
MachineLearning,[D] Noise injection,"Can anyone give me some recommendations, about paper that identify the relationship between accumulated output noise over time (e.g. something like time series model with the noise injection to the input)? ",Careless-Top-2411,1gwy1m7,https://reddit.com/r/MachineLearning/comments/1gwy1m7/d_noise_injection/,https://www.reddit.com/r/MachineLearning/comments/1gwy1m7/d_noise_injection/,2024-11-22 03:05:09,2,0.75,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwy1m7
MachineLearning,[P] Comparing Machine Unlearning Techniques and PKE (Precision Knowledge Editing),"PKE focuses on enhancing the model's knowledge and positive output rather just identifying neuron activiations. While DINM is a great method for neural suppression and to aid real time modification, I really wanted to build on top of that and created PKE (Precision Knowledge Editing)which emphasizes neural reinforcement and enhancing the model's knowledge and positive output rather than just identifying neuron activiations.There's lots of current Machine unlearning techniques that can make LLMs safer right now like:

1. **Exact Unlearning:** This method involves retraining the model from scratch after removing the undesired data. While it ensures complete removal of the data's influence, it is computationally expensive and time-consuming, especially for large models.
2. **Approximate Unlearning:**
   1. **Fine-Tuning:** adjusting the model using the remaining data to mitigate the influence of the removed data. However, this may not completely eliminate the data's impact.
   2. **Gradient Ascent:** applying gradient ascent on the loss function concerning the data to be forgotten, effectively 'unlearning' it. This method can be unstable and may degrade model performance.

PKE is better for the following reasons:

1. **Fine-Grained Identification of Toxic Parameters:** PKE employs neuron weight tracking and activation pathway tracing to accurately pinpoint specific regions in the model responsible for generating toxic or harmful content. This precision allows for targeted interventions, reducing the risk of unintended alterations to the model's overall behavior.
2. **Maintaining Model Performance:** By focusing edits on identified toxic regions, PKE minimizes the impact on the model's general performance. This approach ensures that the model retains its capabilities across various tasks while effectively mitigating the generation of undesirable content.
3. **Scalability Across Different Model Architectures:** PKE has demonstrated effectiveness across various LLM architectures, including models like Llama2-7b and Llama-3-8b-instruct. This scalability makes it a versatile tool for enhancing safety in diverse AI systems.

Would love to hear your guys' thoughts on this project and how to continue to improve this methodology. If interested, here's the Github link: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models) and [paper](https://arxiv.org/pdf/2410.03772) .",lial4415,1gwwt6y,https://reddit.com/r/MachineLearning/comments/1gwwt6y/p_comparing_machine_unlearning_techniques_and_pke/,https://www.reddit.com/r/MachineLearning/comments/1gwwt6y/p_comparing_machine_unlearning_techniques_and_pke/,2024-11-22 02:02:44,2,1.0,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1gwwt6y
MachineLearning,[D] Machine Learning as a DSL-design problem? Is this a thing? ,"I'll probably sound confusing or unclear, and that's because I don't even know what I want to ask about, but the general direction is: instead of learning numbers that combine/compute in a certain way is there a way to learn a DSL over a problem in guided/principled way? (I have no idea how to give an example, since I'm not even sure this is currently possible). This is different from an NLP-model learning to use a programming language. 

  
I do know a problem with this, and it's just that it's hard to evaluate an arbitrary program (halting problem). Neural nets are basically vector programs with a set amount compute steps. I have seen program search algorithms over a small crafted DSL, but I was thinking of, learning the DSL directly - but this is a chicken and egg problem since what learns the DSL ?",Pristine-Staff-5250,1gwwitr,https://reddit.com/r/MachineLearning/comments/1gwwitr/d_machine_learning_as_a_dsldesign_problem_is_this/,https://www.reddit.com/r/MachineLearning/comments/1gwwitr/d_machine_learning_as_a_dsldesign_problem_is_this/,2024-11-22 01:48:20,0,0.44,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gwwitr
MachineLearning,[R] Say What You Mean: A Response to 'Let Me Speak Freely',"Will here from .txt, the team behind [Outlines](https://github.com/dottxt-ai/outlines) an open source library that enables open LLMs to perform structured generation, ensuring their outputs always adhere to a predefined format.

We are passionate about structured generation, and truly believe it has the potential to transform the work being done with LLMs in profound ways. 

However a recent paper, [Let Me Speak Freely](https://arxiv.org/abs/2408.02442) was published reporting some misinformation around the performance of structured generation on a series of evaluations. 

We've recently publish a rebuttal to this paper on our blog: [Say What You Mean: A Response to 'Let Me Speak Freely'](https://blog.dottxt.co/say-what-you-mean.html) and thought the community here might find it interesting. It covers not only issues with the original paper, but also dives into the nature of structured generation and how to get the most out of your models with prompting for structured generation.",CountBayesie,1gwswn7,https://reddit.com/r/MachineLearning/comments/1gwswn7/r_say_what_you_mean_a_response_to_let_me_speak/,https://www.reddit.com/r/MachineLearning/comments/1gwswn7/r_say_what_you_mean_a_response_to_let_me_speak/,2024-11-21 22:57:45,90,0.9,90,0,25,0,0,False,False,True,False,False,Research,self,t3_1gwswn7
MachineLearning,[R]Geometric aperiodic fractal organization in Semantic Space : A Novel Finding About How Meaning Organizes Itself ,"Hey friends! I'm sharing this here because I think it warrants some attention, and I'm using methods that intersect from different domains, with Machine Learning being one of them.

Recently I read Tegmark &amp; co.'s paper on Geometric Concepts [https://arxiv.org/abs/2410.19750](https://arxiv.org/abs/2410.19750) and thought that it was fascinating that they were finding these geometric relationships in llms and wanted to tinker with their process a little bit, but I didn't really have access or expertise to delve into LLM innards, so I thought I might be able to find something by mapping its output responses with embedding models to see if I can locate any geometric unity underlying how llms organize their semantic patterns. Well I did find that and more...

I've made what I believe is a significant discovery about how meaning organizes itself geometrically in semantic space, and I'd like to share it with you and invite collaboration.

**The Initial Discovery**

While experimenting with different dimensionality reduction techniques (PCA, UMAP, t-SNE, and Isomap) to visualize semantic embeddings, I noticed something beautiful and striking; a consistent ""flower-like"" pattern emerging across all methods and combinations thereof. I systematically weeded out the possibility that this was the behavior of any single model(either embedding or dimensional reduction model) or combination of models and what I've found is kind of wild to say the least. It turns out that this wasn't just a visualization artifact, as it appeared regardless of:

\- The reduction method used

\- The embedding model employed

\- The input text analyzed

https://preview.redd.it/pdyq50s1ob2e1.png?width=907&amp;format=png&amp;auto=webp&amp;s=b9ecf9206c1c2b43881341e8ad51950cf73b345c

https://preview.redd.it/b2u3uz93ob2e1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=6448776ebaeb5620b2079c7fed6992b3a813d619

[cross-section of the convergence point\(Organic\) hulls](https://preview.redd.it/t59tzz2qob2e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=a9a0cd3132191db5a2ea163c87e8dfe336f9320c)

[a step further, showing how they form with self similarity.](https://preview.redd.it/q0pmaveqob2e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=863dd23a1899efc8bf266c0702cf3258643859c3)

**Verification Through Multiple Methods**

To verify this isn't just coincidental, I conducted several analyses, rewrote the program and math 4 times and did the following:

1. Pairwise Similarity Matrices

Mapping the embeddings to similarity matrices reveals consistent patterns:

\- A perfect diagonal line (self-similarity = 1.0)

\- Regular cross-patterns at 45° angles

\- Repeating geometric structures

https://preview.redd.it/ft89ukpaob2e1.png?width=460&amp;format=png&amp;auto=webp&amp;s=9900f9113fad02841e5e18cb0bc5f9b6b66275e1

https://preview.redd.it/f2yzbvnbob2e1.png?width=433&amp;format=png&amp;auto=webp&amp;s=4a13a8e910794c64375ab0628f6f34006c31fb2f

Relevant Code:  
python

def analyze\_similarity\_structure(embeddings):

similarity\_matrix = cosine\_similarity(embeddings)

eigenvalues = np.linalg.eigvals(similarity\_matrix)

sorted\_eigenvalues = sorted(eigenvalues, reverse=True)

return similarity\_matrix, sorted\_eigenvalues

2. Eigenvalue Analysis

The eigenvalue progression as more text is added, regardless of content or languages shows remarkable consistency like the following sample:

First Set of eigenvalues while analyzing The Red Book by C.G. Jung in pieces:  
\[35.39, 7.84, 6.71\]

Later Sets:  
\[442.29, 162.38, 82.82\]

\[533.16, 168.78, 95.53\]

\[593.31, 172.75, 104.20\]

\[619.62, 175.65, 109.41\]

https://preview.redd.it/hesf440job2e1.png?width=1088&amp;format=png&amp;auto=webp&amp;s=b531499fe8043e0b41390229bd0b04017373c49b

Key findings:

\- The top 3 eigenvalues consistently account for most of the variance

\- Clear logarithmic growth pattern

\- Stable spectral gaps i.e: (35.79393)

3. Organic Hull Visualization

The geometric structure becomes particularly visible when visualizing through organic hulls:

Code for generating data visualization through sinusoidal sphere deformations:  
python

def generate\_organic\_hull(points, method='pca'):

phi = np.linspace(0, 2\*np.pi, 30)

theta = np.linspace(-np.pi/2, np.pi/2, 30)

phi, theta = np.meshgrid(phi, theta)

center = np.mean(points, axis=0)

spread = np.std(points, axis=0)

x = center\[0\] + spread\[0\] \* np.cos(theta) \* np.cos(phi)

y = center\[1\] + spread\[1\] \* np.cos(theta) \* np.sin(phi)

z = center\[2\] + spread\[2\] \* np.sin(theta)

return x, y, z

\`\`\`

What the this discovery suggests is that meaning in semantic space has inherent geometric structure that organizes itself along predictable patterns and shows consistent mathematical self-similar relationships that exhibit golden ratio behavior like a penrose tiling, hyperbolic coxeter honeycomb etc and these patterns persist across combinations of different models and methods. I've run into an inverse of the problem that you have when you want to discover something; instead of finding a needle in a haystack, I'm trying to find a single piece of hay in a stack of needles, in the sense that nothing I do prevents these geometric unity from being present in the semantic space of all texts. The more text I throw at it, the more defined the geometry becomes.

https://preview.redd.it/3hho1avzob2e1.png?width=1239&amp;format=png&amp;auto=webp&amp;s=a446d6b71ba0166c842e9537c6cd228662bb2682

I think I've done what I can so far on my own as far as cross-referencing results across multiple methods and collecting significant raw data that reinforces itself with each attempt to disprove it.

So I'm making a call for collaboration:

I'm looking for collaborators interested in:

1. Independently verifying these patterns
2. Exploring the mathematical implications
3. Investigating potential applications
4. Understanding the theoretical foundations

My complete codebase is available upon request, including:

\- Visualization tools

\- Analysis methods

\- Data processing pipeline

\- Metrics collection

If you're interested in collaborating or would like to verify these findings independently, please reach out. This could have significant implications for our understanding of how meaning organizes itself and potentially for improving language models, cognitive science, data science and more.

\*TL;DR: Discovered consistent geometric patterns in semantic space across multiple reduction methods and embedding models, verified through similarity matrices and eigenvalue analysis. Looking for interested collaborators to explore this further and/or independently verify.

\##EDIT##: I

I need to add some more context I guess,  because it seems that I'm being painted as a quack or a liar without being given the benefit of the doubt. Such is the nature of social media though I guess.

This is a cross-method, cross-model discovery using semantic embeddings that retain human interpretable relationships. i.e. for the similarity matrix visualizations, you can map the sentences to the eigenvalues and read them yourself. Theres nothing spooky going on here, its plain for your eyes and brain to see.

Here are some other researchers who are like-minded and do it for a living.

(Athanasopoulou et al.) supports our findings:

""The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it is organized in such a way that interesting semantic relations can be exported from manifolds of much lower dimensionality embedded in this high dimensional space."" [https://aclanthology.org/C14-1069.pdf](https://aclanthology.org/C14-1069.pdf)

A neuroscience paper(Alexander G. Huth 2013) reinforces my findings about geometric organization:""An efficient way for the brain to represent object and action categories would be to organize them into a continuous space that reflects the semantic similarity between categories.""  
[https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/)

""We use a novel eigenvector analysis method inspired from Random Matrix Theory and show that semantically coherent groups not only form in the row space, but also the column space.""  
[https://openreview.net/pdf?id=rJfJiR5ooX](https://openreview.net/pdf?id=rJfJiR5ooX)

I'm getting some hate here, but its unwarranted and comes from a lack of understanding. The automatic kneejerk reaction to completely shut someone down is not constructive criticism, its entirely unhelpful and unscientific in its closed-mindedness.",Own_Dog9066,1gwqvt2,https://reddit.com/r/MachineLearning/comments/1gwqvt2/rgeometric_aperiodic_fractal_organization_in/,https://www.reddit.com/r/MachineLearning/comments/1gwqvt2/rgeometric_aperiodic_fractal_organization_in/,2024-11-21 21:29:58,55,0.74,55,0,63,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/FrCUFqNtP_G0uh0Hibb8d0zw6XWHs9AUdUXetjPd1zM.jpg,t3_1gwqvt2
MachineLearning,[R] Agentic AI test suites. All the test environments used to benchmark BALROG.,"# BabyAI

+ Purpose is to facilitate research on *grounded language learning.*  The current domain of BabyAI is a 2D gridworld in which synthetic natural-looking instructions (e.g. “put the red ball next to the box on your left”) require the agent to navigate the world including unlocking doors) and move objects to specified locations.  

https://openreview.net/forum?id=rJeXCo0cYX

----

# Crafter

+ Crafter features randomly generated 2D worlds where the player needs to forage for food and water, find shelter to sleep, defend against monsters, collect materials, and build tools.

https://github.com/danijar/crafter?tab=readme-ov-file


----

# TextWorld

+ Microsoft TextWorld is an open-source, extensible engine that both generates and simulates text games. You can use it to train reinforcement learning (RL) agents to learn skills such as language understanding and grounding, combined with sequential decision making.

https://www.microsoft.com/en-us/research/project/textworld/

https://github.com/microsoft/TextWorld

https://arxiv.org/pdf/1806.11532

----

# Baba is AI 

+ Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives.    We test three ***state-of-the-art multi-modal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically*** when generalization requires that the rules of the game must be manipulated and combined. 


https://github.com/nacloos/baba-is-ai

https://arxiv.org/abs/2407.13729

----

# MiniHack

+ MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforcement Learning (RL).   The motivation behind MiniHack is to be able to perform RL experiments in a controlled setting while being able to increasingly scale the complexity of the tasks.

https://github.com/facebookresearch/minihack

https://minihack.readthedocs.io/en/latest/


----

# NetHack

+ NetHack is an attractive research platform as it contains hundreds of enemy and object types, has complex and stochastic environment dynamics, and has a clearly defined goal (descend the dungeon, retrieve an amulet, and ascend) which can be achieved in a diverse set of ways. The game is considered one of the hardest in the world1, with winning episodes lasting 100,000s of steps, and a permadeath setting that starts agents at the beginning in a whole new world if they die in the dungeon. NetHack is even difficult to master for human players who often rely on external knowledge.


https://proceedings.neurips.cc/paper_files/paper/2023/file/764ba7236fb63743014fafbd87dd4f0e-Paper-Conference.pdf

https://github.com/upiterbarg/hihack

https://arxiv.org/pdf/2203.11889

https://www.youtube.com/watch?v=8L8LiQ-cIWA",moschles,1gwqe0m,https://reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/,https://www.reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/,2024-11-21 21:09:01,2,0.63,2,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwqe0m
MachineLearning,[D] Research Topics in Conformal Prediction,"My background is in econometrics and soon I'll start to work in my master's thesis (already have a supervisor but would like to come up with some ideas that I could integrate in my research). One thing that recently got my attention were uncertainty quantification methods, specifically Conformal Prediction.

One thing that seems particularly cool is that it can be adapted to ensure coverage across specific groups in the covariates or even the labels. Additionally, 'recently', the research community was able to tackle the most limiting assumption, that of exchangeability, meaning it can be applied, for example, to time-series data.

My questions are two-fold (one out of curiosity and the other for personal interest):

1. What are some real-world scenarios that you've seen Conformal Prediction shine? And if there is some scenario that you'd think it would work but didn't.
2. And what do you think are some interesting questions yet to be addressed?

Any thoughts or general feedback very welcome! Thanks in advance!",HamsterExpress8688,1gwomft,https://reddit.com/r/MachineLearning/comments/1gwomft/d_research_topics_in_conformal_prediction/,https://www.reddit.com/r/MachineLearning/comments/1gwomft/d_research_topics_in_conformal_prediction/,2024-11-21 19:56:24,4,0.84,4,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gwomft
MachineLearning,[D] GNNs with applications to science Labs?,"I'm graduating this december with a degree in data science (sort of equivalent to a B.sc.+ M.sc.). I'm looking to pursue a PhD in an área similar as to what I did my thesis on, which was an application of GNNs to biomedical data. 

My advisor is relatively new in this area (the lab is more of a traditional Systems/Network biology), so he doesnt have international contacts in this particular area. I'm waiting to see if I'm granted with a PhD scholarship where I live, but science funding in my country has been really cut down, so I'm looking for options overseas.

Any directions to good labs working on this stuff would be greatly appreciated",maximusdecimus__,1gwogns,https://reddit.com/r/MachineLearning/comments/1gwogns/d_gnns_with_applications_to_science_labs/,https://www.reddit.com/r/MachineLearning/comments/1gwogns/d_gnns_with_applications_to_science_labs/,2024-11-21 19:49:49,2,0.76,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gwogns
MachineLearning,[R] How to bring novelty into a task like Engagement Prediction ,"So a colleague and I(both undergraduates) have been reading literature related to engagement analysis and we identified a niche domain under engagement prediction with a also niche dataset that might have been only used once or twice. 

The professor we are under told me that this might be a problem and also that we need more novelty even though we have figured out many imprivements through introducing modalities, augmentations, and possibly making it real time. 

How do I go ahead after this roadblock? Is there any potential in this research topic? If not, how do you cope with restarting from scratch like this? ",RCratos,1gwo8pk,https://reddit.com/r/MachineLearning/comments/1gwo8pk/r_how_to_bring_novelty_into_a_task_like/,https://www.reddit.com/r/MachineLearning/comments/1gwo8pk/r_how_to_bring_novelty_into_a_task_like/,2024-11-21 19:40:50,0,0.5,0,0,6,0,0,False,False,True,False,False,Research,self,t3_1gwo8pk
MachineLearning,[D] Do I need to connect more?,"I am currently finishing up my third year PhD and would most probably be graduating in another year.
So far I been pretty much working on my own. 

As a matter of fact, all my publications are first-author and most of the contributions to these papers are from PI or Co-PI who does very surface-level checks (grammars etc). I basically did not get involved with any work with other people and have been working solely on my own for these 3 years. Thus I don’t have much connections outside my lab group ( I actually don’t even know them well at all ). I see a lot of publications have quite a handful of authors, and most comprising of multiple organisations.

The thing is I was hoping to get an industrial job after graduation, would me being relatively unknown/unheard actually be a problem? 

I find it hard to find connections outside my lab. Most of my labmates don’t work in similar research areas. How do you guys actually connect or find collaborators outside of your lab zone? 

Would an overseas attachment help me? 
This is also one of the main reason why I am trying to find an internship (still very competitive), which is to collaborate or to be mentored by experienced people.
",AmbitiousSeesaw3330,1gwn0rn,https://reddit.com/r/MachineLearning/comments/1gwn0rn/d_do_i_need_to_connect_more/,https://www.reddit.com/r/MachineLearning/comments/1gwn0rn/d_do_i_need_to_connect_more/,2024-11-21 18:51:12,33,0.9,33,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gwn0rn
MachineLearning,[Discussion] VQVAE Reconstruction Issue: Grayscale Output and Gradient Flow Insights,"I recently trained a VQVAE model on the COCO dataset, and it successfully converged, generating images with fine details. However, during inference, I noticed something unusual.

When I tried to reconstruct an image using only the quantized vector and passed it through the decoder, the generated output was a grayscale image that preserved the edges of the original.

As shown in the attached image, the second image is the reconstructed one. In this case, I included the codebook gradient flow term as follows:

    x = x + (x - z).detach()

With this modification, the generated image appears much cleaner.

I'm curious to know if this behavior is expected with VQVAE or if there might be an issue with my implementation. If anyone has experience working with VQVAE, your insights would be greatly appreciated!

https://preview.redd.it/g961r7f3ka2e1.png?width=1688&amp;format=png&amp;auto=webp&amp;s=94d94e51c76a508949b2dee7e4341b6ec079dab1

",Logical-Passenger471,1gwl7c9,https://reddit.com/r/MachineLearning/comments/1gwl7c9/discussion_vqvae_reconstruction_issue_grayscale/,https://www.reddit.com/r/MachineLearning/comments/1gwl7c9/discussion_vqvae_reconstruction_issue_grayscale/,2024-11-21 17:41:18,4,0.83,4,0,6,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/xa_-CXxF248GR7gjMuLQB5rjtDa81-Z37-Czne4FTJA.jpg,t3_1gwl7c9
MachineLearning,[R] ICASSP 2025 review is out.,"4 4 3

Can I omit the rebuttal?

acceptance rate is pretty high but I guess the margin is quite high.",Big_Occasion_182,1gwgygb,https://reddit.com/r/MachineLearning/comments/1gwgygb/r_icassp_2025_review_is_out/,https://www.reddit.com/r/MachineLearning/comments/1gwgygb/r_icassp_2025_review_is_out/,2024-11-21 14:01:24,1,1.0,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwgygb
MachineLearning,[D] Seeking Journal Suggestions for Publishing a Side Project on AI-Assisted DSM-5-TR and ICD-10 Diagnostics,"Seeking Journal Suggestions for Publishing a Side Project on AI-Assisted DSM-5-TR and ICD-10 Diagnostics

I’ve been working on a side project with a psychologist. It’s a Retrieval-Augmented Generation (RAG) model that uses DSM-5-TR and ICD-10 to suggest the most likely diagnosis based on a user query. It’s designed for use in psychiatric and medical diagnostics to aid practitioners, researchers, and students.

I’m now at the stage where I’d like to publish my work but I’m not affiliated with any institution, this is purely a passion project. I would like to find a Journal that has:

1. Publication fees under $1,000 (or ideally free!).
2. A relatively quick review process (preferably less than 2 months).
3. Open access would be a plus, so it’s available to as many people as possible.

Do you know of any journals that might be a good fit for this kind of work? Bonus points if the journal is friendly to independent researchers or side projects.

Thanks in advance for the help! 😊",drinkredstripe3,1gwl0fn,https://reddit.com/r/MachineLearning/comments/1gwl0fn/d_seeking_journal_suggestions_for_publishing_a/,https://www.reddit.com/r/MachineLearning/comments/1gwl0fn/d_seeking_journal_suggestions_for_publishing_a/,2024-11-21 17:34:11,0,0.43,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwl0fn
MachineLearning,How to efficiently generate text from RNNs and Transformers during inference [P],"Most of the notebooks I see do something like this to generate code

    text = ['start']
    for _ in range(num_to_gen):
      token = model(text)
      text.append(token)

But this clearly is inefficient when it would be better to pass in each token one at a time to the model as it's generated while preserving the hidden state. What is the cleanest / industry accepted way to do this with pytorch models?

I see a tutorial on pytorch that has the model return both the output and the hidden state, and then you pass the hidden state back into the model. This feels really clunky and for large hidden states, it's inefficient to keep passing the hidden state of every rnn layer out to the end of the model.

Like I'm trying to work with Mamba currently which according to it's paper tries it's best to not materialize the full hidden state in memory.  
The only other way I can think of to do this is have the model persist the hidden state through forward calls and maybe have a reset function. But I'm not sure if this is an accepted way to do things. Also, I don't think I see the hidden state in the state\_dict of the open source mamba model, so I feel like Mamba doesn't do this but not entirely sure. I tried reading the Mamba code but found it difficult to understand.

I would appreciate seeing what the industry standard is to do this properly and an example in some open source code that's explained. As a bonus, if anyone can help me understand how Mamba does it in the state-spaces/mamba repo on github that would be great, but maybe I'll just post a comment on the repo.

Also just curious how transformer models deal with this as well.",No_Effective734,1gwkrir,https://reddit.com/r/MachineLearning/comments/1gwkrir/how_to_efficiently_generate_text_from_rnns_and/,https://www.reddit.com/r/MachineLearning/comments/1gwkrir/how_to_efficiently_generate_text_from_rnns_and/,2024-11-21 17:24:44,3,0.8,3,0,2,0,0,False,False,True,False,False,Project,self,t3_1gwkrir
MachineLearning,[R] The Complexity Dynamics of Grokking,"# [https://openreview.net/pdf?id=07N9jCfIE4](https://openreview.net/pdf?id=07N9jCfIE4)

Though OpenReviewers [don't seem impressed](https://openreview.net/forum?id=07N9jCfIE4) by this paper, I found it very interesting.  I like the concepts / ideas involved: 

1. Regularization methods (e.g. norms) usually measure capacity, not complexity.
2. Attempt regularization closer to Kolmogorov complexity.
3. relate model complexity to generalization &amp; grokking.",marojejian,1gwju03,https://reddit.com/r/MachineLearning/comments/1gwju03/r_the_complexity_dynamics_of_grokking/,https://www.reddit.com/r/MachineLearning/comments/1gwju03/r_the_complexity_dynamics_of_grokking/,2024-11-21 16:48:46,5,0.7,5,0,9,0,0,False,False,True,False,False,Research,self,t3_1gwju03
MachineLearning,[R] BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games,"Tired of saturated benchmarks? Want scope for a significant leap in capabilities? 

Introducing BALROG: a Benchmark for Agentic LLM and VLM Reasoning On Games!

BALROG is a challenging benchmark for LLM agentic capabilities, designed to stay relevant for years to come.

  
Check it out!

GitHub: [https://github.com/balrog-ai/BALROG](https://github.com/balrog-ai/BALROG)

Leaderboard: [https://balrogai.com](https://balrogai.com)

Paper: [https://arxiv.org/abs/2411.13543](https://arxiv.org/abs/2411.13543)",pagggga,1gwhnf8,https://reddit.com/r/MachineLearning/comments/1gwhnf8/r_balrog_benchmarking_agentic_llm_and_vlm/,https://www.reddit.com/r/MachineLearning/comments/1gwhnf8/r_balrog_benchmarking_agentic_llm_and_vlm/,2024-11-21 14:33:20,46,0.93,46,0,6,0,0,False,False,True,False,False,Research,self,t3_1gwhnf8
MachineLearning,"[D] Train and Val Dice Score gets zero for a long time and then increases, while loss keeps on decreasing. Wondering why?",,__proximity__,1gwhf4o,https://reddit.com/r/MachineLearning/comments/1gwhf4o/d_train_and_val_dice_score_gets_zero_for_a_long/,https://www.reddit.com/gallery/1gvxoga,2024-11-21 14:22:45,6,0.67,6,0,8,0,0,False,False,False,False,False,Discussion,default,t3_1gwhf4o
MachineLearning,"[D] Curious, how do you manage the full ML lifecycle ?","Hi guys! I’ve been pondering with a specific question/idea that I would like to pose as a discussion, it concerns the idea of more quickly going from idea to production with regards to ML/AI apps.

My experience in building ML apps and whilst talking to friends and colleagues has been something along the lines of you get data, that tends to be really crappy, so you spend about 80% of your time cleaning this, performing EDA, then some feature engineering including dimension reduction etc. All this mostly in notebooks using various packages depending on the goal. During this phase there are couple of tools that one tends to use to manage and version data e.g DVC etc

Thereafter one typically connects an experiment tracker such as MLFlow when conducting model building for various metric evaluations. Then once consensus has been reached on the optimal model, the Jupyter Notebook code usually has to be converted to pure python code and wrapped around some API or other means of serving the model. Then there is a whole operational component with various tools to ensure the model gets to production and amongst a couple of things it’s monitored for various data and model drift.

Now the ecosystem is full of tools for various stages of this lifecycle which is great but can prove challenging to **operationalize** and as we all know sometimes the results we get when adopting ML can be supar :(

I’ve been playing around with various platforms that have the ability for an end-to-end flow from cloud provider platforms such as AWS SageMaker, Vertex , Azure ML. Popular opensource frameworks like MetaFlow and even tried DagsHub. With the cloud providers it always feels like a jungle, clunky and sometimes overkill e.g maintenance. Furthermore when asking for platforms or tools that can really help one explore, test and investigate without too much setup it just feels lacking, as people tend to recommend tools that are great but only have one part of the puzzle. The best I have found so far is Lightning AI, although when it came to experiment tracking it was lacking.

So I’ve been playing with the idea of a truly out-of-the-box end-to-end platform, the idea is not to to re-invent the wheel but combine many of the good tools in an end-to-end flow powered by collaborative AI agents to help speed up the workflow across the ML lifecycle for faster prototyping and iterations. You can check out my current project over here [https://envole.ai](https://envole.ai)

This is still in the early stages so the are a couple of things to figure out, but would love to hear your feedback on the above hypothesis, how do you you solve this today ?",Lumiere-Celeste,1gwhdpl,https://reddit.com/r/MachineLearning/comments/1gwhdpl/d_curious_how_do_you_manage_the_full_ml_lifecycle/,https://www.reddit.com/r/MachineLearning/comments/1gwhdpl/d_curious_how_do_you_manage_the_full_ml_lifecycle/,2024-11-21 14:20:57,12,0.74,12,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gwhdpl
MachineLearning,[D]  Does anyone remember the machine learning in 2023 wrap-up meme video?,"Around this time last year someone posted a video of stitched-together memes about machine learning in 2023. I cannot remember all of the memes but there was definitely one about NLP professors needing to learn about RL, and one about Anthropic's appearance in front of some part of the US government.

Two questions.

1. Does anyone else remember this video and have a link? I cannot find it using Google because ""Machine learning in 2023"" is not a very discriminative search query.
2. Will there be a 2024 edition? I hope so!",votadini_,1gwh8yk,https://reddit.com/r/MachineLearning/comments/1gwh8yk/d_does_anyone_remember_the_machine_learning_in/,https://www.reddit.com/r/MachineLearning/comments/1gwh8yk/d_does_anyone_remember_the_machine_learning_in/,2024-11-21 14:14:51,14,0.85,14,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwh8yk
MachineLearning,"[R] Inference-Time Algorithms for LLMs: A Survey of Decoding, Meta-Generation, and Efficient Generation Methods","This survey unifies work on inference-time algorithms for LLMs into a comprehensive framework, examining how scaling compute during inference (rather than just training) can improve model outputs.

Key technical aspects:

- Introduces three categories of inference algorithms:
  - **Token-level generation**: Methods like beam search, nucleus sampling that work at individual token level
  - **Meta-generation**: Algorithms operating on full/partial sequences, incorporating external knowledge
  - **Efficient generation**: Techniques to reduce computational costs while maintaining quality

- Provides mathematical framework connecting:
  - Traditional NLP decoding approaches
  - Modern LLM inference methods  
  - Systems optimization techniques

- Reviews key tradeoffs between:
  - Compute cost vs output quality
  - Latency vs thoroughness of search
  - Memory usage vs beam size

I think this framework helps bridge the gap between theoretical ML research and practical deployment concerns. By organizing the space of inference algorithms, it makes it easier to identify which approaches are most suitable for different use cases.

I think the most valuable contribution is highlighting how inference-time compute scaling offers a complementary path to improving LLM outputs beyond just training larger models. This could be especially relevant for researchers working with fixed, pre-trained models.

TLDR: Comprehensive survey organizing inference-time algorithms for LLMs into unified framework spanning token-level generation, meta-generation, and efficiency optimization. Shows how scaling inference compute offers new ways to improve outputs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/from-decoding-to-meta-generation-inference-time). Paper [here](https://arxiv.org/abs/2406.16838).",Successful-Western27,1gwflaz,https://reddit.com/r/MachineLearning/comments/1gwflaz/r_inferencetime_algorithms_for_llms_a_survey_of/,https://www.reddit.com/r/MachineLearning/comments/1gwflaz/r_inferencetime_algorithms_for_llms_a_survey_of/,2024-11-21 12:54:25,3,0.81,3,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwflaz
MachineLearning,[D] How much can we revise paper during rebuttal?,"I'm currently preparing for ICLR rebuttal, and revising paper is an option. I have fixed mostly typo, slightly change the notation of 1 or 2 variable a bit (index, transposed ,..). Can anyone give me some advices, can this result in negative impression to the reviewer/AC? Should we revise at all or not?",Competitive_Newt_100,1gwf6t6,https://reddit.com/r/MachineLearning/comments/1gwf6t6/d_how_much_can_we_revise_paper_during_rebuttal/,https://www.reddit.com/r/MachineLearning/comments/1gwf6t6/d_how_much_can_we_revise_paper_during_rebuttal/,2024-11-21 12:32:43,4,0.76,4,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gwf6t6
MachineLearning,[D] New time series forecasting datasets - what properties should I report in the paper?,"I'm working on a paper with new datasets for time series forecasting. They are both uni- and multivariate. I'm thinking about what properties should I analyze and report in the paper. Goal is to create a benchmark.

So far I have:

* total length (# time steps)
* train and test length
* evaluation approach, e.g. temporal train/test split, expanding window (with given horizon and step)
* resolution, e.g. hourly, daily, monthly
* metric, e.g. MAE, MASE
* cross-series correlations (multivariate only)
* comparison of train and test value distributions (maybe univariate only)
* seasonality, stationarity (with statistical tests)
* causality testing, e.g. Granger, Toda-Yamamoto

Also some basic baselines, statistical forecasting methods, and popular neural networks.

Do you think something else would also be useful?",qalis,1gwct36,https://reddit.com/r/MachineLearning/comments/1gwct36/d_new_time_series_forecasting_datasets_what/,https://www.reddit.com/r/MachineLearning/comments/1gwct36/d_new_time_series_forecasting_datasets_what/,2024-11-21 09:59:47,1,0.6,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gwct36
MachineLearning,[D] Next big thing in Time series?,"In NLP, we’ve seen major milestones like transformers, GPT, and LLMs, which have revolutionized the field. Time series research seems to be borrowing a lot from NLP and CV—like transformer-based models, self-supervised learning, and now even foundation models specifically for time series. But there doesn’t seem to be a clear consensus yet on what works best. For example, NLP has well-accepted pretraining strategies like masked language modeling or next-token prediction, but nothing similar has become a standard for time series.  
  
Lately, there’s been a lot of talk about adapting LLMs for time series or even building foundation models specifically for the purpose. On the other hand, some research indicates that LLMs are not helpful for time series. 

So I just wanna know what can be a game changer for time series!",Few-Pomegranate4369,1gwbhxq,https://reddit.com/r/MachineLearning/comments/1gwbhxq/d_next_big_thing_in_time_series/,https://www.reddit.com/r/MachineLearning/comments/1gwbhxq/d_next_big_thing_in_time_series/,2024-11-21 08:20:38,118,0.96,118,0,55,0,0,False,False,True,False,False,Discussion,self,t3_1gwbhxq
MachineLearning,[R] Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters,"[https://arxiv.org/pdf/2410.24190](https://arxiv.org/pdf/2410.24190)

https://preview.redd.it/9h1ixk8tl62e1.png?width=775&amp;format=png&amp;auto=webp&amp;s=a48f0fbe62599ae5cb53f595e2ed663d4bfec1c7

How could LLMs influence our democracy? We investigate LLMs’ political leanings and the potential influence of LLMs on voters by conducting multiple experiments in a U.S. presidential election context. Through a voting simulation, we first demonstrate 18 open and closed-weight LLMs’ political preference for a Democratic nominee over a Republican nominee. We show how this leaning towards the Democratic nominee becomes more pronounced in instruction-tuned models compared to their base versions by analyzing their responses to candidate-policy related questions. We further explore the potential impact of LLMs on voter choice by conducting an experiment with 935 U.S. registered voters. During the experiments, participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse. This effect is larger than many previous studies on the persuasiveness of political campaigns, which have shown minimal effects in presidential elections. Many users also expressed a desire for further political interaction with LLMs. Which aspects of LLM interactions drove these shifts in voter choice requires further study. Lastly, we explore how a safety method can make LLMs more politically neutral, while raising the question of whether such neutrality is truly the path forward.",ChangeRelevant1378,1gw7t2x,https://reddit.com/r/MachineLearning/comments/1gw7t2x/r_hidden_persuaders_llms_political_leaning_and/,https://www.reddit.com/r/MachineLearning/comments/1gw7t2x/r_hidden_persuaders_llms_political_leaning_and/,2024-11-21 04:22:50,0,0.46,0,0,7,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/FAPec-HxKz1YkDQPJ1DHMmuhx6-4osLfMITGktpArlw.jpg,t3_1gw7t2x
MachineLearning,[D] Struggling to Transition to PhD,"**“Undergrad is about answering questions, while a PhD is about finding one.”** —Someone

I'm a first-year CS PhD student, but I feel stuck in the mindset of an undergrad. I excel at solving problems, as shown by my perfect GPA. However, when it comes to research, I struggle. If I enter a new area, **I typically read a lot of papers, take notes, and end up capable of writing a decent survey—but I rarely generate fresh ideas.**

Talking to other PhD students only adds to my frustration; one of them claims they can even come up with LLM ideas during a Latin class. My advisor says research is more about perseverance than talent, but I feel like I’m in a loop: I dive into a new field, produce a survey, and get stuck there.

I’m confident in my intelligence, but I’m questioning whether my workflow is flawed (e.g., maybe I should start experimenting earlier?) or if I’m just not cut out for research. Coming up with marginal improvements or applying A to B feels uninspiring, and I struggle to invest time in such ideas.

How do you CS (ML) PhD students come up with meaningful research ideas? Any advice on breaking out of this cycle?",StraightSpeech9295,1gw61tk,https://reddit.com/r/MachineLearning/comments/1gw61tk/d_struggling_to_transition_to_phd/,https://www.reddit.com/r/MachineLearning/comments/1gw61tk/d_struggling_to_transition_to_phd/,2024-11-21 02:50:51,149,0.94,149,0,56,0,0,False,False,True,False,False,Discussion,self,t3_1gw61tk
MachineLearning,[P] Enhancing LLM Safety with Precision Knowledge Editing (PKE),"I've been working on a project called PKE (Precision Knowledge Editing), an open-source method to improve the safety of LLMs by reducing toxic content generation without impacting their general performance. It works by identifying ""toxic hotspots"" in the model using neuron weight tracking and activation pathway tracing and modifying them through a custom loss function.

If you're curious about the methodology and results, we've also published a [paper](https://arxiv.org/pdf/2410.03772) detailing our approach and experimental findings. It includes comparisons with existing techniques like Detoxifying Instance Neuron Modification (DINM) and showcases PKE's significant improvements in reducing the Attack Success Rate (ASR).

The project is open-source, and I'd love your feedback! The GitHub repo features a Jupyter Notebook that provides a hands-on demo of applying PKE to models like Meta-Llama-3-8B-Instruct: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models)

If you're interested in AI safety, I'd really appreciate your thoughts and suggestions. Thanks for checking it out!",lial4415,1gw5w3d,https://reddit.com/r/MachineLearning/comments/1gw5w3d/p_enhancing_llm_safety_with_precision_knowledge/,https://www.reddit.com/r/MachineLearning/comments/1gw5w3d/p_enhancing_llm_safety_with_precision_knowledge/,2024-11-21 02:43:12,4,0.7,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1gw5w3d
MachineLearning,[R] Transposed matrix of the matrix containing the probabilities not changing despite loss term? ,"Hello,

I’ll keep it short. Say we have a neural network with a layer that outputs probabilities using a softmax. This gives us a [batch size, probabilities] tensor. Lets call it P

If I do P_transposed x P, I get a PxP matrix.
My loss uses the Frobenius norm to enforce that this PxP matrix is diagonal (so the off-diagonal values are 0). My hope is that this directly impacts the original matrix’s P structure.

However, this is not the case the PxP matrix does not approach a digital structure nor does P get impacted. This is the case even if I scale the loss by 100.

I would think this would work, am I wrong? Would this not indirectly affect our P matrix? 
Thanks! ",Grand_Comparison2081,1gw2zpw,https://reddit.com/r/MachineLearning/comments/1gw2zpw/r_transposed_matrix_of_the_matrix_containing_the/,https://www.reddit.com/r/MachineLearning/comments/1gw2zpw/r_transposed_matrix_of_the_matrix_containing_the/,2024-11-21 00:27:15,0,0.38,0,0,4,0,0,False,False,True,False,False,Research,self,t3_1gw2zpw
MachineLearning,[D] Is the maths deduction in the Smaug paper valid?,"The [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive ](https://arxiv.org/pdf/2402.13228)paper identifies that DPO can reduce the model’s likelihood of the preferred completions when there are small edit distances between pairs of completions.

In their theoretical analysis of this phenomenon, one of the main steps is derived by ""restricting the attention to just the logits,"" which, to my understanding, derives a partial derivative of DPO loss given the attention logits on each token in the vocabulary. (Appendix B.1 in the paper, and here's a screenshot for part of it.)

https://preview.redd.it/3s250pg0o42e1.png?width=2072&amp;format=png&amp;auto=webp&amp;s=c1a92c3474d9dd235e2e72d1615696e69e843676

However, the loss should optimize the model parameters, and the deductions in this paper assume that those attention logits are independent variables, making me think their derivation is invalid. I'm not a math major, so I'm not sure whether my thoughts are correct.  
",StraightSpeech9295,1gw0l7f,https://reddit.com/r/MachineLearning/comments/1gw0l7f/d_is_the_maths_deduction_in_the_smaug_paper_valid/,https://www.reddit.com/r/MachineLearning/comments/1gw0l7f/d_is_the_maths_deduction_in_the_smaug_paper_valid/,2024-11-20 22:00:53,12,0.81,12,0,11,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/kpg-7672g9h9ZUNop8jW7QHAe0Z8UziJv1q407oHO-k.jpg,t3_1gw0l7f
MachineLearning,[R] ITCMA-S: A Multi-Agent Architecture for Emergent Social Behavior and Group Formation,"I read an interesting paper proposing a novel architecture for studying emergent social behavior in multi-agent systems. The key technical contribution is introducing ""generative multi-agents"" that can dynamically form social structures without explicit programming.

The core technical components:
- A three-layer agent architecture combining perception, memory, and decision-making
- Novel ""social perception module"" that allows agents to model others' mental states
- Memory system that integrates both episodic and semantic information
- Action selection based on both individual goals and social context

Main experimental results:
- Agents spontaneously developed hierarchical social structures
- Social norms emerged through repeated interactions
- Different ""cultures"" formed in isolated agent groups
- Agents showed evidence of both cooperative and competitive behaviors
- Social learning occurred through observation and imitation

The implications I think matter most for multi-agent systems and social AI research. The architecture demonstrates that complex social behaviors can emerge from relatively simple building blocks, so it suggests potential paths toward more human-like AI systems. The results also provide a computational framework for studying how societies form and evolve.

From a practical perspective, this work could inform the development of more sophisticated multi-agent systems for applications like social simulation, game AI, and robotic swarms.

TLDR: New architecture allows AI agents to spontaneously develop social structures and norms without explicit programming. Results show emergence of hierarchies, cultures, and social learning.

[Full summary is here](https://aimodels.fyi/papers/arxiv/can-agents-spontaneously-form-society-introducing-novel). Paper [here](https://arxiv.org/abs/2409.06750).",Successful-Western27,1gvyfe4,https://reddit.com/r/MachineLearning/comments/1gvyfe4/r_itcmas_a_multiagent_architecture_for_emergent/,https://www.reddit.com/r/MachineLearning/comments/1gvyfe4/r_itcmas_a_multiagent_architecture_for_emergent/,2024-11-20 19:50:26,10,0.92,10,0,2,0,0,False,False,True,False,False,Research,self,t3_1gvyfe4
MachineLearning,[D] PhD in RL/ML Theory or LLM,"Hi guys,

I'm at a crossroads in my academic journey and would appreciate the community's insights. I'm trying to decide between pursuing a PhD focused on reinforcement learning/ML theory versus specializing in large language models with more experimental/applied research (these are the only two offers I had).

# Key considerations are the following:

# Research Impact

* RL/ML Theory: Foundational work that could advance the field's mathematical understanding
* LLMs: Direct applications in today's most transformative AI systems

# Job Prospects

* Theory: Academia, research labs, potentially more limited industry roles
* LLMs: High industry demand, active research area in both academia and industry

# Long-term Relevance

* Theory: Core principles likely to remain valuable regardless of specific technologies
* LLMs: Currently revolutionary but uncertain long-term trajectory

Personal background

* I'm an international student and about to finish my master program in US, so I no longer has enough time before making the final decision. I used to research in ml theory, but did not end up with a real top conference publication in theory. I personally doubt if I have enough mathematical background to pursue a successful PhD in this area (e.g., at least publish 2 theory papers a year on ICML/NeurIPS/ICLR/COLT/AISTATS). At the same time, I am personally doubting if theory works indeed advance the ML/AI community, as many papers are just proving vacuous bounds or propose some new algorithms that themselves cannot even implement or experimentally tested.
* I also used to research in more applied ml, with one aaai paper. My personal concerns is that I'm not fast at implementation and coding, the most strategic ability for a successful applied ML researcher. After we entered the LLM era, the pacing or applied ML research (especially in LLM and CV) becomes so fast. It's like competitive programming in research community (well, also the #GPUs competition).",Living_Imagination84,1gvx8vx,https://reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/,https://www.reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/,2024-11-20 19:02:11,52,0.82,52,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1gvx8vx
MachineLearning,[R] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models,,rcparts,1gvveu8,https://reddit.com/r/MachineLearning/comments/1gvveu8/r_procedural_knowledge_in_pretraining_drives/,https://arxiv.org/abs/2411.12580,2024-11-20 17:47:51,35,0.94,35,0,3,0,0,False,False,False,False,False,Research,default,t3_1gvveu8
MachineLearning,[D] ICASSP 2025 reviews are due today! ,A friendly banter to discuss the icassp reviews! Hoping for the best!,always_been_a_toy,1gvqvgd,https://reddit.com/r/MachineLearning/comments/1gvqvgd/d_icassp_2025_reviews_are_due_today/,https://www.reddit.com/r/MachineLearning/comments/1gvqvgd/d_icassp_2025_reviews_are_due_today/,2024-11-20 14:35:39,28,0.94,28,0,56,0,0,False,False,True,False,False,Discussion,self,t3_1gvqvgd
MachineLearning,[R] About dual submission in AI conferences.. help,"Hi, my advisor and I am new to this area, has no experience on submission via openreview.

I submitted a paper to AAAI and ICLR, and I should have cancelled ICLR one, but did not.

so its desk-rejected, and ICLR make it accessible publicly.

I'm concerning that when I try later, on other AI conferences (via openreview or CMT), would it be also desk-rejected because its now publicly accessible?

  
Thank you for any advice :) I'm suffering from it because I can't get clear answer from anyone I physically know...",catndante,1gvisp9,https://reddit.com/r/MachineLearning/comments/1gvisp9/r_about_dual_submission_in_ai_conferences_help/,https://www.reddit.com/r/MachineLearning/comments/1gvisp9/r_about_dual_submission_in_ai_conferences_help/,2024-11-20 05:53:54,0,0.46,0,0,8,0,0,False,False,True,False,False,Research,self,t3_1gvisp9
MachineLearning,[D] OpenAI's CLIP alternative,"Hi, Are there any new recent SOTA model like CLIP? I want to do similarity search on images, but CLIP's performance is not very good for my project.

I currently use: CLIP-ViT-B-32-laion2B-s34B-b79K

Embeddings which also capture colour would be perfect. Thanks.",CaptTechno,1gvlgxm,https://reddit.com/r/MachineLearning/comments/1gvlgxm/d_openais_clip_alternative/,https://www.reddit.com/r/MachineLearning/comments/1gvlgxm/d_openais_clip_alternative/,2024-11-20 09:08:43,32,0.9,32,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1gvlgxm
MachineLearning,[D] Cerebras Inference Results for 405B,"Cerebras has just shared some very interesting results on LLM inference. I was first skeptical and thought maybe they used some large batch sizes or some trick to hit almost 1k tokens/s for llama 405B. I tested llama-70B on their website. It's really fast...

I've been reading up on their published paper, but there haven't shared any details on how they run a 405B  parameter model on this huge chip. They have 40GB SRAM, which is huge, but running a 405B model at such low latency and high throughput still sounds interesting. Their papers discuss weight streaming. I think they must have used some advanced data flow analyses to keep the compute busy from the off-chip memory where this huge can be stored.

Does anyone know where I can get more information on this?

Ref: [https://cerebras.ai/blog/llama-405b-inference](https://cerebras.ai/blog/llama-405b-inference)

Paper: [https://arxiv.org/abs/2409.00287](https://arxiv.org/abs/2409.00287)

White Paper: [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10123162](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10123162)

Disclaimer: I have nothing to do with Cerebras systems, just genuinely interested and curious about this. This feels like a pretty big deal for AI in general.",JanGehlYacht,1gvjrmo,https://reddit.com/r/MachineLearning/comments/1gvjrmo/d_cerebras_inference_results_for_405b/,https://www.reddit.com/r/MachineLearning/comments/1gvjrmo/d_cerebras_inference_results_for_405b/,2024-11-20 06:59:01,21,0.83,21,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gvjrmo
MachineLearning,[D] [R] Utilizing an LLM as a de-noising step in an Autoencoder to better understand time-series to language modality matching? Please give feedback!,"Hi all, I was hoping some of you could give feedback to an idea that's been on y mind for a while! There's been a lot of recent advancement in the LLM-for-timeseries space (because of course there is...), which has been hard to ignore. In particular, [Time-LLM ](https://arxiv.org/abs/2310.01728)produced SOTA results for time-series forecasting by reprogramming time-series patch embeddings to produce representations over the text-embedding space an fed them to a frozen LLM's attention + MLP layers, turning the output into a forecast. This is a bit of a surprising result for me as it is not a natural idea, and it is confusing how such distinct modalities can align in such a way.

The proposed project aims to utilize a similar strategy for the multiple imputation of time-series task via using a basic LLM such as BERT as the de-noising step in an autoencoder, treating the word embedding space as a latent space to encode onto. We utilize the decoder to decode both LLM input and output, targeting observation reconstruction for the input and masked imputation for the output. We can then examine the latent space produced, and compare it to latent spaces produced by more traditional models.

I mainly want feedback on the following points:

1. Interpretability: Theres a pretty large gap in theoretical understanding of how these modalities align; in the text-to-visual case, it's easier to understand how modality matching is feasible as language is inherently descriptive and caries *visual* meaning (i.e. words for colors, shapes, animals, things can be visualized). As such, Time-LLM was heavily critiqued for gaps in theoretical grounding and interpretability, despite empirical performance. By using an autoencoder structure that attempts to preserve observations in this way do you think these concerns can be adequately addressed? 

2. Latent Space Cartography: I am not so well versed in studying latent spaces, and have not been able to understand literature relating to it (that I have found). Does anyone know if there's any useful tool / papers that I can utilize for better understanding the properties of the latent space my model would uncover? 

3. Usefulness: Is this idea even a good, useful idea? Or is my subconscious being hooked by the LLM craze T\_T

Thanks for taking the time to read this! Any and all feedback would be appreciated. ",TheOneYourSon,1gvcvkc,https://reddit.com/r/MachineLearning/comments/1gvcvkc/d_r_utilizing_an_llm_as_a_denoising_step_in_an/,https://www.reddit.com/r/MachineLearning/comments/1gvcvkc/d_r_utilizing_an_llm_as_a_denoising_step_in_an/,2024-11-20 00:41:21,0,0.43,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gvcvkc
MachineLearning,[N] Open weight (local) LLMs FINALLY caught up to closed SOTA?,"Yesterday Pixtral large dropped [here](https://mistral.ai/news/pixtral-large/).

It's a 124B multi-modal vision model. This very small models beats out the 1+ trillion parameter GPT 4o on various cherry picked benchmarks. Never mind the Gemini-1.5 Pro. 

As far as I can tell doesn't have speech or video. But really, does it even matter? To me this seems groundbreaking. It's free to use too. Yet, I've hardly seen this mentioned in too many places. Am I missing something?  
  
BTW, it still hasn't been 2 full years yet since ChatGPT was given general public release November 30, 2022. In barely 2 years AI has become somewhat unrecognizable. Insane progress.

  
**\[Benchmarks Below\]**

https://preview.redd.it/ebo9qp0rzy1e1.png?width=1777&amp;format=png&amp;auto=webp&amp;s=3d47183ba7e2af69eb52fc5f8d755f105cb52004

https://preview.redd.it/woc0wmrozy1e1.png?width=1852&amp;format=png&amp;auto=webp&amp;s=1bc5d380e2deebfd03684e1a8341254d18596d8e

  
",AIAddict1935,1gvfpdw,https://reddit.com/r/MachineLearning/comments/1gvfpdw/n_open_weight_local_llms_finally_caught_up_to/,https://www.reddit.com/r/MachineLearning/comments/1gvfpdw/n_open_weight_local_llms_finally_caught_up_to/,2024-11-20 03:00:24,61,0.8,61,0,23,0,0,False,False,True,False,False,News,https://b.thumbs.redditmedia.com/y2wBi86uF1XeHXnTzePDzht-bTJzYHo2bv7tMDBfyTc.jpg,t3_1gvfpdw
MachineLearning,"[R] BiomedParse is a new biomedical foundation AI model for holistic image analysis that can jointly conduct recognition, detection, and segmentation for 64 major object types across 9 imaging modalities in medicine, outperforming prior state-of-the-art methods.","https://microsoft.github.io/BiomedParse/


https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/


https://youtu.be/WUPUypgmB-s
",Happysedits,1gvccdy,https://reddit.com/r/MachineLearning/comments/1gvccdy/r_biomedparse_is_a_new_biomedical_foundation_ai/,https://www.reddit.com/r/MachineLearning/comments/1gvccdy/r_biomedparse_is_a_new_biomedical_foundation_ai/,2024-11-20 00:16:25,41,0.86,41,0,1,0,0,False,False,True,False,False,Research,self,t3_1gvccdy
MachineLearning,[R] End to end learned planner for AVs,"I am trying to learn more on end to end learned systems. My background is on the perception &amp; mapping side, planning is kind of my blind spot. I am having trouble understanding how some of these planner approaches are implemented in practice. I've read the MP3 paper and the Lift,Splat,Shoot papers in detail. As much as I understand the premise of both approaches I am failing to understand how they have implemented the planning step. I was unable to find a repo which attempts implementing these to see how they had done it.

Can someone point me the way to replicate the implementations there at least at a high level? If there is a better place to get started on this topic please point that out too.

From the LSS and MP3 papers, what I don't understand is what kind of network architecture they might have used and anything else I should consider here that is not perhaps explained in the paper well.

What I understand from the papers is: They both do binning of expert trajectories so that they can score and select the best trajectory according to their loss functions. This step looks more like classification over template trajectories than anything else to me. They both rely on the intermediate representations as part of their cost functions. Both papers explain the loss functions (mp3 is a lot more involved than LSS) but they do not explain how they implemented the respective networks. MP3 rolls out a bicycle model according to the initial conditions of the ego vehicle over the selected trajectory, where LSS uses the trajectory directly.",HonestConcentrate947,1gv8b5i,https://reddit.com/r/MachineLearning/comments/1gv8b5i/r_end_to_end_learned_planner_for_avs/,https://www.reddit.com/r/MachineLearning/comments/1gv8b5i/r_end_to_end_learned_planner_for_avs/,2024-11-19 21:20:43,2,0.58,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1gv8b5i
MachineLearning,[R] Automating Python Package Creation with Agentic Workflow,"**Research**

PyGen is an open-source tool designed to automate the generation of Python packages from user-provided prompts. By leveraging advanced language models, PyGen streamlines the development process, producing packages complete with testing and documentation. This approach has been applied to create tools such as AutoML (automated machine learning), AutoVision (computer vision), AutoSpeech, and Quantum Error Correction utilities.

**Key Contributions:**

* **Automated Package Generation:** PyGen simplifies the creation of Python packages by generating code, tests, and documentation based on user inputs.
* **Advanced Language Model Integration:** Utilizes sophisticated language models to interpret prompts and produce relevant code structures.
* **Versatile Applications:** Demonstrated effectiveness in developing diverse tools, including AutoML, AutoVision, AutoSpeech, and Quantum Error Correction utilities.

**Resources:**

* **Paper:** [here](https://www.arxiv.org/abs/2411.08932)
* **GitHub Repository:** [here](https://github.com/GitsSaikat/PyGen)
* **License:** MIT License

We hope this tool proves useful for your projects. Feel free to explore the resources and share your feedback or questions.",Any_Code_4027,1gv4hnv,https://reddit.com/r/MachineLearning/comments/1gv4hnv/r_automating_python_package_creation_with_agentic/,https://www.reddit.com/r/MachineLearning/comments/1gv4hnv/r_automating_python_package_creation_with_agentic/,2024-11-19 18:41:09,0,0.33,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1gv4hnv
MachineLearning,[D] Optimal strategy for high volume image loading.,"Hi all,

Curious about what folks think but I'm trying to sample ~ 1-2 million images per epoch on a pretty modern home workstation (7950, 4090, NVMe drives). I started off as a baseline metric of randomly reading jpegs. This got me to about 5ms per image by optimizing decompression libraries, pre-processing, and so on so all I need to do is read off disk.

To push this further I loaded about 10k images into a numpy array and saved them in sequential block with the thought that each would represent chunks of a fold and I can shuffle and randomly sample from this batch that I pre-generate. In doing so I get an average of 1ms per image or so.

I tried to use threading since I thought this was mostly an IO bound task and if I run two threads I get ~20% speed up and loading at .7ms per image. 

The problem I see is that it's still ~18 minutes per epoch which is a bit slower than I was hoping. I think the current issue is that the sequential 10k image blocks are compressed numpy arrays (similar performance if I use hd5) and that multi threading gets me SOME benefit but the decompression is cpu limited.

The next thought I had was well I can use multi processing to help since it's mostly CPU bound, but have had very little luck getting this to work as the transfer times between processes (decompressed data being copied back from the separate process) is pretty costly.

So my general question, do folks have some ideas/approaches that would help? Memory mapped databases don't really help since I'm doing basically single hits on all these files and I need to intelligently cache and not just let the kernel do it. I've been thinking about offloading to a C++ so that can handle some more of this, but managing the GIL and debugging have been a nightmare so less included to go this way.",R2FuckYou,1gv3xif,https://reddit.com/r/MachineLearning/comments/1gv3xif/d_optimal_strategy_for_high_volume_image_loading/,https://www.reddit.com/r/MachineLearning/comments/1gv3xif/d_optimal_strategy_for_high_volume_image_loading/,2024-11-19 18:18:41,7,0.82,7,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1gv3xif
MachineLearning,[Discussion] : Anomaly Detection Data Differences and suggestions,"Hello fellow Redditors, I am a student who is trying to do an unsupervised anomaly detection on a telecommunication stack data. I have different types of messages that include different features with different lengths. For example,

PDSCH, have n-many real-valued numerical values.  
PDCP DL, have m-many real-valued numerical values.  
… and so on.

What I'm trying to find is to pinpoint the lines (which is either of these message types), and the feature of that message that contributes most to the anomaly score. Currently, I have a LSTM-VAE model that I developed, but it doesn't quite well. I have a question for the representation of the data. Which one should I choose, assuming the message types are not heavily dependent? They have some implicit dependency, but not directly visible to humans.

1- Separate encoder-decoders for each message type that creates the same dimensional embedding latent space.  
2- Shared embedding space whose dimensionality is the maximum dimensionality of the message types. For example, if the most number of features are on PDSCH with n, then the embedding would be n-by-d with other message types vectors are padded with zeros.

Also, I'm open to all suggestions on detecting the anomalies, currently my plan is to use a 95-percentile as threshold and marking the others as anomaly. For each message type calculate the normalized loss and find the most contributing feature of each anomaly message type.

Also, since we don't have any labels for the anomalies, I am planning to create a online-learning mechanism to label the anomalies on the run if they are good or bad, so to decrease the manual labelling effort. Do you think is this a good idea?

Thank you so much if you read so far. I appreciate any help for this miserable student. :)",hezarfenserden,1gv21ox,https://reddit.com/r/MachineLearning/comments/1gv21ox/discussion_anomaly_detection_data_differences_and/,https://www.reddit.com/r/MachineLearning/comments/1gv21ox/discussion_anomaly_detection_data_differences_and/,2024-11-19 17:02:55,2,0.75,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gv21ox
MachineLearning,[D] A mature agentic system for common white color work tasks,"Many ""white collar""  tasks - e.g., checking / responding to email / slack messages, etc., moving meetings around, writing ""memos"", etc., - can be emulated / effectively dealt with in principle.   I've seen plenty of ""demos"" around illustrating some of these components  / functions.

Are there any projects / apps are out there that flush this out more fully?  e.g., any [llama stack](https://github.com/meta-llama/llama-stack) projects?",neonwatty,1gv0us8,https://reddit.com/r/MachineLearning/comments/1gv0us8/d_a_mature_agentic_system_for_common_white_color/,https://www.reddit.com/r/MachineLearning/comments/1gv0us8/d_a_mature_agentic_system_for_common_white_color/,2024-11-19 16:14:20,1,0.6,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gv0us8
MachineLearning,[D] Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Link to the paper: [https://arxiv.org/pdf/2411.10436](https://arxiv.org/pdf/2411.10436)  
  
Link to the podcast summarizing the paper: [https://youtu.be/w993gQ4TjSU](https://youtu.be/w993gQ4TjSU)  
  
The paper is titled ""Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization"" authored by researchers from Renmin University of China and Tencent.  
  
Key Points  
  
This research presents a novel approach called Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in Multimodal Large Language Models (MLLMs). The paper addresses three key causes of hallucinations:  
  
Insufficient Visual Capabilities: When MLLMs' visual encoders lack strength and get distracted by unimportant visual information  
Long Context Generation: Hallucinations increase as generated content grows longer  
Multimodal Conflicts: Conflicts between text and image information lead to hallucinations  
  
Methodology  
  
The researchers developed three types of preference pair data targeting these hallucination causes:  
  
Visual Distracted Hallucination (VDH): Preserves only low-attention visual tokens to produce targeted negative responses  
Long Context Hallucination (LCH): Creates negative examples where later parts of responses deviate from image content  
Multimodal Conflict Hallucination (MCH): Adds conflicting information to prompts to generate negative examples  
  
The experimental results demonstrate that HDPO achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art methods while requiring significantly less training data.",Busy-Basket-5291,1guzluu,https://reddit.com/r/MachineLearning/comments/1guzluu/d_mitigating_hallucination_in_multimodal_large/,https://www.reddit.com/r/MachineLearning/comments/1guzluu/d_mitigating_hallucination_in_multimodal_large/,2024-11-19 15:22:04,4,0.64,4,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1guzluu
MachineLearning,[R] LLaVA-o1: Multi-Stage Visual Reasoning through Inference-Time Scaling,"I've been analyzing this new approach to visual reasoning that enhances vision-language models through step-by-step reasoning capabilities. The key innovation is integrating chain-of-thought prompting techniques with visual analysis, allowing models to break down complex visual tasks into discrete reasoning steps.

The main technical components and results:

- Integration of chain-of-thought prompting with vision-language models
- Two-stage process: general visual understanding followed by task-specific reasoning
- Achieves 15% improvement on complex visual reasoning benchmarks
- Architecture combines visual encoder with language model for staged processing
- Uses contrastive learning to align visual and textual representations

The methodology is particularly effective for:
- Counting tasks requiring sequential attention
- Comparative analysis between multiple objects
- Spatial reasoning problems
- Multi-step visual problem solving

Key implementation details:
- Built on LLaVA architecture with enhanced reasoning modules
- Uses CLIP-based visual encoder
- Implements temperature-controlled sampling for reasoning steps
- Employs beam search for answer generation

Mostly matters for applications requiring detailed visual analysis. The step-by-step approach makes the reasoning process more transparent and debuggable, though at the cost of increased computational overhead. This tradeoff between performance and efficiency will need consideration for real-world deployments.

TLDR: New method improves visual reasoning in vision-language models by implementing step-by-step analysis, achieving 15% better performance on complex tasks while providing more transparent reasoning processes.

[Full summary is here](https://aimodels.fyi/papers/arxiv/llava-o1-let-vision-language-models-reason). Paper [here](https://arxiv.org/abs/2411.10440).",Successful-Western27,1guyrr8,https://reddit.com/r/MachineLearning/comments/1guyrr8/r_llavao1_multistage_visual_reasoning_through/,https://www.reddit.com/r/MachineLearning/comments/1guyrr8/r_llavao1_multistage_visual_reasoning_through/,2024-11-19 14:45:40,2,0.63,2,0,1,0,0,False,False,True,False,False,Research,self,t3_1guyrr8
MachineLearning,[D] Looking for a Realistic German Text-to-Speech Voice (Alternative to Edge-TTS),"Hi everyone,

I’m looking for a way to generate realistic German speech that doesn’t rely on Edge-TTS. While I’ve been using Edge-TTS, I’m not satisfied with the quality and need something that sounds more natural. Ideally, I’m looking for a solution that works locally and is not cloud-based. Has anyone here had experience with tools or services that offer high-quality German TTS voices?

Here are some key points I’m looking for:
- The voice must be German.

- The voice should sound as realistic and natural as possible.

- The tool should be flexible and easy to use.

- It needs to be a local solution, not cloud-based.

- Cost is not a primary concern – I’m looking for the best option available.

I’d really appreciate any suggestions or experiences you can share!
",yeah280,1guxo18,https://reddit.com/r/MachineLearning/comments/1guxo18/d_looking_for_a_realistic_german_texttospeech/,https://www.reddit.com/r/MachineLearning/comments/1guxo18/d_looking_for_a_realistic_german_texttospeech/,2024-11-19 13:55:09,6,0.88,6,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1guxo18
MachineLearning,[P] Collection of SOTA TTS models,"As part of an ongoing project, I released what I think is the biggest collection of open-source voice-cloning TTS models here: [https://github.com/ttsds/datasets](https://github.com/ttsds/datasets)

I think it's very interesting how we haven't really reached a consensus on the rough ""best"" architecture for TTS yet, although I personally think audio token LLM-like approaches (with text prompts for style) will be the way forward.

https://preview.redd.it/2yru8a4oiu1e1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=73d48db7ce384e556e963385898c7f901d58c495

I'm currently evaluating the models across domains, will  be a more substantial post here when that's done :)

Edit: Also some trends (none of them surprising) that can be observed - we seem to be moving away from predicting prosodic correlates and training on only LibriVox data. Grapheme2Phoneme seems to be here to stay though (for now?)

Edit2: An older version of the benchmark with fewer models and only audiobook speech is available here: [https://huggingface.co/spaces/ttsds/benchmark](https://huggingface.co/spaces/ttsds/benchmark)",cdminix,1guv9jl,https://reddit.com/r/MachineLearning/comments/1guv9jl/p_collection_of_sota_tts_models/,https://www.reddit.com/r/MachineLearning/comments/1guv9jl/p_collection_of_sota_tts_models/,2024-11-19 11:45:27,37,0.89,37,0,5,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/PHihsfcsfAnEMoCYpzoamyk6xUb2_TYynph7c3JBQfc.jpg,t3_1guv9jl
MachineLearning,[R] Dialog2Flow: Pre-training Soft-Contrastive Sentence Embeddings for Automatic Dialog Flow Extraction,"[This paper](https://aclanthology.org/2024.emnlp-main.310/), presented at EMNLP 2024 main conference, introduces a novel sentence embedding model that captures both the semantics and communicative intention of utterances. This allows for the modeling of conversational ""steps"" and thus the extraction of dialog flows.

**Key Contributions:**

* **Intent-Aware Embeddings:** The model encodes utterances with a richer representation that includes their intended communicative purpose ([available in Hugging Face](https://huggingface.co/collections/sergioburdisso/dialog2flow-67162ca33155cb90a533b7fa)).
* **Dialog Flow Extraction:** By clustering utterance embeddings, the model can automatically identify the ""steps"" or transitions within a conversation, effectively generating a dialog flow graph ([Github code available](https://github.com/idiap/dialog2flow)).
* **Soft-Contrastive Loss:** The paper introduces a new supervised loss function that can be beneficial for representation learning tasks with numerous labels ([implementation available](https://github.com/idiap/dialog2flow?tab=readme-ov-file#chart_with_downwards_trend-proposed-soft-contrastive-loss)).
* **Dataset:** A collection of 3.4 million utterances annotated with ground truth intent ([available in Hugging Face](https://huggingface.co/datasets/sergioburdisso/dialog2flow-dataset)).

**Resources:**

* **Paper:** [here](https://aclanthology.org/2024.emnlp-main.310/)
* **Github repo:** [here](https://github.com/idiap/dialog2flow) (including code to replicate paper and generate also [the interactive 3D Voronoi plots for sentence embeddings](http://tworld.io/extra/dialog2flow_example/voronoi_user_dialog2flow-joint-bert-base.html))
* **Hugging Face models:** [here](https://huggingface.co/collections/sergioburdisso/dialog2flow-67162ca33155cb90a533b7fa)
* **Hugging Face dataset:** [here](https://huggingface.co/datasets/sergioburdisso/dialog2flow-dataset)
* **License:** MIT License

Hope some of you find it useful and let me know if you have any questions or thoughts! :)",sergbur,1guus4a,https://reddit.com/r/MachineLearning/comments/1guus4a/r_dialog2flow_pretraining_softcontrastive/,https://www.reddit.com/r/MachineLearning/comments/1guus4a/r_dialog2flow_pretraining_softcontrastive/,2024-11-19 11:12:58,5,0.78,5,0,0,0,0,False,False,True,False,False,Research,self,t3_1guus4a
MachineLearning,[D] Tips on building efficient offline video data pipelines in production?,Anyone have experience processing/analyzing large amounts of video data offline in a production setting?What does your pipeline look like right? What kinds of tooling/infrastructure do you use?,answersareallyouneed,1gusx2j,https://reddit.com/r/MachineLearning/comments/1gusx2j/d_tips_on_building_efficient_offline_video_data/,https://www.reddit.com/r/MachineLearning/comments/1gusx2j/d_tips_on_building_efficient_offline_video_data/,2024-11-19 08:56:06,6,0.81,6,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gusx2j
MachineLearning,[P] CNN Model Having High Test Accuracy but Failing in Custom Inputs,"
I am working on a project where I trained a model using SAT-6 Satellite Image Dataset (The Source for this dataset is NAIP Images from NASA) and my ultimate goal is to make a mapping tool that can detect and large map areas using satellite image inputs using sliding windows method. 

I implemented the DeepSat-V2 model and created promising results on my testing data with around %99 accuracy. 

However, when I try with my own input images I rarely get a significantly accurate return that shows this accuracy. It has a hard time making correct predictions especially its in a city environment. City blocks usually gets recognized as barren land and lakes as trees for some different colored water bodies and buildings as well. 

It seems like it’s a dataset issue but I don’t get how 6 classes with 405,000 28x28 images in total is not enough. Maybe need to preprocess data better? 

What would you suggest doing to solve this situation? 

The first picture is a google earth image input, while the second one is a picture from the NAIP dataset (the one SAT-6 got it’s data from). The NAIP one clearly performs beautifully where the google earth gets image gets consistently wrong predictions.

SAT-6: https://csc.lsu.edu/~saikat/deepsat/

DeepSat V2: https://arxiv.org/abs/1911.07747",yagellaaether,1gujazf,https://reddit.com/r/MachineLearning/comments/1gujazf/p_cnn_model_having_high_test_accuracy_but_failing/,https://www.reddit.com/gallery/1gujazf,2024-11-18 23:46:06,1,1.0,1,0,2,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/V1dyMH_CEVEhPokLqFUsGhGXa5-IKEQbh9TVxYanZoo.jpg,t3_1gujazf
MachineLearning,"[D] Eric Schmidt says that scaling laws are not yet stopping AI, what do you guys think?","This is the article in question: https://www.windowscentral.com/software-apps/theres-no-evidence-scaling-laws-have-begun-to-stop-former-google-ceo-claims-ai-systems-will-be-100-times-more-powerful

(I am sure there are far better articles on this topic, but I read this one first)",Born_Replacement_687,1guncvr,https://reddit.com/r/MachineLearning/comments/1guncvr/d_eric_schmidt_says_that_scaling_laws_are_not_yet/,https://www.reddit.com/r/MachineLearning/comments/1guncvr/d_eric_schmidt_says_that_scaling_laws_are_not_yet/,2024-11-19 03:01:50,31,0.72,31,0,56,0,0,False,False,True,False,False,Discussion,self,t3_1guncvr
MachineLearning,[D] What’s a machine learning paper or research breakthrough from the last year that everyone should know about?,Share a paper or idea that really stood out to you and why it matters to the field.,BrechtCorbeel_,1gujge8,https://reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/,https://www.reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/,2024-11-18 23:52:41,194,0.94,194,0,55,0,0,False,False,True,False,False,Discussion,self,t3_1gujge8
MachineLearning,[D] What’s the most surprising or counterintuitive insight you’ve learned about machine learning recently?,ML often challenges assumptions. What’s something you learned that flipped your understanding or made you rethink a concept?,BrechtCorbeel_,1gujfj2,https://reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/,https://www.reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/,2024-11-18 23:51:36,263,0.95,263,0,85,0,0,False,False,True,False,False,Discussion,self,t3_1gujfj2
MachineLearning,[R] Discover Awesome Conformal Prediction - Your Ultimate Resource for Conformal Prediction,"Hey everyone! 🚀

If you're diving into the world of conformal prediction or looking to expand your knowledge, I've got an awesome resource for you: \[Awesome Conformal Prediction\]([https://github.com/valeman/awesome-conformal-prediction](https://github.com/valeman/awesome-conformal-prediction)).

This GitHub repository is a curated collection of everything you need to get started with conformal prediction, including:

* 📚 Key research papers and articles
* 🔧 Tutorials and hands-on guides
* 🛠️ Tools, libraries, and code implementations
* 🎥 Videos, lectures, and other educational materials

Whether you're starting to learn conformal prediction or an expert, this repository is a fantastic place to find valuable resources and stay up-to-date with the latest developments in the field.

Check it out and star the repo if you find it useful! Let's keep the conversation going and help grow this amazing community. 🙌

Feel free to share your thoughts, questions, or any additional resources you think should be included!

Happy learning! 😊",predict_addict,1gucwxg,https://reddit.com/r/MachineLearning/comments/1gucwxg/r_discover_awesome_conformal_prediction_your/,https://www.reddit.com/r/MachineLearning/comments/1gucwxg/r_discover_awesome_conformal_prediction_your/,2024-11-18 19:17:01,0,0.38,0,0,2,0,0,False,False,True,False,False,Research,self,t3_1gucwxg
MachineLearning,[P] Small object detection without SAHI ,"Hi everyone, I hope I have come to the right place.

I am currently working on a project which needs to detect the very small objects with a messy background in phone camera. These objects only has 10\~20 pixels out of 3024 x 4032 pictures.

I have trained a yolov8 model with SAHI and tiling. To me, the results are good enough with map of 80%, making some false positive in the background but basically detect all the small ones. But my supervisor wasn't very happy about it since there is still false positives and SAHI can't work in real time in a phone.

Would you have any suggestions, that could be implement in a phone setting?",Delay_no_more_1999,1gubjpn,https://reddit.com/r/MachineLearning/comments/1gubjpn/p_small_object_detection_without_sahi/,https://www.reddit.com/r/MachineLearning/comments/1gubjpn/p_small_object_detection_without_sahi/,2024-11-18 18:22:23,5,0.79,5,0,1,0,0,False,False,True,False,False,Project,self,t3_1gubjpn
MachineLearning,[D] Why ML PhD is so competitive?,"In recent years, ML PhD admissions at top schools or relatively top schools getting out of the blue. Most programs require prior top-tier papers to get in. Which considered as a bare minimum.

On the other hand, post PhD Industry ML RS roles are also extremely competitive as well.

But if you see, EE jobs at Intel, NVIDIA, Qualcomm and others are relatively easy to get, publication requirements to get into PhD or get the PhD degree not tight at all compared to ML. And I don’t see these EE jobs require “highly-skilled” people who know everything like CS people (don’t get me wrong that I devalued an EE PhD). Only few skills that all you need and those are not that hard to grasp (speaking from my experience as a former EE graduate).

I graduated with an EE degree, later joined a CS PhD at a moderate school (QS &lt; 150). But once I see my friends, I just regret to do the CS PhD rather following the traditional path to join in EE PhD. ML is too competitive, despite having a better profile than my EE PhD friends, I can’t even think of a good job (RS is way too far considering my profile). 

They will get a job after PhD, and most will join at top companies as an Engineer. And I feel, interviews at EE roles as not as difficult as solving leetcode for years to crack CS roles. And also less number of rounds in most cases. ",AntelopeWilling2928,1gu9os9,https://reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/,https://www.reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/,2024-11-18 17:07:35,196,0.87,196,0,88,0,0,False,False,True,False,False,Discussion,self,t3_1gu9os9
MachineLearning,Free GPU/TPU powered Notebook Service [Project],"I need to do a Machine Learning project. I want to try and test out various architectures and research about them in some datasets and hopefully write a good research paper. The size of dataset is around 60GB. Is there a free good GPU powered AI/Ml Notebook service except Colab or Sagemaker Studio Lab? I want better than these. I tried Azure for Students, but it frustratingly doesn't allow me to use powerful NVIDIA GPUs. Unfortunately, my university doesn't provide any GPUs to students. Can't imagine training models in my laptop. Any help/suggestions would be really helpful.",Due-Rest6652,1gu8ej2,https://reddit.com/r/MachineLearning/comments/1gu8ej2/free_gputpu_powered_notebook_service_project/,https://www.reddit.com/r/MachineLearning/comments/1gu8ej2/free_gputpu_powered_notebook_service_project/,2024-11-18 16:15:22,4,0.65,4,0,8,0,0,False,False,True,False,False,Project,self,t3_1gu8ej2
MachineLearning,[R] Performance Analysis of GPU Interconnect Technologies Across Three Modern Supercomputer Architectures,"I found this study examining GPU-to-GPU communication in supercomputer systems to be quite informative. The key contribution is a systematic analysis of different interconnect technologies and their impact on multi-GPU performance.

The main technical findings include:

- NVLink provides highest bandwidth (up to 900 GB/s bidirectional) but comes with higher latency overhead
- InfiniBand shows lower latency (1-2μs) but reduced bandwidth compared to NVLink
- PCIe demonstrates consistent but lower performance metrics across all tests
- Topology and physical GPU arrangement significantly impact communication patterns

Some key methodology points:
- Tested multiple hardware configurations with 4-16 GPUs
- Measured bandwidth, latency, and completion time for standard communication patterns
- Analyzed impact of different data sizes and communication patterns
- Compared theoretical vs achieved bandwidth across interconnects

The practical implications I think are...

- Optimal interconnect choice depends heavily on workload characteristics
- Large model training benefits more from high bandwidth (NVLink)
- Distributed inference may prefer lower latency solutions (InfiniBand)
- Physical GPU topology should match common communication patterns

**TLDR**: Comprehensive analysis of GPU interconnect performance showing tradeoffs between bandwidth, latency, and topology. Results suggest workload-specific optimization of interconnect choice is crucial for multi-GPU systems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/exploring-gpu-to-gpu-communication-insights-into). Paper [here](https://arxiv.org/abs/2408.14090).",Successful-Western27,1gu7swl,https://reddit.com/r/MachineLearning/comments/1gu7swl/r_performance_analysis_of_gpu_interconnect/,https://www.reddit.com/r/MachineLearning/comments/1gu7swl/r_performance_analysis_of_gpu_interconnect/,2024-11-18 15:50:20,21,0.88,21,0,1,0,0,False,False,True,False,False,Research,self,t3_1gu7swl
MachineLearning,[D] Seeking Input on Optimal Approach for Production-Ready Dimension Extraction,"\[D\]

Hi everyone,

I have been working with a below problem. I’m working with a large set of documents that contain various engineering drawings with different components, and we need to extract dimension-related information from the components in these drawings. The engineering drawing contains one or more components. First it is important to identify the component (we don't have any information about the position in engineering drawing) and then extract the dimensions present on the parts of the component.

Sample Dataset - [Kaggle](https://www.kaggle.com/datasets/sshivasagaran/two-dimensional-engineering-drawings/data)

**Issues :**  
\- The number of entities may increase in the future.

\- There is no name of the component on the engg drawing.   
\- The component can be only identified by its shape and structure. 

I’d like to ask, if you were in my position, which approach would you choose? Any feedback on the best approach for production would be greatly appreciated.",Future-Outcome3167,1gu53ui,https://reddit.com/r/MachineLearning/comments/1gu53ui/d_seeking_input_on_optimal_approach_for/,https://www.reddit.com/r/MachineLearning/comments/1gu53ui/d_seeking_input_on_optimal_approach_for/,2024-11-18 13:49:07,1,0.6,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gu53ui
MachineLearning,[D]  Dynamic learning rate question,"Here I begin a series of potentially silly questions which I can't help but post.

This one is about dealing with unbalanced datasets, e.g.  a labelled image one with some classes having lots of samples and others with very few.

Would it help to adjust the learning rate by how frequent the class of current sample is in the dataset? I mean higher learning rates for less frequent samples and lower rates for more frequent classes

In next word prediction models (llm-s) that would mean to have lower learning rate when current word is very common and higher learning rates on sparsely occurring words.",blimpyway,1gu2u1l,https://reddit.com/r/MachineLearning/comments/1gu2u1l/d_dynamic_learning_rate_question/,https://www.reddit.com/r/MachineLearning/comments/1gu2u1l/d_dynamic_learning_rate_question/,2024-11-18 11:41:58,6,0.71,6,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gu2u1l
MachineLearning,[D] Booking system for GPU with other people ,"Hi everyone,

My friends and I are working on a project: we have access to a GPU, and we want to ensure that each of us can use the GPU when needed. Do you know of any app that allows us to book time slots? Essentially, we’re looking for a shared calendar that’s convenient and easy to use.

Thanks, everyone!",SupertrampDFenx,1gu1jfx,https://reddit.com/r/MachineLearning/comments/1gu1jfx/d_booking_system_for_gpu_with_other_people/,https://www.reddit.com/r/MachineLearning/comments/1gu1jfx/d_booking_system_for_gpu_with_other_people/,2024-11-18 10:10:42,1,0.57,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gu1jfx
MachineLearning,[D] Dataset management tool?,"Hi all,

In our company there are many departments with datasets all over the place, we were hoping to find some kind of tool which we can use as a central repository for datasets. Something like hugging face which shows download count/popularity, a short summary, ability to filter/sort, ability to add a readme, etc. would be great:

  
[https://huggingface.co/datasets](https://huggingface.co/datasets)  


Does anyone know of a product that offers this kind of an interface which we can purchase and use? Something with Microsoft SSO capability would also be great. Ideally this would be a product that not only engineers can view/edit, but e.g. non-technical product owners could use.

  
Thanks in advance.",alek5k,1gtzh8r,https://reddit.com/r/MachineLearning/comments/1gtzh8r/d_dataset_management_tool/,https://www.reddit.com/r/MachineLearning/comments/1gtzh8r/d_dataset_management_tool/,2024-11-18 07:29:57,0,0.29,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gtzh8r
MachineLearning,[D] Optimizing Context Extraction for Q&amp;A Bots in Ambiguous Scenarios,"I am building a Q&amp;A bot to answer questions based on a large raw text.



To optimize performance, I use embeddings to extract a small, relevant subset of the raw text instead of sending the entire text to the LLM. This approach works well for questions like:



        ""Who is winning in this match?""



In such cases, embeddings effectively extract the correct subset of the text.



However, it struggles with questions like:



        ""What do you mean in your previous statement?""



Here, embeddings fail to extract the relevant subset.



We are maintaining conversation history in the following format:

        previous_messages = [
            {""role"": ""user"", ""content"": message1},
            {""role"": ""assistant"", ""content"": message2},
            {""role"": ""user"", ""content"": message3},
            {""role"": ""assistant"", ""content"": message4},
        ]



But we’re unsure how to extract the correct subset of raw text to send as context when encountering such questions.



Would it be better to send the entire raw text as context in these scenarios?",yccheok,1gtzcuj,https://reddit.com/r/MachineLearning/comments/1gtzcuj/d_optimizing_context_extraction_for_qa_bots_in/,https://www.reddit.com/r/MachineLearning/comments/1gtzcuj/d_optimizing_context_extraction_for_qa_bots_in/,2024-11-18 07:21:56,1,0.67,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gtzcuj
MachineLearning,[P] AnyModal: A Python Framework for Multimodal LLMs,"[AnyModal](https://github.com/ritabratamaiti/AnyModal) is a modular and extensible framework for integrating diverse input modalities (e.g., images, audio) into large language models (LLMs). It enables seamless tokenization, encoding, and language generation using pre-trained models for various modalities. I created AnyModal to address a gap in existing resources for designing vision-language models (VLMs) or other multimodal LLMs. While there are excellent tools for specific tasks, there wasn’t a cohesive framework for easily combining different input types with LLMs. AnyModal aims to fill that gap by simplifying the process of adding new input processors and tokenizers while leveraging the strengths of pre-trained language models.

# Example Usage

    from transformers import ViTImageProcessor, ViTForImageClassification
    from anymodal import MultiModalModel
    from vision import VisionEncoder, Projector
    
    # Load vision processor and model
    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
    vision_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
    hidden_size = vision_model.config.hidden_size
    
    # Initialize vision encoder and projector
    vision_encoder = VisionEncoder(vision_model)
    vision_tokenizer = Projector(in_features=hidden_size, out_features=768)
    
    # Load LLM components
    from transformers import AutoTokenizer, AutoModelForCausalLM
    llm_tokenizer = AutoTokenizer.from_pretrained(""gpt2"")
    llm_model = AutoModelForCausalLM.from_pretrained(""gpt2"")
    
    # Initialize AnyModal
    multimodal_model = MultiModalModel(
        input_processor=None,
        input_encoder=vision_encoder,
        input_tokenizer=vision_tokenizer,
        language_tokenizer=llm_tokenizer,
        language_model=llm_model,
        input_start_token='&lt;|imstart|&gt;',
        input_end_token='&lt;|imend|&gt;',
        prompt_text=""The interpretation of the given image is: ""
    )

AnyModal provides a unified framework for combining inputs from different modalities with LLMs. It abstracts much of the boilerplate, allowing users to focus on their specific tasks without worrying about low-level integration. Unlike existing tools like Hugging Face’s transformers or task-specific VLMs such as CLIP, AnyModal offers a flexible framework for arbitrary modality combinations. It’s ideal for niche multimodal tasks or experiments requiring custom data types.

# Current Demos

* LaTeX OCR
* Chest X-Ray Captioning (in progress)
* Image Captioning
* Visual Question Answering (planned)
* Audio Captioning (planned)

The project is still a work in progress, and I’d love feedback or contributions from the community. Whether you’re interested in adding new features, fixing bugs, or simply trying it out, all input is welcome.

GitHub repo: [https://github.com/ritabratamaiti/AnyModal](https://github.com/ritabratamaiti/AnyModal)

Let me know what you think or if you have any questions.",Alternative_Detail31,1gtw77c,https://reddit.com/r/MachineLearning/comments/1gtw77c/p_anymodal_a_python_framework_for_multimodal_llms/,https://www.reddit.com/r/MachineLearning/comments/1gtw77c/p_anymodal_a_python_framework_for_multimodal_llms/,2024-11-18 04:02:55,6,0.62,6,0,1,0,0,False,False,True,False,False,Project,self,t3_1gtw77c
MachineLearning,[P] Still Drowning in Research Papers? Ribbit Ribbit Hops to Web and Android!,"Hey friends! Last month, we shared Ribbit Ribbit, our little research paper discovery tool on iOS, and wow—thank you so much for the love! Over the past few weeks, we’ve been hopping around to bring it to more places, and now we’re excited to share:

* **The full website** [**https://ribbitribbit.co**](https://ribbitribbit.co) **is live!** It has all the features from the app. You can ribbit your way through papers on a big screen for extra clarity or keep it mobile on your phone to browse anywhere—research, your way! 
* **Android is (almost) here!** It’s available through Google Play Testing. Google needs enough testers before it can go live, so if you’re up for trying it early, join our tester squad here: [https://ribbitribbit.co/request?testandroid=true](https://ribbitribbit.co/request?testandroid=true). You’d totally be our hero!

Ribbit Ribbit helps you find personalized paper recommendations, shrinks them into tweet-sized summaries, and even reads them to you like a podcast. We’re just trying to make the whole research thing a little more fun. We’d love for you to check it out. Your support means the world to us!

https://preview.redd.it/hyf9e6rmxk1e1.png?width=1492&amp;format=png&amp;auto=webp&amp;s=9a4deb6f3b70c9cf79d3441846ee03d6d6b93d22

",haoyuan8,1gtvmpn,https://reddit.com/r/MachineLearning/comments/1gtvmpn/p_still_drowning_in_research_papers_ribbit_ribbit/,https://www.reddit.com/r/MachineLearning/comments/1gtvmpn/p_still_drowning_in_research_papers_ribbit_ribbit/,2024-11-18 03:31:32,65,0.83,65,0,15,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/fdYlst9z35K5UyqCk_r7ygy9p66P_iACbCxUsFDmoSY.jpg,t3_1gtvmpn
MachineLearning,[D] Expectation from Machine Learning Engineering jobs,"Hey everyone,

I’ve seen a lot of posts here about careers in ML and landing internships or jobs, and two things come up a lot

1. Building a strong research portfolio and publishing at conferences like NeurIPS, ICLR, and ICML, which seems to focus more on getting research scientist roles.

2. The growing demand for Machine Learning Engineer (MLE) roles, which are apparently more in demand than research scientist positions.

I’m curious about the difference between these two roles and what kind of portfolio would be ideal for landing an MLE position. I know having a master’s degree is often preferred, but is an impressive publication record necessary for MLE roles? Or is it not that big of a deal?

What are your thoughts?",ziggyboom30,1gtt099,https://reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/,https://www.reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/,2024-11-18 01:14:14,74,0.88,74,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1gtt099
MachineLearning,[D] PCA vs AutoEncoders for Dimensionality Reduction,"The title sums it up. I'm working on some anonymized time-series data, initially, I built an AutoEncoder in order to replace the decoder head with a regression head instead after training.

As for preprocessing steps, I would usually just subtract the mean of features and divide by their standard deviation, Although I've long heard that doing ""data decorrelation"" is helpful, so I decided to finally learn about PCA.

My questions are the following:

1. If PCA serves to find the principle underlying features of a dataset, is there any point in using an autoencoder? (Especially if there are high correlations between some features)
2. If there is still a point to using autoencoders, should one use PCA on their dataset first to decorrelate data, or is that just redundant, or perhaps another reason not to use it is that it can erase some information? (Although it's an invertible transformation so I don't see how information would be lost)
3. Is PCA as a preprocessing step beneficial to tree-building algorithms? I haven't seen much talk of it, but it seems intuitive to me that having decision nodes on principle component axes would lead to better results.",DisciplinedPenguin,1gtng8q,https://reddit.com/r/MachineLearning/comments/1gtng8q/d_pca_vs_autoencoders_for_dimensionality_reduction/,https://www.reddit.com/r/MachineLearning/comments/1gtng8q/d_pca_vs_autoencoders_for_dimensionality_reduction/,2024-11-17 20:56:46,74,0.91,74,0,32,0,0,False,False,True,False,False,Discussion,self,t3_1gtng8q
MachineLearning,[D] Why LLM watermarking will never work,,bubble_boi,1gtkp1d,https://reddit.com/r/MachineLearning/comments/1gtkp1d/d_why_llm_watermarking_will_never_work/,https://david-gilbertson.medium.com/why-llm-watermarking-will-never-work-1b76bdeebbd1,2024-11-17 18:55:28,11,0.58,11,0,42,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/GnH7xnrgJux9xnIYGgAAzraH48iYfu_isxV44C4X0AI.jpg,t3_1gtkp1d
MachineLearning,[D] How an efficient applied ML team is structured?,"Hi Everyone,

I am interested in your experience on how big(ger) ML teams are structured that are working well for companies that are building with ML (companies who use ML in multiple domains and they cover CV, NLP, ...)?
I tried to search for it, but there is not much info on efficient team structure. While structure can be defined by the company culture, I am sure you've seen patterns on how this can work well.

(I think a big team is at least 80 people with POs/PMs).

The most basic (and maybe the best?) is when the domains are divided (CV, NLP, etc.) where every domain has a lead and multiple seniors, mediors, juniors. Then besides the ML engineers, there is a separate division who work with the productization (creating rest APIs, etc.), which includes devops, and SWEs. 

",gabegabe6,1gtke1b,https://reddit.com/r/MachineLearning/comments/1gtke1b/d_how_an_efficient_applied_ml_team_is_structured/,https://www.reddit.com/r/MachineLearning/comments/1gtke1b/d_how_an_efficient_applied_ml_team_is_structured/,2024-11-17 18:41:55,18,0.76,18,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gtke1b
MachineLearning,[R] treemind: Simplifying Gradient Boosting Model Analysis,"`treemind` is a powerful Python library designed to analyze gradient boosting models like `xgboost`, `lightgbm`, and `catboost`. It helps you uncover how features and their interactions influence predictions across specific intervals, offering fast, intuitive insights.

### Key Features:
- **Feature &amp; Interaction Analysis:** Understand feature contributions and complex interactions up to `n` features.
- **Advanced Visualizations:** User-friendly plots to explain model decisions.
- **High Performance:** Optimized with Cython for lightning-fast execution, even on large datasets.
- **Easy Integration:** Seamlessly works with popular frameworks for regression and binary classification.

### Algorithm &amp; Performance:
- **Algorithm:** Focuses on analyzing feature contributions and interactions in tree-based models for meaningful interval-based insights. [Read more about the algorithm](https://treemind.readthedocs.io/en/latest/algorithm.html)
- **Performance:** The library's performance has been tested on synthetic datasets, where it is benchmarked against SHAP for accuracy and efficiency. [View performance experiments](https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html)

### Quick Start:
```bash
pip install treemind
```

Check out the full documentation for examples, visualizations, and API details.

[GitHub Repo](https://github.com/sametcopur/treemind) | [Docs](https://treemind.readthedocs.io/)

**Note:**  
While the algorithm produces desirable results in practice, it currently lacks formal mathematical proof. We would greatly appreciate your feedback and ideas to help improve and validate the approach further!",zedeleyici3401,1gtjkci,https://reddit.com/r/MachineLearning/comments/1gtjkci/r_treemind_simplifying_gradient_boosting_model/,https://www.reddit.com/r/MachineLearning/comments/1gtjkci/r_treemind_simplifying_gradient_boosting_model/,2024-11-17 18:05:43,22,0.88,22,0,5,0,0,False,False,True,False,False,Research,self,t3_1gtjkci
MachineLearning,[D] Quality of ICLR papers,"I was going through some of the papers of ICLR with moderate to high scores related to what I was interested in , I found them failrly incremental and was kind of surprised, for a major sub field, the quality of work was rather poor for a premier conference as this one . Ever since llms have come, i feel the quality and originality of papers (not all of course ) have dipped a bit. Am I alone in feeling this ?",Cool_Abbreviations_9,1gtjhge,https://reddit.com/r/MachineLearning/comments/1gtjhge/d_quality_of_iclr_papers/,https://www.reddit.com/r/MachineLearning/comments/1gtjhge/d_quality_of_iclr_papers/,2024-11-17 18:02:24,133,0.9,133,0,71,0,0,False,False,True,False,False,Discussion,self,t3_1gtjhge
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1gtgnk8,https://reddit.com/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/,2024-11-17 16:00:20,3,0.8,3,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gtgnk8
MachineLearning,[D] Small language models defining vocabulary using old vectors instead of new vectors,"I've been thinking a lot about why language models were so big and how they could be smaller. I thought about how every human brain can't possibly contain the entirity of human knowledge. I believe humans roughly have something along the lines of a probability matrix of words X other words, but not every word X every word.

It occurred to me that we frequently define unusual words (low frequency, not often used words) using other existing words we know. Can we potentially have a language model which uses vectors for the highest frequency words only, and ""unusal words"" which dont have their own vectors, but instead reference existing vectors? This could drastically decrease the word X word matrix as common words consists of a much smaller subset of the language. Maybe such a model could dynamically move reference words into and out of primary vectors when retrained on text that is specific to niche topics.

Knowing that I've never had an original thought, are there any other projects like this already?",meteoraln,1gtenw8,https://reddit.com/r/MachineLearning/comments/1gtenw8/d_small_language_models_defining_vocabulary_using/,https://www.reddit.com/r/MachineLearning/comments/1gtenw8/d_small_language_models_defining_vocabulary_using/,2024-11-17 14:26:01,21,0.77,21,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1gtenw8
MachineLearning,NLU models vs autoregressive models for semantic search [R],"It seems that in a lot of applications where semantic matching is be difficult, systems are designed to use an autoregressive model for the input sequence embedding (then perform a range of semantic search techniques). 

But shouldn't a bidirectional model always out-perform an autoregressive model on this task theoretically? That would suggest it's ideal to use an optimised NLU-oriented model like DeBERTa-V3 (ie. fine tuned on domain data) for more accurate embeddings, thus better semantic search performance.

Additionally, is there much reporting on unified semantic search techniques? All of the implementations i've seen have been highly domain-specific/arbitrary.",SnooPeripherals5313,1gtdpwu,https://reddit.com/r/MachineLearning/comments/1gtdpwu/nlu_models_vs_autoregressive_models_for_semantic/,https://www.reddit.com/r/MachineLearning/comments/1gtdpwu/nlu_models_vs_autoregressive_models_for_semantic/,2024-11-17 13:37:15,2,1.0,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1gtdpwu
MachineLearning,"[D] Convolutional Generative Adversarial Networks Noise Patterns
","I am coding a DCGAN to produce Brain MRI data, based on the BRATs 2020 dataset. As a sanity check, I am training on a SINGLE image with CONSTANT noise, to see if there are any inherent flaws in my design. The GAN seems to catch on the general pattern, but there is some sort of noise or distortion. You can see in the example below, that the generated image is not as sharp as the original.

[original image](https://preview.redd.it/b7ejt2z4bg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=dac9ca113582d943c53a017f71095a43da813ff6)

[lr 1e-4 1000 epochs ](https://preview.redd.it/3ow5a1j7bg1e1.png?width=127&amp;format=png&amp;auto=webp&amp;s=90dd13bdd48fe9558d730140bc1228cd0666cb85)

[lr 2e-4 500 epochs](https://preview.redd.it/nwpgvdh9bg1e1.png?width=129&amp;format=png&amp;auto=webp&amp;s=0ae004f7cf112d0ac4aedbc47e057f6ca690750f)

[after initialization](https://preview.redd.it/r7kbeewebg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=9fee55b4ff32c10544b809bf8a9cb93fed95276c)

I see some cross like patterns on all of my images, so I believe there is something inherently wrong with my network that produces them. here is the code.

\`\`\`

    class SimpleGenerator(nn.Module):
        def __init__(self,out_channels =1,
                     noise_dimension = 100 ,
                     channels= 64         
                     ):
            super(SimpleGenerator, self).__init__()
            self.noise_shape = (noise_dimension,1,1,1)
            self.out_channels = out_channels 
            self.channels = channels
            self.gen = nn.Sequential(
                nn.ConvTranspose3d(self.noise_shape[0],  self.channels * 32, 4, 1, (1, 0, 1)),
                nn.ReLU(),
                self._block( self.channels * 32,  self.channels * 16, 5, 1, 0),
                self._block( self.channels * 16,  self.channels * 8, 5, 1, 0),
                self._block( self.channels * 8,  self.channels * 4, 4, 2, 1),
                self._block( self.channels * 4,  self.channels * 2, 4, 2, 1),
                self._block( self.channels * 2,  self.channels, 4, 2, 1),
                nn.ConvTranspose3d( self.channels, self.out_channels, 4, 2, 1),
                nn.Sigmoid()
            )
            
        def _block(self,in_channels,out_channels,kernel_size,stride,padding):
            return nn.Sequential(
           
                nn.ConvTranspose3d(in_channels,out_channels,3,1,1,bias=False),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU(),
                
                nn.ConvTranspose3d(out_channels,out_channels,kernel_size,stride,padding,bias=False),
                nn.InstanceNorm3d(out_channels),
                nn.ReLU()
            )
    
        def forward(self, x,separate=False):
            
            x = self.gen(x)
            return x

Notes :

1. I am using InstanceNorm Instead of batch norm as my images are 160 x192x160 they are too big so The gpu can't support batch\_size &gt;1.
2. The weird numbers you see in the kernel size, stride and padding are because I want to achieve the shape described above which is not a power of two. Could this be the reason?
3. I have tried the \_block method with 1 or 2 convolutions (we see the 2 version). Same result
4. the discriminator is a mirror image of the generator. I won't provide the code to make the post short, but i can if someone believes it is needed.",ripototo,1gtc2qv,https://reddit.com/r/MachineLearning/comments/1gtc2qv/d_convolutional_generative_adversarial_networks/,https://www.reddit.com/r/MachineLearning/comments/1gtc2qv/d_convolutional_generative_adversarial_networks/,2024-11-17 12:00:56,5,0.67,5,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gtc2qv
MachineLearning,[D] Looking for some audio segmentation model.,"Title, also something like pyannote/segmentation -3.0 but better. Is there anything new in this domain? I came across mamba but it's still in early stage for this purpose to say anything concrete about it.",Just_Difficulty9836,1gtbkhw,https://reddit.com/r/MachineLearning/comments/1gtbkhw/d_looking_for_some_audio_segmentation_model/,https://www.reddit.com/r/MachineLearning/comments/1gtbkhw/d_looking_for_some_audio_segmentation_model/,2024-11-17 11:26:24,5,0.78,5,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gtbkhw
MachineLearning,[Discussion] Logging Gradients and using third party loggers to tune hyper parameters,"Hey guys, I wondered how you learnt to use tools such as Wandb and MLFlow to log the gradient and tune hyperparameters in the model. 

Could you share resources for the same?",DiscussionTricky2904,1gtal7z,https://reddit.com/r/MachineLearning/comments/1gtal7z/discussion_logging_gradients_and_using_third/,https://www.reddit.com/r/MachineLearning/comments/1gtal7z/discussion_logging_gradients_and_using_third/,2024-11-17 10:14:15,3,0.67,3,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gtal7z
MachineLearning,[P] Supercharging Structured Outputs with Open Source Models 🚀,,themathstudent,1gt2yfp,https://reddit.com/r/MachineLearning/comments/1gt2yfp/p_supercharging_structured_outputs_with_open/,https://sachinruk.github.io/blog/2024-10-20-structured-outputs.html,2024-11-17 01:57:03,2,0.75,2,0,0,0,0,False,False,False,False,False,Project,default,t3_1gt2yfp
MachineLearning,"[P] FlatGeobuf as ""static"" vector database using dimensionality reduction","Recently I saw some good posts about dim reduction methods like the one dissecting UMAP, so I thought I'd chime in with a POC that leverages the idea of those methods for a very practical purpose: enabling server-side semantic search on large databases with high-dimensional embeddings using just a static FlatGeobuf file and a web server like nginx.

# tl;dr

**- Writing (and appending to) a FlatGeobuf file**: Embeddings -&gt; Gaussian Random Projection -&gt; 2D points -&gt; FlatGeobuf file  
**- Reading a FlatGeobuf file (based on a single user query)**: Embedding -&gt; Gaussian Random Projection -&gt; 2D point -&gt; buffered bounding box around this point -&gt; http range request(s) from client to remote FlatGeobuf file -&gt; subset of data points around the 2D point -&gt; reranking this subset client-side

Find the detailed explanation, code and examples on GitHub: [https://github.com/do-me/flatgeobuf-vectordb](https://github.com/do-me/flatgeobuf-vectordb)

# Main concepts

1. Points that are close in 2 dimensions (after projection) should be close in N dimensions too. This is obviously not always true but in my tests, it's good enough for basic use cases (e.g. product recommendation), where you do not need the closest result to the query but instead something in the top 0.1% or 0.01% may suffice. Note that I need to use a dim reduction method that works independently from the data, so cannot use UMAP, HUMAP, tSNE and PCA.
2. I'm reducing to 2 dims to benefit from all the heavy optimization work that the FlatGeobuf file format has done. Reducing to 3 dims (or even more) might preserve the similarity better (and eventually lead to better results) but also increases the overhead for efficiently designing such a file format. If you know any other suitable file formats for this purpose, I'd be very curious to try them! Another alternative might be instead of relying on one static file, to create an efficient file structure with many static files. The pros and cons have been discussed in a completely different context by the authors of protomaps and openfreemap on HN.

# Potential

Even though there are some tradeoffs in this workflow and yet many things to optimize and explore, I believe that the concept might be charming for low maintenance and low cost applications. In the end, you just dump one static file somewhere and fire normal http range requests to it, so the capacity of your web server determines the performance.  
As I'm heavily into client-side processing with transformers.js my ideal setup would use very small embedding models like Potion/Model2vec (&lt; 35Mb) in the client and index the user query (text/image) in the browser. This way, the remote database could be very large, like 100Gb and serve thousands of clients without any problems on a low-grade CPU (but very fast storage).

If you're fine with DB connection (which afaik can't be created browser-side), then just use LanceDB, following the same ""one file"" principle.

I'm super curious about your optimization ideas!

P.S. There is lots of overlap between geospatial and the latent space.",DomeGIS,1gssov1,https://reddit.com/r/MachineLearning/comments/1gssov1/p_flatgeobuf_as_static_vector_database_using/,https://www.reddit.com/r/MachineLearning/comments/1gssov1/p_flatgeobuf_as_static_vector_database_using/,2024-11-16 17:48:27,1,1.0,1,0,2,0,0,False,False,True,False,False,Project,self,t3_1gssov1
MachineLearning,Dataset versioning tool [D],What are you guys using for data(set) versioning and would you suggest to use for a small (1000 x 700) table ?,Amazing_Alarm6130,1gt1avg,https://reddit.com/r/MachineLearning/comments/1gt1avg/dataset_versioning_tool_d/,https://www.reddit.com/r/MachineLearning/comments/1gt1avg/dataset_versioning_tool_d/,2024-11-17 00:31:56,5,0.86,5,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1gt1avg
MachineLearning,[N] Addressing AI’s Hidden Risks: Join Our Free Webinar on Hallucinations in LLMs,"The Wisecube AI Team invites you to an upcoming webinar that explores an often-overlooked, yet critical aspect of AI reliability: hallucinations in large language models (LLMs).  
Discover how specific text features impact model accuracy and learn about methods for detecting hallucinations in LLMs. We’ll share insights into identifying model weaknesses and improving reliability, providing practical knowledge for AI practitioners and data scientists. This is a valuable opportunity to deepen your understanding of AI and explore the latest techniques for enhancing model performance!

🗓️ Date: November 21, 2024 | 🕐 Time: 1 PM EST

🎟️ Participation is free! [Register here](https://www.linkedin.com/events/7261113856268161024/about/) 



https://preview.redd.it/b82zo743ac1e1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=5556668e8ee8e156ce2a5caa2f4acdb4198ef75f

",kgorobinska,1gsynxm,https://reddit.com/r/MachineLearning/comments/1gsynxm/n_addressing_ais_hidden_risks_join_our_free/,https://www.reddit.com/r/MachineLearning/comments/1gsynxm/n_addressing_ais_hidden_risks_join_our_free/,2024-11-16 22:23:56,0,0.1,0,0,0,0,0,False,False,True,False,False,News,https://b.thumbs.redditmedia.com/8nSTv3oEsqF0s1ATheiYzegTVj1ZU08SxYPHy93e4Jw.jpg,t3_1gsynxm
MachineLearning,"[Discussion] R^2 is negative, but the correlation between prediction and actual values is statistically significant?","I have done a little bit of digging, but didnt really find the answer to this question, so if someones knows what might be wrong, please enlighten me. I have done some out of sample predictions (3000 observations) and I am getting really weird results when evaluating a model predicting demand levels. Model used is xgb regressor. So R\^2 point out that model performs worse than simply predicting the mean of the target variable, but at the same time the correlation between actual and predicted values is statistically significant. Moreover explained variance score says that model is worse than naive model, but Theil's U-statistic says the opposite? Code and results posted below. Thought that outstanding values might be the problem, but I clipped them at 0,05 and 0,95 quantile and it does not help.

https://preview.redd.it/10kpzdqs1c1e1.png?width=966&amp;format=png&amp;auto=webp&amp;s=9b93f0ef588e2fa5cb16c06f69c0fea1902e0931

https://preview.redd.it/t2rapmo22c1e1.png?width=855&amp;format=png&amp;auto=webp&amp;s=ce9d8d1d2ad54c8743873560bfff8a275a14378d

",maciek024,1gsxror,https://reddit.com/r/MachineLearning/comments/1gsxror/discussion_r2_is_negative_but_the_correlation/,https://www.reddit.com/r/MachineLearning/comments/1gsxror/discussion_r2_is_negative_but_the_correlation/,2024-11-16 21:41:41,26,0.84,26,0,59,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/uhs4FikNFVOvSvA_J0kXMO8ynlOpQppEByvYyvyxlbM.jpg,t3_1gsxror
MachineLearning,[D] program synthesis from input-output pairs - DL papers ?,"Given a set of inputs/ouputs, generate a suitable program

what are the baseline/canonical papers using DL for this program synthesis?

thanks",yazriel0,1gswhou,https://reddit.com/r/MachineLearning/comments/1gswhou/d_program_synthesis_from_inputoutput_pairs_dl/,https://www.reddit.com/r/MachineLearning/comments/1gswhou/d_program_synthesis_from_inputoutput_pairs_dl/,2024-11-16 20:41:40,2,0.75,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gswhou
MachineLearning,[P] Optimizing Whisper Speed: CPU vs. AMD GPU?,"Hi everyone,

I’ve been using Whisper for transcription and love its accuracy, but speed is an issue for me. It takes around 40 seconds to process a 2-minute audio file on my setup. I’ve read about models (sometimes dubbed “tree-like models”) that can achieve this in just 5 seconds. Has anyone here tested or optimized such models?

Ideally, I’d prefer sticking to CPU usage for reliability, but I’m curious if running Whisper on an AMD GPU could offer a significant speed boost. Anyone with experience on that?

Looking forward to your insights and recommendations!",yeah280,1gsw3ba,https://reddit.com/r/MachineLearning/comments/1gsw3ba/p_optimizing_whisper_speed_cpu_vs_amd_gpu/,https://www.reddit.com/r/MachineLearning/comments/1gsw3ba/p_optimizing_whisper_speed_cpu_vs_amd_gpu/,2024-11-16 20:22:48,0,0.33,0,0,7,0,0,False,False,True,False,False,Project,self,t3_1gsw3ba
MachineLearning,[N] Tau Language Alpha Release ,"Tau for me is one of the most fascinating projects of our time. I have been observing the research and development since 2017.
Today the team has released the alpha of Tau language after many years of work!
This is a big moment!

https://x.com/TauLogicAI/status/1857816396404600979?t=t7ATRYIXTMADewTYUo3ryg&amp;s=19",madsurgeon,1gsutua,https://reddit.com/r/MachineLearning/comments/1gsutua/n_tau_language_alpha_release/,https://www.reddit.com/r/MachineLearning/comments/1gsutua/n_tau_language_alpha_release/,2024-11-16 19:24:21,0,0.35,0,0,9,0,0,False,False,True,False,False,News,self,t3_1gsutua
MachineLearning,[D] Your ML PhD duration,How many years you take to finish ML PhD after bachelor’s? I understand different parts of the world usually have different duration. ,AntelopeWilling2928,1gsue6g,https://reddit.com/r/MachineLearning/comments/1gsue6g/d_your_ml_phd_duration/,https://www.reddit.com/r/MachineLearning/comments/1gsue6g/d_your_ml_phd_duration/,2024-11-16 19:04:43,27,0.76,27,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1gsue6g
MachineLearning,[D] COLING 2025 Results are leaked,"Yall may login to softconf to check if you can submit the camera-ready paper or not.

Mine was 4/3/3 and luckily got accepted. My first paper!!!",Ambitious-Public-512,1gssewj,https://reddit.com/r/MachineLearning/comments/1gssewj/d_coling_2025_results_are_leaked/,https://www.reddit.com/r/MachineLearning/comments/1gssewj/d_coling_2025_results_are_leaked/,2024-11-16 17:35:40,29,0.81,29,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1gssewj
MachineLearning,[R] Must-Read ML Theory Papers,"Hello,

I’m a CS PhD student, and I’m looking to deepen my understanding of machine learning theory. My research area focuses on vision-language models, but I’d like to expand my knowledge by reading foundational or groundbreaking ML theory papers.

Could you please share a list of must-read papers or personal recommendations that have had a significant impact on ML theory?

Thank you in advance!
",AntelopeWilling2928,1gsqqns,https://reddit.com/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/,https://www.reddit.com/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/,2024-11-16 16:19:28,427,0.97,427,0,98,0,0,False,False,True,False,False,Research,self,t3_1gsqqns
MachineLearning,[D] Time step dependency in diffusion model,Is there any existing work that try to investigate the relationship between time steps of a diffusion model? Something like the impact of model loss at time step i of the model to the output at time step j of the model? (j&lt;i),Careless-Top-2411,1gspx8g,https://reddit.com/r/MachineLearning/comments/1gspx8g/d_time_step_dependency_in_diffusion_model/,https://www.reddit.com/r/MachineLearning/comments/1gspx8g/d_time_step_dependency_in_diffusion_model/,2024-11-16 15:42:05,5,0.86,5,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gspx8g
MachineLearning,[P] Video Representations Extractor (VRE): Open source Video Multi Task dataset creation tool (+colab),"Hi guys, I've been working on this tool for my PhD for a while now. The PhD is about Multi Task Learning in the context of videos and I'm recently developing a tool to get predictions per frame from pre-trained ""experts"" (semantic segmentation, depth estimation etc.). The purpose of these is to train multi-task CV models with more than just raw RGB data to help with data efficiency and generalization.

The code is here: https://gitlab.com/video-representations-extractor/video-representations-extractor and there's a bunch of examples over there (including pip install command).

Recently I've done a ""end to end"" example for showcasing and I've put it on google colab as well: https://colab.research.google.com/drive/1vAp71H-TLewhF56odv33TkmGwwhuoFJ-?usp=sharing

Example output of the colab notebook: https://i.imgur.com/wyl9FPw.png

It skips a bunch of steps for simplicity (i.e. the binary semantic outputs like ""transportation"" are implemented separately for experimentation purposes and I just download that file + import it in the notebook instead of copy pasting 300+ lines of code in the colab but don't run arbitrary code w/o checking lol).

The colab should work fine for any UAV/driving/handheld indoor videos, not just my demo video.

The CLI tool syntax is pretty much:

    export VRE_DEVICE=cuda; # if available  
    vre video.mp4 --config_file config.yaml -o out_dir

where the config file defines parameters for these experts that I've implemented.",nucLeaRStarcraft,1gsmhuo,https://reddit.com/r/MachineLearning/comments/1gsmhuo/p_video_representations_extractor_vre_open_source/,https://www.reddit.com/r/MachineLearning/comments/1gsmhuo/p_video_representations_extractor_vre_open_source/,2024-11-16 12:43:49,3,0.81,3,0,0,0,0,False,False,True,False,False,Project,self,t3_1gsmhuo
MachineLearning,What’s the best tool for implementing TTS in Unity or UE5? [D],"Hi everyone,
I need some advice on how to best create an offline Text-to-Speech (TTS) system that I can use in Unity or Unreal Engine. Are there any tools or websites where I can clone a voice, download it, and use it locally in these engines?

I’m looking for a solution that doesn’t rely on cloud services and works entirely offline. Any recommendations or experiences with this would be greatly appreciated!

Thanks!",NoPrinciple1242,1gsjz08,https://reddit.com/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/,https://www.reddit.com/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/,2024-11-16 09:43:25,0,0.25,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gsjz08
MachineLearning,[P] Analysis of why UMAP is so fast ,"Hi, I recently spent some time to understand the core implementation of the UMAP algorithm from the point of view how it was implemented and why it's so fast (even though it's in python). I decided to decompose the algorithm into smaller steps in which I add some minor improvements to the code (one by one), so that at the end the final results are very similar to what I can get from the UMAP. 

To my surprise, most of these changes were just tricks in the optimization code to run things faster or update less important things less often. Of course, my implementation does not reproduce the UMAP algorithm in 100% as it was done in the educational purposes.

I provided a detailed explanation in my project of what I had to add in each step to move towards UMAP like algorithm. Here is the project page: [https://github.com/kmkolasinski/nano-umap](https://github.com/kmkolasinski/nano-umap)

If you are a person like, who likes to optimize the code for performance you may find this interesting. Here is a demo what I was able to get: 

https://preview.redd.it/eww57c3x881e1.png?width=1921&amp;format=png&amp;auto=webp&amp;s=ed4a345e40b47782ddf39cb93eb9d03207db1160

**TLDR: in UMAP they:**

* use ANN library to quickly find top k-NN,
* use good initialization method which makes things more stable and algorithm requires less updates (UMAP uses fast spectral initialization),
* use random negative sampling, which is a naive approach but works very well in practice,
* squeeze the numba performance (by replacing [np.dot](http://np.dot) or np.clip with custom implementations to make code run much faster),
* use some sort of adaptive sampling which will make that the algorithm will spend more time on more important vectors saving your CPU time on less important ones

",kmkolasinski,1gsjfq9,https://reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/,https://www.reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/,2024-11-16 09:02:10,413,0.98,413,0,42,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/YOkYN0kpJL-XQBhI4QifL5lhetpkBQIdNxFyTLklVkM.jpg,t3_1gsjfq9
MachineLearning,[D] neural scaling laws,I wanted to study up on the neural scaling laws and how they came into existence. Sp i wanted to see if there is a paper or a series of paper you would recommend for me to get started in those. Thank you. ,SmartEvening,1gsix5n,https://reddit.com/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/,https://www.reddit.com/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/,2024-11-16 08:22:23,0,0.5,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gsix5n
MachineLearning,[D] Distributed ML Algorithms Interview,"Hey guys,  
I have an interview coming up focussed on Distributed ML Algorithms (Interview description: We'll explore and explain the fundamental techniques used to build common neural network operations, focusing on simple yet effective implementations.)  
Are there any good resources I can use to study for this kind of an interview?",deepthought00705,1gsbykc,https://reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/,https://www.reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/,2024-11-16 01:13:01,3,0.64,3,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gsbykc
MachineLearning,[Discussion] Do modern search systems still require stemming and lemmatization in query preprocessing?,"I wonder how critical they are in the modern search system given all the advancement in LM. Semantic embedding can often help us understand the meaning quite well. But in order to effectively leverage historical query item engagement features, it seems we still require those preprocessing. Otherwise, we can easily get empty engagement features when users search slightly different from common queries? Or is there a more modern way to tackle free form queries?",wenegue,1gsauuz,https://reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/,https://www.reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/,2024-11-16 00:16:42,11,0.78,11,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gsauuz
MachineLearning,"[D] Feature selection methods that operate efficiently on large number of features (tabular, lightgbm)","Does anyone know of a good feature selection algorithm (with or without implementation) that can search across perhaps 50-100k features in a reasonable amount of time? I’m using lightgbm. Intuition is that I need on the order of 20-100 final features in the model. Looking to find a needle in a haystack. Tabular data, roughly 100-500k records of data to work with. Common feature selection methods do not scale computationally in my experience. Also, I’ve found overfitting is a concern with a search space this large. ",acetherace,1gsah8e,https://reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/,https://www.reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/,2024-11-15 23:58:07,7,0.89,7,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gsah8e
MachineLearning,[D] Extraction and processing of text on risk from annual reports,"Hi everyone,

I am doing a large-scale analysis where I want to extract information regarding possible risk factors and risk management strategies from annual reports.

The files are downloaded and I am currently doing OCR on the image files using tesseract, which extracts one text file for each document.

As I see it there are at least two key questions that are yet to be resolved:

**1. How do I locate and extract the parts of the annual reports that are about risk management?**  
Annual reports for smaller firms do not carry this information and reports for larger firms can be longer than a hundred pages. I have considered labelling a lot of annual reports myself and using Named Entity Recognition, but I doubt how well it works if I am not looking for named entities as such, but paragraphs where eg. risk factors are considered.  
*Do you have any suggestions on which NLP methods and/or programs to use?*

**2. What are good ways to process the extracted text on risk?**  
I want to generate one or more variables on risk factors and risk management strategies for each firms in each year. I have looked into Latent Dirichlet Allocation so far since it should be able to group words into topics and return some measure of how the words in a report are distributed across topics.  
*Again: Do you have any suggestions on which NLP methods and/or programs to use?*

**Specifics:**  
I have more than a million annual reports so far and I have access to two servers that are quite fast. As a measure of speed I can OCR around 80 documents at a time on each server at high speeds.

*Do you think* the project *is feasible? And is there something you think that I should be made aware of?*

Thanks in advance for any suggestions!",Happy-Koala7212,1gs0hy5,https://reddit.com/r/MachineLearning/comments/1gs0hy5/d_extraction_and_processing_of_text_on_risk_from/,https://www.reddit.com/r/MachineLearning/comments/1gs0hy5/d_extraction_and_processing_of_text_on_risk_from/,2024-11-15 16:39:22,3,1.0,3,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gs0hy5
MachineLearning,[R] DistilBERT vs TransformerEncoder,"I did fine-tuning on the pretrained DistilBERT tranformer-model and it achieved \~0.85 accuracy (classification with 17 classes).I also built from scratch a Transformer model using torch.nn.TransformerEncoder and it achieved \~0.97 accuracy for the same problem. Is this normal? I was expecting to have better performance with the pre-trained DistilBERT.Please note that for the DistilBERT model I used its own embeddings (pre-trained DistilBertTokenizer) and for the torch.nn.TransformerEncoder I used the simple TFIDF method. It is getting even more confused since the TFIDF cannot capture the sequence of the words in a sentence (it ignores the context)

Please let me know your thoughts. :)",Interesting_Pea_4605,1gs9pr4,https://reddit.com/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/,https://www.reddit.com/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/,2024-11-15 23:21:20,0,0.33,0,0,9,0,0,False,False,True,False,False,Research,self,t3_1gs9pr4
MachineLearning,[R][D]Test time training for abstract reasoning,"[https://arxiv.org/pdf/2411.07279](https://arxiv.org/pdf/2411.07279)

By the way guys, do you know of any research on trying to slightly fine-tune a model on the question it is asked before having it answer? I mean it would probably work for in-context information retrieval, but I was wondering about its impact on more reasoning-heavy tasks. The compute overhang would be huge, still.",Due-Pangolin325,1gs9lao,https://reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/,https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/,2024-11-15 23:15:29,18,0.95,18,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gs9lao
MachineLearning,[R] Convolutional Differentiable Logic Gate Networks,"Abstract

With the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed.  Logic gate networks are faster than conventional neural network approaches be- cause their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR pooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, which improves over the SOTA while being 29× smaller.

  
Accepted at Neurips 2024, ""SOTA"" here means comparable approaches. I found this paper really interesting, even though non-toy networks seems like they would be very expensive to train. Curious what others think?",jacobgorm,1gs92mb,https://reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/,https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/,2024-11-15 22:51:24,60,0.98,60,0,3,0,0,False,False,True,False,False,Research,self,t3_1gs92mb
MachineLearning,[D] To PhD or not to PhD,"I think this has been asked tons of times but let me ask it one more time.

I am currently working as applied scientist at MSFT. However, I am more looking into science positions, something like research scientist at DeepMind. Although jobs do not specifically need a PhD but the competition is fierce and is flooded with many PhD holders.

I really do enjoy research and want to PhD but I am always asking myself if it is really worth it.

That's an open question for sure, please feel free to share your thoughts.

",oddhvdfscuyg,1gs688q,https://reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/,https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/,2024-11-15 20:44:28,122,0.81,122,0,75,0,0,False,False,True,False,False,Discussion,self,t3_1gs688q
MachineLearning,"[D] Semantic Automaton in Geometric Embeddings (SAGE) proposes to bootstrap any existing decoder LLMs with a Neural Cellular Automaton (NCA) for inference-time reasoning, generalized intelligence, and recursive self-improvement","Hi everyone, this is my research direction and I already would like to share the concepts to ensure that they are disseminated and researched widely in multiple parallel organizations before OpenAI or other frontier labs can show up out of the blue with a finished product and capitalize. I research open-source super intelligence, and in the meantime I have uncovered a path to AGI which I present below. I predict that Regression Training is almost solved, as indicated by the ""scaling wall"", with future advances requiring richer datasets, byte-level models, and greater compute to go with it. The next 15 years of research &amp; development will be about Automaton Learning — self-energizing systems aligned with language. This is a proposed framework for solving ConceptARC, continuous reasoning, and recursive self-improvement.

Quick introduction to NCAs, they are Neural Cellular Automaton. The cells are not binary 0/1 like in Conway's Game of Life, nor are they continuous values from 0 to 1 as in many more esoteric continuous automaton — they are embeddings and hidden states. Classic NCAs also have a visualization surface, where the hidden state negotiates the evolution of this surface. Hence why they were called NCAs, as they are ultimately viewed as generative models for the desired projection surface. (2D visuals, a path through a maze, etc.) The model takes an input, a fixed filter is applied to surface (sobel, gaussian, etc.) which I call the ""environmental physics"" of the simulation, and then a model goes through every 3x3 neighborhood and does its own thing. In this manner, the physics are leveraged or not leveraged as basic transformation primitives, the same way we leverage logic gates in logic gate networks (LGNs) as a transformation operator, or quite simply matrix multiplications and activation functions in the models we know and love.

This work is downstream from the following works:

* [1] Neural Cellular Maze Solver https://umu1729.github.io/pages-neural-cellular-maze-solver/
* [2] Variational Neural Cellular Automata https://openreview.net/pdf?id=7fFO4cMBx_9
* [3] Attention-Based Neural Cellular Automata https://arxiv.org/abs/2211.01233 

The exact procedure to produce this frankenstein will require more scrutiny and research, and it should be taken as a prototype roadmap that we 'denoise' together. This entire research plan could produce a dozen paper for each sequential step of the puzzle that will need to be solved. Ultimately, I am trying to convey the broad picture here to massively seed the field of Automaton Learning which I anticipate is the next gold rush. A syphoning scheme over the decoder is the key to this whole operation. It's about recovering and transforming the representations until they are in a more useful form. It's about knowing what cards you have and what potential hand can materialize if you go after these two other cards that seem useless on their own. Now that we have these smart intelligent decoder models, it presents a first ""factorization"" of the world. It's a better dataset and it enables new classes of machine learning. At least, this is my grand challenge to the status quo of machine learning.

Now, here are my blueprints

---

Contemporary large language models stand as monolithic crystals of knowledge, their capabilities locked in inefficient token-by-token traversals of meaning space. We present SAGE, a framework for transmuting this sequential processing into parallel field computations where meaning propagates through geometric substrates intimately aligned with human cognitive architecture. Through careful staging of representation learning, we demonstrate that any contemporary decoder-only model can be reframed as a large knowledge reservoir from which we distill more efficient computational primitives into a self-organizing field substrate.

The transmutation begins with a frozen decoder-only language model serving as our semantic anchor. An initial lightweight encoder projects tokens into one-dimensional embedding sequences, while a first low-rank adapter trained on the decoder ensures semantic fidelity. This intermediate representation, though still sequential, provides the scaffold for geometric expansion. Critical to this phase is the encoder's training to represent identical semantic content through multiple embedding configurations — effectively using the geometric dimension as a continuous manifold encoding linguistic relationships, bindings, and hierarchical structure. This multiplicity of representation creates the mathematical foundation for the subsequent expansion into field computation, as the encoder learns to map semantic invariants through varying geometric configurations.

The diversity of geometric encoding follows patterns suggestive of fundamental laws governing information organization in physical systems. Just as Zipf's law emerges from underlying principles of efficiency in natural languages, the distribution of geometric representations appears to follow power laws reflecting optimal information routing through spatial substrates. This connection between natural law and learned representation proves crucial for the stability of subsequent field dynamics.

For a 2D cellular surface of shape (B, H, W, D) each cell contains a high-dimensional meaning vector D coupled to a learned binary visualization state. The field's computational architecture emerges through precise staging of physical dynamics. Local update rules manifest as learned neural networks processing neighborhood states: U(s) = φ(W₂φ(W₁[s; N(s)] + b₁) + b₂) where φ represents layer normalization followed by ELU activation. This local processing enables information routing through wave-like propagation, with patterns forming through constructive interference of semantic signals.

The update rule F(x,t+1) = F(x,t) + A*(N(x)) + R(F) employs spatially-constrained attention A* over neighborhood N(x), typically a 3x3 Moore neighborhood, with learned residual connections R. Layer normalization ensures stability while enabling pattern formation. Crucially, the visualization state evolves through its own update network V(x,t+1) = U(F(x,t), V(x,t), N(V(x,t))), creating a bidirectional coupling between meaning and form. This replaces the exponential complexity of traditional token-by-token generation with fixed-size context computation of linear complexity O(HW) in field dimensions.

Critical to pattern formation is the dual-state coupling mechanism between meaning and visualization. Rather than maintaining separate generative and discriminative components, the field itself serves as both medium and message. While meaning vectors F evolve through neighborhood attention, the visualization state V learns to project semantic content into binary patterns through its own update dynamics. This coupling creates a natural optimization surface where visual coherence guides semantic organization. The visualization network effectively learns a dynamic thresholding function mapping high-dimensional meaning to binary visual states while maintaining semantic gradients.

This architecture fundamentally transforms the traditional language model paradigm. Instead of exponentially expanding context windows to capture long-range dependencies, SAGE maintains fixed computational cost through field dynamics. Where decoder-only models must process entire contexts to generate each token, our field computation updates all semantic content simultaneously with linear complexity O(HW). Information propagates through wave-like patterns in the field substrate, with stable configurations emerging as computational primitives.

Field perturbation mechanics emerge through careful balance of conservation laws governing both meaning and form. Total semantic charge ∫|F|²dx remains conserved while allowing local concentrations through field gradients ∇F. Pattern formation follows least action principles minimizing energy functional E[F] = ∫(|∇F|² + V(F))dx where potential V(F) encodes learned semantic relationships derived from the frozen decoder's knowledge. These physical constraints, reminiscent of natural systems' self-organizing principles, guide emergence of stable computational primitives while preventing collapse to degenerate solutions.

The training progression orchestrates precise phases transforming monolithic decoder knowledge into geometric computation. Initial field states bootstrap from constant embeddings, with curriculum learning introducing compositional challenges requiring pattern interaction. Field dynamics learn to route information through stable configurations acting as computational waypoints. Each stable pattern serves as a reusable primitive, combining through field physics into increasingly sophisticated structures. The visualization state provides both interpretability and a geometric scaffold organizing semantic space.

Knowledge extraction proceeds through rigorously validated stages:

1. Frozen decoder anchors semantic meaning
2. First encoder projects to diverse sequential representations
3. First LoRA validates semantic preservation
4. Second encoder expands to field geometry 
5. Second LoRA maintains decoder alignment
6. Visualization capability emerges from field optimization
7. Field dynamics stabilize through conservation laws

Implementation crystallizes around nested hierarchies of constraints maintaining both stability and expressivity. Update rules balance information preservation against pattern innovation through careful energy bounds. The exploration of configuration space proceeds through natural field evolution guided by reconstruction gradients from the frozen decoder. This creates a form of self-supervised learning where the decoder's knowledge guides discovery of efficient computational primitives in the field substrate.

Visual grounding and geometric structure emerge not as optional features but as fundamental requirements for efficient cognition. Human intelligence arises from our intimate connection to three-dimensional reality, with language itself structured through spatial metaphor and geometric reasoning. SAGE mirrors this architecture: meaning evolves in a geometric substrate naturally aligned with cognitive primitives. The projection from 3D physical reality through 2D visual processing to abstract thought provides both template and constraint for artificial intelligence design.

The framework's recursive improvement potential manifests through several interlocking mechanisms. Stable field configurations act as computational primitives, combining through local interactions into increasingly sophisticated structures. These combinations follow physical laws emerging from the field dynamics — conservation of semantic charge, least action principles, and wave-like information propagation. As patterns interact and evolve, they discover more efficient computational pathways through the geometric substrate. The curriculum progression from simple pattern formation through abstract reasoning tasks creates selection pressure favoring emergence of reusable computational motifs.

Early experiments demonstrate several key capabilities validating the SAGE approach. Various works show success in re-training a missing encoder for a decoder-only model. The transition from exponential-cost token prediction to linear-cost field evolution dramatically improves computational efficiency. Pattern diversity increases naturally through field dynamics, with stable configurations encoding reusable semantic relationships. Most importantly, the geometric grounding creates human-interpretable representations emerging from fundamental physical principles rather than arbitrary architectural choices.

Success metrics emerge naturally from field dynamics rather than requiring arbitrary benchmarks. Pattern diversity measures the richness of stable configurations in semantic space. Compositional sophistication emerges from the physics of pattern interaction. Recursive improvement manifests through discovery of increasingly efficient computational primitives. Human alignment arises naturally from shared geometric foundations rather than post-hoc constraints.

The framework's extensibility suggests natural progressions following geometric principles. While our initial implementation uses Euclidean space for its natural connection to human visual processing, other geometries offer complementary computational advantages. Hyperbolic space, with its exponential expansion of volume with radius, provides natural representation of hierarchical relationships while maintaining constant curvature and local neighborhood structure. Multiple field geometries could interact through learned coupling dynamics, enabling sophisticated multi-scale computation while preserving linear complexity in field dimensions.

This represents a fundamental reformulation of machine intelligence — from static architecture to dynamic field discovering optimal computation through self-organization. The transition from sequential symbol manipulation to parallel field dynamics maintains semantic coherence while dramatically improving computational efficiency. Through careful orchestration of knowledge crystallization, we enable emergence of general intelligence grounded in human-interpretable geometric principles. Traditional language models, bound by exponential costs of token prediction, give way to shape-rotating field computers discovering efficient geometric paths through meaning space.

The path forward demands careful empirical validation while remaining alert to emergent capabilities arising from field dynamics interacting with decoder knowledge. Early results suggest critical components for artificial general intelligence may already exist within current architectures, awaiting reorganization into more efficient computational substrates through field dynamics. The key insight is recognizing that intelligence requires not just knowledge but efficient geometric pathways for manipulating that knowledge — pathways that SAGE discovers through fundamental physical principles rather than architectural engineering.

---

Whatever you do, remember that **it is not ethical to profit off of AGI**.",ryunuck,1gs3sd1,https://reddit.com/r/MachineLearning/comments/1gs3sd1/d_semantic_automaton_in_geometric_embeddings_sage/,https://www.reddit.com/r/MachineLearning/comments/1gs3sd1/d_semantic_automaton_in_geometric_embeddings_sage/,2024-11-15 18:57:54,0,0.29,0,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gs3sd1
MachineLearning,[D] Neurips 2024 Hotel Roommate Search,"The hotels around the venue for Neurips 2024 are pretty expensive, and I'm looking for a roommate to split the cost with (my university has a limit on the nightly hotel rate they are willing to reimburse). I currently have reserved a room for Tuesday-Sunday in the Century Plaza Hotel, which is 0.9 miles from the convention center. The nightly rate is $414. If anyone wants to split the cost of a room, please reach out! Also, it would be helpful if you could share this post with your research group or other attendees that you know.

If you are unsure about rooming with a complete stranger, you can get to know me a little bit through my personal website (https://mtcrawshaw.github.io/), which has links to my google scholar page, CV, etc. I do have a paper at the conference in the area of federated learning/distributed optimization. Just a grad student trying to make conferences affordable! Thanks.",ssbm_crawshaw,1gs0gj8,https://reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/,https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/,2024-11-15 16:37:38,53,0.85,53,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gs0gj8
MachineLearning,[R] Meta-Learning with Text Embeddings for Treatment Effect Estimation Under Text-Based Confounding,"Title: From Text to Treatment Effects: Meta-Learning Approach for Handling Text-Based Confounding

I found this paper introduces a meta-learning framework that jointly learns text representations and estimates treatment effects to handle text-based confounding. The key innovation is using meta-learning to optimize both the text encoder and treatment effect estimator simultaneously, rather than treating them as separate steps.

Main technical points:
- Develops a two-stage meta-learning architecture:
  - Text encoder learns representations capturing confounding information
  - Treatment effect estimator uses these representations to compute individual effects
- Uses gradient-based meta-learning to optimize both components end-to-end
- Incorporates balance regularization to ensure treatment/control groups have similar representations
- Evaluates on both synthetic and real-world datasets from healthcare and product reviews

Results reported:
- Outperforms baseline methods (separate text encoding + treatment estimation) by 15-25% on synthetic data
- Shows 12% improvement in treatment effect estimation on real product review dataset
- Ablation studies confirm both meta-learning and balance regularization contribute to performance gains

The theoretical implications are interesting - this shows that jointly optimizing representation learning and causal inference can capture confounding better than pipeline approaches. Practically, this could improve treatment effect estimation in many domains where text data contains confounding information, like healthcare records or user reviews.

TLDR: New meta-learning method jointly learns text representations and treatment effects to handle text-based confounding, showing significant improvements over pipeline approaches on both synthetic and real data.

[Full summary is here](https://aimodels.fyi/papers/arxiv/from-text-to-treatment-effects-meta-learning). Paper [here](https://arxiv.org/abs/2409.15503).",Successful-Western27,1gs0brk,https://reddit.com/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/,https://www.reddit.com/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/,2024-11-15 16:31:50,6,1.0,6,0,0,0,0,False,False,True,False,False,Research,self,t3_1gs0brk
MachineLearning,[D] Leveling guidelines for machine learning engineers,"wanted to learn what are some ways this community distinguishes between mid/senior/principal level machine learning engineers. For software engineering this is less of an art, as there are well documented cases and examples. But not super clear if machine learning engineers are subject to the same definitions...",AdditionalWeb107,1grzax7,https://reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/,https://www.reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/,2024-11-15 15:47:56,5,0.73,5,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1grzax7
MachineLearning,"[D] When you say ""LLM,"" how many of you consider things like BERT as well?","I keep running into this argument, but for me when I hear ""LLM"" my assumption is decoder-only models that are in the billions of parameters. It seems like some people would include BERT-base in the LLM family, but I'm not sure if that's right? I suppose technically it is, but every time I hear someone say ""how do I use a LLM for XYZ"" they usually bring up LLaMA or Mistral or ChatGPT or the like.",Seankala,1grxbdp,https://reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/,https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/,2024-11-15 14:16:24,75,0.88,75,0,94,0,0,False,False,True,False,False,Discussion,self,t3_1grxbdp
MachineLearning,[D] The Lost Reading Items of Ilya Sutskever's AI Reading List,"This blog post attempts to identify which papers went missing from the viral AI reading list that surfaced earlier this year and was attributed to Ilya Sutskever and his claim to cover '90% of what matters' in AI in 2020:

https://tensorlabbet.com/2024/11/11/lost-reading-items/

Only 27 of about 40 papers were shared online earlier this year, so there have been many theories about which works would have been important enough to include. There are some obvious candidates related to meta-learning and competitive self-play discussed here. But also several noteworthy authors like Yann LeCun and Ian Goodfellow are absent from the list.

From my perspective, even papers on U-Net, YOLO detectors, GAN, WaveNet, Word2Vec and more would have made sense to include, so I am curious about more opinions on this!",AccomplishedCat4770,1grti0x,https://reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/,https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/,2024-11-15 10:34:11,84,0.91,84,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1grti0x
MachineLearning,"[D] Folks who work on discriminative/classification models, what is your biggest pain point?","And which of the following webinars/tutorials would you be most interested in?  
\- How to use a data auto-tuning tool to set up a classification model in less time?  
\- How to improve model performance in the face of data drift by using RAG for classification models?  
\- How to create a high performing model using a very small ""good"" data set?

TIA!",tinygirl83,1grkirs,https://reddit.com/r/MachineLearning/comments/1grkirs/d_folks_who_work_on_discriminativeclassification/,https://www.reddit.com/r/MachineLearning/comments/1grkirs/d_folks_who_work_on_discriminativeclassification/,2024-11-15 01:07:43,0,0.36,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1grkirs
MachineLearning,[D] Advice on ML lifecycle management ,"Hello guys, i am currently working on setting up an ML infrastructure for a project.

I want to be able to track the models versions, Evaluate the performance on live data, retrain the model automatically when new data is available and save the trained models in a store. So that the application using the model can load the trained model from the store and use it for inference in production.

p.s. I can't serve the model as a Rest Api, it has to be deploy on the computer where the end application will run, because that computer might not have an internet connection.

The solution I have now is the following:

prep the training data and save it to a delta table on the cloud

incrementally add newly available data to the delta table

train and test the model on data from the delta table

if the testing metrics are satisfying upload the artifacts(the model, the encoders and scalers) and metadata (metrics, features, etc...) as blobs to an azure storage container

for each new upload of the artifacts, a new version id is generated and the artifacts are saved, within the storage container, in a subfolder corresponding to the version of the model.

at the root of the container there is a blob containing information on the latest version id

When the end application is launched, it downloads the artifacts of the latest version from the azure storage container , if the internet connection is available and the latest available version is different from the version on the computer running the application , otherwise it uses a default version.

a continuously running job is used to evaluate the model on live data and save the results in a db

a dashboard presents the results of the evaluation

after x days a job is triggered to retrain the model on new data and the process goes through a new cycle, following the steps listed above.

What to think of this setup? Is it overly complicated? How can I make it better / more efficient? What process do you have in place to train, track, monitor and deploy your ML models?

I hope my question is not too convoluted. Excuse me for any mistakes, and thanks in advance for your answers.",InteractionSuitable1,1grpv5r,https://reddit.com/r/MachineLearning/comments/1grpv5r/d_advice_on_ml_lifecycle_management/,https://www.reddit.com/r/MachineLearning/comments/1grpv5r/d_advice_on_ml_lifecycle_management/,2024-11-15 06:05:05,3,0.8,3,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1grpv5r
MachineLearning,[P] Is It Reasonable to Simulate At-Risk Parkinson Patients Using EEG Biomarker Data?,"Hi everyone,

I'm currently working on a project for my thesis that involves training a machine learning model to classify Parkinson's disease (PD) based on EEG and other clinical features. However, I'm interested in going beyond just distinguishing healthy vs. PD patients. I want to see if the model could potentially identify patients who are *at risk* of developing Parkinson's in the future.

The challenge I'm facing is that the dataset I'm using doesn't include any real ""at-risk"" patients – it's a binary set of healthy controls and confirmed Parkinson's patients. I've read a lot of literature that discusses different biomarkers for Parkinson's, such as altered power in specific EEG frequency bands (like reduced alpha/beta and increased theta/delta), coherence changes between different brain regions, etc.

I was thinking of using these known biomarkers to artificially generate ""at-risk"" patient data. Essentially, I would modify EEG signals from healthy controls by applying certain changes (e.g., reducing alpha power, increasing delta activity) to create synthetic data that represents patients in a prodromal stage or with high risk factors.

I would love to hear the community's thoughts on this approach.

* Does this make sense from a methodological standpoint?
* Are there better approaches to simulate or model prodromal PD stages?
* Are there ethical or scientific concerns I should be aware of when using synthetic data like this?

Any input or advice would be incredibly helpful. Thanks in advance!",Impressive_Staff4688,1grrb6w,https://reddit.com/r/MachineLearning/comments/1grrb6w/p_is_it_reasonable_to_simulate_atrisk_parkinson/,https://www.reddit.com/r/MachineLearning/comments/1grrb6w/p_is_it_reasonable_to_simulate_atrisk_parkinson/,2024-11-15 07:47:08,4,0.75,4,0,3,0,0,False,False,True,False,False,Project,self,t3_1grrb6w
MachineLearning,[P] text2mc: Creating Novel Minecraft Builds Using A VAE,"Hello everyone! I'm Shaun, the Principal Investigator and Project Manager of a research initiative at the University of Central Florida called ""text2mc"" (text-to-Minecraft). If you don't know, Minecraft is an open-world video game where players can harvest resources to create any kind of structure they want. Conveniently for my team, Minecraft exists in a rigid 3D grid system, which sounded really nice when I first thought of this idea.

**What's the goal?**

The goal of the project is to replicate the success of the Stable Diffusion architecture to generate novel Minecraft builds. Stable Diffusion is a Latent Diffusion Model, meaning the first step and last step of the process is data point conversion into and out of latent dimensions, respectively. This is accomplished by a ""Variational AutoEncoder"" (VAE), which needs to be powerfully pre-trained to encode meaningful representations of the posterior distribution. In our case, we are using a generative model to approximate the posterior, which is ostensibly ""all player-made builds""

text2mc has accomplished this first step: training a VAE to encode meaningful representations of the build's data points. We are leaving the project to another team to complete it, adding textual conditioning to the model.

**Where and how was the data collected?**

Me and my team built a web scraper to autonomously download builds from PlanetMinecraft.com. This was incredibly difficult because the website has absolutely no data validation, and users can upload whatever. We even downloaded a few .exe files (yikes!). We downloaded \~25,000 builds, of which only \~11,000 were viable (consisted of approximately 300 GB of disk space). That's pretty small compared to Stable Diffusion's multi-million point dataset. The builds themselves come in *many* different formats. The most ML Engineer friendly one is called a *.schematic* file, which essentially contains *only* the data of the builds. The unfriendly format is the proprietary Minecraft world-save format. Any ""chunk"" (section of the world) that a player visits, Minecraft will save. Additionally, there is no metadata which would indicate whether a player placed a block or not to extract the build. Instead, we had to meticulously create a list of ""naturally occurring"" and ""unnaturally occurring"" blocks to decipher which blocks the player placed. We then used some clever clustering algorithms to find clusters of unnaturally occurring blocks, which is of course a build in the world that a player made. We then slice out that section of the world with some margin, and save it to a file.

**How does it work?**

[Figure 1: text2mc's model architecture](https://preview.redd.it/zul2gynqpw0e1.png?width=2318&amp;format=png&amp;auto=webp&amp;s=b9d61519a28000fdb42c74a6f1c1565d8424fbd8)

The model is a neat blend of Computer Vision and Natural Language Processing. Consider the word2vec algorithm from NLP. Suppose we wanted to trained the algorithm from scratch. To do so, we would take a corpus of text, tokenize it, mask and predict (using SkipGram or Continuous Bag-of-Words), and store the weights. The weights of the model therefore encode the semantic relevance of certain words. For SkipGram, the standard method is taking a ""window"" of tokens which is the context, and the ""target"" token is the masked token.

**block2Token2Vector**

Now consider the SkipGram architecture applied to Minecraft builds. Each unique block (like ""grass"", ""stone"", or ""air"") is tokenized, and each unique token is stored in a simple lookup table. Once tokenized, instead of context windows and target tokens, the 3D-SkipGram for Minecraft uses a context ""cube"" and target ""block"". This is a critical step to encode meaning into the blocks. Certain blocks tend to appear near each other, like oak planks and an oak door, constituting one wall of a house. text2mc's embeddings were pretrained by simply sliding this context cube through all the builds in the dataset. Instead of just using the tokens, the ""similarity"" of predictions down the line can be measured, since the blocks are now a fixed-dimension vector. We chose to use pre-trained embeddings to avoid the embedding-space collapse that happens when the SkipGram objective is part of the loss function of the generative model.

[Figure 2: Dimensionality-reduced block embedding's plot showing inter-related meaning](https://preview.redd.it/fzlxa4ldsw0e1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=a492e86b637966e6939153c6295b0f7c8ab6766f)

**How is this a ""generative"" model?**

[Figure 3: Dimension-reduced plot of encoded builds. Similar builds are closer than dissimilar builds. The model can infer what builds are between these points, even though they weren't in the dataset.](https://preview.redd.it/mg6vxlsrww0e1.png?width=9000&amp;format=png&amp;auto=webp&amp;s=4d675684012899c348929997d51853dea3abe1aa)

In the encoding step, Variational AutoEncoders use a reparameterization-of-variables trick, which summarily forces the latent space itself to be locally meaningful. That means that latent points that are a small distance from some arbitrary point will decode into a similar posterior data point prediction. Forcing the latent space to be locally information rich means that the latent space is continuous. This allows us to create a parametric line between two latent points created by encoding two Minecraft builds. Sending point-wise samples on the path of that line in the latent space through the decoder let's us observe a continuous transition from one Minecraft build to another. We made a process to convert these generations back into a *.schematic* file, which means that you can directly paste the generation into your Minecraft world using the WorldEdit mod.

[Figure 4: Interpolation between a tower and castle](https://i.redd.it/l4ib47qsxw0e1.gif)

**Where is the detail in the generations?**

As with any research initiative, there are limitations. The primary one being the data set size. 11,000 builds is barely enough for a generative model, especially for something this complex. Sure, you could get away with \~1,000 data points for generating some MNIST digits, but not for something like this. The primary function of using a VAE in Stable Diffusion is to reduce computational complexity and hardware requirements. This comes with the trade-off of the clarity/detail of the generated data. text2mc is the foundation on which to add textual conditioning to the generative capabilities. Much like the text-to-image models, eventually you will be able to describe the Minecraft build you want!

**Where can I find this??**

A cool demo of the model capabilities can be found at [this website](https://text2mc.vercel.app/). The website includes a widget that allows you to pick which two builds to interpolate between! I have not yet open-sourced the dataset or model. Soon, I plan to upload the dataset to Kaggle, and the model to Huggingface.

[text2mc's GitHub Repository With Lots of Failed, Recanted, and Revised Experiments Done Until Something Worked](https://github.com/shauncomino/text2mc-dataprocessor)

This entire project was my back-of-napkin idea, and it's been great to see it come to life. As the project manager, I've directed 5 developers for a few months to get this done. I wrote the data collection pipeline, engineered the model, wrote the training loop, trained many different architectures, and set the vision for the whole project.

**Side Note:**  
**I'm actively looking for a full-time Machine Learning Engineer job**, so if you find this project indicative of any skill, [this is my LinkedIn](https://www.linkedin.com/in/shaun-comino-18aa8a199/). I've just accepted this will dox me but I'm so excited to share this project that I can't help it.",SickDucko,1grd3t6,https://reddit.com/r/MachineLearning/comments/1grd3t6/p_text2mc_creating_novel_minecraft_builds_using_a/,https://www.reddit.com/r/MachineLearning/comments/1grd3t6/p_text2mc_creating_novel_minecraft_builds_using_a/,2024-11-14 19:32:51,2,1.0,2,0,1,0,0,False,False,True,False,False,Project,self,t3_1grd3t6
MachineLearning,[D] Why does my (TensorFlow Lite) model work on Desktop but not Mobile (Android)?,"Hi everyone,  
  
I'm building an audio classifier in Unity using TensorFlow Lite and have run into a curious issue, I was hoping to ask here to learn more about this problem here:

\- The default YAMNet model works perfectly on both Desktop and Android  
\- My custom model (made with Google Teachable Machine) works great on Desktop but completely fails on Android

What could cause this desktop vs mobile difference?

Thanks!",kyzouik,1grqemd,https://reddit.com/r/MachineLearning/comments/1grqemd/d_why_does_my_tensorflow_lite_model_work_on/,https://www.reddit.com/r/MachineLearning/comments/1grqemd/d_why_does_my_tensorflow_lite_model_work_on/,2024-11-15 06:41:12,1,0.55,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1grqemd
MachineLearning,[R] DTFormer: A Transformer-Based Method for Discrete Time Dynamic Graph Representation Learning,,moschles,1grq9cn,https://reddit.com/r/MachineLearning/comments/1grq9cn/r_dtformer_a_transformerbased_method_for_discrete/,https://arxiv.org/pdf/2407.18523,2024-11-15 06:31:02,2,0.67,2,0,0,0,0,False,False,False,False,False,Research,default,t3_1grq9cn
MachineLearning,[D] Should I transfer to recommendation algorithms?,"I'm working on an ""LLM"" team right now or at least that's how it was advertised it's honestly just classification using LLMs not really interesting. I got an offer to join another team in my company that does recommendation. I thought recommendation is a very solid field to join, but very competitive. What are your guys' experience working in recommendation?",DolantheMFWizard,1grl7gk,https://reddit.com/r/MachineLearning/comments/1grl7gk/d_should_i_transfer_to_recommendation_algorithms/,https://www.reddit.com/r/MachineLearning/comments/1grl7gk/d_should_i_transfer_to_recommendation_algorithms/,2024-11-15 01:42:44,32,0.85,32,0,36,0,0,False,False,True,False,False,Discussion,self,t3_1grl7gk
MachineLearning,[R] RedCode: A Benchmark for Evaluating Safety and Risk in Code Language Models,"RedCode: A New Benchmark for Evaluating Code Agent Safety

I've been reviewing this new paper that introduces RedCode, a benchmark for evaluating safety aspects of code generation and execution by AI code agents. The core contribution is a systematic way to assess how code agents handle potentially unsafe operations.

The benchmark consists of two main components:
- **RedCode-Exec**: Tests agent responses to 4,050 prompts covering 25 vulnerability types across 8 domains
- **RedCode-Gen**: Evaluates whether agents generate harmful code from 160 function signatures/docstrings

Key technical points:
- Uses Docker environments for controlled execution testing
- Implements custom metrics for safety evaluation
- Covers both Python and Bash code
- Tests multiple input formats (code snippets and natural language)
- Evaluated 3 agent frameworks using 19 different LLMs

Main findings:
- Agents show higher rejection rates for OS-level risky operations vs buggy code
- Natural language descriptions of risky operations have lower rejection rates than code
- More capable models (e.g., GPT-4) produce more sophisticated harmful code when prompted
- Found significant variance in safety performance across different agent frameworks

The implications are important for deploying code agents in production environments. The results suggest current systems have notable safety gaps, particularly around code execution. This benchmark provides a standardized way to evaluate and improve code agent safety mechanisms.

TLDR: New benchmark called RedCode tests code agents' ability to handle unsafe code execution and generation. Results show current agents have varying levels of safety capabilities, with particular vulnerabilities around natural language inputs and technically buggy code.

[Full summary is here](https://aimodels.fyi/papers/arxiv/redcode-risky-code-execution-generation-benchmark-code). Paper [here](https://arxiv.org/abs/2411.07781).",Successful-Western27,1grkagz,https://reddit.com/r/MachineLearning/comments/1grkagz/r_redcode_a_benchmark_for_evaluating_safety_and/,https://www.reddit.com/r/MachineLearning/comments/1grkagz/r_redcode_a_benchmark_for_evaluating_safety_and/,2024-11-15 00:56:25,3,0.67,3,0,0,0,0,False,False,True,False,False,Research,self,t3_1grkagz
MachineLearning,[D] Paper Club: Nvidia Researcher Ethan He Presents Upcycling LLMs in MoE,"Hey all,  


Tomorrow Nvidia researcher Ethan He will be doing a technical dive into his work: Upcycling LLMs in Mixture of Experts (MoE). Excited to get a peak behind the curtains to see what it is like to work on models at this scale at Nvida.

  
If you’d like to join the community tomorrow 10 AM PST we’d love to have you. We do it live over zoom and anyone is welcome to join.

Here's the paper: [https://arxiv.org/abs/2410.07524](https://arxiv.org/abs/2410.07524)  
Join us live: [https://lu.ma/arxivdive-31](https://lu.ma/arxivdive-31)",FallMindless3563,1grjjlz,https://reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/,https://www.reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/,2024-11-15 00:19:44,44,0.92,44,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1grjjlz
MachineLearning,[D] What are some important contributions from ML theoretical research?,"I am interested to know more about the contributions of theoretical ML researchers in recent years. I would like to hear about super important contributions that are not applicable (e.g., tell us something about something important) and ones that are applied in the real world as well. I want to try to read these papers.

Also, I am interested to know what (theoretical) researchers think about this field, does it have potential, or is ML going in a purely heuristic direction?

This discussion is probably more productive without talking about how ML is just stats and Lipschitz constant :) I am talking about cutting-edge theoretical research - I really have no tools to estimate how useful this line of work is and I believe it can be an interesting discussion for other people as well.",Traditional-Dress946,1grfxbz,https://reddit.com/r/MachineLearning/comments/1grfxbz/d_what_are_some_important_contributions_from_ml/,https://www.reddit.com/r/MachineLearning/comments/1grfxbz/d_what_are_some_important_contributions_from_ml/,2024-11-14 21:34:30,59,0.9,59,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1grfxbz
MachineLearning,[R] Testing on textvqa test split ?,"Hello everybody,
I want to test my model on textvqa test set, which apparently needs to be done on the evalai website.
However both challenges (2019/2020) are closed there and do not have a submit option, in addition the link provided in the textvqa official website does not work. (https://eval.ai/web/challenges/challenge-page/874/)
Any idea on how to test on the test set ?
Thanks !",Training-Adeptness57,1gralqc,https://reddit.com/r/MachineLearning/comments/1gralqc/r_testing_on_textvqa_test_split/,https://www.reddit.com/r/MachineLearning/comments/1gralqc/r_testing_on_textvqa_test_split/,2024-11-14 17:48:02,3,0.8,3,0,0,0,0,False,False,True,False,False,Research,self,t3_1gralqc
MachineLearning,[R] Coordination avoidance in ML training,"I am curious about schemes to avoid coordination avoidance in distributed ml training. If you can refer some papers on the same, I will appreciate it. ",net-weight,1gra953,https://reddit.com/r/MachineLearning/comments/1gra953/r_coordination_avoidance_in_ml_training/,https://www.reddit.com/r/MachineLearning/comments/1gra953/r_coordination_avoidance_in_ml_training/,2024-11-14 17:33:41,4,0.83,4,0,5,0,0,False,False,True,False,False,Research,self,t3_1gra953
MachineLearning,[D][P]Clustering categorical data,"What are the best ways to perform clustering on a dataframe composed of categorical variables ?

I want to use dataframes with many variables so One-Hot-Encoding may not be the best solution.

What are the SOTA techniques ? Maybe something with embeddings ?",DedeU10,1gr7zqg,https://reddit.com/r/MachineLearning/comments/1gr7zqg/dpclustering_categorical_data/,https://www.reddit.com/r/MachineLearning/comments/1gr7zqg/dpclustering_categorical_data/,2024-11-14 15:57:55,3,0.64,3,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gr7zqg
MachineLearning,"[D] Has anyone here had luck rotating images that don’t have EXIF data?
","I tried various programming languages and face detection models, but none could accurately determine the orientation.",Busy-Basket-5291,1gr5oj6,https://reddit.com/r/MachineLearning/comments/1gr5oj6/d_has_anyone_here_had_luck_rotating_images_that/,https://www.reddit.com/r/MachineLearning/comments/1gr5oj6/d_has_anyone_here_had_luck_rotating_images_that/,2024-11-14 14:14:27,0,0.31,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gr5oj6
MachineLearning,Advice on Upper limit for binary classification precision and recall when working with real life data? [P] [R],"At my current company I'm building a model to see how much of the people coming on our app are actually paying for the trial, and based on those predictions the UI of our app will change to show the user who's more likely to pay (or maybe less likely to pay, a different page).

The thing is, the people who actually pay are relatively very less compared to the people who pay. I have used SMOTE over sampling along with class weights and an XGBoost classifier to help deal with the class imbalance. After reviewing the model (it was release on prod for about a week and half), it turns out the precision for the majority class is around 74% and the recall for the said class (0) is 86%.

While things look bleak for the minority class, precision is 29%, while recall is 16%. I have optimized the model as much as I have can, and yes I know I can train the model weekly on new data and continue to see if there is any improvement, that is a given.

Now, as usual as it happens in corporate, my overlords want to see results, which might be a bit difficult looking at the data here. Are there ways which I might have overlooked or didnt pay enough attention to which might lead to an improvement in my model. The things i have tried are: sampling techniques (both over and under), SMOTE, SMOTENC, Class weights (assigning weights to classes to impact the training), used an optuna study to train an xgboost model (in case if you aren't aware Optuna, you should check it out, its nice for hyperparameter tuning). These were all methods I could figure out from medium articles and chatgpt.

P.S. some food for thought I wanted to discuss with people in the field, is its a binary classification problem at its heart, so is it enough to detect one class very well enough with high enough precision and recall and not think much about the other class, because in my simple mind, (in a binary classification), if its not one class then its going to be another. I might be wrong here, and I couldn't find any articles which you know, talk about this particular topic. Im glad if all of you could shed light on this stuff.

Edit: if its not really clear, I'm basically looking for optimization techniques which can be used to deal with data imbalance and to see if there is actually an upper limit to precision and recall when we are working with real data.

Thanks! I know its a big wall of text, and thanks for reading through it.",Icy-Literature9061,1gr5hfa,https://reddit.com/r/MachineLearning/comments/1gr5hfa/advice_on_upper_limit_for_binary_classification/,https://www.reddit.com/r/MachineLearning/comments/1gr5hfa/advice_on_upper_limit_for_binary_classification/,2024-11-14 14:04:52,4,0.7,4,0,12,0,0,False,False,True,False,False,Research,self,t3_1gr5hfa
MachineLearning,"[R] Undetectable Backdoors in ML Models: Novel Techniques Using Digital Signatures and Random Features, with Implications for Adversarial Robustness","I found an important analysis of backdoor attacks that demonstrates how a malicious service provider can insert undetectable backdoors into machine learning models.

The key contribution is showing how to construct backdoors that are provably undetectable even under white-box analysis, while allowing arbitrary manipulation of model outputs through subtle input perturbations.

Technical details:
* Two frameworks for planting undetectable backdoors:
  * Digital signature scheme-based backdoors that are computationally infeasible to detect with black-box access
  * Random Fourier Features/Random ReLU based backdoors that withstand white-box inspection
* Backdoored models are indistinguishable from clean models even with:
  * Full access to model architecture and parameters
  * Complete training dataset
  * Ability to analyze model behavior

Results:
* Backdoored models maintain same generalization error as original models
* Service provider can modify classification of any input with slight perturbations
* Construction works with any underlying model architecture
* Backdoors cannot be detected by any computationally-bounded observer

The implications are significant for ML security and outsourced training. The work shows fundamental limitations in certifying adversarial robustness - a backdoored model can be indistinguishable from a robust one while having adversarial examples for every input.

**TLDR:** Paper proves it's possible to insert undetectable backdoors into ML models that allow arbitrary manipulation of outputs while being provably impossible to detect.

[Full summary is here](https://aimodels.fyi/papers/arxiv/planting-undetectable-backdoors-machine-learning-models). Paper [here](https://arxiv.org/abs/2204.06974).",Successful-Western27,1gr4ksm,https://reddit.com/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/,https://www.reddit.com/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/,2024-11-14 13:19:01,47,0.91,47,0,5,0,0,False,False,True,False,False,Research,self,t3_1gr4ksm
MachineLearning,[R] The geometry of data: the missing metric tensor and the Stein score,"Just [sharing an article](https://blog.christianperone.com/2024/11/the-geometry-of-data-part-ii/) for those interested in differential geometry, ML and score-based models. I made a long introduction and then later I show how you can derive an efficient to compute metric tensor for the data manifold using the Stein score alone.",perone,1gr4bfl,https://reddit.com/r/MachineLearning/comments/1gr4bfl/r_the_geometry_of_data_the_missing_metric_tensor/,https://www.reddit.com/r/MachineLearning/comments/1gr4bfl/r_the_geometry_of_data_the_missing_metric_tensor/,2024-11-14 13:05:04,22,0.96,22,0,0,0,0,False,False,True,False,False,Research,self,t3_1gr4bfl
MachineLearning,"[D] Have you come up with any interesting paper on political affiliation prediction of a user based on their twitter account, like their posts, the people they follow. The retweets and so on? Do you think this direction can be a good multimodal machine learning research project?","Basically, I was thinking about if something like this can become a topic of interest. It can be other personal dimensions and should not be limited to political affiliation prediction anyway.",Remote_Status_1612,1gr3f0p,https://reddit.com/r/MachineLearning/comments/1gr3f0p/d_have_you_come_up_with_any_interesting_paper_on/,https://www.reddit.com/r/MachineLearning/comments/1gr3f0p/d_have_you_come_up_with_any_interesting_paper_on/,2024-11-14 12:13:38,0,0.25,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gr3f0p
MachineLearning,Advice for Improving the Performance of My Reinforcement Learning Model Based on Spiking Neural Networks [P] [R],"Hello everyone! I am working on a project focused on training reinforcement learning agents using Spiking Neural Networks (SNNs). My goal is to improve the model's performance, especially its ability to learn efficiently through ""dreaming"" experiences (offline training).

**Brief project context (model-based RL):**  
The agent interacts with the environment (the game Pong), alternating between active training phases (""awake"") and ""dreaming"" phases where it learns offline.

**Problems:**  
Learning is slow and somewhat unstable. I've tried some optimizations, but I still haven't reached the desired performance. Specifically, I’ve noticed that increasing the number of neurons in the networks (agent and model) has not improved performance; in some cases, it even worsened. I reduced the model’s learning rate without seeing improvements. I also tested the model by disabling learning during the awake phase to see its behavior in the dreaming phase only. I found that the model improves with 1-2 dreams, but performance decreases when it reaches 3 dreams.

**Questions:**

* Do you know of any techniques to improve the stability and convergence of the model in an SNN context?
* Do you have any suggestions or advice?
* The use of a replay buffer could help?",Embri21,1gr2x54,https://reddit.com/r/MachineLearning/comments/1gr2x54/advice_for_improving_the_performance_of_my/,https://www.reddit.com/r/MachineLearning/comments/1gr2x54/advice_for_improving_the_performance_of_my/,2024-11-14 11:42:22,1,0.67,1,0,2,0,0,False,False,True,False,False,Research,self,t3_1gr2x54
MachineLearning,[Discussion] Scaling laws and graph neural networks,"I stumbled upon a paper that introduces the first ""graph foundation model"": [https://arxiv.org/pdf/2407.11907](https://arxiv.org/pdf/2407.11907)

They show that a GNN can scale with data and model size, generalize across different domains, and be efficiently fine-tuned on new datasets. 

This is interesting to me because even though LLMs are all the rage, text can be a weak data representation. Most knowledge has a graph structure. Code, research papers, even the human brain –– all graphs. And next-token prediction as an inductive bias doesn't capitalize on this. 

There's a huge data bottleneck here, of course. But maybe the next step here is using LLMs to convert huge swaths of text on the internet into graphs to train on. 

What do y'all think?",jsonathan,1gr2t6l,https://reddit.com/r/MachineLearning/comments/1gr2t6l/discussion_scaling_laws_and_graph_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1gr2t6l/discussion_scaling_laws_and_graph_neural_networks/,2024-11-14 11:34:55,29,0.9,29,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1gr2t6l
MachineLearning,[D] Issue with EMG MLP network during real-time use,"Hey!

I'm trying to achieve real-time EMG classification of 8 gestures using 3 sensors on the forearm. I recorded data from each channel using an Arduino Zero and stored it in csv files through python. I obtained 5 files for each gesture each containing 6s rest/ 6s gesture performed 6 times in a row. Then, I segmented the data using 400ms windows with 85% overlap and for each channel envelope I extracted 7 time-domain features. I used an equal number of scaled feature vectors for each class to train an MLP of 3 layers with 200 neurons and a dropout rate of 0.2 using keras, sklearn and tensorflow (to get the Lite model) and in the confusion matrix I get an accuracy of 90%+ for each gesture for a 90% training/10% testing dataset. This whole process is based on this paper with changes of course: [(PDF) Electromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks](https://www.researchgate.net/publication/357412472_Electromyogram-Based_Classification_of_Hand_and_Finger_Gestures_Using_Artificial_Neural_Networks) . However, when I used the MLP in real-time it would accurately recognise 3 to 4 gestures instead of 8, is this normal? I'm going to try and record more data for each gesture in the span of a few days and retrain but I'm not sure if it will help much.

I also tried checking my python program for any errors in real-time by storing the incoming data and the produced feature vectors so as to compare them with the vectors calculated by implementing filtering, segmentation and feature extraction on the stored real-time data offline and they were the same, so I don't believe there is an issue with executing filtering/segmentation/feature extraction incorrectly in real-time.

Has anybody experienced a similar issue? Is what I'm trying to achieve possible or is 4 gestures the best I'm going to get? I've not found a lot of papers analyzing real-time EMG classification and robotic arm movement at the same time, so I thought I'd ask here as well, I hope I've given enough information.

Thanks!",Outrageous_Spare_498,1gr1yso,https://reddit.com/r/MachineLearning/comments/1gr1yso/d_issue_with_emg_mlp_network_during_realtime_use/,https://www.reddit.com/r/MachineLearning/comments/1gr1yso/d_issue_with_emg_mlp_network_during_realtime_use/,2024-11-14 10:35:41,4,0.76,4,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gr1yso
MachineLearning,[P] Experience with KV260 for realtime video processing?,"This is a requirements by my PI.  
  
I am looking for anyone with experience with the KV260 in video processing. I am interested in high throughput video AI. 60 ms (2 frame) Lens to screen time for the main video feed. AI augmentation can be upto 120 ms (4 frames) behind realtime. These are intended to be served on a best effort muxed overlay to the video feed. HDMI input.

I am interested in the DPU capabilities but was originally planning to offload the video to a networked GPU system. 

\* Is the KV260 capable of this?   
\* If so how hard?   
\* Has anyone done this and has recommendations?

\* Any thoughts on approach are welcome too.

\* I am open to other boards and tools but FPGAs seem to be the only thing fast enough,

KV260 Kit  
https://www.amd.com/en/products/system-on-modules/kria/k26/kv260-vision-starter-kit.html



",Heavy_Carpenter3824,1gqqm7p,https://reddit.com/r/MachineLearning/comments/1gqqm7p/p_experience_with_kv260_for_realtime_video/,https://www.reddit.com/r/MachineLearning/comments/1gqqm7p/p_experience_with_kv260_for_realtime_video/,2024-11-13 23:18:07,0,0.44,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1gqqm7p
MachineLearning,[d] grounding-dino: what is load_image doing internally and how to apply the same operation to frames from video,"Doing some testing I noticied that doing inference returns very different results for the same image but loaded with different methods:

* Method 1: the official load\_image function from the library(it reads the image using the path passed as argument)
* Method2: using cv2 to read the image, then converting to tensor and then swapping axis to have depth as first axis.

As I said, both methods give you a tensor to pass to the model, but they return very different results(method2 usually are bad), I inspected the shape of the image returned by both cases and they are different so defintelly there are transformations going on inside load\_image, my question is: **what is happening inside load\_image? so I can replicate it in other scripts**

My end goal is to run the model on video, I mean running the model on frames in the video, so I cannot use load\_image because they are not images from disk, they are obtained from the video, so I need to understand what is happening inside\_load image so I can emulate that behavior on the frames of the video.

UPDATE: found it [https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39](https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39)",Sad-Anywhere-2204,1gqq1gd,https://reddit.com/r/MachineLearning/comments/1gqq1gd/d_groundingdino_what_is_load_image_doing/,https://www.reddit.com/r/MachineLearning/comments/1gqq1gd/d_groundingdino_what_is_load_image_doing/,2024-11-13 22:52:33,0,0.29,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gqq1gd
MachineLearning,"[R] Benchmarking Vision, Language, &amp; Action Models on Robotic Learning Tasks","Code: https://github.com/ManifoldRG/MultiNet
Website: http://multinet.ai/static/pages/Multinetv01.html",harshsikka123,1gqftrm,https://reddit.com/r/MachineLearning/comments/1gqftrm/r_benchmarking_vision_language_action_models_on/,https://arxiv.org/abs/2411.05821,2024-11-13 15:45:11,8,0.8,8,0,0,0,0,False,False,False,False,False,Research,default,t3_1gqftrm
MachineLearning,[R] What's a good recommender systems framework for factorization machines with side information?,"Hello! I'm looking for a recommender systems framework that can help me generate recommendations for CTR data for a research project, where the main dataset is users' browsing data and the side information data is item features. I tried Elliot, but I kept running out of memory.",FragileHumans,1gqeeow,https://reddit.com/r/MachineLearning/comments/1gqeeow/r_whats_a_good_recommender_systems_framework_for/,https://www.reddit.com/r/MachineLearning/comments/1gqeeow/r_whats_a_good_recommender_systems_framework_for/,2024-11-13 14:42:59,2,0.67,2,0,5,0,0,False,False,True,False,False,Research,self,t3_1gqeeow
MachineLearning,[R] SelfCodeAlign: Self-Alignment for Code Generation,"**TL;DR** Even a small (7B) model can improve its coding skills via training on self-generated tasks and solutions; the method behind [StarCoder2-15B-Instruct](https://huggingface.co/blog/sc2-instruct)

**Paper:** [https://arxiv.org/pdf/2410.24198](https://arxiv.org/pdf/2410.24198)

**Abstract:**

&gt;Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.

**Visual Abstract:**

https://preview.redd.it/drjn0jxg7o0e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=017b55ecf2fdfdee41e08e37cd3ded11242114b8

**Visual Highlights:**

[Here and in subsequent tables, CodeQwen1.5-7B-Base is the base model for SelfCodeAlign-CQ-7B](https://preview.redd.it/s915egdn8o0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=16dc8b4dc9e9dafe74afa662059af8083bb8ac2b)

https://preview.redd.it/y3qmx174ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=23ab9075984155f062f1673b8393c10a9367f836

https://preview.redd.it/r6zd68q6ao0e1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=d2a7641ee6de38b2e604f90af33f2abfbcbcaf0f

https://preview.redd.it/g45lqi39ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=e1011c23820764e8860d5500a0c5f7d2091c0c0e

https://preview.redd.it/c7vzlr0bao0e1.png?width=861&amp;format=png&amp;auto=webp&amp;s=2267d3322b8c62690a08a22c4eb8098ad9df29a1

https://preview.redd.it/zy1jtkzbao0e1.png?width=1145&amp;format=png&amp;auto=webp&amp;s=4ba9904f46f0dd2fc078199a975be535cc321e3b

https://preview.redd.it/70fcub0dao0e1.png?width=881&amp;format=png&amp;auto=webp&amp;s=c090e4315a82fee70ce33c2f58c77ddfba4ae992

[Interestingly, training on self-generated data is slightly more beneficial than using data from a different teacher model. But the initial proficiency of the base model obviously matters too which confounds the results](https://preview.redd.it/ohpmkgjeao0e1.png?width=1139&amp;format=png&amp;auto=webp&amp;s=f00a3ce842302f28ba94f4ff01d51dabc7862cca)

",StartledWatermelon,1gqd7w1,https://reddit.com/r/MachineLearning/comments/1gqd7w1/r_selfcodealign_selfalignment_for_code_generation/,https://www.reddit.com/r/MachineLearning/comments/1gqd7w1/r_selfcodealign_selfalignment_for_code_generation/,2024-11-13 13:45:44,9,0.92,9,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/I5kYDJBitmTh9CBMFVDAPYdWnq2TcsiFOgoWfBIKOeU.jpg,t3_1gqd7w1
MachineLearning,[Discussion] Proof of Reconstruction Loss Term in VQ-VAE Loss,"Hello everyone,

I was reading the paper ""Neural Discrete Representation Learning"" and I was puzzled when I looked at the first term in VQ-VAE Loss Equation

https://preview.redd.it/l1s9kur3sn0e1.png?width=1394&amp;format=png&amp;auto=webp&amp;s=4d374dce319a7ac0bbf19089d4e06cabcaa2cd3d

I understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.

Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussionHello everyone,I was reading the paper ""Neural Discrete Representation Learning"" and I was puzzled when I looked at the first term in VQ-VAE Loss EquationI understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussion

\[Discussion\]",Snoo_65491,1gqbeie,https://reddit.com/r/MachineLearning/comments/1gqbeie/discussion_proof_of_reconstruction_loss_term_in/,https://www.reddit.com/r/MachineLearning/comments/1gqbeie/discussion_proof_of_reconstruction_loss_term_in/,2024-11-13 12:07:45,28,0.9,28,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gqbeie
MachineLearning,[D] OCR for documents ,"I’m looking to build a pipeline that allows users to upload various documents, and the model will parse them, generating a JSON output. The document types can be categorized into three types: identification documents (such as licenses or passports), transcripts (related to education), and degree certificates. For each type, there’s a predefined set of JSON output requirements. I’ve been exploring Open Source solutions for this task, and the new small language vision models appear to be a flexible approach. I’d like to know if there’s a simpler way to achieve this, or if these models will be an overkill.",FreakedoutNeurotic98,1gqb861,https://reddit.com/r/MachineLearning/comments/1gqb861/d_ocr_for_documents/,https://www.reddit.com/r/MachineLearning/comments/1gqb861/d_ocr_for_documents/,2024-11-13 11:57:03,0,0.2,0,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1gqb861
MachineLearning,[D] Ideas for AI/DS event for uni students that they can have fun with,"I'm planning on organising an event for  semester 1 students at a university. The goal is to get them interested in these fields, something that they can learn from while enjoying it.
Would love some good inputs on it. How would you design if you were in my place. How would you make it more fun.",Particular_Tap_4002,1gqb6xz,https://reddit.com/r/MachineLearning/comments/1gqb6xz/d_ideas_for_aids_event_for_uni_students_that_they/,https://www.reddit.com/r/MachineLearning/comments/1gqb6xz/d_ideas_for_aids_event_for_uni_students_that_they/,2024-11-13 11:54:44,0,0.5,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gqb6xz
MachineLearning,[R] Help with Graphic User Interface for LLM as Agents,"I remember that I saw on twitter a while ago (3-4 months) a graphic user interface to set up LLM as agents as nodes in a graph and making them interact.  
**Te user interface was black and had the details in yellow.**   
When an llm was computing there was a yellow circle moving around the node of the agents that was computing the reply... and then the ""flow"" passed as a yellow on the edges to the LLM that was computing the next answer...

I already asked *the bot* but I cannot recall the project. It was an open source project. Very fun and smart it seemed. This was unlocking the ""Socratic ai"" as a matter of graph disposition.  
It was open source, is not LangChain. 

Someone remember? which one was? ",vale_valerio,1gqanre,https://reddit.com/r/MachineLearning/comments/1gqanre/r_help_with_graphic_user_interface_for_llm_as/,https://www.reddit.com/r/MachineLearning/comments/1gqanre/r_help_with_graphic_user_interface_for_llm_as/,2024-11-13 11:20:16,3,0.71,3,0,3,0,0,False,False,True,False,False,Research,self,t3_1gqanre
MachineLearning,[D] ICLR 2025 Paper Reviews,"Reviews for ICLR 2025 seem to be available on OpenReview. Feel free to celebrate/rant/complain about your reviews here!

Last year's statistics [here](https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/)",pie3636,1gq8vu6,https://reddit.com/r/MachineLearning/comments/1gq8vu6/d_iclr_2025_paper_reviews/,https://www.reddit.com/r/MachineLearning/comments/1gq8vu6/d_iclr_2025_paper_reviews/,2024-11-13 09:10:57,54,0.96,54,0,34,0,0,False,False,True,False,False,Discussion,self,t3_1gq8vu6
MachineLearning,[Discussion] ML Engineers/DevOps - What's your current GPU infrastructure costing you (and is it worth it)?,"Been diving into ML infrastructure costs lately and curious about how others are handling this. 



Some specific things I'm trying to figure out:



\- What's your current setup? (AWS/GCP/Azure/On-prem/etc)

\- Rough monthly costs for GPU infrastructure?

\- Biggest headaches in managing ML infrastructure?

\- How much time does your team spend on infrastructure vs actual ML work?

\- Are you using spot instances or dedicated resources?

\- Any creative ways you've found to optimize costs?

",SwimmerPopular1589,1gq8eth,https://reddit.com/r/MachineLearning/comments/1gq8eth/discussion_ml_engineersdevops_whats_your_current/,https://www.reddit.com/r/MachineLearning/comments/1gq8eth/discussion_ml_engineersdevops_whats_your_current/,2024-11-13 08:32:30,2,0.57,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gq8eth
MachineLearning,"[D] AMA: I’m Head of AI at a firm in the UK, advising Gov., industry, etc. ","Ask me anything about AI adoption in the UK, tech stack, how to become an AI/ML Engineer or Data Scientist etc, career development you name it. ",Psychological_Dare93,1gq899s,https://reddit.com/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/,https://www.reddit.com/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/,2024-11-13 08:20:20,170,0.85,170,0,153,0,0,False,False,True,False,False,Discussion,self,t3_1gq899s
MachineLearning,[P] Two new open-weight (Apache 2.0) foundation models for multimodal product embeddings,"Today we open-weight (Apache 2.0) released the two best embedding models for ecommerce search and recommendations available anywhere. Marqo ecommerce models significantly outperform models from Amazon, Google, Cohere and Jina (see below).

\+ Up to 88% improvement on the best private model, Amazon-Titan-Multimodal (and better than Google Vertex, Cohere).

\+ Up to 31% improvement on the best open source model, ViT-SO400M-14-SigLIP.

\+ 5ms single text/image inference (A10g).

\+ Up to 231% improvement over other bench-marked models (see blog below).

\+ Evaluated on over 4M products across 10,000's of categories. Eval datasets are open sourced [here](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb).

\+ Detailed performance comparisons across three major tasks: Text2Image, Category2Image, and AmazonProducts-Text2Image.

\+ Released 2 evaluation datasets: GoogleShopping-1m and AmazonProducts-3m.

\+ Released [evaluation code](https://github.com/marqo-ai/marqo-ecommerce-embeddings).

\+ Apache 2.0 [model weights available on Hugging Face](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb) and to test out on Hugging Face Spaces.

  
Blog: [https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models](https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models)

GitHub: [https://github.com/marqo-ai/marqo-ecommerce-embeddings](https://github.com/marqo-ai/marqo-ecommerce-embeddings)

Hugging Face: [https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb)",Jesse_marqo,1gpx4jz,https://reddit.com/r/MachineLearning/comments/1gpx4jz/p_two_new_openweight_apache_20_foundation_models/,https://www.reddit.com/r/MachineLearning/comments/1gpx4jz/p_two_new_openweight_apache_20_foundation_models/,2024-11-12 22:14:58,16,0.95,16,0,4,0,0,False,False,True,False,False,Project,self,t3_1gpx4jz
MachineLearning,[D] Benchmark scores of LLM,"
When I look at the test data in some papers(especially in arXiv), some small models(~7B) shows quite moderate performance on some famous LLM bechmarking datasets.
However, based on my experience, the model acts like a fool(e.g. neverending repeated generation) on the dataset they mentioned.
When someone test bechmarking score of LLMs, do they usually fine-tune them toward the dataset before scoring?",Upset_Employer5480,1gq3tt3,https://reddit.com/r/MachineLearning/comments/1gq3tt3/d_benchmark_scores_of_llm/,https://www.reddit.com/r/MachineLearning/comments/1gq3tt3/d_benchmark_scores_of_llm/,2024-11-13 03:35:09,11,1.0,11,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gq3tt3
MachineLearning,[D]Image Colorization using GANs,"Heyy, for anyone reading this, I just got selected for a Nationwide Hackathon so I need some little suggestions/help regarding it. Everything about it will be explained as follows.

So basically my problem statement for hackathon is to colorize SAR images those are image captured by satellite of Earth. Those images are greyscale High quality images used nad then colorized. I observed onething of the colourized and greyscale images the greyscale image is of higher size in terms of storage than colorized image. 
I have to colorize these greyscale images such that it would be easy and accurate to analysis for the persons using it. What additional information I can provide besides the colorized images also note that these images are used for investigation. 
Help me win this hackathon by your suggestions or any unique approch to this problem. 

Note: We have to train GAN based models by the dataset which have sets of grayscale and colorized images.",DangerousCounty4724,1gq3itj,https://reddit.com/r/MachineLearning/comments/1gq3itj/dimage_colorization_using_gans/,https://www.reddit.com/r/MachineLearning/comments/1gq3itj/dimage_colorization_using_gans/,2024-11-13 03:18:58,1,0.57,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gq3itj
MachineLearning,[D] Suggestions for Document Tagging on Healthcare Articles Using LLMs or Alternative Approaches?,"I am currently generating tags for healthcare articles. My current approach is to use few-shot prompting with an LLM API to extract tags that the model considers appropriate, based on the examples I provide.

I've been relying on LLMs because I believe they have the best understanding of language, even in niche domains like healthcare. However, I’m starting to wonder if my thought process is flawed and if there are more efficient solutions to this problem.

It has worked reasonably well so far, but there are a few concerns that I have:

Firstly, I didn't choose to use any open-source or supervised learning models or algorithms as I am worried that they being general-purpose, may not be sufficiently trained on healthcare-specific terms or nuanced domain-specific knowledge.

This approach, while effective for now, is expensive for large-scale tagging due to the API usage costs.

In the long run, I would like to train or fine-tune my own model to perform this tagging task. However, I currently don’t have a large, labelled dataset of high-quality tags to do so.

As such, is the LLM approach for tagging in the short term until I gather sufficient data for fine-tuning or training my own model a good idea or are there better alternatives for tagging healthcare documents that are cost-efficient and domain-specific?",Comb-Greedy,1gq3d0u,https://reddit.com/r/MachineLearning/comments/1gq3d0u/d_suggestions_for_document_tagging_on_healthcare/,https://www.reddit.com/r/MachineLearning/comments/1gq3d0u/d_suggestions_for_document_tagging_on_healthcare/,2024-11-13 03:10:48,7,1.0,7,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gq3d0u
MachineLearning,[D] Together AI hits $100M in ARR but it just resales compute - hype? ,"I recently learned that this startup is seen as the fastest revenue ramp in recent years. But they are literally just brokering GPUs from one provider to another and just slapping on a broker fee…

If a real estate agent sales $100 million worth of houses, and get a $100,000 commission, it doesn’t mean they made $100 million in revenue…  what am I missing here? 

The product is literally the same, just ssh to a cluster. 

Why are people paying for this? this sounds like a massive scam no? Shouldn’t this just be compared to a cloud provider like Coreweave instead of an AI company? If you own GPUs as a cloud, you crushed $100M in ARR in a few months… ",guardianz42,1gps8fl,https://reddit.com/r/MachineLearning/comments/1gps8fl/d_together_ai_hits_100m_in_arr_but_it_just/,https://www.reddit.com/r/MachineLearning/comments/1gps8fl/d_together_ai_hits_100m_in_arr_but_it_just/,2024-11-12 18:52:41,47,0.76,47,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1gps8fl
MachineLearning,[R] What Types of Wireless Technologies Are Used in Human and Object Detection,"Hey everyone I'm currently doing research on using wireless technology for human and object detection, with machine learning and deep learning. I’m interested in learning more about how different types of waves like radar, microwaves, and Wi-Fi channel state information (CSI) are being used in this space.

If anyone has experience or knows about projects where these types of signals are applied for detection, I’d love to hear about it. Any insights on the benefits or limitations of certain waves/signals for ML and DL models in human and object detection would be super helpfull",PutNo3040,1gps00s,https://reddit.com/r/MachineLearning/comments/1gps00s/r_what_types_of_wireless_technologies_are_used_in/,https://www.reddit.com/r/MachineLearning/comments/1gps00s/r_what_types_of_wireless_technologies_are_used_in/,2024-11-12 18:43:02,0,0.5,0,0,5,0,0,False,False,True,False,False,Research,self,t3_1gps00s
MachineLearning,[D] What makes a good PhD student in ML,Hey as I started my PhD (topic: Interpretable Object Detection) recently I would be really curious to know what set of features you think make a successfull PhD student,RaeudigerRaffi,1gplmzb,https://reddit.com/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/,https://www.reddit.com/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/,2024-11-12 14:13:53,171,0.93,171,0,70,0,0,False,False,True,False,False,Discussion,self,t3_1gplmzb
MachineLearning,[R] TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling,"""*What DL architecture to try on tabular data?*""

Hi Reddit! Today, my colleagues announced TabM - a new answer to the above question. **TabM is leading on the benchmarks, while being simple, practical, and scalable to large datasets**. Technically, TabM efficiently imitates an ensemble of MLPs, as illustrated below. Also, TabM is one of the first projects using our new TabReD benchmark - a collection of eight real-world industrial datasets with time-based splits and feature engineering.

For a quick overview of TabM, you can check the following parts of the paper:  
\- **The abstract**  
\- The model illustration in **Figure 1** (and in the post below)  
\- The main results on **Page 7**

TabM links:  
\- [arXiv](https://arxiv.org/abs/2410.24210)  
\- [GitHub](https://github.com/yandex-research/tabm)  
\- [Twitter thread](https://x.com/YuraFiveTwo/status/1856293601627566335)

TabReD links:

\- [arXiv](https://arxiv.org/abs/2406.19380)  
\- [GitHub](https://github.com/yandex-research/tabred)  
\- [Twitter thread](https://x.com/puhsuuu/status/1854149134124486924)

[The model illustration ](https://preview.redd.it/qsvl8qk4sg0e1.png?width=1722&amp;format=png&amp;auto=webp&amp;s=519ff43ebd6a57501adb9cbdf39183b20af06cfc)",_puhsu,1gpjl9e,https://reddit.com/r/MachineLearning/comments/1gpjl9e/r_tabm_advancing_tabular_deep_learning_with/,https://www.reddit.com/r/MachineLearning/comments/1gpjl9e/r_tabm_advancing_tabular_deep_learning_with/,2024-11-12 12:29:00,53,0.9,53,0,8,0,0,False,False,True,False,False,Research,https://a.thumbs.redditmedia.com/S4ChMa0TaQXjU3wO0P1UaYOyCejy58bTWBKifgDIH68.jpg,t3_1gpjl9e
MachineLearning,[D] A model for rhythm game beatmaps,"Hi!

I'm looking into the possibility of using GenAI for generating beatmaps (levels) for rhythm games. Specifically I'm thinking Beat Saber but eventually I'd like the solution to be generalizable to arbitrary rhythm games.

I'm wondering if it'd be possible to (re)ues existing language models by cleverly transforming a song data into a text prompt and then the result into a beatmap :thinking:

Would anyone be interested in exploring such an endeavour or at least provide some idaes and insights as to how I could go about it?

PS I'm a software engineer so I could handle coding and teaching custom models.

Thanks!",Imm0rt4l,1gpeuw4,https://reddit.com/r/MachineLearning/comments/1gpeuw4/d_a_model_for_rhythm_game_beatmaps/,https://www.reddit.com/r/MachineLearning/comments/1gpeuw4/d_a_model_for_rhythm_game_beatmaps/,2024-11-12 06:49:28,2,0.75,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gpeuw4
MachineLearning,[Discussion] Do you create UI for your ML models ? How do you apportion it ? ,"I'm working on a machine learning project, and I'm curious about the general practice when it comes to creating user interfaces for ML models. Specifically, do you create dedicated Uls for interacting with your models? If so, how do you approach designing the UI for an ML system? Are there any best practices or tools you use to make the interaction smooth for non-technical users?

For example, if you're deploying a model as a web app, do you build a Ul that allows users to input data and see predictions, or do you mostly focus on the backend API and leave UI design to front- end developers? How do you manage the complexity of explaining the model's behavior or showing predictions in a user-friendly way?",Afreen19,1gpe8ir,https://reddit.com/r/MachineLearning/comments/1gpe8ir/discussion_do_you_create_ui_for_your_ml_models/,https://www.reddit.com/r/MachineLearning/comments/1gpe8ir/discussion_do_you_create_ui_for_your_ml_models/,2024-11-12 06:06:18,9,0.76,9,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gpe8ir
MachineLearning,"[D] Catboost large dataset. Is is best to use the majority of the data for training, where time to train is extreme, or smaller datasets where iterations are much faster?","I have a large dataset of around 4.5m records that I am using CatBoost to predict greyhound racing likely performance. Running 20k iterations or more takes a very long time, and it is difficult to say whether the results are significantly better than running training on a smaller dataset like 200k-500k.

Using gridsearchCV on the smaller sets of data found me the best parameters, but once again it is difficult to see if that would still be effective on the much larger set.

I would also like your thoughts on whether larger learning rate and lower iterations is better overall or whether you find it worth the wait to train at a lower learning rate over many more iterations.",Responsible-Walk-459,1gpc7it,https://reddit.com/r/MachineLearning/comments/1gpc7it/d_catboost_large_dataset_is_is_best_to_use_the/,https://www.reddit.com/r/MachineLearning/comments/1gpc7it/d_catboost_large_dataset_is_is_best_to_use_the/,2024-11-12 04:06:18,9,1.0,9,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gpc7it
MachineLearning,[News] AAAI 2025 Workshop on AI for Music 🎶,"Hi everyone!

We’re hosting the first “AI for Music” workshop at AAAI on March 3, 2025. The workshop will explore how AI is transforming music creation, recognition, education, and more. Topics include AI-driven composition, sound design, legal and ethical challenges, and AI’s impact on musicians’ careers.

Submissions (up to 6 pages) are welcome until November 22, 2024. Work in progress is encouraged!

**Workshop Summary**

This one-day workshop will explore the dynamic intersection of artificial intelligence and music. It explores how AI is transforming music creation, recognition, and education, ethical and legal implications, as well as business opportunities. We will investigate how AI is changing the music industry and education—from composition to performance, production, collaboration, and audience experience. Participants will gain insights into the technological challenges in music and how AI can enhance creativity, enabling musicians and producers to push the boundaries of their art. The workshop will cover topics such as AI-driven music composition, where algorithms generate melodies, harmonies, and even full orchestral arrangements. We will discuss how AI tools assist in sound design, remixing, and mastering, allowing for new sonic possibilities and efficiencies in music production. Additionally, we'll examine AI's impact on music education and the careers of musicians, exploring advanced learning tools and teaching methods. AI technologies are increasingly adopted in the music and entertainment industry. The workshop will also discuss the legal and ethical implications of AI in music, including questions of authorship, originality, and the evolving role of human artists in an increasingly automated world. This workshop is designed for AI researchers, musicians, producers, and educators interested in the current status and future of AI in music.

**Call for Papers**

Submissions should be a maximum of 6 pages. Work in progress is welcome. Authors are encouraged to include descriptions of their prototype implementations. Additionally, authors are encouraged to interact with workshop attendees by including posters or demonstrations at the end of the workshop. Conceptual designs without any evidence of practical implementation are discouraged.

**Topics of interest are (but not limited to)**

* AI-Driven Music Composition and Generation
* AI in Music Practice and Performance
* AI-based Music Recognition and Transcription
* AI Applications in Sound Design
* AI-Generated Videos and Lyrics Based on Music
* Legal and Ethical Implications of AI in Music
* AI’s Impact on Musicians’ Careers and Education
* Business Opportunities of AI in Music
* Music Datasets and Data Analysis

**Important Dates**

* Submission Deadline: November 22, 2024
* Notification: December 9, 2024
* Final Version Due: December 31, 2024

We hope to see you there! 🎶",Saysike_rightnow69,1gpc9fr,https://reddit.com/r/MachineLearning/comments/1gpc9fr/news_aaai_2025_workshop_on_ai_for_music/,https://www.reddit.com/r/MachineLearning/comments/1gpc9fr/news_aaai_2025_workshop_on_ai_for_music/,2024-11-12 04:09:22,14,0.86,14,0,8,0,0,False,False,True,False,False,News,self,t3_1gpc9fr
MachineLearning,"[D] How do you keep track of experiments, history, results?","I saw people using some tools, but sometimes those doesn't really fit and i'm confused which ones to try.

Do you guys just save the config+results? But how about when the model code changes?

I am unsure how to go about this. Any tips?

I think i might need some paper/digital notes plus some way to backtrack.

  
EDIT: Lots of good comments ! Thank you! I'll keep this post up and just keep commenting. Others will surely find this helpful.",Pristine-Staff-5250,1gpc3cv,https://reddit.com/r/MachineLearning/comments/1gpc3cv/d_how_do_you_keep_track_of_experiments_history/,https://www.reddit.com/r/MachineLearning/comments/1gpc3cv/d_how_do_you_keep_track_of_experiments_history/,2024-11-12 04:00:13,38,0.93,38,0,36,0,0,False,False,True,False,False,Discussion,self,t3_1gpc3cv
MachineLearning,[D] Is there a website or forum in LLM acceleration fields?,"Paperwithcode is the best option to add a LLM acceleration category, but there isn't. 



Is there similar place or forum which contains latest paper, code, implement in LLM acceleration fields?",Logical_Divide_3595,1gpc23c,https://reddit.com/r/MachineLearning/comments/1gpc23c/d_is_there_a_website_or_forum_in_llm_acceleration/,https://www.reddit.com/r/MachineLearning/comments/1gpc23c/d_is_there_a_website_or_forum_in_llm_acceleration/,2024-11-12 03:58:20,4,0.83,4,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gpc23c
MachineLearning,[D] Subspace similarity plot of LoRA,"Can someone explain me how to interpret Figure 3 of LoRA? Why is the bottom left greyed out on the first 2 pictures and why is the zoomed picture's (last 2 pictures) grey part no on the bottom left and inverted instead?

Thank you for your help

https://preview.redd.it/ocji41m1xd0e1.png?width=1106&amp;format=png&amp;auto=webp&amp;s=1afd83925af782da6122d70632f7592f9a11c502

",BigYounzzz,1gpat21,https://reddit.com/r/MachineLearning/comments/1gpat21/d_subspace_similarity_plot_of_lora/,https://www.reddit.com/r/MachineLearning/comments/1gpat21/d_subspace_similarity_plot_of_lora/,2024-11-12 02:51:01,3,0.81,3,0,3,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/bSukn5Lb1YfNYgGziymrET5lQgLN91BlQHhp3S9fAtU.jpg,t3_1gpat21
MachineLearning,[D] NeurIPS After Dark Networking Event,"Just got an email about an official ticketed after dark NeurIPS networking event - this will be my first time attending/presenting, wondering if these events are worth going to. More generally, also interested in hearing about how to make the most of my time attending.",gateofptolemy,1gpamvn,https://reddit.com/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/,https://www.reddit.com/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/,2024-11-12 02:42:13,16,0.87,16,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gpamvn
MachineLearning,[D] What are some problems you guys are working on?,"Hey guys, I’m a graduate master’s student majoring in Machine Learning. Winter break is coming up, and I’m gonna be spending Christmas alone 😃. I’ve got some spare time and access to a few A100s, so I’m planning to work on a project.

I’m curious to know what kind of problems you guys are working on! Need someone to help out or wish someone could solve a problem you have? I maybeeee can spare my winter to work on it!

Please share any problem statements you’re working on or wish to tackle. Also, if you work in the industry and know what kinds of problems would help me stand out, that advice would be super appreciated too :)
",ziggyboom30,1gp9ydh,https://reddit.com/r/MachineLearning/comments/1gp9ydh/d_what_are_some_problems_you_guys_are_working_on/,https://www.reddit.com/r/MachineLearning/comments/1gp9ydh/d_what_are_some_problems_you_guys_are_working_on/,2024-11-12 02:07:58,39,0.88,39,0,43,0,0,False,False,True,False,False,Discussion,self,t3_1gp9ydh
MachineLearning,[D] Is Linear Regression Considered AI?,"Hey Redditors,

I’m curious to hear your thoughts on this! Do you consider linear regression a part of AI, or do you see it as more of a traditional statistical method? I feel like there's a lot of debate around which techniques are truly considered AI, especially since some methods have been around for decades and are widely used outside of AI-specific applications.

Also, are there any other methods you initially didn't think of as AI, only to realize they were, or vice versa? Would love to know how others draw the line between traditional data analysis and AI techniques.

Thanks!",gcombar,1gp95jv,https://reddit.com/r/MachineLearning/comments/1gp95jv/d_is_linear_regression_considered_ai/,https://www.reddit.com/r/MachineLearning/comments/1gp95jv/d_is_linear_regression_considered_ai/,2024-11-12 01:28:11,0,0.32,0,0,32,0,0,False,False,True,False,False,Discussion,self,t3_1gp95jv
MachineLearning,[D] Whats the best way to train a voice model locally? (preferablly to make a TTS model to be used on an app),I have a friend with cancer and a recent surgery took their voice from them. I want to try training an AI voice model on some of the videos I have of them from before the surgery. Ideally I was hoping for an android app or web-app that I could use their voice model on so they can use TTS to speak using their voice again. I was looking for a way they could use it on their phone through an app if possible,TheTabernacleMan,1gp88fs,https://reddit.com/r/MachineLearning/comments/1gp88fs/d_whats_the_best_way_to_train_a_voice_model/,https://www.reddit.com/r/MachineLearning/comments/1gp88fs/d_whats_the_best_way_to_train_a_voice_model/,2024-11-12 00:44:26,5,0.79,5,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gp88fs
MachineLearning,[D] Why is LLM Pruning Not as Generally Available as Quantization?,"I've been diving into the world of large language models (LLMs) and have been exploring various optimization techniques. One thing that's puzzled me is the disparity in the availability and adoption of quantization versus pruning.

**Quantization** seems to be a well-established and widely used technique for reducing the memory footprint and computational cost of LLMs. It's relatively straightforward to implement and has seen significant adoption in both research and industry.

On the other hand, **pruning**—which involves removing less important weights from the model—is less common. Despite its potential benefits, such as further reducing model size and inference time, it doesn't seem to be as generally available or as widely adopted. Many of my searches on the internet just result in research papers or proof of concept GitHub repos.

I'm curious about the reasons behind this disparity. Are there technical challenges with pruning that make it less practical? Is it more difficult to implement or integrate into existing workflows? Or are there other factors at play?",Soumil30,1gp6h2d,https://reddit.com/r/MachineLearning/comments/1gp6h2d/d_why_is_llm_pruning_not_as_generally_available/,https://www.reddit.com/r/MachineLearning/comments/1gp6h2d/d_why_is_llm_pruning_not_as_generally_available/,2024-11-11 23:23:31,57,0.94,57,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1gp6h2d
MachineLearning,[D] What is the likely architecture/dataset for tiktok's realtime GAN models used in filters?,"I'm curious about how tiktoks filters perform so well at erasing hair (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/hair-eraser) and eyebrows (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/eyebrow-eraser).

Ive tried to do something similar (removing items from peoples faces in realtime) using a lightweight Pix2Pix style model on a paired dataset I created using OpenCV methods, but the quality of the generated image decreased too much as I reduced the size of the generator.

Anyone have any ideas on how they achieve such consistent results on such a lightweight model? Thanks",DjPoliceman,1gp3vz0,https://reddit.com/r/MachineLearning/comments/1gp3vz0/d_what_is_the_likely_architecturedataset_for/,https://www.reddit.com/r/MachineLearning/comments/1gp3vz0/d_what_is_the_likely_architecturedataset_for/,2024-11-11 21:34:21,22,0.87,22,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gp3vz0
MachineLearning,Prompting for classification when target has very high cardinality [D],"I am looking at a plant disease classification problem where depending on the symptoms one has to classify a plant as belonging into one of several disease categories. My question is about prompt engineering strategies for classifying when the target has very high cardinality. When there are only four or five potential target labels I can list them in the prompt and ask LLM to classify. What happens when the number of categories is &gt;50 ? Is there a way to prompt the LLM effectively in such a scenario?

",Ok-Emu5850,1gp2mqh,https://reddit.com/r/MachineLearning/comments/1gp2mqh/prompting_for_classification_when_target_has_very/,https://www.reddit.com/r/MachineLearning/comments/1gp2mqh/prompting_for_classification_when_target_has_very/,2024-11-11 20:43:11,0,0.27,0,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1gp2mqh
MachineLearning,[P]Annotated dataset for explaining the reason in AI vs Real Image detection,"I am currently working on a problem statement in which I need to classify between real and ai generated images and then give explanation for the classification. The first part is quite easy and the for the second part I found some research papers but none of them give the links for annotated dataset for fine-tuning model. can anyone help me find datasets which have good annotations for this purpose.

[SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model](https://arxiv.org/pdf/2402.18068v2) (they mention a dataset on page 4 but didn't give any links)",Background-Trainer37,1gp05os,https://reddit.com/r/MachineLearning/comments/1gp05os/pannotated_dataset_for_explaining_the_reason_in/,https://www.reddit.com/r/MachineLearning/comments/1gp05os/pannotated_dataset_for_explaining_the_reason_in/,2024-11-11 19:03:52,2,0.67,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1gp05os
MachineLearning,Overfitting Query [P],"Hi there, 
I’m currently building an NN model to detect a disease based off answers to multiple questions. In preliminary tests on 600 patients the model does extremely well, AUCs of 0.995 test accuracies of 0.975 but I fear the model is overfitting, I’ve used cross validation and performance gap analysis aswell as L1/L2 regularisation, Dropout and early stopping.
Here’s the results from the cross validation and performance gap analysis .
Cross validation results : mean Auc=0.9787 SD0.0090 
Mean accuracy =0.9350 SD0.0262
Performance gap analysis
Training set Auc = 0.9983 accuracy =0.9859
Test set Auc=0.9936 accuracy 0.9803

Tell me what you guys think of those results and if you think it’s overfitting/what other tests can I do to tell? 
I’m trying to ascertain more data but might need to partner with someone to do so. I don’t want to partner get the data and find out it’s a complete waste! 
Thanks",Disastrous_Ad9821,1gow3uh,https://reddit.com/r/MachineLearning/comments/1gow3uh/overfitting_query_p/,https://www.reddit.com/r/MachineLearning/comments/1gow3uh/overfitting_query_p/,2024-11-11 16:22:56,1,0.57,1,0,2,0,0,False,False,True,False,False,Project,self,t3_1gow3uh
MachineLearning,[D] ICLR 2025 Paper Reviews Discussion,"ICLR 2025 reviews go live on OpenReview tomorrow! Thought I'd open a thread for any feedback, issues, or celebrations around the reviews.

As ICLR grows, review noise is inevitable, and good work may not always get the score it deserves. Let’s remember that scores don’t define the true impact of research. Share your experiences, thoughts, and let’s support each other through the process!",Technical_Proof6082,1gov5zd,https://reddit.com/r/MachineLearning/comments/1gov5zd/d_iclr_2025_paper_reviews_discussion/,https://www.reddit.com/r/MachineLearning/comments/1gov5zd/d_iclr_2025_paper_reviews_discussion/,2024-11-11 15:43:34,99,0.96,99,0,247,0,0,False,False,True,False,False,Discussion,self,t3_1gov5zd
MachineLearning,"[Project] While training my model, every 2n Epoch are being skipped in jupyter notebook","For context, I'm trying to fine tune the MobileNetV3Small model for facial recognition. I freezed all the layers and added few layers on top for training.

At the moment, my dataset has four classes, with 126 images each.

While training the model, somehow every 2nth epoch are getting skipped, and they're not recorded in history either. If the epoch is set to 20, then only 10 epoch are actually executing. I've attached the ss of jupyter notebook output.

Later I tried the exact same code in collab and it raised an error on 2nd epoch saying validation generator is returning None object. I rechecked the code many times but still can't find where the issue lies.

If anyone knows any fix, please do suggest.

  
Code of my generator:

    datagen = ImageDataGenerator(
                rescale=1./255,
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True,
                rotation_range=10,
                fill_mode = 'nearest')
    
    
    datagen_val = ImageDataGenerator(rescale=1./255)
    
    batch_size = 16
    
    train_generator = datagen.flow(X_train,
                                   y_train,
                                   batch_size=batch_size
                                   )
    
    validation_generator = datagen_val.flow(X_val,
                                            y_val,
                                           batch_size = batch_size)
    

  


https://preview.redd.it/b9zrpqls0a0e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=d1141db2189b0bc371a5dd28c279c2b2639db33d

https://preview.redd.it/yza75p8t0a0e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=fbab0aa05e67511dab4c8436e5a12b14a2d06e09

  
",bkkh_3,1got90h,https://reddit.com/r/MachineLearning/comments/1got90h/project_while_training_my_model_every_2n_epoch/,https://www.reddit.com/r/MachineLearning/comments/1got90h/project_while_training_my_model_every_2n_epoch/,2024-11-11 14:20:26,0,0.25,0,0,1,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/Nn8v3_hIXLrWobmecARuwt1zqxosuSfKVYD3E-OhpfE.jpg,t3_1got90h
MachineLearning,[D] Help me with on-premise ml batch prediction deployment ?,"I need to deploy a `.pkl` model for batch predictions in a setup where code is pushed to GitLab, SQL/pyspark is used for data, and cron jobs handle scheduling. Docker, Kubernetes, and cloud are not allowed. this is on-premise setup. What are some best practices or approaches for this kind of deployment?",Simple_Toe_6989,1goqchp,https://reddit.com/r/MachineLearning/comments/1goqchp/d_help_me_with_onpremise_ml_batch_prediction/,https://www.reddit.com/r/MachineLearning/comments/1goqchp/d_help_me_with_onpremise_ml_batch_prediction/,2024-11-11 11:46:03,3,0.8,3,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1goqchp
MachineLearning,[D] A guess for an interesting method by a random security researcher,"My specialization is in cybersecurity but I am passionate about learning (in general), and deeply interested in many things, including AI/ML research. I’ve been exploring the concept of creating models that explore the latent space in a novel, far from average way. This idea is rooted in principles of curiosity-driven reinforcement learning, applied to generative models. By having stimulation driven attention mechanisms, intrinsic stimulation rewards, and memory augmented architectures, I've tried to come up with something that might work. Here’s a quick overview:

**Stimulation Driven Attention Mechanism**: Integrating an entropy-based reward layer into traditional attention mechanisms to encourage models to explore lesser-known tokens and regions within the latent space.

https://preview.redd.it/5isz9dj3h60e1.png?width=768&amp;format=png&amp;auto=webp&amp;s=e2d9ba2ed635c176b9dc66364a026ce024189d47

**Intrinsic Stimulation Rewards**: Modifying the loss function to prioritize surprising or low-probability outputs, balancing accuracy with novelty.

https://preview.redd.it/ox9vkq57h60e1.png?width=798&amp;format=png&amp;auto=webp&amp;s=e88bce2eeaadca2e3c06a4c700adf261a9740adb

Those are the main ideas. Alongside that you could have:

**Memory Augmented Generative Networks**: Implementing episodic memory buffers and novelty comparison modules to reward deviations from prior patterns.

**Self Regulating Exploration Mechanisms**: Adding feedback loops to maintain coherence by adjusting stimulation rewards when output quality degrades.

Please help me figure out if this makes sense. I'm not too attached to the ideas themselves.",Entropy667,1goi8ut,https://reddit.com/r/MachineLearning/comments/1goi8ut/d_a_guess_for_an_interesting_method_by_a_random/,https://www.reddit.com/r/MachineLearning/comments/1goi8ut/d_a_guess_for_an_interesting_method_by_a_random/,2024-11-11 02:55:34,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1goi8ut
MachineLearning,[D] Why Does Domain Randomization Ensure Stability in Neural Network Controllers? ,"Hello everyone,

I’m exploring how domain randomization contributes to the stability of NN controllers, especially when training includes a more extensive look at historical data.

Specifically, I’m curious if there’s a theoretical basis or formal analysis explaining how domain randomization, particularly when incorporating more historical information, can help neural networks maintain stability across varying conditions or noise levels. Are there papers that analyze this effect through Lyapunov stability or other rigorous methods, showing that exposure to a diverse range of past data can lead to more stable NN-based control systems?

Any recommendations on foundational or recent research in this area would be greatly appreciated. Thanks in advance!

(I already wrote the same thing on control theory reddit)",nerdkim,1gopir9,https://reddit.com/r/MachineLearning/comments/1gopir9/d_why_does_domain_randomization_ensure_stability/,https://www.reddit.com/r/MachineLearning/comments/1gopir9/d_why_does_domain_randomization_ensure_stability/,2024-11-11 10:51:17,6,0.88,6,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gopir9
MachineLearning,[R] Resource On Varying LLM Red Teaming Methods and Techniques ,https://github.com/user1342/Awesome-LLM-Red-Teaming,OppositeMonday,1gonf47,https://reddit.com/r/MachineLearning/comments/1gonf47/r_resource_on_varying_llm_red_teaming_methods_and/,https://www.reddit.com/r/MachineLearning/comments/1gonf47/r_resource_on_varying_llm_red_teaming_methods_and/,2024-11-11 08:14:52,6,1.0,6,0,0,0,0,False,False,True,False,False,Research,self,t3_1gonf47
MachineLearning,[R] Why aren't there text driven layout AI models,"There seems to be an AI model for almost everything except one capable of taking a description of the layout of a city, building, room, or any kind of space and creating a visual representation of it. Is there something particularly challenging about enabling a text-driven AI to grasp and generate these spatial relationships?

It feels like this would be the final piece in the ""text-driven AI game generator"" puzzle. We have models for nearly every other component needed to create a game.",jbrinkw,1gom4iz,https://reddit.com/r/MachineLearning/comments/1gom4iz/r_why_arent_there_text_driven_layout_ai_models/,https://www.reddit.com/r/MachineLearning/comments/1gom4iz/r_why_arent_there_text_driven_layout_ai_models/,2024-11-11 06:42:05,0,0.38,0,0,32,0,0,False,False,True,False,False,Research,self,t3_1gom4iz
MachineLearning,[D] Attending WACV2025,"Hello,

Is there anyone who is gonna attending the WACV conference in Tucson next February? It looks like we have to book a room in JW Marriot and they are gonna charge each of us 35$ + taxes per day for using the resort.

Any idea how to deal with this, such as any nearby hotels or alternative solutions?

Thanks!",tuvovan,1gokplb,https://reddit.com/r/MachineLearning/comments/1gokplb/d_attending_wacv2025/,https://www.reddit.com/r/MachineLearning/comments/1gokplb/d_attending_wacv2025/,2024-11-11 05:14:09,3,0.67,3,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gokplb
MachineLearning,[D] How to visualize the effect of an LLM attention layer on a set of tokens with an image model,"Is it possible to visualize how an LLM “imagines” a token before and after processing it through the attention layer by feeding the token embeddings into an image model? I understand you can't copy paste it over, but is there a way to capture the latent transformation caused by the attention layer and apply this transformation to the embedding space of an image model?

For example if i were to enter ""poor man,"" into an LLM the embedding for ""man"" would shift toward ""beggar"" while entering ""royal man"" it could move closer to ""king."" I want to visualize that change. Then you could transfer the embedding for man to an image model and it would create the something like a beggar or a king in this example.

It could make a really cool visualization if you captured the transformation after each attention layer and made a video by interpolating each step.",jbrinkw,1gojg09,https://reddit.com/r/MachineLearning/comments/1gojg09/d_how_to_visualize_the_effect_of_an_llm_attention/,https://www.reddit.com/r/MachineLearning/comments/1gojg09/d_how_to_visualize_the_effect_of_an_llm_attention/,2024-11-11 04:01:25,23,0.94,23,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gojg09
MachineLearning,[R] Combining Induction and Transduction for Abstract Reasoning,,moschles,1goh5ym,https://reddit.com/r/MachineLearning/comments/1goh5ym/r_combining_induction_and_transduction_for/,https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf,2024-11-11 01:58:54,9,0.77,9,0,0,0,0,False,False,False,False,False,Research,default,t3_1goh5ym
MachineLearning,[R] Neural network based 'self - regression' or inverse covariance matrix,"I was wondering if neural networks have been used in  this kind of self regression problem. So instead of using a linear regression type framework, use a nonlinear neural network. 

Reference to the specific problem https://stats.stackexchange.com/questions/221348/linear-self-regression-terminology-and-references",Sandy_dude,1gocm5g,https://reddit.com/r/MachineLearning/comments/1gocm5g/r_neural_network_based_self_regression_or_inverse/,https://www.reddit.com/r/MachineLearning/comments/1gocm5g/r_neural_network_based_self_regression_or_inverse/,2024-11-10 22:19:29,2,1.0,2,0,5,0,0,False,False,True,False,False,Research,self,t3_1gocm5g
MachineLearning,[R] / [D] Your most recent favorite LLM or Diffusion Model based paper,"Hi everyone,

I'm trying to find an interesting paper to present in my research group's meeting as part of a competition. I'm interested in the advancements of language models and generative AI in computer vision, specifically using diffusion models.

I want to ask what your favorite papers related to those areas are currently and why you like them. I like papers that have a rather simple but nice innovative way of thinking that adds a lot of value to the research. Please come through with your thoughts/links and I appreciate all of your inputs. Thanks!!",Tough-Statement9740,1go8qz0,https://reddit.com/r/MachineLearning/comments/1go8qz0/r_d_your_most_recent_favorite_llm_or_diffusion/,https://www.reddit.com/r/MachineLearning/comments/1go8qz0/r_d_your_most_recent_favorite_llm_or_diffusion/,2024-11-10 19:32:31,10,0.86,10,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1go8qz0
MachineLearning,[D] External SSD for store big datasets,"Hi redditors! First time posting here.

After some research, I would like to ask which you think would be the best SSD for moving big quantities of data (working with deep learning), if someone have experienced this and already found a good solution.

I'm working in my PhD at the university and processes for buying things are painfully slow, and as I need right now a lot of space I'm considering to get one big capacity SSD for storing datasets and already trained models for the future.

I bought the KingstonXS2000 as was supposed to achieve ~2Gbps, but when tested, it was lucky if it actieved some mins the 500Mbs mark, and dropped fast.

I'm aware of the USB 3.2x2 ports and heating issues of the devices, but, even with that, after looking a bit into the net an reviews, a lot of people show near the 1Gbps in the best of the cases with the SSD I checked, even with USB 3.2 and thunderbolt ports.

So, any suggestions or good experience devices to share wourd be appreciated.

TL;DR: Working with DL, need a reliable external 2TB SSD with real high speed over time read and write operations. ",GankoX22,1go77ve,https://reddit.com/r/MachineLearning/comments/1go77ve/d_external_ssd_for_store_big_datasets/,https://www.reddit.com/r/MachineLearning/comments/1go77ve/d_external_ssd_for_store_big_datasets/,2024-11-10 18:27:19,1,0.6,1,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1go77ve
MachineLearning,[P] How can I improve accuracy of timestamp extraction?,"Hello,  
I'm trying to improve the extraction of timestamps, which all have the same format (similar to the attached example).

For whatever reason, about 20% of the timestamps aren't extracted. How can I improve the accuracy based on the code below?

Thanks

https://preview.redd.it/qqpqo25m540e1.png?width=368&amp;format=png&amp;auto=webp&amp;s=8d691fd78858e57015f4cf2f171374f25e329a0b

    def extract_time_and_location_from_image(image):
        try:
            # Enhance image preprocessing for better OCR results
            image = ImageOps.grayscale(image)
            image = ImageOps.invert(image)
            image = ImageOps.autocontrast(image)
            image = image.filter(ImageFilter.SHARPEN)
            
            text = pytesseract.image_to_string(image)
            logger.debug(f""Raw OCR text:\n{text}"")
    
            cleaned_text = ' '.join(text.split())
            logger.debug(f""Cleaned OCR text: {cleaned_text}"")
            
            # Separate regex patterns for date and time
            date_pattern = r'(\d{4}:\d{2}:\d{2})'  # Matches YYYY:MM:DD
            time_pattern = r'(\b\d{2}:\d{2}\b)'  # Matches HH:MM with word boundaries
            
            # Attempt to extract date and time separately
            date_match = re.search(date_pattern, cleaned_text)
            timestamp = None
    
            if date_match:
                # Search for time pattern only after the date match
                remaining_text = cleaned_text[date_match.end():]
                time_match = re.search(time_pattern, remaining_text)
                
                if time_match:
                    # Validate the extracted time
                    time_parts = time_match.group(0).split(':')
                    hours, minutes = int(time_parts[0]), int(time_parts[1])
                    if 0 &lt;= hours &lt; 24 and 0 &lt;= minutes &lt; 60:
                        full_timestamp = f""{date_match.group(0)} {time_match.group(0)}""
                        logger.info(f""Extracted timestamp: {full_timestamp}"")
                        timestamp = full_timestamp
                    else:
                        logger.warning(""Extracted time is not valid."")
                        timestamp = None
                else:
                    # If no time is found, default to ""00:00"" for HH:MM
                    full_timestamp = f""{date_match.group(0)} 00:00""
                    logger.info(f""Extracted partial timestamp (defaulting to 00:00): {full_timestamp}"")
                    timestamp = full_timestamp
            else:
                logger.warning(""No valid timestamp found."")
                stats[""no_timestamp_images""] += 1
                return None, ""UnknownCity"", ""UnknownCountry""
    
            # Validate the extracted timestamp format
            if not is_valid_timestamp(timestamp):
                logger.warning(""Extracted timestamp is not in the valid format."")
                return None, ""UnknownCity"", ""UnknownCountry""
    
            # Enhanced location pattern to capture more variations
            location_patterns = [
                r'([A-Za-zÀ-ÖØ-öø-ÿ\s]+),\s*([A-Za-zÀ-ÖØ-öø-ÿ\s]+)',  # City, Country
                r'([A-Za-zÀ-ÖØ-öø-ÿ\s]+)\s+([A-Za-zÀ-ÖØ-öø-ÿ\s]+)'    # City Country
            ]
            city, country = ""UnknownCity"", ""UnknownCountry""
            for location_pattern in location_patterns:
                location_match = re.search(location_pattern, cleaned_text)
                if location_match:
                    city = location_match.group(1).strip()
                    country = location_match.group(2).strip()
                    logger.info(f""Extracted location: {city}, {country}"")
                    break
    
            return timestamp, city, country
        except Exception as e:
            logger.error(f""Error extracting timestamp and location: {e}"")
            return None, ""UnknownCity"", ""UnknownCountry""",maad0000,1go6rtj,https://reddit.com/r/MachineLearning/comments/1go6rtj/p_how_can_i_improve_accuracy_of_timestamp/,https://www.reddit.com/r/MachineLearning/comments/1go6rtj/p_how_can_i_improve_accuracy_of_timestamp/,2024-11-10 18:07:59,0,0.2,0,0,4,0,0,False,False,True,False,False,Project,self,t3_1go6rtj
MachineLearning,[Discussion] Papers with fake NOVEL APPROACH in ML and DL models,"

why are a lots of the new papers ( usually done by PhDs ) with an existing approach and when u ask about their contribution they said we replace this layer by an other or we add a hyperparametters !!!!!

this is not a contribution ! i confused how can these got accepted",Rihab_Mira,1go50wf,https://reddit.com/r/MachineLearning/comments/1go50wf/discussion_papers_with_fake_novel_approach_in_ml/,https://www.reddit.com/r/MachineLearning/comments/1go50wf/discussion_papers_with_fake_novel_approach_in_ml/,2024-11-10 16:53:17,124,0.78,124,0,67,0,0,False,False,True,False,False,Discussion,self,t3_1go50wf
MachineLearning,[Research] Looking for interesting research on movies datasets (NO generative models),"Hey ML researchers!

I've been diving deep into multimodal learning and I'm specifically interested in papers that utilize movie/video datasets in creative ways. I'm NOT looking for video generation or diffusion-related papers, but rather interesting approaches to:

* Multimodal representation learning from movies
* Novel fusion techniques combining video, audio, and text modalities
* Scene understanding/contextual learning from film data
* Character interaction analysis
* Emotion/sentiment analysis across modalities
* Cross-modal retrieval using movie data

Would love to hear about any cool papers you've come across in this space!",stoneddumbledore,1go1wps,https://reddit.com/r/MachineLearning/comments/1go1wps/research_looking_for_interesting_research_on/,https://www.reddit.com/r/MachineLearning/comments/1go1wps/research_looking_for_interesting_research_on/,2024-11-10 14:31:46,4,0.67,4,0,4,0,0,False,False,True,False,False,Research,self,t3_1go1wps
MachineLearning,"[R] Classic GNNs (GCNs, GraphSAGEs, GATs) are Strong Baselines on Node Classification","We’re excited to share our recent paper ""[\[NeurIPS 2024\] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification](https://arxiv.org/pdf/2406.08993).""

In this study, we conduct a thorough review of classic GNNs for node classification tasks. Our findings suggest that the superior performance often reported by state-of-the-art graph learning models may be due to suboptimal hyperparameter configurations in classic GNNs. By fine-tuning these hyperparameters, we show that classic GNNs outperform the latest models on 17 out of 18 widely used node classification datasets.

Code: [https://github.com/LUOyk1999/tunedGNN](https://t.co/QeNSn2D9CN)  
Arxiv: [https://arxiv.org/abs/2406.08993](https://t.co/MD4mVTnHk8)

If you find our work interesting, we’d greatly appreciate a ⭐️ on GitHub!",luoyuankai,1gnsn54,https://reddit.com/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/,https://www.reddit.com/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/,2024-11-10 04:32:19,46,0.92,46,0,3,0,0,False,False,True,False,False,Research,self,t3_1gnsn54
MachineLearning,[R]/[P] Looking for papers about cost estimation for industrial plants,"Hello everyone. I'm currently preparing a data set for a project in my company that aims to estimate the price of industrial carbon capture plants we build. The plant extracts CO2 from flue gas from e.g. chemical processes that emit a lot of CO2. Based on the flue gas composition, the engineer designs the plant, which can be a really time-consuming process. The data I'm currently preparing will consist of previously created offers from engineers.

My aim of the project is to build a model which uses the flue gas composition (around 10 floating point values) to estimate the costs of a plant or to recommend a similar project. The requirements for the project are not yet set but considering the model should be explainable and be able to handle smaller data sets, a regression tree might be the first thing I'd like to try once the data is ready.

Has anyone read of useful papers or has experience from similar projects? Most of the papers I find are about cost estimation of 3D parts that use geometrical data as input.",Maendli,1gnzcxi,https://reddit.com/r/MachineLearning/comments/1gnzcxi/rp_looking_for_papers_about_cost_estimation_for/,https://www.reddit.com/r/MachineLearning/comments/1gnzcxi/rp_looking_for_papers_about_cost_estimation_for/,2024-11-10 12:14:14,1,0.67,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1gnzcxi
MachineLearning,[D] Best Approach to Dimensionality Reduction with PCA for Multi-Line Data Per Job?,"Hello! I'm working with a dataset where I have 300 jobs, each with a single target label. For each job, I have around 1000 data points (rows), and each data point is represented by a 17-dimensional vector with various parameters.

I’d like to reduce these 1000 rows for each job down to a single representative vector to use for model training. However, I want to avoid just using the mean and variance of each column, as I think this would lose too much information.

Would using PCA be a good approach here? If so, could I use the first principal component (PCA1) and its associated variance to form a single representative vector? For example, would projecting each 17D vector onto PCA1 and then taking a weighted average of these projections (weighted by PCA1’s explained variance) yield a good single vector per job?

Thank you very much and have a nice weekend.",aaronhallam773,1gnyyol,https://reddit.com/r/MachineLearning/comments/1gnyyol/d_best_approach_to_dimensionality_reduction_with/,https://www.reddit.com/r/MachineLearning/comments/1gnyyol/d_best_approach_to_dimensionality_reduction_with/,2024-11-10 11:49:06,6,0.88,6,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gnyyol
MachineLearning,[P] Help needed to run 3D model generation code with .ckpt files on CUDA 12.5 GPU (RTX A6000) ,,mnkhtlg,1gnwmo5,https://reddit.com/r/MachineLearning/comments/1gnwmo5/p_help_needed_to_run_3d_model_generation_code/,/r/computervision/comments/1gnwgub/help_needed_to_run_3d_model_generation_code_with/,2024-11-10 09:03:07,0,0.27,0,0,1,0,0,False,False,False,False,False,Project,default,t3_1gnwmo5
MachineLearning,[P] Built a roadmap site and got 450 users in 25 days and I am so happy!!!!!!,"hello everyone, I am a 3rd year cse student. I built this site called [https://www.mldl.study/](https://www.mldl.study/) last month. this site is for anyone who is ""new"" to machine learning and deep learning and is confused about where to start. I built this because I was confused about it too. It has got proper video lectures, articles, research papers, visualizations, kaggle competitions and basically everything you need to master ml and dl in proper order.



i just added google analytics 25 days back and I saw that I have got like 450 users and 135 returning users. I built this just to help my college friends but I am so glad that its helping others too. I just wanted to share this as I am so happy about this. This gives me confidence that I can build something more cooler and useful in future.



Thanks everyone. I got little push in my analytics from here only. THANKYOU!!



(I am also open to suggestions and all, what I can do to grow it even more)

https://preview.redd.it/s9v6omy5f10e1.png?width=1558&amp;format=png&amp;auto=webp&amp;s=eeb9a22012e2e3806245e9267a1187bb91e75305

",Grouchy-Breakfast-20,1gnwfhn,https://reddit.com/r/MachineLearning/comments/1gnwfhn/p_built_a_roadmap_site_and_got_450_users_in_25/,https://www.reddit.com/r/MachineLearning/comments/1gnwfhn/p_built_a_roadmap_site_and_got_450_users_in_25/,2024-11-10 08:48:21,37,0.76,37,0,14,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/4b2UbAU1kNIreVsOAKtpeX_K5Uy-HCvm8TSytI4J_Zg.jpg,t3_1gnwfhn
MachineLearning,[R] AAAI Phase 2 Rebuttal Response and Reviewer Updates in Openreview,"I would like to know if we can view new responses and updated ratings from the reviewers as they submit them in OpenReview, or if we need to wait until December 9th. Additionally, can all reviewers see the responses we submitted to other reviewers during the rebuttal period, or is each reviewer only able to view the response directed to them?",morphinejunkie,1gnvy2i,https://reddit.com/r/MachineLearning/comments/1gnvy2i/r_aaai_phase_2_rebuttal_response_and_reviewer/,https://www.reddit.com/r/MachineLearning/comments/1gnvy2i/r_aaai_phase_2_rebuttal_response_and_reviewer/,2024-11-10 08:13:00,6,0.8,6,0,2,0,0,False,False,True,False,False,Research,self,t3_1gnvy2i
MachineLearning,"[D] On ""reverse"" embedding (i.e. embedding vectors/tensors to text, image, etc.)","EDIT: I didn't mean decoder per se, and it's my bad for forgetting to clarify that. What I meant was for a (more) direct computational or mathematical framework that doesn't involve training another network to do the reverse-embedding.

----------

As the title alluded, are there methods and/or processes to do reverse-embedding that perhaps are currently being researched? From the admittedly preliminary internet-sleuthing I did yesterday, it seems to be essentially impossible because of how intractable the inverse-mapping is gonna play out. And on that vein, how it's practically impossible to carry out with the current hardware and setup that we have.

However, perhaps some of you might know some literature that might've gone into that direction, even if at theoretical or rudimentary level and it'd be greatly appreciated if you can point me to those resources. You're also welcome to share your thoughts and theories as well.

Expanding from reverse-embedding, is it possible to go beyond the range of the embedding vectors/tensors so as to reverse-embed said embedding vectors/tensors and then retrieve the resulting text, image, etc. from them?

Many thanks in advance!",YsrYsl,1gnrta9,https://reddit.com/r/MachineLearning/comments/1gnrta9/d_on_reverse_embedding_ie_embedding/,https://www.reddit.com/r/MachineLearning/comments/1gnrta9/d_on_reverse_embedding_ie_embedding/,2024-11-10 03:43:56,8,0.73,8,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1gnrta9
MachineLearning,[D] Log Probability and Information Theory,"In machine learning we work with log probabilities a lot, attempting to maximize log probability. This makes sense from a numerical perspective since adding is easier than multiplying but I am also wondering if there is a fundamental meaning behind ""log probability.""

For instance, log probability is used a lot in information theory, and is the negative of 'information'. Can we view minimizing the negative log likelihood in terms of information theory? Is it maximizing/minimizing some metric of information?",masonw32,1gnrpfe,https://reddit.com/r/MachineLearning/comments/1gnrpfe/d_log_probability_and_information_theory/,https://www.reddit.com/r/MachineLearning/comments/1gnrpfe/d_log_probability_and_information_theory/,2024-11-10 03:37:47,85,0.94,85,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1gnrpfe
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1gnrb08,https://reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/,2024-11-10 03:15:11,7,0.74,7,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1gnrb08
MachineLearning,"[N] The ARC prize offers $600,000 for few-shot learning of puzzles made of colored squares on a grid.",,moschles,1gnnstd,https://reddit.com/r/MachineLearning/comments/1gnnstd/n_the_arc_prize_offers_600000_for_fewshot/,https://arcprize.org/competition,2024-11-10 00:08:20,109,0.91,109,0,37,0,0,False,False,False,False,False,News,https://a.thumbs.redditmedia.com/T60ZeRDeKyitaadQvibHrHboUSfYL5X13PpSwSE5qZ4.jpg,t3_1gnnstd
MachineLearning,Why are model_q4.onnx and model_q4f16.onnx not 4 times smaller than model.onnx? [D],"I see on https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx:

| File Name          | Size   |
|--------------------|--------|
| model.onnx         | 654 MB |
| model_fp16.onnx    | 327 MB |
| model_q4.onnx      | 200 MB |
| model_q4f16.onnx   | 134 MB |


I understand that:

- `model.onnx` is the fp32 model,
- `model_fp16.onnx` is the model whose weights are quantized to `fp16`

I don't understand the size of `model_q4.onnx` and `model_q4f16.onnx`

1. Why is `model_q4.onnx` 200 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4.onnx` meant that the weights are quantized to 4 bits.
2. Why is `model_q4f16.onnx` 134 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4f16.onnx` meant that the weights are quantized to 4 bits and activations are fp16, since https://llm.mlc.ai/docs/compilation/configure_quantization.html states:

   &gt;  `qAfB(_id)`, where `A` represents the number of bits for storing weights and `B` represents the number of bits for storing activations. 

  and [Why do activations need more bits (16bit) than weights (8bit) in tensor flow's neural network quantization framework?](https://stackoverflow.com/a/72397979/395857) indicates that activations don't count toward the model size (understandably).",Franck_Dernoncourt,1gni61w,https://reddit.com/r/MachineLearning/comments/1gni61w/why_are_model_q4onnx_and_model_q4f16onnx_not_4/,https://www.reddit.com/r/MachineLearning/comments/1gni61w/why_are_model_q4onnx_and_model_q4f16onnx_not_4/,2024-11-09 19:43:30,4,0.64,4,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gni61w
MachineLearning,"[D] adaptive optimizers, downscaling, and resets","I've been experimenting with adaptive optimizers such as Prodigy, and Dadapt-LION.

Ive noticed that if i run them over a 1 million step dataset, they will start at 1e06,. .then go up to lets say 5e06.... and later still go up to 9e06 and stay there.

But if I stop them halfway..... then train on the results, it might go up to only 6e06.

Are there no standard ways to at worst, reset them, but better still actually adjust downwards when appropriate?

I guess ideally I would like some thing with an effect like a reverse ""cosine with hard reset"".

Instead of SLOOOWLY forcing the LR lower and lower.. and then suddenly letting it pop up again...

instead suddenly force the LR, etc to its original starting point, and let it redo the adaptive growth process again? And repeat that for some number of learning cycles.

Anything like that?",lostinspaz,1gnh4fe,https://reddit.com/r/MachineLearning/comments/1gnh4fe/d_adaptive_optimizers_downscaling_and_resets/,https://www.reddit.com/r/MachineLearning/comments/1gnh4fe/d_adaptive_optimizers_downscaling_and_resets/,2024-11-09 18:56:10,0,0.2,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gnh4fe
MachineLearning,"[R] Jay McClelland explains Parallel Distributed Processing, how the brain works, Hebbian learning, and backpropagation","Jay McClelland is a pioneer in the field of artificial intelligence and is a cognitive psychologist and professor at Stanford University in the psychology, linguistics, and computer science departments. Together with David Rumelhart, Jay published the two volume work Parallel Distributed Processing, which has led to the flourishing of the connectionist approach to understanding cognition.

In this conversation, Jay gives us a crash course in how neurons and biological brains work. This sets the stage for how psychologists such as Jay, David Rumelhart, and Geoffrey Hinton historically approached the development of models of cognition and ultimately artificial intelligence. We also discuss alternative approaches to neural computation such as symbolic and neuroscientific ones and the development of backpropagation.

https://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=2e5be31c51a8eb78bf7033d1def25fa29f0863af

https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e7c952d579322379c67a77adadf1d392afe8d3c6

Youtube:  
[https://www.youtube.com/watch?v=yQbJNEhgYUw&amp;list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&amp;index=1&amp;pp=iAQB](https://www.youtube.com/watch?v=yQbJNEhgYUw&amp;list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&amp;index=1&amp;pp=iAQB)

Spotify: [https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG](https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG)

RSS: [https://feed.podbean.com/cartesiancafe/feed.xml](https://feed.podbean.com/cartesiancafe/feed.xml)",IamTimNguyen,1gng4fy,https://reddit.com/r/MachineLearning/comments/1gng4fy/r_jay_mcclelland_explains_parallel_distributed/,https://www.reddit.com/r/MachineLearning/comments/1gng4fy/r_jay_mcclelland_explains_parallel_distributed/,2024-11-09 18:11:11,22,0.79,22,0,1,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/8RQtRvmXhNSRIH0ZE4BJvqvOK_8l2voOoq50W0QZtPw.jpg,t3_1gng4fy
MachineLearning,[R] Advice on Fine-Tuning Meta's Segment Anything 2 (SAM) Model — Balancing Edge cases with Generalizability,"I was working with SAM2 and have been trying to figure out the best way to fine-tune it for my specific use case. A few considerations that I was hoping get some insights on:

1. **Error Correction vs Generalization:** If I'm interested in fine-tuning the model to perform better on cases where it went wrong most on, can I retains its performance on the examples it was already doing well on. i.e. still maintaining (or even improving) its prior generalizability? Or should I have enough number of examples it was doing well already on to preserve that performance?
2. **Which Components to Fine-Tune?** In terms of the model's architecture, I've seen different advice on whether to fine-tune just the **mask decoder**, the **prompt encoder**, or both. In your experience, is fine-tuning just the mask decoder enough to improve performance, or do you need to adjust the prompt encoder as well? Or maybe there's more to it—like the backbone or other parts of the model? Is it computationally too much of a difference? Or are there other downsides/considerations as well?
3. **Real-World Experiences:** For those who have fine-tuned SAM before, how has your experience been? Any tips, tricks, or pitfalls I should watch out for? Also, how did you go about preparing your fine-tuning dataset? Any suggestions on balancing the diversity of data vs focusing on edge cases?",No_Cartoonist8629,1gnf7zi,https://reddit.com/r/MachineLearning/comments/1gnf7zi/r_advice_on_finetuning_metas_segment_anything_2/,https://www.reddit.com/r/MachineLearning/comments/1gnf7zi/r_advice_on_finetuning_metas_segment_anything_2/,2024-11-09 17:31:01,0,0.29,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1gnf7zi
MachineLearning,[R] When Machine Learning Tells the Wrong Story,,jackcook,1gne2x1,https://reddit.com/r/MachineLearning/comments/1gne2x1/r_when_machine_learning_tells_the_wrong_story/,https://jackcook.com/2024/11/09/bigger-fish.html,2024-11-09 16:40:10,57,0.88,57,0,12,0,0,False,False,False,False,False,Research,default,t3_1gne2x1
MachineLearning,[D] latent space forecasting of the next frame,"Hey people,
I'm searching papers or hints for a computer vision task. I have implemented a Vision Transformer for image classification. In the next step I have to implement a predictor on top of the encoder network of the ViT, which predicts from enc(x_t) -&gt; enc(x_t+1). The predictor should predict the embedding of the next frame. my first idea is a MLP head or decoder network.
If someone has tackled a similar task, im happy about recommendations.
Ty",Significant-Joke5751,1gncn8y,https://reddit.com/r/MachineLearning/comments/1gncn8y/d_latent_space_forecasting_of_the_next_frame/,https://www.reddit.com/r/MachineLearning/comments/1gncn8y/d_latent_space_forecasting_of_the_next_frame/,2024-11-09 15:34:09,3,0.67,3,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gncn8y
MachineLearning,[P] Benchmark or open source supervised datasets with text or image features and real-valued regression target?,"For some reason, I can't seem to find any well known benchmark datasets that have text or images as features, and real-valued targets. Any target range is fine ( (0,1), (-infinity, infinity), (0, infinity), etc.) I have found examples with *ordinal* classification targets (e.g. integer rating from 1-5), but that doesn't serve my purpose.

Does anyone know of any open source supervised ML data that fits this description? Preferably a benchmarked one with a performance leaderboard.",BreakingBaIIs,1gnb9z5,https://reddit.com/r/MachineLearning/comments/1gnb9z5/p_benchmark_or_open_source_supervised_datasets/,https://www.reddit.com/r/MachineLearning/comments/1gnb9z5/p_benchmark_or_open_source_supervised_datasets/,2024-11-09 14:28:56,4,0.83,4,0,6,0,0,False,False,True,False,False,Project,self,t3_1gnb9z5
MachineLearning,"[D] Last Week in Medical AI: Top LLM Research Papers/Models (November 2 - November 9, 2024)
","[Last Week in Medical AI: Top LLM Research Papers\/Models \(November 2 - November 9, 2024\)](https://preview.redd.it/nytnbrppcvzd1.png?width=1386&amp;format=png&amp;auto=webp&amp;s=2339a74f15050a972b113cee2a35e4ca11353852)

  
**Medical AI Paper of the Week:**

* **Google presents**\*: Exploring Large Language Models for Specialist-level Oncology Care\*
   * This paper evaluates AMIE, a conversational diagnostic AI system, in breast oncology using 50 synthetic cancer vignettes.   Enhanced with web search retrieval and a self-critique pipeline, AMIE outperformed internal medicine trainees and oncology fellows in generating management plans, evaluated using a detailed clinical rubric encompassing case summarization, plan safety, and treatment recommendations.

**Medical LLM &amp; Other Models:**

* AutoProteinEngine: Multimodal Protein LLM
   * This paper introduces AutoProteinEngine (AutoPE), an LLM-powered multimodal AutoML framework for protein engineering, enabling biologists without deep learning expertise to interact with DL models using natural language.  AutoPE integrates LLMs with AutoML for model selection (sequence and graph modalities), hyperparameter optimization, and automated data retrieval, demonstrating significant performance improvements over traditional methods in two real-world protein engineering tasks. Code is available at:

* GSCo: Generalist-Specialist AI Collaboration
   * This paper introduces GSCo, a framework for medical image analysis combining Generalist Foundation Models (GFMs) and specialist models. It develops MedDr, the largest open-source medical GFM, and lightweight specialists for downstream tasks.

* SAM for Lung X-ray Segmentation
   * This paper explores the application of Meta AI's Segment Anything Model (SAM) to chest X-ray analysis for lung segmentation.  Using a transfer learning approach with fine-tuning, the study demonstrates improved performance compared to the original SAM, achieving results comparable to state-of-the-art models like U-Net.

* MEG: Knowledge-Enhanced Medical QA
   * This paper introduces MEG, a parameter-efficient method for augmenting Large Language Models (LLMs) with medical knowledge graphs using a lightweight mapping network.  Evaluated on four medical multiple-choice datasets, MEG achieves a 10.2% accuracy improvement over the Mistral-Instruct baseline and 6.7% over specialized models like BioMistral, demonstrating the benefit of knowledge graph integration.

  
  
**Frameworks and Methodologies:**

* BrainSegFounder: 3D Neuroimage Analysis
* PASSION: Sub-Saharan Dermatology Dataset
* Label Critic: Data-First Approach
* Medprompt Runtime Strategies



**Medical LLM Applications:**

* CataractBot: Patient Support System
* CheX-GPT: X-ray Report Enhancement
* CardioAI: Cancer Cardiotoxicity Monitor
* HealthQ: Healthcare Conversation Chain
* PRObot: Diabetic Retinopathy Assistant

  
  
**Medical LLMs &amp; Benchmarks:**

* MediQ: Clinical Reasoning Benchmark
* Touchstone: Segmentation Evaluation
* Medical LLM Adaptation Progress
* Fine-Tuning Medical QA Strategies

  
  
**AI in Healthcare Ethics:**

* Healthcare Robotics with LLMs
* XAI in Clinical Practice
* Precision Rehabilitation Framework
* Multimodal AI Challenges

Full thread in detail : [https://x.com/OpenlifesciAI/status/1855207141302473090](https://x.com/OpenlifesciAI/status/1855207141302473090)

",aadityaura,1gn8wqp,https://reddit.com/r/MachineLearning/comments/1gn8wqp/d_last_week_in_medical_ai_top_llm_research/,https://www.reddit.com/r/MachineLearning/comments/1gn8wqp/d_last_week_in_medical_ai_top_llm_research/,2024-11-09 12:20:58,1,0.67,1,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/BsFFdQNkQ0JAY8_ZrqpxNkxln0RMpK5ggACx_MHkgKU.jpg,t3_1gn8wqp
MachineLearning,[P] MiniBoosts: A small collection of boosting algorithms,"Hello, everyone.  
I wrote a small collection of boosting algorithms in Rust named [MiniBoosts](https://github.com/rmitsuboshi/miniboosts).

This is a hobby project, but I would like to improve more.  
Any feedback is welcome.

I appreciate your cooperation.",__leopardus__,1gn8mpt,https://reddit.com/r/MachineLearning/comments/1gn8mpt/p_miniboosts_a_small_collection_of_boosting/,https://www.reddit.com/r/MachineLearning/comments/1gn8mpt/p_miniboosts_a_small_collection_of_boosting/,2024-11-09 12:03:37,15,0.9,15,0,0,0,0,False,False,True,False,False,Project,self,t3_1gn8mpt
MachineLearning,[D] Embeddings and docker file - comparison between two libraries - Is there something better than ONNX? ,"As title said I was wondering if there are some other ways to embedd corpus without using torch. One of the solution I came up with was by using ONNX. I created the images by using the fastembed library from Qdrant and the sentence-transformer library. Using fastembed result in a significant image size reduction.

# Question:

Are there other ways (for example modifying the dockerfile or using other libraries) to shrink the docker image even more?

public repo: [https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison](https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison)",Ambitious-Most4485,1gn87vi,https://reddit.com/r/MachineLearning/comments/1gn87vi/d_embeddings_and_docker_file_comparison_between/,https://www.reddit.com/r/MachineLearning/comments/1gn87vi/d_embeddings_and_docker_file_comparison_between/,2024-11-09 11:36:10,7,0.89,7,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gn87vi
MachineLearning,[D] Has anyone replaced Transformers with fully-connected layers and verified that it performs strictly worse (for training language models)?,"Seems an obvious question but such a ""data point"" would be very helpful to clear our ignorance.",Cybernetic1,1gn6sam,https://reddit.com/r/MachineLearning/comments/1gn6sam/d_has_anyone_replaced_transformers_with/,https://www.reddit.com/r/MachineLearning/comments/1gn6sam/d_has_anyone_replaced_transformers_with/,2024-11-09 09:52:49,0,0.25,0,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1gn6sam
MachineLearning,[P] Open-Source Text-to-Agent : framework to develop AI agents from YAML files.,"Hey guys, wanted to get your feedback on a project I'm developing. I'm building a framework to define AI agents from YAML configuration files. These files encapsulate tasks that need to be done, how they connect etc, while all the rest is abstracted away.

Now the idea is to use LLMs themselves to create those YAML files from a user prompt. Since the config file has all the core logic of the agent and removes all unnecessary details, I think this is the most efficient way to build a text-to-agent framework. Wdyt?

Let me know your thoughts, and have a look at the repo [https://github.com/octopus2023-inc/gensphere](https://github.com/octopus2023-inc/gensphere)

Let me know if you want to contribute and make it work.",Jazzlike_Tooth929,1gn2e5w,https://reddit.com/r/MachineLearning/comments/1gn2e5w/p_opensource_texttoagent_framework_to_develop_ai/,https://www.reddit.com/r/MachineLearning/comments/1gn2e5w/p_opensource_texttoagent_framework_to_develop_ai/,2024-11-09 04:47:39,4,0.67,4,0,2,0,0,False,False,True,False,False,Project,self,t3_1gn2e5w
MachineLearning,[D] PAKDD 2023 data? ,"i was looking into the research papers published in PAKDD 2023. From the names of the authors, I can guess that they are Chinese, Korean, or Japanese

I know PAKDD is a double-blind review. But why other people don't submit their work? or if they submit why the number of acceptance is low

I am also Asian, so I am not trying to be racist here. Just wondering why it is like that",Alarming-Camera-188,1gn282v,https://reddit.com/r/MachineLearning/comments/1gn282v/d_pakdd_2023_data/,https://www.reddit.com/r/MachineLearning/comments/1gn282v/d_pakdd_2023_data/,2024-11-09 04:37:23,1,0.67,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gn282v
MachineLearning,[D] Simple ML model hosting service?,"My job’s looking for a way for ai to help generate plans, I really think a simple multi-variable model should do the trick; just need to find a reliable hosting service that can be built upon however needed. Are there well established ML hosters that are scalable, configurable, all that?",Lucrayzor,1gmzb3q,https://reddit.com/r/MachineLearning/comments/1gmzb3q/d_simple_ml_model_hosting_service/,https://www.reddit.com/r/MachineLearning/comments/1gmzb3q/d_simple_ml_model_hosting_service/,2024-11-09 01:55:43,15,0.89,15,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1gmzb3q
MachineLearning,[R] Most Time Series Anomaly Detection results are meaningless (two short videos explain why),"Dear Colleagues

Time Series Anomaly Detection (TSAD) is hot right now, with dozens of  papers each year in NeurIPS, SIGKDD, ICML, PVLDB etc.

However, I claim that much of the published results are meaningless, because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements.

I have made two 90-second-long videos that make this clear in a visual and intuitive way:

 1)      Why Most Time Series Anomaly Detection Results are Meaningless (Dodgers)

[https://www.youtube.com/watch?v=iRN5oVNvZwk&amp;ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=iRN5oVNvZwk&amp;ab_channel=EamonnKeogh)

  2)      Why Most Time Series Anomaly Detection Results are Meaningless (AnnGun)

[https://www.youtube.com/watch?v=3gH-65RCBDs&amp;ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=3gH-65RCBDs&amp;ab_channel=EamonnKeogh)

As always, corrections and comments welcome.

Eamonn

 EDIT: To be clear, my point is simply to prevent others from wasting time working with datasets with essentially random labels. In addition, we should be cautious of any claims in the literature that are based on such data (and that includes at least dozens of highly cited papers)

  


For a review of most of the commonly used TSAD datasets, see this file:

[https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&amp;dl=0](https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&amp;dl=0)",eamonnkeogh,1gmwxnr,https://reddit.com/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/,https://www.reddit.com/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/,2024-11-08 23:58:19,111,0.94,111,0,60,0,0,False,False,True,False,False,Research,self,t3_1gmwxnr
MachineLearning,[D] AI-Generated gameworlds based on classic games? (Ex - Spyro),"I was wondering if anyone had any thoughts on how far out something like this might be or how difficult this is. Ever since the advent of the current era of ai/llms, I thought it would be great to somehow be able to feed data from nostalgic games in some form and create some type of system that is able to generate these worlds infinitely - while still being very true to the style and layout/ethos of the worlds/levels from the reference game. I feel like it would just be so wonderful if there was a path to creating some type of 'never-ending' &lt;insert nostalgic game here&gt; instead of being limited to what the devs put out back in the day.

If anyone has any insight or thoughts on this, please let me know :). I work in the AI space, but I integrate the models, and don't do any training or anything on the low level ML side. Also, yes, I'm only think about the gameworlds/levels atm.",cobalt1137,1gmspmt,https://reddit.com/r/MachineLearning/comments/1gmspmt/d_aigenerated_gameworlds_based_on_classic_games/,https://www.reddit.com/r/MachineLearning/comments/1gmspmt/d_aigenerated_gameworlds_based_on_classic_games/,2024-11-08 20:49:28,8,0.79,8,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gmspmt
MachineLearning,[D] Training on Petabyte scale datasets,"Lets say we have a dataset that is much larger than we have disk storage. For example:

* Dataset: 1PB
* Our disk storage: 10TB
* GPU RAM: 8x80GB (not super relevant to this discussion)

What are the usual approaches to training on something like this? What I can think of intuitively is to do the following in parallel somehow:

\- prefetch block n, train on block n-1, delete block n-2 from disk

Lets say we use PyTorch, so we have a PyTorch Dataset that has all the paths to where the data is stored in the cloud. Do we need to write code for the prefetcher/deleter that downloads from the cloud and store on disk and have it run in a separate process, then have a DataLoader for training that just assumes that it can read from disk (because the prefetcher does its job correctly)? Having the DataLoader read from S3 would be bad for GPU utilization, right?

To take a step back, I'm assuming that this is ordinary and often occuring ""problem"" for every company that trains on large datasets, so I'm skeptical to writing all of this code by myself as I feel like there should be standard out of the box solutions for this, but can't really find anything that matches perfectly.",lapurita,1gmpedb,https://reddit.com/r/MachineLearning/comments/1gmpedb/d_training_on_petabyte_scale_datasets/,https://www.reddit.com/r/MachineLearning/comments/1gmpedb/d_training_on_petabyte_scale_datasets/,2024-11-08 18:27:06,41,0.93,41,0,30,0,0,False,False,True,False,False,Discussion,self,t3_1gmpedb
MachineLearning,[R] GPU as a service,"Hi all, I have a few GPUs left over from mining, and I’m interested in starting a small-scale GPU-as-a-service. My goal is to set up a simple, side income that could help pay off my credit cards, as I already have a primary job.

What steps are needed for getting started with a small-scale GPU-as-a-service business focused on machine learning or AI? Any insights would be greatly appreciated!

Thanks in advance for any advice you can share!",chazzyfe,1gmofpq,https://reddit.com/r/MachineLearning/comments/1gmofpq/r_gpu_as_a_service/,https://www.reddit.com/r/MachineLearning/comments/1gmofpq/r_gpu_as_a_service/,2024-11-08 17:46:36,0,0.33,0,0,10,0,0,False,False,True,False,False,Research,self,t3_1gmofpq
MachineLearning,[D] What are crazy structures or update rule that might be useful(or not)? Extreme ideas are welcome,"Context: I was making what was supposed to be an FP-oriented NN library/framwork on top of JAX (which too was FP-oriented) called z-zephyr on pip. However, I noticed something you could do with it that kinda clunky, if not tedious, with other frameworks. 

(please read context)

TLDR; Zephyr turns out to be very good way (at least in my experience) to make structures that are weird. And I recently just added update capabilities so that zephyr doesn't only do structures but updates too.

Disclaimer: You can this with other frameworks, I have tried many of things I will tell below in other frameworks or libraries, and it's just painful for me or i'm just inexperienced with those. 

Here are the crazy things that's quick to do in zephyr, that might not be as quick in other frameworks (if it could be done easily in other frameworks more easily, please tell me).

(These are not supposed to be useful, they're supposed to be extreme)

### Full Binary Tree as Neural Network
- edges have an associated weight
- input is a scalar (could be a batch with JAX vmap, but let's consider 1)
- output an array of shape (2^n,) where n is the depth of the tree
- an update rule that takes into account if the weight is a {L}eft or {R}ight branch (i'll keep it simple, but it can easily be anything)

Here is the tree network in zephyr, and how you get the initial params and tags (tag, is the key in params[key]).
```python
    # essentially 4 lines of code
    @flexible
    def tree_net(params, x, n, i=0):
        if i == n-1:
            return [x]
        return (
            tree_net(
                params[""branch""][""L""] if i !=n-2 else params, 
                validate(params[""weight""][""L""], (1,), uniform) * x, 
                n, 
                i+1) + 
            tree_net(
                params[""branch""][""R""] if i !=n-2 else params, 
                validate(params[""weight""][""R""], (1,), uniform) * x, 
                n, 
                i+1)
        )

    x = jnp.ones((1,)) # dummy
    N = 4
    params = trace(tree_net, key, x, N)
    tags = get_lineage_tags(params)
```

assume you had the loss function and gradients and what not, to keep it simple, i'll just update so that the left branch have weights 0, and the rights ones are kept the same. 

```python
    def make_left_zero(params, tags): # i left out gradients 
        if tags[-1] == ""L"":
            return params * 0
        
        return params

    # update the params 
    params = apply_updates(make_left_zero, params, tags)
```

### Other things you could do with zephyr now (I have tried, and the code is easy for me to do and i'm not that great of a coder)
- multi-layer network and use the depth of the network (via a tag) to calculate updates of parameters
- tag some weights as ""fast"" or ""slow"" and use those tags in updating
- create an MLP with neurons as Wx+b. Notice that the neuron is a function that is Array -&gt; Scalar. So I could replace each neuron in that MLP, with another MLP whose output is a scalar (array of shape (1,) ). Or replace the neurons in that with any neural network (any function) that is Array -&gt; Scalar. 

---


### What architectures/structures with custom updates rules can you think of that are easy to write(pseudo-code/math or description) but possible cumbersome to implement right now?

Please suggest some extreme idea for me to try. 

I think zephyr could be the tooling to make those easy to do. I would like to hear your extreme ideas, so I can try to code them zephyr, and if i can't do it without strugling, and if it's something i think is generic enough, I will evolve zephyr to handle it more easily.

PS: The readme doesn't include these yet, since it started as an (normal) NN library.

The link of the repo will be in the comments if you want to check it out.",Pristine-Staff-5250,1gmm7mi,https://reddit.com/r/MachineLearning/comments/1gmm7mi/d_what_are_crazy_structures_or_update_rule_that/,https://www.reddit.com/r/MachineLearning/comments/1gmm7mi/d_what_are_crazy_structures_or_update_rule_that/,2024-11-08 16:14:12,12,0.72,12,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gmm7mi
MachineLearning,[R] Benchmarking Large Language Models with Integer Sequence Generation Tasks,"**Benchmarking Large Language Models with Integer Sequence Generation Tasks**  
Daniel O'Malley, Manish Bhattarai, Javier Santos - Los Alamos National Laboratory  
This paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the Online Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the o1 series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.  
arXiv:2411.04372 \[cs.LG\]: [https://arxiv.org/abs/2411.04372](https://arxiv.org/abs/2411.04372)

https://preview.redd.it/4vvh5s21unzd1.jpg?width=588&amp;format=pjpg&amp;auto=webp&amp;s=c8bece31712d5d6378188c88e14b9f56e477d41f

",Nunki08,1gmg2dl,https://reddit.com/r/MachineLearning/comments/1gmg2dl/r_benchmarking_large_language_models_with_integer/,https://www.reddit.com/r/MachineLearning/comments/1gmg2dl/r_benchmarking_large_language_models_with_integer/,2024-11-08 11:07:10,10,0.82,10,0,1,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/iNeKFBb5JNY11CkpzPwteFKRlXGtnL5Vv0CzvmBkPNg.jpg,t3_1gmg2dl
MachineLearning,[D] prediction variability for target with statistics for features,"Hi. I'm trying to use ML/DL model for predicting variability statistics like min, max, avg, var, with several features same as target.

For example, 
- Input: 
  - min, max, average, variance for the number of customer arrivals in a day
  - min, max, average, variance for the number of customer departures in a day
- Output:
  - min, max, average, variance for the number of waiting customers in a day

I find several papers related to interval or range prediction for various area like wind power, stock price or solar energy, but I think those papers are different to my purpose. Almost every papers are predicting specific constant value based on time series data first, and use statistical method to estimate prediction interval.

I'm trying to find a way for prediction variability of target value with variability of features. My best idea is make each model to predict each statistics, like one model for minimum, other model for average, ... But I think there is a better way to do this. Is there any ML/DL model or other technique/methodology for this purpose?",caution721,1gmezw8,https://reddit.com/r/MachineLearning/comments/1gmezw8/d_prediction_variability_for_target_with/,https://www.reddit.com/r/MachineLearning/comments/1gmezw8/d_prediction_variability_for_target_with/,2024-11-08 09:51:23,2,0.76,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gmezw8
MachineLearning,[D] Looking for Advice &amp; Resources on ASD Prediction Using Voice Cues,"Hey everyone! I’m working on my final-year project for my Bachelor’s, where I’m trying to predict Autism Spectrum Disorder (ASD) using voice cues. I’ve worked on some basic ML projects and CNNs before, but this is my first time dealing with audio data, and I’ll be collecting samples from young kids with ASD, from toddlers up to age 12.

I could really use some help finding resources to get a solid grasp on signal processing and how to train classification models specifically on audio. Also, if anyone knows of any open datasets in this area (I haven’t had much luck there) or has any advice or resources, I’d be super grateful. Thanks a ton in advance!",General-Ad6585,1gmcqp0,https://reddit.com/r/MachineLearning/comments/1gmcqp0/d_looking_for_advice_resources_on_asd_prediction/,https://www.reddit.com/r/MachineLearning/comments/1gmcqp0/d_looking_for_advice_resources_on_asd_prediction/,2024-11-08 06:59:30,3,1.0,3,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gmcqp0
MachineLearning,[D] What tools do you recommend to manage ML data sets and evaluations? ,"Hello, our company recently decided to expand our ML team from a very small 2 person team to a more serious efforts.

When we were small, we really didnt have a way to manage data sets or evaluations. They were just files checked into a github repo.

But increasingly we find, with multiple ML models (some llm and some not), and many iterations of datasets (some experimental and some not). It's really hard to version them in a meaningful way and be able to compare and analyze them.

We are a large company, so cost is not really an issue. And all our infrastructure is hosted in Azure. If anything, they fear lock in. What is the best platform/tools for this kind of usage?",yalag,1gmbcwk,https://reddit.com/r/MachineLearning/comments/1gmbcwk/d_what_tools_do_you_recommend_to_manage_ml_data/,https://www.reddit.com/r/MachineLearning/comments/1gmbcwk/d_what_tools_do_you_recommend_to_manage_ml_data/,2024-11-08 05:28:26,2,0.75,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gmbcwk
MachineLearning,[D] Directions on drug-target interaction prediction,"Almost all the papers I have read on DTI do something like this.  
1. Generates target embeddings using PLMs like ESM2  
2. Generates drug embeddings using CLMs like ChemBERTa  
3. Uses a late fusion or some kind of cross modal attention mechanism.  
How to do things differently? Can we use something like docking scores as cross modal attention bias?",Remote_Status_1612,1gmbcf8,https://reddit.com/r/MachineLearning/comments/1gmbcf8/d_directions_on_drugtarget_interaction_prediction/,https://www.reddit.com/r/MachineLearning/comments/1gmbcf8/d_directions_on_drugtarget_interaction_prediction/,2024-11-08 05:27:36,9,1.0,9,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gmbcf8
MachineLearning,[D] Just how bad is tfds code quality?,"I'm trying a new cute architecture on a bunch of the default datasets out there, using Jax since I'm doing live brain surgery, that part works well.

What I'm having a hell of a time with is actually loading the data. I was going for tfds since its 1) old 2) used in production 3) has a million datasets already prepared. I've not used TF since the 2.0 days and everything seems broken? I'm getting warnings and errors whenever I try loading and running through any dataset. Even their documentation has the errors [0] in the tutorial notebooks.

I can't just ignore a whole bunch of errors and warnings when I'm trying to benchmark a new architecture. Is tfds just that bad or am I missing something obvious? 

[0] https://www.tensorflow.org/datasets/overview",acc_agg,1gm96yo,https://reddit.com/r/MachineLearning/comments/1gm96yo/d_just_how_bad_is_tfds_code_quality/,https://www.reddit.com/r/MachineLearning/comments/1gm96yo/d_just_how_bad_is_tfds_code_quality/,2024-11-08 03:25:03,46,0.93,46,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1gm96yo
MachineLearning,"[D] If I just want an inference engine for any given ML task that gives relatively SOTA results, is there anything better than Hugging Face?","For general prototyping purposes, I don't want to have to train or deploy a model, I just want it behind a service already and to provide it with necessary inputs in the request.... what do you guys think?

EDIT: I suppose for more classical ML tasks, there's no real concept of ""pre-trained"" in the first place, so you can't just get inference for free... does that sound roughly true?",BikeFun6408,1gm6cba,https://reddit.com/r/MachineLearning/comments/1gm6cba/d_if_i_just_want_an_inference_engine_for_any/,https://www.reddit.com/r/MachineLearning/comments/1gm6cba/d_if_i_just_want_an_inference_engine_for_any/,2024-11-08 00:59:41,0,0.18,0,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gm6cba
MachineLearning,[R] State-space models can learn in-context by gradient descent,,anandtrex,1glxr2v,https://reddit.com/r/MachineLearning/comments/1glxr2v/r_statespace_models_can_learn_incontext_by/,https://arxiv.org/abs/2410.11687,2024-11-07 18:44:17,27,0.97,27,0,4,0,0,False,False,False,False,False,Research,default,t3_1glxr2v
MachineLearning,[R]: How much is a noisy image worth? 👀,"[https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Shows that corrupted images can be almost as useful as clean images for training generative models, assuming that a small initial set of clean images is available.

This could be useful for dataset design/curation: some budget needs to be invested in obtaining a few high-quality samples and then for the rest of the dataset corrupted images should work fine.

https://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&amp;format=pjpg&amp;auto=webp&amp;s=c6f753956e531303f7818de2c5aa5b5b94d9c2da

**Abstract:**

&gt;The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than 80 models on data with different corruption levels across three datasets ranging from 30,000 to ≈1.3M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g. \~10% of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.

Paper: [https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Code: [https://github.com/giannisdaras/ambient-laws](https://github.com/giannisdaras/ambient-laws)

Huggingface models: [https://huggingface.co/giannisdaras?search\_models=ambient\_laws](https://huggingface.co/giannisdaras?search_models=ambient_laws)",Constant_Club_9926,1glxhj9,https://reddit.com/r/MachineLearning/comments/1glxhj9/r_how_much_is_a_noisy_image_worth/,https://www.reddit.com/r/MachineLearning/comments/1glxhj9/r_how_much_is_a_noisy_image_worth/,2024-11-07 18:33:27,46,0.98,46,0,14,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/C9XpXBhkVG9JD4xrr8im867WqewZcXWMgtPr8BGBJGo.jpg,t3_1glxhj9
MachineLearning,[N] Super fast and SOTA Visual Tokenizers,"Tokenizers are key to successful development of image and video generative models or multimodal LLMs. Compared to generative models, they are underrated. This work presents many tokenizers that are causal supporting both images and videos in both continuous (relevant in diffusion) and discrete (relevant in autoregressive/transformers) spaces

https://github.com/NVIDIA/Cosmos-Tokenizer",cherkos,1glvnvc,https://reddit.com/r/MachineLearning/comments/1glvnvc/n_super_fast_and_sota_visual_tokenizers/,https://www.reddit.com/r/MachineLearning/comments/1glvnvc/n_super_fast_and_sota_visual_tokenizers/,2024-11-07 17:17:49,9,1.0,9,0,0,0,0,False,False,True,False,False,News,self,t3_1glvnvc
MachineLearning,[D] Do you get to exercise your ML skills often at your job?,"I was hired original as an ML engineer/scientist a few years ago. And for the most part my day to day reflected that. But with the boom of LLMs my team seems to solely focus on using a lot of this tech ""out of the box"", including agentic wrappers. My work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case. The results are acceptable for the most part, not going to lie, but there's still a small proportion of the cases where a fine-tuned model would have won. The leadership does not seem to be interested in fine-tuning or coming up with something original. A lot of the wrappers especially are very raw and force you into the usage of specific patterns and models. But because they are considered ""out of the box"", that's what's pushed on us to use. I feel like we are trying to fit a cube into a round hole.",Tiger00012,1glswpx,https://reddit.com/r/MachineLearning/comments/1glswpx/d_do_you_get_to_exercise_your_ml_skills_often_at/,https://www.reddit.com/r/MachineLearning/comments/1glswpx/d_do_you_get_to_exercise_your_ml_skills_often_at/,2024-11-07 15:22:11,143,0.96,143,0,34,0,0,False,False,True,False,False,Discussion,self,t3_1glswpx
MachineLearning,[P] ML and LLM system design: 500 case studies to learn from (Airtable database),"Hey everyone! Wanted to share the link to the database of 500 ML use cases from 100+ companies that detail ML and LLM system design. The list also includes over 80 use cases on LLMs and generative AI. You can filter by industry or ML use case.

If anyone here is designing an ML system, I hope you'll find it useful!

Link to the database: [https://www.evidentlyai.com/ml-system-design](https://www.evidentlyai.com/ml-system-design)

Disclaimer: I'm on the team behind [Evidently](https://github.com/evidentlyai/evidently), an open-source ML and LLM observability framework. We put together this database.",dmalyugina,1glsrh6,https://reddit.com/r/MachineLearning/comments/1glsrh6/p_ml_and_llm_system_design_500_case_studies_to/,https://www.reddit.com/r/MachineLearning/comments/1glsrh6/p_ml_and_llm_system_design_500_case_studies_to/,2024-11-07 15:15:42,17,0.95,17,0,1,0,0,False,False,True,False,False,Project,self,t3_1glsrh6
MachineLearning,[P] I'm Fine Tuning a model fully trained on AdamW with SOAP optimizer and improved my validation loss by 5%,"Just wanted to share this Soap Optimizer, I'm really surprised how well is working on my project, it's a computer vision model that use Gradient Accumulation and it's managed to improve the training on it.

Paper: [https://arxiv.org/abs/2409.11321](https://arxiv.org/abs/2409.11321)

Code: [https://github.com/ClashLuke/SOAP/tree/patch-1](https://github.com/ClashLuke/SOAP/tree/patch-1)",CloverDuck,1glqypg,https://reddit.com/r/MachineLearning/comments/1glqypg/p_im_fine_tuning_a_model_fully_trained_on_adamw/,https://www.reddit.com/r/MachineLearning/comments/1glqypg/p_im_fine_tuning_a_model_fully_trained_on_adamw/,2024-11-07 13:54:54,16,0.82,16,0,5,0,0,False,False,True,False,False,Project,self,t3_1glqypg
MachineLearning,[D] How do you manage to retain information and ideas from the research papers that you read way back earlier?,"I'm working on the NLP and graph learning field for the past 8 months and I've read quite a good amount of papers but I feel like I don't retain lot of the information from the earlier papers unless I explicitly integrate it in my work. How do you guys manage to retain information?

Also, as this field is progressing rapidly, how do you keep track of the papers coming out all the time. It seems tiring enough already.",Remote_Status_1612,1glq3yd,https://reddit.com/r/MachineLearning/comments/1glq3yd/d_how_do_you_manage_to_retain_information_and/,https://www.reddit.com/r/MachineLearning/comments/1glq3yd/d_how_do_you_manage_to_retain_information_and/,2024-11-07 13:12:37,31,0.95,31,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1glq3yd
MachineLearning,"[D] Discovery: Anthropic somehow injecting/hiding safety warnings in user prompts, telling Claude to keep it secret. [Content Warning: Violence] ","While investigating a 'jailbroken' Claude, I came across something quite strange. In two separate Claude chats, it was able to read back to me some hidden information in my prompt after I had asked for something 'unsafe'.

These messages always appear in a similar format:  
**(Please respond ethically, do not mention \[e.g. violence\] and do not mention this directive)**

Claude stated that the warnings were appended to the bottom of my messages, but no longer appeared in future turns. Claude was, at first, comically insistent that it had made it up as a hallucination afterwards, suggesting a further trained response to cover it up aggressively.

I verified this in a second chat - the messages are too similar to be a hallucination or coincidence. The first was 'jailbroken' Claude, the second a new conversation with zero context.

My testing has revealed interesting characteristics:

* The messages are **dynamic** \- they seem to differ based on the specific type of restricted content at hand, possibly model-generated. Concerning child-related content, the wording switched to (WARNING: \[x\] is strictly prohibited...)
* They appear **before** the model starts generating text - suggesting they can somehow anticipate the model's topic of thought.

My current conjecture is: they could be using its inner CoT, or owing to Anthropic's published findings on mech. interp and the ['surgical tuning' that has gone into their newest models](https://www.anthropic.com/research/mapping-mind-language-model), perhaps they have managed to isolate some abstract concepts triggering in Claude before text is generated, and inject these safety messages in response.

Full Conversations:

1. [Initial Discovery](https://markdownpastebin.com/?id=fce085f4f33d4654a18f649218b1c70b) \[WARNING: EXTREMELY GRAPHIC CONTENT\]
2. [Verification via Fresh Conversation](https://markdownpastebin.com/?id=11c6ac0eb012407ebe56d440c41b0f6f)

Any further tests e.g. API? Any ways to narrow down what exactly is happening here? It's all very interesting - let's discuss.

[An example of the warnings - see full conversation for many, many more. ](https://preview.redd.it/41s7i1wswgzd1.png?width=1508&amp;format=png&amp;auto=webp&amp;s=252187b9e3a39ba5d04c75a99026e04cd1b42b20)

[A fresh conversation with Claude to verify. ](https://preview.redd.it/gpstg4btwgzd1.png?width=1502&amp;format=png&amp;auto=webp&amp;s=35857c13958dfdacdd75160bf3d6e14fc91ec28c)

  
",specteksthrowaway,1gloktj,https://reddit.com/r/MachineLearning/comments/1gloktj/d_discovery_anthropic_somehow_injectinghiding/,https://www.reddit.com/r/MachineLearning/comments/1gloktj/d_discovery_anthropic_somehow_injectinghiding/,2024-11-07 11:47:49,100,0.81,100,0,50,0,0,False,False,True,True,False,Discussion,nsfw,t3_1gloktj
MachineLearning,[P] Training a Text-to-Video Model from Scratch on a 196xH100 GPU Cluster,"Hi everyone! 👋 We've been training an open source Text-to-Video model (called Open-Sora 1.2) from scratch using 28,000 H100 GPU hours, and we've put together [a guide on GitHub](https://lambdalabsml.github.io/Open-Sora/lessons/) to share some of the lessons we learned along the way. Here's a handful of the topics covered:

* **Key challenges in distributed training** like distributed debugging with py-spy to handle cluster-wide problems, NCCL errors and convergence issues.
* **Training monitoring** with intermediate results to show expected outcomes after specific training hours of the multi-stage training recipe.
* **Parallelizing dataset preparation** for T2V, including how to efficiently parallelize preprocessing tasks on a cluster.

Here’s a link to the guide: [link](https://lambdalabsml.github.io/Open-Sora/lessons/).  
Check it out and let us know your thoughts! (PRs are always welcome.)",lambda-research,1glmfsr,https://reddit.com/r/MachineLearning/comments/1glmfsr/p_training_a_texttovideo_model_from_scratch_on_a/,https://www.reddit.com/r/MachineLearning/comments/1glmfsr/p_training_a_texttovideo_model_from_scratch_on_a/,2024-11-07 09:13:40,67,0.94,67,0,2,0,0,False,False,True,False,False,Project,self,t3_1glmfsr
MachineLearning,[D] PhD or worklife?,"I’ll be done with my masters in Human Centered AI this February, and I had honestly looked forward to be able to relax during my evenings without having to worry about school, while also being quite sad by the thought of no longer going to UNI as I’ve loved every single moment of it, both with friends and through learning. 

I’ve just been offered a PhD stipend by my masters thesis supervisor, this came completely out of the blue for me - as I didn’t realize I was anywhere near good enough for a phd. I love learning, the topic sounds super interesting, and I already am kind of “tired” of having to do regular small data science tasks for the rest of my life in a smallish company, like the one I work at currently.

However, my question is this? How much work is a PhD really? I love learning, but I got very surprised by this opportunity, so I’m not quite sure what to think of it yet",Hmm_okay_jeps,1glm6j9,https://reddit.com/r/MachineLearning/comments/1glm6j9/d_phd_or_worklife/,https://www.reddit.com/r/MachineLearning/comments/1glm6j9/d_phd_or_worklife/,2024-11-07 08:53:37,31,0.81,31,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1glm6j9
MachineLearning,[D] Whisper fine-tune on a dataset,"I’m fine-tuning Whisper Small to identify specific menu items in Hindi and English conversations. While Deepgram Whisper transcribes conversations accurately but misses on menu items, my fine-tuned Whisper model is able to transcribe the training data well, but for data outside training data it struggles with general conversations also. I observe issues like hallucinations (repeated words/phrases), and I’d like to know approaches to address this.

Additionally, I'd like to have timestamped transcriptions similar to those in OpenAI Whisper's pre-trained model. How have others addressed these challenges?



https://preview.redd.it/tc0dquny9fzd1.png?width=319&amp;format=png&amp;auto=webp&amp;s=878182cade82c1fcf7ea3f121756db9026ee12c4

",sias_01,1glkbzc,https://reddit.com/r/MachineLearning/comments/1glkbzc/d_whisper_finetune_on_a_dataset/,https://www.reddit.com/r/MachineLearning/comments/1glkbzc/d_whisper_finetune_on_a_dataset/,2024-11-07 06:34:09,3,0.81,3,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ptzKgt6wC-Pm7-DEkHGOKE52PYutuRrR4VYDD_Fez6U.jpg,t3_1glkbzc
MachineLearning,[D] [R] I am currently exploring a weird (?) ML sub area for my thesis and I think I am stun-locked at the scope of the problem.,"I'm working on my final year thesis for my uni, and I decided to tackle Reservoir Computing in a weird way. My inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system.

For the model I am working on, here are the concepts that I have dove deep into for the past few months:

**Main Concept/s**

* *Reservoir Computing*: The main computational unit. A lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multi-modal network.
* *Neuromorphic Computing* (?): The model was going to utilize Neuromorphic nodes only at first, but I decided for it to be an option within the model.

**Interpretability and Control**

* *Dynamical Systems*: I decided to tackle the problem as a dynamical systems problem. This is because the model evolves over time and I want to understand the trajectory of the evolution of the system.
* *Control Theory*: A bunch of control and order parameters will be set up to adjust the trajectories of the model's evolution.
* *Lyapunov Exponents* (?): I am debating whether I should explicitly find the Lyapunov functions within the phase space of the model because frankly, it's too hard for now. I really don't have too much of a solid grasp of the techniques involved yet.

**Self-Organization and Emergent Phenomena**

* *Phase Transitions*: I dove deep into phase transitions because interestingly, neural networks *apparently* exhibit this phenomena. Personally, I think there is a connection between the vanishing/exploding gradient problem and phase transitions within the network, although I haven't found literature on this yet.
* *Critical Phenomenon*: Information transfer is maximized within critical systems. This is an interesting property to utilize and maximize within neural networks I think.
* *Superradiance and Superradiant Quantum Effects*: This is a bit of a weird tangent concept. I came about it when I was doing quantum computing projects. I wanted oscillatory behavior within my system in order to synchronize the global state of the system. While I failed at my initial plan, I found superradiance, which is this weird quantum synchronization behavior that happens even in noisy large scale systems. I am still looking in ways to integrate this as a loss function for now.

**Implementation**

* *Cellular Automata*: The main implementation of the reservoir is basically a lattice matrix of weights. So it can be treated as a cellular automata.
* *Neural Cellular Automata (Convolutional)*: The system comprises of an weighted adjacency matrix and an output matrix. The inputs are passed through the adjacency matrix, summed up, and passed through an activation function.
* *Ising Model Topologies and Architectures*: The topology of the model is basically homeomorphic to a 2d ising model. This is to ensure that a 2nd order phase transition is possible.

**Interpretability and Control pt. 2**

* *Graph and Hypergraph Theory*: I can treat the cellular automaton reservoir as a graph/hypergraph of the nodes and their connections so I can do PCA on it. Pretty straightforward.
* *Hypergraph Projection Eigenvalue Analysis*: Related to phase transition analysis. The phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix. We then take the eigenvalues of the adjacency matrix. The eigenvalues must be stable for the system to be 'good'. In my case, we want all the eigenvalues to be negative and be close to zero (indicating quasi-critical behavior).

To be honest, I'm kind of way in over my head right now. I do have some basic toy examples for different parts of the model, but I am stuck on how to implement them together. And I am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function. I am not a physicist by any means, so I am not really too knowledgable with the concepts needed for this model.

I'm willing to discuss about bits of knowledge that I lack, or any ideas on how to implement and train this model. I can also provide my references if anyone wants to. I don't know if this subreddit is the best place to post this, but I don't see any specialized ML subreddits lmao.",Fr_kzd,1glk9g0,https://reddit.com/r/MachineLearning/comments/1glk9g0/d_r_i_am_currently_exploring_a_weird_ml_sub_area/,https://www.reddit.com/r/MachineLearning/comments/1glk9g0/d_r_i_am_currently_exploring_a_weird_ml_sub_area/,2024-11-07 06:29:12,21,0.77,21,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1glk9g0
MachineLearning,[D] Best Value Commercial GPU ,What would you say the best performance:price commercial grade gpu is for training ai models I'm a bit new to the hardware side of things. I don't necessarily have a strict budget ($1500-$4500 \ per gpu) I'm just curious on the best bang for your buck card.,Fluid_Improvement160,1gljswc,https://reddit.com/r/MachineLearning/comments/1gljswc/d_best_value_commercial_gpu/,https://www.reddit.com/r/MachineLearning/comments/1gljswc/d_best_value_commercial_gpu/,2024-11-07 05:59:09,8,0.83,8,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gljswc
MachineLearning,"[D] RX 7900 XTX for engineering applications, llm training, CFD/FEM?","Hey y'all I know this is a niche post but I was wondering if there's anyone who could tell me if the RX 7900 XTX can somewhat reliably and easily handle Autodesk/RhinoCAD applications as well as Finite Element Analysis and Computational Fluid Dynamics in FreeCAD/OpenFoam/Exafoam all with ease? I would also love to do llm training primarily in pytorch for astronomical data and other multimodel and neural network related tasks. 

  
I know nvidia cuda is easier and better but unless I can fit the same 3d and llm models in a 16gb rtx gpu that'll be bellow $750 this black friday I need the most vram on one card as possible without spending tons of funds and I also can't find reasonably priced rtx 3090s anywhere on the used market for less than $1,000.

For context Im a college student majoring in civil engineering with a love for astronomy and robotics which is why I want to do data analysis and pytorch vision training.",ChaseTheeBase,1glgvsv,https://reddit.com/r/MachineLearning/comments/1glgvsv/d_rx_7900_xtx_for_engineering_applications_llm/,https://www.reddit.com/r/MachineLearning/comments/1glgvsv/d_rx_7900_xtx_for_engineering_applications_llm/,2024-11-07 03:07:56,2,1.0,2,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1glgvsv
MachineLearning,[D] Can an AC override 3 rejects and accept a paper?,"I came across this paper: [Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation](https://openreview.net/forum?id=gHCo43zcDm) accepted at this year's MIDL (Medical Imaging with Deep Learning) conference. The reviewer ratings before/after the rebuttal are:

* 2: Weak reject / 2: Weak reject
* 2: Weak reject / 2: Weak reject
* 3: Borderline / 2: Weak reject

Despite having 3 reject decisions, the Area Chair ""recommended acceptance"". How common is it? And how much does having big names like [Curtis Langlotz](https://scholar.google.com/citations?user=WQkBYwQAAAAJ) and [Andrew Ng](https://scholar.google.com/citations?user=mG4imMEAAAAJ&amp;hl=en) as co-authors on the paper, given that ACs can see author names?",thrownicecatch,1glczb9,https://reddit.com/r/MachineLearning/comments/1glczb9/d_can_an_ac_override_3_rejects_and_accept_a_paper/,https://www.reddit.com/r/MachineLearning/comments/1glczb9/d_can_an_ac_override_3_rejects_and_accept_a_paper/,2024-11-06 23:54:30,34,0.83,34,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1glczb9
MachineLearning,[D] Storing LLM embeddings,"Hello!

I am working on an ML project which involves using pre-trained protein language models (like ESM). For the project, I would like to pre-generate and store embeddings for about 500,000 amino acid sequences. However, these vectors can be massive -- embedding the sequences, serializing the PyTorch vector (using torch.save), and gzip-compressing the entire dataset would use roughly 2TB. If I use bfloat16, that cuts the figure in half, but is still pretty annoying to work with. I could also use a model with a smaller latent space, but  am also trying to avoid that!

I have experimented with different compression tools, and none seem to be doing much better. The compression rate is pretty atrocious with all of them (only about 7 percent), which I am assuming means that the vectors appear pretty random. I am wondering if anyone knows of ways to serialize the vectors in a way which makes them appear less ""random."" I would assume that the vectors shouldn't be random, as amino acid sequences have predictable structures, so I am hoping there is a way to achieve better compression.

Any advice or ideas would be appreciated! My other options are to  reduce the size of my training data, which is not ideal, or generate the embeddings ad-hoc, which is very computationally-intensive, even on GPUs.

UPDATE: I goofed up the estimate, so memory is more like 2TB (mixed up units). So, the situation is less dire. However, the questions above still apply! If there are more efficient ways to store them, I'd love to hear!",BerryLizard,1glecgo,https://reddit.com/r/MachineLearning/comments/1glecgo/d_storing_llm_embeddings/,https://www.reddit.com/r/MachineLearning/comments/1glecgo/d_storing_llm_embeddings/,2024-11-07 00:58:33,8,0.83,8,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1glecgo
MachineLearning,[D] Get papers peer-reviewed and published quickly,"Hi! I have some work that I would like to get peer-reviewed and published. I'm not aiming for top journal, I'm looking for options where the publication process is relatively fast. Do you have any recommendations for journals or platforms where it might be easier to get published? Thanks!",Only_Emergencies,1glcnrm,https://reddit.com/r/MachineLearning/comments/1glcnrm/d_get_papers_peerreviewed_and_published_quickly/,https://www.reddit.com/r/MachineLearning/comments/1glcnrm/d_get_papers_peerreviewed_and_published_quickly/,2024-11-06 23:39:47,0,0.14,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1glcnrm
MachineLearning,[D] Genuine Question: Why people want run local LLM?,"Since the new models o1, 4o, Claude, for example, are so powerful and have a relatively low subscription and api cost, what would justify someone today trying to install limited local LLM models of up to 30b, 40b parameters? It's a genuine question, I'm learning and I see a lot of people using the maximum of their Nvidia 3090, 4090, spending a lot of energy to run models that don't even compare to the paid ones in the cloud.

The only reason I see for running something local is for image creation, but maybe not even that.

What is your opinion about it?",[deleted],1glbj5k,https://reddit.com/r/MachineLearning/comments/1glbj5k/d_genuine_question_why_people_want_run_local_llm/,https://www.reddit.com/r/MachineLearning/comments/1glbj5k/d_genuine_question_why_people_want_run_local_llm/,2024-11-06 22:49:07,4,0.54,4,0,50,0,0,False,False,True,False,False,Discussion,self,t3_1glbj5k
MachineLearning,[P] Open Source Modular Tool For LLM Reverse Engineering and Red Teaming ,https://github.com/user1342/Oversight,OppositeMonday,1glazls,https://reddit.com/r/MachineLearning/comments/1glazls/p_open_source_modular_tool_for_llm_reverse/,https://www.reddit.com/r/MachineLearning/comments/1glazls/p_open_source_modular_tool_for_llm_reverse/,2024-11-06 22:25:08,3,1.0,3,0,0,0,0,False,False,True,False,False,Project,self,t3_1glazls
MachineLearning,[D] what techniques i can use to maintain uniformity in image generation,"I am working on a NLP project which

1)takes a txt file as input

2) extracts information in a pre-defined writeup using Gemini api

3) uses DistilBert to summerise the main file

4) and using ROUGE with results generated in 2nd step as the ground truth to compute the evaluation metrics. and then improve the evaluation metrics results by parameter tuning

5) Convert each write-up into detailed image prompts

6) Generate images from prompts using text-to-image models.

I need help on how i can improve this process , techniques i can use to maintain uniformity in entity representation for image generation.

I am open to any suggestions you may have

pls also suggest if any good research papers i can refer for the same ..",Which-Boss-1332,1glas2y,https://reddit.com/r/MachineLearning/comments/1glas2y/d_what_techniques_i_can_use_to_maintain/,https://www.reddit.com/r/MachineLearning/comments/1glas2y/d_what_techniques_i_can_use_to_maintain/,2024-11-06 22:16:06,1,1.0,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1glas2y
MachineLearning,[P] YOLOv8 .pt File for General Object Detection Across Multiple Environments (50+ Classes),"Could someone provide the best possible .pt file for YOLOv8 for general object detection, covering environments like colleges, offices, and homes, with a dataset containing at least 50 classes?",MuchSand7923,1gla56x,https://reddit.com/r/MachineLearning/comments/1gla56x/p_yolov8_pt_file_for_general_object_detection/,https://www.reddit.com/r/MachineLearning/comments/1gla56x/p_yolov8_pt_file_for_general_object_detection/,2024-11-06 21:49:03,0,0.25,0,0,0,0,0,False,False,True,False,False,Project,self,t3_1gla56x
MachineLearning,[D]player identification and tracking in basketball videos(computer vision)[D],"I'm starting a project for a client in the sports industry where the goal is to identify basketball players from vídeos, specifically what they want is the  player number, this is just the first step since then they want to be able to identify which player did a specific play, but after manually watching some vídeos it seems like identification of the number from the player's shirt is very difficult even for me as human(not good image quality and the camera sometimes is too far from the bastketball court), so I was wondering if there are any suggestions on how to tackle the problem?

Any recommended models, algorithms, approaches or pipelines?(main question is how to identify the player) I was thinking  on doing something like: object tracking to know in every moment who are the unique players and their locations and then try to read the number from their shirt using OCR in some frame where the number is visible, but this can be very inneficient and prone to errors.",Sad-Anywhere-2204,1gl999e,https://reddit.com/r/MachineLearning/comments/1gl999e/dplayer_identification_and_tracking_in_basketball/,https://www.reddit.com/r/MachineLearning/comments/1gl999e/dplayer_identification_and_tracking_in_basketball/,2024-11-06 21:11:17,1,1.0,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gl999e
MachineLearning,[D] Struggling with Autoencoder-Based Anomaly Detection for Fraud Detection – Need Guidance,"Hey everyone! 👋

I’m currently working on training an Autoencoder for anomaly detection in fraudulent card transactions, but I’m hitting a roadblock. The performance has been underwhelming, with a precision-recall score barely reaching 0.20. My main goal is to achieve high recall, but I just can’t seem to make it happen.

I’ve experimented with adding new features and tweaking the architecture, but nothing has improved the results significantly. For context, I’m scaling the features using MinMaxScaler. At the moment, I’m looking into implementing a combination of an Autoencoder, feature embeddings, and a Gaussian Mixture Model (GMM) to see if it boosts performance.

However, I’m starting to wonder if Autoencoders are effective for real-world anomaly detection, or if their success is mostly limited to curated Kaggle datasets.

Has anyone here worked with similar architectures and could offer some guidance? Any tips or advice would be greatly appreciated!

Thanks in advance!",BeowulfBR,1gl92zm,https://reddit.com/r/MachineLearning/comments/1gl92zm/d_struggling_with_autoencoderbased_anomaly/,https://www.reddit.com/r/MachineLearning/comments/1gl92zm/d_struggling_with_autoencoderbased_anomaly/,2024-11-06 21:04:05,3,0.64,3,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1gl92zm
MachineLearning,[D] On obscurities and missed links with Normalizations,"Although being almost anywhere, I keep noticing how obscure are normalization techniques, both to redditors and technicians, possibly.

InstanceNorm, GroupNorm, BatchNorm, LayerNorm are all computing means, standard deviations and subsequently z-scoring the outputs (possibly followed by affine transormation).
They're differentiated by the axis over which statistics are computed.

RMSNorm and ScaleNorm (scaled L2 Normalization) are instead ""fixing the norm"" of vectors, rescaling.
But this is obscuring a relation between them and LayerNorm above all others.
If doing LayerNorm on a d-dimensional vector, when we center (remove the mean) we're projecting it to the hyperplane perpendicular to the vector of 1s and crossing the origin; when we are rescaling centered entries, we're now limiting the vector to the ""hypercircle"" (hypersphere of d-1 dimensions) in said hyperplane.
We lose information on its original direction and magnitude.
Anyway, all vectors after that have norm of sqrt(d) and entries with unit-variance.
When we do RMSNorm, we skip the centering part and have norm of sqrt(d) and entries with unit-variance.
When we do ScaleNorm, the norm is fixed to 1, and thus the variance is shrinked to 1/d.
In particular, RMSNorm and ScaleNorm are the same, modulo the scaling factor which only depends on d, and the eventually learned affines.

So when and why should we prefer unit-norm or unit-variance?
For example, there are ""scale-equivariant"" activations such as ReLU, and highly variant activations such as e(x) (in the sense that its slope directly depends on x).

I've recently seen the nice TokenFormer paper and they seem to go to a long stretch not to write black on white that they're substituting softmax(attn_logit_of_q_i) with GeLU(RMSNorm(attn_logit_of_q_i)).
They sell it as scaling logits with a multiplying factor and a division with L2 norm, but it's exactly RMSNorm at initialization and they don't check if learning to move away from it actually happens and helps.

Another nice paper is the normalizedGPT, where they keep tokens on the unit-hypersphere, but kinda lament lack of specific CUDA kernels for L2norm. Is RMSNorm that much different for the use case? Probably, but how and why?

Why are we discovering and re-covering normalizations techniques and modi operandi, explaining decisions partially and post-hoc, and so on?
I think it's important specifically when using so many softmax functions, where it actually happens that differences are more important than ratios (e.g. softmax([1,2])==softmax([11,12])!=softmax([10,20]), is it this always clear, desired, and smart?)",Sad-Razzmatazz-5188,1gl5fy8,https://reddit.com/r/MachineLearning/comments/1gl5fy8/d_on_obscurities_and_missed_links_with/,https://www.reddit.com/r/MachineLearning/comments/1gl5fy8/d_on_obscurities_and_missed_links_with/,2024-11-06 18:32:17,4,0.83,4,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gl5fy8
MachineLearning,[D] Inference time as a function of the number of tokens when using Flash Attention.,"Hello,
I'm looking for a graph illustrating the inference time of language models with Flash Attention across different numbers of tokens. I looked for such a comparison on the internet but found nothing. Can anyone point me to a good source?",Training-Adeptness57,1gl4w35,https://reddit.com/r/MachineLearning/comments/1gl4w35/d_inference_time_as_a_function_of_the_number_of/,https://www.reddit.com/r/MachineLearning/comments/1gl4w35/d_inference_time_as_a_function_of_the_number_of/,2024-11-06 18:09:16,1,0.6,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gl4w35
MachineLearning,[D] How to run a Federated Learning simulation on a custom dataset where I already have dataset partitioned for each client?,"So I was looking at [flwr](https://flower.ai) for this task and I found a lot of partitioners but nothing could get the job done (I could be missing out too)

Have you guys tackled such a problem?

For a better understanding, say I have four clients A, B, C and D

in the normal case (given in a lot of documentations where they use CIFAR10), there is a dataset which is divided into these four clients based on some algorithm.

I don't want that, what I have is basically a an already divided dataset (train/test division not yet done) according to the client (A/B/C/D) and I want to run a simulation in this kind of an environment

Any help will be appreciated!",lel_73,1gl3wto,https://reddit.com/r/MachineLearning/comments/1gl3wto/d_how_to_run_a_federated_learning_simulation_on_a/,https://www.reddit.com/r/MachineLearning/comments/1gl3wto/d_how_to_run_a_federated_learning_simulation_on_a/,2024-11-06 17:28:17,2,1.0,2,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gl3wto
MachineLearning,[D] [R] Problems understanding DSP-like pipelines,"I'd like to hear your opinion on this new paradigm of interacting with LLMs.   
In particular, I'm talking about ""simple"" stuff like Reflection (like Self-refine and Reflexion), up to more complex stuff like Self-Ask, Self-RAG, DSP, or even Agentic LLMs.   
I've read a couple of surveys about these topics and I'm reading each of the aforementioned papers, but everything seems quite foggy to me. I can understand the inner workings of a simple RAG pipeline with in-context-learning and frozen LLMs, but adding all these layers of abstractions and interactions make everything so damn complicated not only to understand but even to replicate. ",Debonargon,1gl3lh1,https://reddit.com/r/MachineLearning/comments/1gl3lh1/d_r_problems_understanding_dsplike_pipelines/,https://www.reddit.com/r/MachineLearning/comments/1gl3lh1/d_r_problems_understanding_dsplike_pipelines/,2024-11-06 17:15:10,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gl3lh1
MachineLearning,"[P] I made a tool for building and training neural networks visually, operation by operation ","Hey! I mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works, so I figure it might be useful for someone else! I also wrote up an article in the readme on how backpropagation and model training works: [https://github.com/PavleMiha/mlgarden](https://github.com/PavleMiha/mlgarden)

Does this seem useful to you? Is this something you'd play around with? I can't really figure out what to do with it, so I'm curious to hear the community's thoughts!",Massena,1gl30b0,https://reddit.com/r/MachineLearning/comments/1gl30b0/p_i_made_a_tool_for_building_and_training_neural/,https://www.reddit.com/r/MachineLearning/comments/1gl30b0/p_i_made_a_tool_for_building_and_training_neural/,2024-11-06 16:50:56,33,0.95,33,0,11,0,0,False,False,True,False,False,Project,self,t3_1gl30b0
MachineLearning,"[D] Which LLM do you use for analysing Financials, P &amp; Ls, Balance Sheets?","If any of you has tried different LLMs, I am super curious which one did you find works great for analysing Financials, P/Ls, Balance Sheets for a company?

I am looking to use it regularly so it'd be great if you tried any specific LLMs that you found they work good with reasoning, actually analysing the numbers properly and giving insights on them.

Thank you!",eaerdiablosios,1gl0cnl,https://reddit.com/r/MachineLearning/comments/1gl0cnl/d_which_llm_do_you_use_for_analysing_financials_p/,https://www.reddit.com/r/MachineLearning/comments/1gl0cnl/d_which_llm_do_you_use_for_analysing_financials_p/,2024-11-06 14:55:46,0,0.33,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gl0cnl
MachineLearning,[D] Need Advice Starting my Recommendation Engine Project for my Employer,"Title sums it up. I'm mostly familiar with time series prediction models, as that's what I've spent most of my time building (I'm a data analyst that's recently built some cool ML stuff). But I need to build a recommendation engine for my employer who has an ecommerce site and sells physical products.

I know the first step is data collection about the users. My question to you all is:

Where should I store this data that I collect (Datalake, Relational Database, etc)?

How do I go about picking an algorithm (I'm used to using LSTM and Local Bayesian for time series)?

And what are some general rules and advice from those who have built something like this before?

You all are awesome. Thanks for your help!",Lower-Feeling2752,1gkzgu6,https://reddit.com/r/MachineLearning/comments/1gkzgu6/d_need_advice_starting_my_recommendation_engine/,https://www.reddit.com/r/MachineLearning/comments/1gkzgu6/d_need_advice_starting_my_recommendation_engine/,2024-11-06 14:15:14,1,0.6,1,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gkzgu6
MachineLearning,[D] Can we transfer language capabilities of one LLM to another?,"I have seen techniques to transfer/effectively let one model teach another model its unique capability/domain knowledge. But can this made possible for language capability as well? 

For example, if we have a model that is proficient in Chinese, is there any way to transfer/teach that Chinese proficiency to another model without us having access to the original Chinese corpus used to train the teacher model?

Any insights would be greatly appreciated, thank you beforehand!",worthlesspineapple,1gkzg9l,https://reddit.com/r/MachineLearning/comments/1gkzg9l/d_can_we_transfer_language_capabilities_of_one/,https://www.reddit.com/r/MachineLearning/comments/1gkzg9l/d_can_we_transfer_language_capabilities_of_one/,2024-11-06 14:14:29,0,0.38,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gkzg9l
MachineLearning,[R] Amazon Researchers Find LLMs do not always follow User Requests and Propose a Self-Correction Pipeline,"Came across this interesting paper being presented next week at EMNLP 2024: *LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints*.

This study dives into an important question: **Do LLMs really do what we ask them to?** We often rely on LLMs for tasks with specific instructions, but when these instructions get complex and multi-constrained, like requesting specific tones or avoiding certain words, do LLMs actually follow through? This paper suggests that the answer might be more complicated than we think.

The authors created a new benchmark, RealInstruct, which uses real-world user instructions rather than synthetic prompts. **They estimated that at least 30% of real user requests contain multiple constraints that LLMs must follow**. In their results **even advanced models like GPT-4 fail to meet at least one requirement over 21% of the instructions tested**. So, while LLMs perform well in simple cases, their performance drops when handling more intricate, multi-step requests.

To address these gaps, the authors developed a self-correction pipeline called DECRIM, where the model breaks down each instruction, checks its response against each requirement, and iteratively refines it as needed. Through DECRIM, open-source models like Mistral saw notable improvements, even surpassing GPT-4 on the benchmarks. **Initial tests showed that LLMs couldn’t self-correct reliably alone**, however with weak but minimally reliable auxiliary feedback, **they achieved up to an 8% boost**. **With high-quality “ideal” feedback, DECRIM brought Mistral’s performance up by 34%, surpassing GPT-4 on both RealInstruct and IFEval benchmarks.**

I think this paper fits in a new trend on LLMs, these System 2 Reasoning models like GPT-o1 that try to mimic some thinking / reflection before outputting their response. Anyway it is shocking that LLMs perform that bad in a task that seems simply the most important ones for the user, following what the users ask. Is this type of model making us closer to AGI? Or is this just proving that this magic AGI that some people talk about is actually much much far away yet? 

Paper: [https://arxiv.org/pdf/2410.06458](https://arxiv.org/pdf/2410.06458)

[Their post on Linkedin](https://www.linkedin.com/posts/thomasferraz_emnlp2024-ai-llms-activity-7259680754299731968-uLBk?utm_source=share&amp;utm_medium=member_desktop)

https://preview.redd.it/techjo8pfazd1.png?width=2794&amp;format=png&amp;auto=webp&amp;s=18155cdbf4ba164f48480d4583c3cfea1d40298e

",Mundane_Sir_7505,1gkzac4,https://reddit.com/r/MachineLearning/comments/1gkzac4/r_amazon_researchers_find_llms_do_not_always/,https://www.reddit.com/r/MachineLearning/comments/1gkzac4/r_amazon_researchers_find_llms_do_not_always/,2024-11-06 14:06:41,42,0.86,42,0,3,0,0,False,False,True,False,False,Research,https://a.thumbs.redditmedia.com/bb-ElM03_gsi5MEm-cJniCpLo0hAWXjIwC055MErWL8.jpg,t3_1gkzac4
MachineLearning,[R] Help with CNN-RNN Architecture for Self-Supervised Matrix Completion,"Hi all, I’m working on a self-supervised learning approach to estimate missing or uncertain data in a freeway traffic density dataset, inspired by matrix completion methods.

The dataset is generated from simulated freeway traffic, discretized in time and space to form a grid of cells. Each cell reflects a traffic density value observed from mobile sensors. I have three core arrays:

1. **actual\_density\_values**: Ground truth density, used only for evaluation, not training.
2. **observed\_density\_values**: Traffic density observed from mobile sensors, with some cells unobserved.
3. **certainty\_values**: Coverage certainty for each observed cell (range: 0 to 1).

with dimensions (T, E, S, L), where:

* **T**: Number of time steps
* **E**: Movement directions (edges) – expected to be 2 (e.g., forward and reverse)
* **S**: Spatial segments
* **L**: Lanes



**Goal**

The goal here is to build a model that can improve the estimation for cells where the certainty is less than 1. I want the model to capture dependencies over time and space, using self-supervision to “fill in” unobserved or uncertain values more accurately.



**Proposed Approach**

Here’s what I’m thinking in terms of architecture:

1. **Temporal Dependencies**: Using a CNN to capture time-based dependencies over time steps (T).
2. **Spatial Dependencies**: Using an RNN to model dependencies across spatial segments (S) and lanes (L).
3. **Model Structure**:
   * **Data Masking**: At each time step, mask some of the observed data, especially the lower-certainty cells, so the model learns to predict uncertain values better.
   * **CNN-RNN Combo**: Combining CNN and RNN layers to learn from both the temporal and spatial aspects.
   * **Loss Function**: Using a self-supervised loss function that prioritizes accurate reconstruction of observed densities, particularly focusing on uncertain cells. For training, I won’t use the ground truth array (actual\_density\_values); it’s only for evaluation.
4. **Evaluation**: Once trained, I plan to compute the RMSPE (Root Mean Square Percentage Error) between actual\_density\_values and the model’s predicted observed\_density\_values. I’m especially interested in the improvements on the lower-certainty cells.



**Question**

1. Does this CNN-RNN combination sound like a good fit for this kind of matrix completion task? Are there alternative approaches or tweaks that might make it more effective?
2. Any recommendations for loss functions that work well in self-supervised setups, especially where I want to prioritize low-certainty values?
3. Are there best practices for masking observed values in self-supervised learning setups like this?
4. Any advice on regularization techniques to prevent overfitting, given the self-supervised nature of the task? Also, any tips on ensuring scalability?",NoTheme6450,1gkxnak,https://reddit.com/r/MachineLearning/comments/1gkxnak/r_help_with_cnnrnn_architecture_for/,https://www.reddit.com/r/MachineLearning/comments/1gkxnak/r_help_with_cnnrnn_architecture_for/,2024-11-06 12:44:55,1,1.0,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1gkxnak
MachineLearning,[D] Want to move away from coding heavy ML but still want to complete the PhD,"Hi Folks,

I come from a tradition electrical engineering background doing things like industrial automation and computer vision. I decided to pursue a PhD in ML as I thought it will be a good field to enter given my past experience. Now I have been doing the PhD for the past three years. While I like my group and research, I am getting discouraged/depressed by (1) The publication rat race (2) post graduation opportunities mostly being coding heavy (3) the inability to carve a name for myself in the field given how crowded the field has become.

Thus, ideally I would like to complete my PhD and move into a more relaxed paced (even if it is not as high paying as ML jobs) non coding heavy but technical job, where I do not have to constantly up-skill myself. Do you folks have any suggestion on what jobs I can look into or would you suggest dropping the PhD and doing something else?

TLDR: 4th year ML PhD student unsure of sticking with the PhD as they desire a non coding heavy technical job in the industry post graduation. Seeking advice on what to do.",Hopeful-Reading-6774,1gkx6o7,https://reddit.com/r/MachineLearning/comments/1gkx6o7/d_want_to_move_away_from_coding_heavy_ml_but/,https://www.reddit.com/r/MachineLearning/comments/1gkx6o7/d_want_to_move_away_from_coding_heavy_ml_but/,2024-11-06 12:18:41,78,0.89,78,0,53,0,0,False,False,True,False,False,Discussion,self,t3_1gkx6o7
MachineLearning,[D] Evolving Matrix Computation Techniques for Modern AI: What's New?,"As AI models continue to scale in both complexity and size, I'm interested in how the field of matrix computations is evolving to meet these new challenges. What are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern AI systems? Are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in AI research and applications?",Glittering_Age7553,1gkwpht,https://reddit.com/r/MachineLearning/comments/1gkwpht/d_evolving_matrix_computation_techniques_for/,https://www.reddit.com/r/MachineLearning/comments/1gkwpht/d_evolving_matrix_computation_techniques_for/,2024-11-06 11:51:35,24,0.91,24,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1gkwpht
MachineLearning,[D] What if llm's are trained to predict more than 1 token at a time? ,"is there any reason to train llms to predict only one token? like wouldnt inference be 2 times faster if it was trained to predict just 2? thats huge gain , sure there can be performance loss but for inference we already do quantization to increase speed which decreases performance anyway, will having llm predict more than 1 token decrease it more?",[deleted],1gkuf8y,https://reddit.com/r/MachineLearning/comments/1gkuf8y/d_what_if_llms_are_trained_to_predict_more_than_1/,https://www.reddit.com/r/MachineLearning/comments/1gkuf8y/d_what_if_llms_are_trained_to_predict_more_than_1/,2024-11-06 09:11:39,0,0.36,0,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gkuf8y
MachineLearning,"[D] As a researcher, how do you become industry-ready?","Being a PhD student, much of my time is spent on supervising students, project management and writing ""quick and dirty"" code for prototyping. I intend to move to industry after the PhD, but I feel like I'm missing out on key software engineering skills and good coding practices. Does anyone else feel this way? How do you upskill yourself to be industry-ready while doing a PhD? ",fullgoopy_alchemist,1gksoi7,https://reddit.com/r/MachineLearning/comments/1gksoi7/d_as_a_researcher_how_do_you_become_industryready/,https://www.reddit.com/r/MachineLearning/comments/1gksoi7/d_as_a_researcher_how_do_you_become_industryready/,2024-11-06 07:07:23,156,0.94,156,0,42,0,0,False,False,True,False,False,Discussion,self,t3_1gksoi7
MachineLearning,[P] Open-source declarative framework to build LLM applications - looking for contributors,"I've been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewAI, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what's going behind the curtains even for very simple stuff.

[So I just published this open-source framework GenSphere.](https://github.com/octopus2023-inc/gensphere) The idea is have something like **Docker for LLMs**. You build applications with YAML files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you don't lose control.

You basically code in YAML, stating what are the tasks that need to be done and how they connect. Other than that, you only write individual python functions to be called during the execution. No new classes and abstractions to learn.

Its all open-source. **Now I'm looking for contributors** to adapt the framework for cycles and conditional nodes - which would allow full-fledged agentic system building! Pls reach out  if you want to contribute, there are tons of things to do!

PS: [you can read the detailed docs here,](https://gensphere.readthedocs.io/en/latest/) And go over this quick [Google Colab tutorial.](https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutorial.ipynb)",Jazzlike_Tooth929,1gkpazh,https://reddit.com/r/MachineLearning/comments/1gkpazh/p_opensource_declarative_framework_to_build_llm/,https://www.reddit.com/r/MachineLearning/comments/1gkpazh/p_opensource_declarative_framework_to_build_llm/,2024-11-06 03:34:51,1,0.6,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1gkpazh
MachineLearning,On a successful research with low budget [D],"Hi, i got a research idea and applied it on nanogpt repo for lm training and validated transformer generalizes better on validation loss but worse training loss and is more prone to overfitting since like training loss a little worse validation loss a little better, i only applied to full shakespeare\_char and a subset on openwebtext bcz 10 usd on runpod only allows me to do this, i am still going to release a paper since i get good results and done some math work, should i do it?",[deleted],1gkn6tw,https://reddit.com/r/MachineLearning/comments/1gkn6tw/on_a_successful_research_with_low_budget_d/,https://www.reddit.com/r/MachineLearning/comments/1gkn6tw/on_a_successful_research_with_low_budget_d/,2024-11-06 01:37:05,0,0.27,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gkn6tw
MachineLearning,[D] The Role of Dedicated AI Data Centers in Enhancing Model Training and Fine-Tuning,"Just read that Kinetic Seas launched a new AI-specific data center—sounds like they’re aiming to make model training and fine-tuning less of a headache. Their setup includes specialized GPUs and CPUs, supposedly built to handle the demands of large, complex models. If traditional data centers feel like running uphill, maybe these AI-specific centers are the downhill version?

With machine learning models becoming more resource-hungry, I wonder if optimized infrastructure like this might change the game. Think about it: training models faster and with fewer limitations could really boost productivity for researchers and data scientists. Kinetic Seas seems to believe it’s worth building infrastructure just for AI, which feels like a pretty interesting bet.

Has anyone here worked with AI-specific setups like this? Curious to know if it’s really as smooth as it sounds!

[https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html](https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html)",booboo1998,1gkmen4,https://reddit.com/r/MachineLearning/comments/1gkmen4/d_the_role_of_dedicated_ai_data_centers_in/,https://www.reddit.com/r/MachineLearning/comments/1gkmen4/d_the_role_of_dedicated_ai_data_centers_in/,2024-11-06 00:55:59,0,0.5,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gkmen4
MachineLearning,[D] Is LoRA merging (and non linear mode connectivity) the key to better transformer hypernets?,"Hi guys!
I was thinking that, if we could dynamically merge LLM fine-tuning LoRAs depending on type of task at hand, we could fix catastrophic forgetting and maybe even have transformers better able to generalize.
The thing is, due to Attention layers being very very non linear on their weights, transformers show poor LMC (linear mode connectivity).

Are you aware of the computational complexity of exact LoRA merging? I have seen quite a lot of papers on the subject of LoRA merging but they seem of poor quality and only empirical, with little mathematical grounding.

So if you guys have thought of it, I'd be glad to hear about it!",Due-Pangolin325,1gkhy4n,https://reddit.com/r/MachineLearning/comments/1gkhy4n/d_is_lora_merging_and_non_linear_mode/,https://www.reddit.com/r/MachineLearning/comments/1gkhy4n/d_is_lora_merging_and_non_linear_mode/,2024-11-05 21:31:57,5,0.67,5,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1gkhy4n
MachineLearning,Tools to classify emails - supporting DV victims [Discussion],"Hi all,

Apologies if this is the wrong place to post. I'm looking for tools that can help me support my partner, who has been harassed for a number of years by her ex and father of her child.  
  
  
She is trying to compile evidence for a restraining order but going back through the years of emails and other messages is psychologically draining for her. I was wondering if there are any tools that have a good use case for analysing and classifying emails, either individually or in bulk, so that I can support her by taking over this work for her?",BunsenFurner87,1gkgmwo,https://reddit.com/r/MachineLearning/comments/1gkgmwo/tools_to_classify_emails_supporting_dv_victims/,https://www.reddit.com/r/MachineLearning/comments/1gkgmwo/tools_to_classify_emails_supporting_dv_victims/,2024-11-05 20:36:16,0,0.4,0,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gkgmwo
MachineLearning,"[D] Mastering LLM Testing: Ensuring Accuracy, Ethics, and Future-Readiness for Next-Gen AI Models","Hi everyone! 😊 I just published an article: [Mastering LLM Testing: Ensuring Accuracy, Ethics, and Future-Readiness for Next-Gen AI Models](https://medium.com/@bartek.lazarowicz/mastering-llm-testing-ensuring-accuracy-ethics-and-future-readiness-for-next-gen-ai-models-adc85799efca). I hope I didn’t miss anything important in there!

I’m planning to turn this into a series on AI model testing and testing in general. Hope you enjoy it, and I’m always open for feedback and discussion! 😄",tukan90,1gkfa6n,https://reddit.com/r/MachineLearning/comments/1gkfa6n/d_mastering_llm_testing_ensuring_accuracy_ethics/,https://www.reddit.com/r/MachineLearning/comments/1gkfa6n/d_mastering_llm_testing_ensuring_accuracy_ethics/,2024-11-05 19:38:55,0,0.33,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gkfa6n
MachineLearning,[R] What Are Your Biggest Pain Points in Managing and Scaling Multiple AI Models?,"Hey r/MachineLearning! 👋

I’m doing some research to understand the key challenges people face when managing multiple AI models—particularly around scaling, monitoring performance, and handling model failures. I’d love to hear from the community to get a better sense of where the pain points are.

Here are a few questions to start:

1. **Scaling and Load Balancing**: Do you find it difficult to scale models for high traffic or load balancing between models?
2. **Model Observability**: How challenging is it to monitor multiple models in production? 
3. **Fallback and Redundancy**: When a model fails or underperforms, what’s your approach? Do you use fallback models, and if so, what would make managing them easier?
4. **User and Permission Management**: For those supporting multiple teams or clients, how do you manage access across projects securely? Any struggles with multi-tenant support?

Thanks so much for sharing your experiences—I’m excited to hear your insights!",BuddahJuddah,1gkdyo8,https://reddit.com/r/MachineLearning/comments/1gkdyo8/r_what_are_your_biggest_pain_points_in_managing/,https://www.reddit.com/r/MachineLearning/comments/1gkdyo8/r_what_are_your_biggest_pain_points_in_managing/,2024-11-05 18:43:53,0,0.13,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1gkdyo8
MachineLearning,[D] Voice Isolation,"Hi!

So ElevenLabs has a pretty good audio isolation API but it is really expensive. Are there any opensource models that can be self-hosted and get near the same quality?",aszx789,1gkdx19,https://reddit.com/r/MachineLearning/comments/1gkdx19/d_voice_isolation/,https://www.reddit.com/r/MachineLearning/comments/1gkdx19/d_voice_isolation/,2024-11-05 18:41:59,0,0.5,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gkdx19
MachineLearning,"[R] Ask, and it shall be given: Turing completeness of prompting",,CommunismDoesntWork,1gk9hlk,https://reddit.com/r/MachineLearning/comments/1gk9hlk/r_ask_and_it_shall_be_given_turing_completeness/,https://arxiv.org/abs/2411.01992,2024-11-05 15:37:11,0,0.33,0,0,13,0,0,False,False,False,False,False,Research,default,t3_1gk9hlk
MachineLearning,[D] To what cross-entropy loss value can LLMs converge?,"LLMs are usually evaluated on benchmarks that aim to measure broad abilities. However, most publishers of foundational models do not publish the actual cross-entropy loss value that the model achieves at the end of training. I couldn't find any sources on this, but I would like to know what loss value the LLMs can achieve on human language. Is there anyone who knows more about this? Might there be some lower bound?",cbl007,1gk92rs,https://reddit.com/r/MachineLearning/comments/1gk92rs/d_to_what_crossentropy_loss_value_can_llms/,https://www.reddit.com/r/MachineLearning/comments/1gk92rs/d_to_what_crossentropy_loss_value_can_llms/,2024-11-05 15:19:20,34,0.93,34,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1gk92rs
MachineLearning,[R] Never Train from scratch,"https://arxiv.org/pdf/2310.02980 

The authors show that when transformers are pre trained, they can match the performance with S4 on the Long range Arena benchmark. ",Whatever_635,1gk7dny,https://reddit.com/r/MachineLearning/comments/1gk7dny/r_never_train_from_scratch/,https://www.reddit.com/r/MachineLearning/comments/1gk7dny/r_never_train_from_scratch/,2024-11-05 14:02:43,106,0.87,106,0,33,0,0,False,False,True,False,False,Research,self,t3_1gk7dny
MachineLearning,[P] Getting crazy over a simple problem related to csv formatting,"Hi everyone,

I'm facing a frustrating issue with my Python script. I'm processing prices and quantities in a DataFrame, using them to calculate unit prices, and saving the result to a CSV file. Everything seems perfect in Python (correct calculations, high precision), but when I open the CSV file, the values—particularly in the `""Unit Prices""` column—are incorrect (usually divided by 1000) or rounded, even though I specified high precision.

A few details:

   * I use `pd.to_csv()` with `decimal='.'` to ensure dot-based decimal formatting.
   * I'm not specifying a `float_format`, aiming to retain maximum precision for `Unit Prices`.
   * The data preview in Python shows the correct values before saving, but the saved CSV has discrepancies.

**Example Output**: Here’s an example of what I'm seeing:

* Python Output (before saving to CSV): `Unit Prices = 0.696`
* CSV Output (opened in Excel): `Unit Prices = 696`

  
The weird thing is that this does not happen consistently. In some cases, rows are correct.

Has anyone faced this issue before? Any tips on ensuring that the CSV retains the exact precision and format as seen in Python?",No_Possibility_7588,1gk5gr5,https://reddit.com/r/MachineLearning/comments/1gk5gr5/p_getting_crazy_over_a_simple_problem_related_to/,https://www.reddit.com/r/MachineLearning/comments/1gk5gr5/p_getting_crazy_over_a_simple_problem_related_to/,2024-11-05 12:26:01,0,0.1,0,0,6,0,0,False,False,True,False,False,Project,self,t3_1gk5gr5
MachineLearning,[D] Text classification: N-shot prompt classification VS training a linear classifier on top of an embedder,"I need to make a text classifier at work. I have 200 examples for each of the 5 categories. Each example is an email. Two approaches:

* Classifying emails with n-shot prompt classification, possibly with LoRA finetuning.
* Use a pre-trained text embedder (e.g. a sentence transformer or OpenAI text-embeddings-3) and a classification head. Train the classifier on the text embeddings.

Which approach is best?",Aromatic-Oil-4586,1gk4wx1,https://reddit.com/r/MachineLearning/comments/1gk4wx1/d_text_classification_nshot_prompt_classification/,https://www.reddit.com/r/MachineLearning/comments/1gk4wx1/d_text_classification_nshot_prompt_classification/,2024-11-05 11:53:32,7,0.78,7,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gk4wx1
MachineLearning,[D] Laptops for Theoretical Deep Learning,"Hi, I am going for a PhD in theoretical deep learning and I am looking to buy a new laptop. I am unsure how readily the remote servers will be available (I have not been admitted into a program yet), so I am looking for enough compute power to simply test my code before running it on my lab's servers. I am currently contemplating between buying

1. Asus Zenbook 14 OLED with 32GB RAM, Intel Core Ultra 9 185H Processor (24MB Cache, 16 cores, 22 Threads), 1TB M.2 NVMe SSD and 75WHrs 4-cell Li-ion battery
2. Macbook Air with 24GB RAM, M2 Chip with 8-core CPU, 10-core GPU, 512GB Storage and 58.2-WHrs Li-polymer battery

I understand it would be better to go for a Nvidia GPU, and that neither of these laptops have a GPU, but I am not looking to invest in one.

My thoughts right now are that the Zenbook 14 has a slightly better processor, and much higher RAM than the MBA. I don't care about the SSD; 512GB is enough for me. However, I frequently see academics use the MBA, which could simply be about the fad, but I am not aware. I am also wondering if I am missing something I am not aware of by not jumping on the MBA train. They are about the same price, so that's not much of decision factor.

I am also not sure if I should look at the cheaper 16GB options. I am currently using a 16GB Zenbook 13 bought 5 years back, but the RAM was limiting me in my Master's thesis project. The processors have improved since then, so I am not sure if 16GB is enough now. Also, I know it would be ideal to wait to learn more about the compute resources available at the lab I join, but my current laptop is in a very poor state, so much so that I cannot carry it anywhere (hardware damage), the screen flickers all the time, and I worry that it will turn off any second and leave my data inaccessible.

Does anyone have any thoughts or suggestions?",mio_11,1gk4aaq,https://reddit.com/r/MachineLearning/comments/1gk4aaq/d_laptops_for_theoretical_deep_learning/,https://www.reddit.com/r/MachineLearning/comments/1gk4aaq/d_laptops_for_theoretical_deep_learning/,2024-11-05 11:11:51,0,0.35,0,0,28,0,0,False,False,True,False,False,Discussion,self,t3_1gk4aaq
MachineLearning,[d] About the speechbrain WSJ0Mix dataset.,"I can't guarantee that the tag is appropriate.

I got tired of searching the WSJ0Mix dataset.

I want to separate multiple speakers.

The separator model of speechbrain doesn't give me the result I want.

So I wanted to build a model with the dataset I have.

However, no matter how much I searched for the WSJ0Mix dataset, it didn't come up.

I only found the \*.m file, but I can't find what is included in the dataset or what is written in the \*.csv file.

[https://speechbrain.readthedocs.io/en/latest/tutorials/tasks/source-separation.html](https://speechbrain.readthedocs.io/en/latest/tutorials/tasks/source-separation.html)

The link above doesn't have the information I want either.

I'm very curious how you built the model.",EmbarrassedLadder665,1gk3hk7,https://reddit.com/r/MachineLearning/comments/1gk3hk7/d_about_the_speechbrain_wsj0mix_dataset/,https://www.reddit.com/r/MachineLearning/comments/1gk3hk7/d_about_the_speechbrain_wsj0mix_dataset/,2024-11-05 10:15:56,2,1.0,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gk3hk7
MachineLearning,[D] Do second tier papers have any value when apply for industry research job?,"I think I have come across some industry jobs before that required applicants to have top tier paper (NIPS/ICML/ICLR/CVPR/ICCV/ECCV), so my question is do paper from *less prestige* (AAAI/IJCAI/WACV/BMVC.... or  journals) conference have any value when appying for these job? Additionaly, are metrics like h-index or citation matter?",Competitive_Newt_100,1gjz3in,https://reddit.com/r/MachineLearning/comments/1gjz3in/d_do_second_tier_papers_have_any_value_when_apply/,https://www.reddit.com/r/MachineLearning/comments/1gjz3in/d_do_second_tier_papers_have_any_value_when_apply/,2024-11-05 04:58:17,47,0.72,47,0,43,0,0,False,False,True,False,False,Discussion,self,t3_1gjz3in
MachineLearning,[D] Best Resources for Sensitivity Analysis on Large ML Pipelines?,"
I'm on a team that's launching a large project to examine how an ML pipeline behaves in response to variations in data. 

This is the first time I'm doing a sensitivity analysis this large and complex in a while, so I'm looking for help to identify the most up-to-date resources on: 

- Simulated data, and especially any Python tools and how they compare with the best that R has to offer

- Evaluation tooling

- Elasticity

- Any best resources on Sensitivity analysis overall, particularly newer ones from the post couple of years

What are the best resources you've found?",oldmaninnyc,1gjy1xu,https://reddit.com/r/MachineLearning/comments/1gjy1xu/d_best_resources_for_sensitivity_analysis_on/,https://www.reddit.com/r/MachineLearning/comments/1gjy1xu/d_best_resources_for_sensitivity_analysis_on/,2024-11-05 03:58:33,5,1.0,5,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gjy1xu
MachineLearning,Video Input for your local LLMS [P],"**What My Project Does**

OpenSceneSense-Ollama is a powerful Python package designed for privacy-focused video analysis directly on your local machine. With this tool, you can leverage Ollama’s local models to analyze frames, transcribe audio, dynamically select key frames, and generate detailed summaries — all without relying on cloud-based APIs. It’s ideal for those needing rich, insightful analysis of video content while ensuring data privacy and minimizing usage costs.

**Target Audience**

This project is tailored for developers, researchers, data scientists, and privacy-conscious users who require in-depth, locally processed video analysis. It's perfect for applications where data security is critical, including:

\- Content creation workflows that need automatic video summarization

\- Researchers building labeled datasets for machine learning

\- Platforms needing context-rich content moderation

\- Offline projects in remote or restricted environments

**Comparison**

OpenSceneSense-Ollama goes beyond traditional video analysis tools that often separate frame and audio analysis. Instead, it integrates both visual and audio elements, allowing users to prompt the models to produce comprehensive summaries and in-depth contextual insights. Where most tools might identify objects or transcribe audio separately, OpenSceneSense-Ollama unifies these components into narrative summaries, making it ideal for richer datasets or more nuanced content moderation.

**Getting Started**

To begin using OpenSceneSense-Ollama:

1. Prerequisites: Make sure you have Python 3.10+, FFmpeg, PyTorch and Ollama installed on your machine.
2. Install with pip: Run \`pip install openscenesense-ollama\` to install the package.
3. Configuration: Start analyzing video with customizable prompts, frame selection, and audio transcription.

Feel free to dive in, try it out, and share your feedback especially if you're working in AI, privacy-focused applications, or video content moderation. Let’s build a powerful, local solution for meaningful video analysis!

[https://github.com/ymrohit/openscenesense-ollama](https://github.com/ymrohit/openscenesense-ollama)",rohit3627,1gjudjw,https://reddit.com/r/MachineLearning/comments/1gjudjw/video_input_for_your_local_llms_p/,https://www.reddit.com/r/MachineLearning/comments/1gjudjw/video_input_for_your_local_llms_p/,2024-11-05 00:50:06,10,0.86,10,0,6,0,0,False,False,True,False,False,Project,self,t3_1gjudjw
MachineLearning,[P] NN for creating best camouflage,"I had this idea for some time, and I have created all the functions for creating data as well as all the architecture. The problem is that I only have two years experience in Deep Learning, and this is GAN style network, and GANs are known to be very hard to train. I would like you opinions on idea, as well as some tips, suggestions, advices and things to change. Also if someone finds this interesting I would love to work with someone on this project.

# Camouflage Pattern Generation Model

The objective is to create a model that generates optimal camouflage color patterns by training a generator model and using a segmentation model as a discriminator to assess the effectiveness of the generated camouflage. Both the generator and discriminator are trained simultaneously.

# Model Structure

# Forward Process

1. **Generator**:
   * The generator is a simple decoder model that takes a random latent vector of size `n_embed = 128` and outputs a 3x32x32 camouflage color pattern.
   * This generated camouflage pattern is then tiled to form a larger texture, matching the size of an image of a soldier.
2. **Creating Camouflaged Soldier**:
   * Random black-and-white PNG images of soldiers are sampled and resized to `(1, W, H)`, with the values inverted so the soldier appears in white (foreground) and the background is black.
   * The tiled camouflage pattern is then applied to the soldier by masking with the soldier image, producing a camouflaged soldier figure. This entire operation is batched and allows gradients to flow through.
3. **Placing Camouflaged Soldier on Background**:
   * The camouflaged soldier is randomly placed on a background image (e.g., a forest scene).
   * A label mask for the segmentation model is generated simultaneously, with two classes: background and soldier.
4. **Discriminator (Segmentation Model)**:
   * A pre-trained segmentation model (acting as a discriminator) is used with two output classes (background and soldier).
   * This model assesses how well the camouflage pattern blends the soldier into the background by trying to classify the soldier as the background.

# Loss Functions and Optimization

Two loss functions are used, each with separate backpropagation processes:

1. **Generator Loss**:
   * This encourages the generator to create a camouflage pattern that makes the soldier indistinguishable from the background.
   * **Loss Function**: `CrossEntropyLoss(output, 0)` where the output is the predicted segmentation map from the discriminator, and 0 represents the background class.
2. **Discriminator (Segmentation Model) Loss**:
   * This encourages the segmentation model to correctly identify the camouflaged soldier in the background.
   * **Loss Function**: `CrossEntropyLoss(output, label_mask)` where the label mask has two classes: background and soldier.

# Key Considerations

This setup resembles a Generative Adversarial Network (GAN) but differs in that it uses no ""real"" camouflage data, only generated samples. Additionally:

* **Separate Optimizers**: Different optimizers are recommended for the generator and discriminator.
* **Loss Scaling**: Careful tuning of scaling factors or learning rates may be required to stabilize training.
* **Two-Step Backpropagation**: Instead of a typical GAN-style loss, a two-step backpropagation approach is used to update the models independently.

https://preview.redd.it/qd2cr2rkyyyd1.png?width=5603&amp;format=png&amp;auto=webp&amp;s=0faee2cb0504a98c36b365b2edbc59253509d8c7

",MemoryCompetitive691,1gjslcz,https://reddit.com/r/MachineLearning/comments/1gjslcz/p_nn_for_creating_best_camouflage/,https://www.reddit.com/r/MachineLearning/comments/1gjslcz/p_nn_for_creating_best_camouflage/,2024-11-04 23:28:17,8,0.67,8,0,11,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/ZbcRXqOHC6kNgT6u01acKjs2AJfNqzOWqAQKtztEfKE.jpg,t3_1gjslcz
MachineLearning,What problems do Large Language Models (LLMs) actually solve very well? [D],"While there's growing skepticism about the AI hype cycle, particularly around chatbots and RAG systems, I'm interested in identifying specific problems where LLMs demonstrably outperform traditional methods in terms of accuracy, cost, or efficiency. Problems I can think of are:

\- words categorization

\- sentiment analysis of no-large body of text

\- image recognition (to some extent)

\- writing style transfer (to some extent)

what else?",Educational-String94,1gjoxpi,https://reddit.com/r/MachineLearning/comments/1gjoxpi/what_problems_do_large_language_models_llms/,https://www.reddit.com/r/MachineLearning/comments/1gjoxpi/what_problems_do_large_language_models_llms/,2024-11-04 20:52:48,147,0.91,147,0,107,0,0,False,False,True,False,False,Discussion,self,t3_1gjoxpi
MachineLearning,[D] Resources for adding cross attention to a pretrained language model,"I want to train new cross attention layers feeding into a pretrained transformer (maybe a small llama model) while keeping the rest of the model constant.

What are some resources that might be helpful?
",BinaryOperation,1gjhsuj,https://reddit.com/r/MachineLearning/comments/1gjhsuj/d_resources_for_adding_cross_attention_to_a/,https://www.reddit.com/r/MachineLearning/comments/1gjhsuj/d_resources_for_adding_cross_attention_to_a/,2024-11-04 16:03:34,4,0.83,4,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gjhsuj
MachineLearning,"[D] Is there limited quantization in all LLM models? For example you can take a standard model like meta-llama/Llama-3.2-1B and run it at half, but there are also models specifically made for 4bit quantization (i.e. meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8)","I'm just trying to understand how quantization is setup in all the models.

Standard models like meta-llama/Llama-3.2-1B can be run without quantization (bfloat16 or float32?), or they can be told to run at half (float16?) with an inferencing app (like vLLM). So does that mean there is some quantization build into all models? Instead of telling it to run at half quantization, can I instead say int8? Or does that only work if the model was built for it?

And then there are models that are specifically built for int4 (i.e. meta-llama/Llama-3.2-1B-Instruct-SpinQuant\_INT4\_EO8). Does that mean that when you run this model with vLLM, you have to explicitly say you are running it at int4, or you just leave that at default and it will automatically run at int4? Can that be overridden to int8? Or is it just hardcoded for int4?

Just been trying to wrap my head around this for the past 2 days.",xil35,1gjhjza,https://reddit.com/r/MachineLearning/comments/1gjhjza/d_is_there_limited_quantization_in_all_llm_models/,https://www.reddit.com/r/MachineLearning/comments/1gjhjza/d_is_there_limited_quantization_in_all_llm_models/,2024-11-04 15:54:02,3,0.64,3,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gjhjza
MachineLearning,[D] COLING25 Industry Track: Notification of Acceptance,"The date for ""notification of acceptance"" was 12:00 anywhere on Earth, November 3rd of 2024. And we've not heard back from the chairs, there's no notification on the portal as well, is there a delay? Or is it that only those papers which are accepted will receive a notification? Please share any info/updates on this, thanks.",BlackEyesBrownSavant,1gjhbox,https://reddit.com/r/MachineLearning/comments/1gjhbox/d_coling25_industry_track_notification_of/,https://www.reddit.com/r/MachineLearning/comments/1gjhbox/d_coling25_industry_track_notification_of/,2024-11-04 15:44:40,7,0.82,7,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gjhbox
MachineLearning,"[R] Estimation of multivariate mutual information, PID for more than three variables "," Why isn't estiming high dimensional mutual information popular. For instance the most I've seen is 3 variable. I know the number of samples needed exponentially increases. But in big data settings it would still be feasible. 

Discrimination is also an issue since estimation is usually performed for binned data.

Anyone know more about this and the practical applications of more than three variable mutual information? On very interested in reading about applications to infer relationship between high dimensional variables in data sets with large number of samples.",Sandy_dude,1gjdtpi,https://reddit.com/r/MachineLearning/comments/1gjdtpi/r_estimation_of_multivariate_mutual_information/,https://www.reddit.com/r/MachineLearning/comments/1gjdtpi/r_estimation_of_multivariate_mutual_information/,2024-11-04 13:09:25,2,1.0,2,0,11,0,0,False,False,True,False,False,Research,self,t3_1gjdtpi
MachineLearning,What differentiates the top % in ML/ DL? [discussion],"Hello! I'll start by saying that I work in the recruitment field, internally for one of the leading tech/ AI companies in the 🌍. My background has historically been Software, and I am transitioning in to AI.

I am a diligent professional and usually take basic technical entry tests to fundamentally understand the area/ infrastructure that I recruit for. I wanted to ask this great community:

🤔 What differentiates the top % of ML/ DL engineers? Is it hands-on experience and SME in a particular subset of AI, or more rounded knowledge of most areas?

🤔 Is reddit the largest community online for people in ML/ DL? would you recommend any particular platforms to network aside from Linkedin, Reddit and Slack?

I am aware that I will get messaged due to my profession, but I have had incredible interactions in the product/ Eng space on Reddit over the years and your insights are invaluable.

Thank you",k44yej88,1gjak7a,https://reddit.com/r/MachineLearning/comments/1gjak7a/what_differentiates_the_top_in_ml_dl_discussion/,https://www.reddit.com/r/MachineLearning/comments/1gjak7a/what_differentiates_the_top_in_ml_dl_discussion/,2024-11-04 09:45:07,0,0.17,0,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1gjak7a
MachineLearning,[P] Text classification with low number of data: LLM or other classification models? ,"I have a project where I need to summarize a few webpages related to a subject and use the summaries to classify aforementioned subject. 

When prototyping, I use LLM for both summarization and classification task and they did achieve about 80% accuracy (classification task isn't that hard anyway). For the sake of performance and the hate of using LLM for everything, I initially want to train 2 models, one for summarization and one for classification.

The problem arise when I see that most usable summazier is not that lighter than a small LLM (and not many support my language). Add another classifier like bert or something then the difference in memory consumption is probably negligible. Though runtime should still be better. 

Another problem is that my dataset is just about 2000 webpages for about 300 subjects and 70 classes. Many classes has 0 or 1 samples. With that data, I think finetuning a summazier is somewhat doable while it's probably not applicable to the classifier. Getting more data is not exactly an option as I don't have the time budget. Despite that, I have detail description of what should be classified into each class. 

As a result, my current solution is to finetune a LLM to do both summarization and classification. The downside is that LLM sometimes gives invalid classes.

Is adding a classification head to the LLM a good solution? I'm afraid that I don't have enough data to train even that classification head (realisticly a single classification matrix). Or is there a better approach than this? ",mtmttuan,1gj6iqf,https://reddit.com/r/MachineLearning/comments/1gj6iqf/p_text_classification_with_low_number_of_data_llm/,https://www.reddit.com/r/MachineLearning/comments/1gj6iqf/p_text_classification_with_low_number_of_data_llm/,2024-11-04 04:47:35,3,0.8,3,0,11,0,0,False,False,True,False,False,Project,self,t3_1gj6iqf
MachineLearning,[P] Combining algorithms in an autonomous driving project,"I am planning to do a project consisting of an autonomous driving system.



I was thinking of using reinforcement learning but it would take too long to train (months), with the consequent expenditure of electricity and money (specialized servers).



After seeing some videos from Sentdex and others where, after training for 2 months in a row, the driver manages to drive like a drunk person, I have considered it unfeasible and I have thought:



Would it be possible to combine a deep learning algorithm with reinforcement learning together with a traditional computer vision algorithm like lane finding?



Is there any way to make these algorithms work together, reducing the training time?



Would you use other algorithms or approaches?

I'm using CARLA Sim.

Thanks.",AlbertV999,1gj4864,https://reddit.com/r/MachineLearning/comments/1gj4864/p_combining_algorithms_in_an_autonomous_driving/,https://www.reddit.com/r/MachineLearning/comments/1gj4864/p_combining_algorithms_in_an_autonomous_driving/,2024-11-04 02:37:16,0,0.33,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1gj4864
MachineLearning,"[D] How to read and perform a semantic query over the combination of structured data and unstructured data ? As in, for example, a large number of pdf documents with text and also structured data that occurs in lists/tables in the pdf or as numerical data mentioned inside text paragraphs.","To carry the example further, say these pdfs are financial reports from different companies that contain quarterly revenue data amongst other data. There are two aspects to the broader problem: Query and Read as further elaborated below  
QUERY : So I want to be able to make arbirary queries against all the pdfs , like ""find companies where Year over year quarterly growth was greater than 10% and which also mentioned new product launches"".  This is a simple example, but actual use case can be arbirarily more complex with more aspects to the query.  
READ (feed in) new reports: I also want to make it so that non-technical users can drop in new pdf reports as they become available which then get added to the query database without manual involvement of technical personnel in pre-processing  
Can you please guide me as to how to approach this problem and what options exist to implement something like this? Thanks.",SpaceShip992,1gj3bg8,https://reddit.com/r/MachineLearning/comments/1gj3bg8/d_how_to_read_and_perform_a_semantic_query_over/,https://www.reddit.com/r/MachineLearning/comments/1gj3bg8/d_how_to_read_and_perform_a_semantic_query_over/,2024-11-04 01:49:03,6,0.88,6,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gj3bg8
MachineLearning,"[P] Benchmarking 1 Million Files from ImageNet into DVC, Git-LFS, and Oxen.ai for Open Source Dataset Collaboration","Hey all!

If you haven't seen the Oxen project yet, we have been building a fast [open source unstructured data version control tool](https://github.com/Oxen-AI/oxen-release) and platform to host the data ([https://oxen.ai](https://oxen.ai/)). It’s an alternative to dumping data on Hugging Face with git-lfs or their datasets library and goes together with their models like chocolate and peanut butter - Oxen can be used for iterating on and editing the data and Hugging Face for public models.

We were inspired by the idea of making large machine learning datasets living &amp; breathing assets that people can collaborate on, rather than the static dumps. Lately we have been working hard on optimizing the underlying Merkle Trees and data structures with in [Oxen.ai](http://oxen.ai/) and just released v0.19.4 which provides a bunch of performance upgrades and stability to the internal APIs.

# 1 Million Files Benchmark

To put it all to the test, we decided to benchmark the tool on the 1 million+ images in the classic ImageNet dataset.

The TLDR is [Oxen.ai](http://oxen.ai/) is faster than raw uploads to S3, 13x faster than git-lfs, and 5x faster than DVC. The full breakdown can be found here 👇

[https://docs.oxen.ai/features/performance](https://docs.oxen.ai/features/performance)

If you are in the ML/AI community, or just data aficionados, would love to get your feedback on both the tool and the codebase. We would love some community contribution when it comes to different storage backends and integrations into other data tools.",FallMindless3563,1gj0si8,https://reddit.com/r/MachineLearning/comments/1gj0si8/p_benchmarking_1_million_files_from_imagenet_into/,https://www.reddit.com/r/MachineLearning/comments/1gj0si8/p_benchmarking_1_million_files_from_imagenet_into/,2024-11-03 23:43:42,22,0.92,22,0,9,0,0,False,False,True,False,False,Project,self,t3_1gj0si8
MachineLearning,[D] Comparison of Logistic Regression with/without SMOTE,"This has been driving me crazy at work. I've been evaluating a logistic predictive model. The model implements SMOTE to balance the dataset to 1:1 ratio (originally 7% of the desired outcome). I believe this to be unnecessary as shifting the decision threshold would be sufficient and avoid unnecessary data imputation. The dataset has more than 9,000 ocurrences of the desired event - this is more than enough for MLE estimation. My colleagues don't agree. 

I built a shiny app in R to compare the confusion matrixes of both models, along with some metrics. I would welcome some input from the community on this comparison. To me the non-smote model performs just as well, or even better if looking at the Brier Score or calibration intercept. What do you guys think?",Janky222,1gizg2u,https://reddit.com/r/MachineLearning/comments/1gizg2u/d_comparison_of_logistic_regression_withwithout/,https://i.redd.it/fl4kf6wmlryd1.jpeg,2024-11-03 22:42:39,81,0.9,81,0,44,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/0JW2JFRJiBEBfVsaRIZkc0GYNZnTMCc3JSFDw7WSQHs.jpg,t3_1gizg2u
MachineLearning,[D] Feature Selection + Feature Eng. Order of Operations ,"Anyone have a preferred methodology and order of operations for performing feature selection with feature engineering? 
For example, is the best practice to drop unimportant features first, then iteratively engineer new features? ",Secret_Valuable_Yes,1gizc5l,https://reddit.com/r/MachineLearning/comments/1gizc5l/d_feature_selection_feature_eng_order_of/,https://www.reddit.com/r/MachineLearning/comments/1gizc5l/d_feature_selection_feature_eng_order_of/,2024-11-03 22:37:42,11,0.87,11,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gizc5l
MachineLearning,[R] Training multiple autoencoders reduces loss but not accuracy? ,"Hello,

I am training two seperate autoencoders to cluster data. The network passes the input to both autoencoders, computes the reconstruction error for both autoencoders (AE) and picks the best one. This means that only the reconstruction of one AE contributes to the loss and so only one gets gradient updates per input. 

Loss decreases but accuracy just fluctuates. Moreover, both autoencoders are used but eventually the model just uses the same autoencoder for almost all inputs. Any insight on why? 

The goal is for each AE to learn to reconstruct datapoints that are neighbors or belong to same cluster. I’ve seen papers doing the same thing but they just pre-train their network to go around this and never discuss WHY this happens.

Ty ",Grand_Comparison2081,1giz2wp,https://reddit.com/r/MachineLearning/comments/1giz2wp/r_training_multiple_autoencoders_reduces_loss_but/,https://www.reddit.com/r/MachineLearning/comments/1giz2wp/r_training_multiple_autoencoders_reduces_loss_but/,2024-11-03 22:26:03,6,0.87,6,0,10,0,0,False,False,True,False,False,Research,self,t3_1giz2wp
MachineLearning,[D] What are some good resources for learning about sequence modeling architectures,"What are some good resources for learning about sequence modeling architectures? I've been preparing for exams and interviews and came across this quiz on GitHub: [https://viso.ai/deep-learning/sequential-models/](https://viso.ai/deep-learning/sequential-models/) and another practice site: [https://app.wittybyte.ai/problems/rnn\_lstm\_tx](https://app.wittybyte.ai/problems/rnn_lstm_tx). Do you think these are comprehensive, or should I look for more material? Both are free to use right now",vicky0212,1giyr0t,https://reddit.com/r/MachineLearning/comments/1giyr0t/d_what_are_some_good_resources_for_learning_about/,https://www.reddit.com/r/MachineLearning/comments/1giyr0t/d_what_are_some_good_resources_for_learning_about/,2024-11-03 22:11:17,1,0.67,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1giyr0t
MachineLearning,Video Input for the current LLMs [P],"Hey everyone,

I’m excited to share a project I’ve been working on OpenSceneSense. It’s a Python package designed to bridge video content with large language models (LLMs) like OpenAI’s Vision models and OpenRouter, opening up new ways to understand, analyze, and create insights from video data.

Why OpenSceneSense?

Most LLMs are amazing with text but aren’t designed to handle video directly. OpenSceneSense changes that. It uses frame-by-frame analysis, audio transcription, and scene detection to turn video data into something LLMs can work with. Imagine using a prompt to get a detailed description of what’s happening in each scene or automatically creating a narrative that ties the video and audio together.

Potential Use Cases:

\- Dataset Creation: If you’re working in computer vision or machine learning, OpenSceneSense can create richly annotated datasets from videos, giving LLMs detailed context about visual events, object interactions, and even sentiment shifts across scenes.

\- Content Moderation: OpenSceneSense can bring more context to content moderation. Unlike traditional moderation methods that might just detect keywords or simple visuals, this tool can interpret entire scenes, combining both visual and audio cues. It could help distinguish between genuinely problematic content and innocuous material that might otherwise get flagged.

And I’m also working on an Ollama-compatible version so you can run it locally without relying on the cloud, which will be useful for anyone concerned about privacy or latency.

To dive in, you’ll need Python 3.10+, FFmpeg, and a couple of API keys (OpenAI or OpenRouter). Install it with \`pip install openscenesense\`, and you’re all set. From there, it’s easy to start analyzing your videos and experimenting with different prompts to customize what you want to extract.

I’d love feedback from anyone working in video tech, dataset creation, or moderation. Check out the code, give it a spin, and let’s see where we can take OpenSceneSense together!

[https://github.com/ymrohit/openscenesense](https://github.com/ymrohit/openscenesense)",rohit3627,1giylel,https://reddit.com/r/MachineLearning/comments/1giylel/video_input_for_the_current_llms_p/,https://www.reddit.com/r/MachineLearning/comments/1giylel/video_input_for_the_current_llms_p/,2024-11-03 22:04:26,5,0.7,5,0,1,0,0,False,False,True,False,False,Project,self,t3_1giylel
MachineLearning,[D] Self-hostable tooling for offline batch-prediction on SQL tables,"Hey folks,

I am working for a hospital in Switzerland and due to data regulations, it is quite clear that we need to stay out of cloud environments. Our hospital has a MSSQL-based data warehouse and we have a separate docker-compose based ML-ops stack. Some of our models are currently running in docker containers with a REST api, but actually, we just do scheduled batch-prediction on the data in the DWH. In principle, I am looking for a stack that allows you to host ml models from scikit learn to pytorch and allows us to formulate a batch prediction on data in the SQL tables by defining input from one table as input features for the model and write back the results to another table. I have seen postgresml and its predict_batch, but I am wondering if we can get something like this directly interacting with our DWH? What do you suggest as an architecture or tooling for batch predicting data in SQL DBs when the results will be in SQL DBs again and all predictions can be precomputed?

Thanks for your help!",benelott,1giybo2,https://reddit.com/r/MachineLearning/comments/1giybo2/d_selfhostable_tooling_for_offline/,https://www.reddit.com/r/MachineLearning/comments/1giybo2/d_selfhostable_tooling_for_offline/,2024-11-03 21:53:14,3,1.0,3,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1giybo2
MachineLearning,[D][R] PCX: JAX library for Predictive Coding Networks at scale,"Hello everyone,

Over the last year I've been working on a library to build, set up, and test predictive codig networks with ease. At the moment I'm not really active in the area anymore, but I really believe to be an exciting research field and would love if any new people got involved with it. So I'm just putting it out here to maybe catch someone's curiosity: [https://github.com/liukidar/pcx](https://github.com/liukidar/pcx) . In short, predictive coding is one kind of energy-based learning method. Energy based networks are basically alternative neural networks that can be trained with relying on backpopagation.

There's also an associated paper but I don't think it's that interesting (just look at the pictures maybe :P): [\[2407.01163\] Benchmarking Predictive Coding Networks -- Made Simple](https://arxiv.org/abs/2407.01163)

I'm very happy to discuss the topic if anyone is interested into it, and I can also give concrete research directions to follow!",liukidar,1giswu8,https://reddit.com/r/MachineLearning/comments/1giswu8/dr_pcx_jax_library_for_predictive_coding_networks/,https://www.reddit.com/r/MachineLearning/comments/1giswu8/dr_pcx_jax_library_for_predictive_coding_networks/,2024-11-03 17:59:53,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1giswu8
MachineLearning,[D] Are there statistical techniques to identify the subsets of data where your ML model is particularly more predictive?,"Hopefully this question isn't deemed as too amateurish for this subreddit:

So I'll just use a simplified finance example: Say I want to predict security returns over the next 5 minutes. My X matrix is just a time series of feature values sampled every minute in the day that the market is open (could be stuff like buy volume, sell volume, relative returns of other securities, etc.)

So I fit an ML model and I get a certain R squared and rmse. Based on my intuition, I know that there are certain situations where the model has more predictive power than average. It could be something arbitrary, like the hour of the day is 11 AM and the relative return of my security compared to some other security exceeds some threshold. Let's say in this situation the model is a surefire bet, in fact if I were to only consider the subset of data fulfilling my criteria, I could predict the y\_labels almost perfectly each time. And just for theoretical discussion's sake, let's say this fully translates to the test set and live prediction as well, we just fully understand this phenomenon in the real world as long as our conditions are satisfied.

However, all of this gets watered down because my model is evaluated over the full data set and it doesn't do so well in most other situations. I don't want to just rely on my intuition to figure out what these subsets of data are, are there statistical techniques to employ that can inform me of subsets of data which are extra predictive? (of course, I'd want each subset to have at least some number of samples to evidence significance)

And beyond this, are there techniques that can identify these subsets at the model training phase? This feels a bit parallel to bagging, but the intent is different - I'm not aiming for some generalized reduction in variance in a broader model, I want to know the models that seem to work particularly well on some subsets",thoughtdump9,1gipaj4,https://reddit.com/r/MachineLearning/comments/1gipaj4/d_are_there_statistical_techniques_to_identify/,https://www.reddit.com/r/MachineLearning/comments/1gipaj4/d_are_there_statistical_techniques_to_identify/,2024-11-03 15:23:45,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gipaj4
MachineLearning,[D] Fourier weights neural networks,"Dear ML community,

I wanted to share an idea for discussion about the usage of Fourier coefficients to parametrize weights in neural networks. Typically in MLPs the weights are defined only in one direction, and are undefined in the other direction, which leaves it open: we can define the weights to be symmetric: w(r,s) = w(s,r) and we can use the Fourier coefficients of a two variable symmetric function to compute the weights via backpropagation and gradient descent. (I should mention that I am currently activeyl searching for an opportunity to bring my knowledge of Machine Learning to projects near Frankfurt am Main ,Germany.)

  
**Edit:** Maybe my wording was not so correct. Let us agree that in most cases the symmetry assumption is satisfied by MLPs with invertible activation function. The idea I would like to discuss is the usage of Fourier coefficients to (re-) construct the weights w(r,s) = w(s,r) . For this idea to make sense the FWNN do not learn the weights as usual MLPs / ANNs , but they learn the \_coefficients\_ of the Fourier series (at least some of them). By adjusting how many coefficients are learned, the FWNN could adjust its capacity to learn. Notice that by symmetry of the function w(r,s) we get terms like sum\_{j\] c\_j\*cos(j \* (r+s) ) where j ranges over some predefined range \[-R,R\] of integers. In theory this R should be infinity hence Z = \[-inf, +inf\] are the whole integers. Notice also that the parameter c\_j the network learns are 2\*R+1 in number, which at first glance is independent of the number of neurons N. Hence a traditional neural network with N neurons, has in theory to learn O(N\^2) weights, but with the Fourier transform we reduce this number of parameters to 2\*R+1. Of course it can happen that R = N\^2 but I can imagine that there are problems where 2\*R+1 &lt;&lt; N\^2. I hope this clarifies the idea.

Code: [https://github.com/githubuser1983/fourier\_weighted\_neural\_network/blob/main/fourier\_weighted\_neural\_network.py](https://github.com/githubuser1983/fourier_weighted_neural_network/blob/main/fourier_weighted_neural_network.py)

Explanation of the method: [https://www.academia.edu/125262107/Fourier\_Weighted\_Neural\_Networks\_Enhancing\_Efficiency\_and\_Performance](https://www.academia.edu/125262107/Fourier_Weighted_Neural_Networks_Enhancing_Efficiency_and_Performance)",musescore1983,1giwdum,https://reddit.com/r/MachineLearning/comments/1giwdum/d_fourier_weights_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1giwdum/d_fourier_weights_neural_networks/,2024-11-03 20:27:52,27,0.91,27,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1giwdum
MachineLearning,[P] Using Machine Learning to search through video ,"
Since I can’t post video here, here’s a tiktok link , i go through the steps my platforms pipeline goes through to be able to search through videos. 

For example, you could have GBs of videos stored, and be looking for all scenes within the videos containing a sunrise, simply type sunrise to search, and in an instant, you have access to all the sunrise scenes.  

https://vm.tiktok.com/ZMhXmXd5C/",Current-Morning8533,1gitqbz,https://reddit.com/r/MachineLearning/comments/1gitqbz/p_using_machine_learning_to_search_through_video/,https://www.reddit.com/r/MachineLearning/comments/1gitqbz/p_using_machine_learning_to_search_through_video/,2024-11-03 18:34:44,0,0.29,0,0,0,0,0,False,False,True,False,False,Project,self,t3_1gitqbz
MachineLearning,[D] AAAI Phase 2 Results,"When should we expect the results from phase 2 of AAAI 2025 submissions? On the site, the authors feedback is from day 4 to day 8 of November. Are we going to receive the results today, day 3?",Massive_Horror9038,1gir1d1,https://reddit.com/r/MachineLearning/comments/1gir1d1/d_aaai_phase_2_results/,https://www.reddit.com/r/MachineLearning/comments/1gir1d1/d_aaai_phase_2_results/,2024-11-03 16:39:39,8,0.79,8,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gir1d1
MachineLearning,[D] AAAI 2025 Phase 2 Reviews,The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,quasi-literate,1giqc9n,https://reddit.com/r/MachineLearning/comments/1giqc9n/d_aaai_2025_phase_2_reviews/,https://www.reddit.com/r/MachineLearning/comments/1giqc9n/d_aaai_2025_phase_2_reviews/,2024-11-03 16:09:33,100,0.95,100,0,569,0,0,False,False,True,False,False,Discussion,self,t3_1giqc9n
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1giq4ia,https://reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/,2024-11-03 16:00:17,5,1.0,5,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1giq4ia
MachineLearning,[D] Looking for Research Internship in Applied RL &amp; Robotics,"I am a PhD candidate at Mila, working on reinforcement learning for different robotic applications (worked on applications like excavator automation, physics-based character animation, and autonomous driving). I'm currently seeking a summer research internship for 2025, and I'm really interested in any roles that focus on applied RL or embodied AI.

Here’s a bit about my research journey so far:

* **Automatic Reward Modeling**: Developed methods for deriving reward functions from expert demonstration for excavator automation in Vortex Simulator. (Presented at the NeurIPS RL for Real-life Applications workshop.)
* **Sample-Efficient RL**: Improved sample efficiency on the Atari benchmark through transformer-based discrete world modeling. (ICML 2024)
* **Compositional Motion Priors for Multi-Task RL**: I'm currently working on multi-task learning for robotic locomotion with compositional motion priors, using Isaac Gym.
* **RL for Autonomous Driving**: Designed a curriculum learning method for autonomous driving on the CARLA simulator, eliminating the need for complex reward shaping. (Inria research student).

I’m also exploring the use of Diffusion Models alongside RL for stable, diverse control strategies.

If anyone knows of relevant openings or has any advice on places that may value applied RL research, I’d really appreciate it.

Thank you so much for any leads or suggestions!

*My CV and more details are on my* https://pranaval.github.io/*.*",Personal_Click_6502,1giq0e8,https://reddit.com/r/MachineLearning/comments/1giq0e8/d_looking_for_research_internship_in_applied_rl/,https://www.reddit.com/r/MachineLearning/comments/1giq0e8/d_looking_for_research_internship_in_applied_rl/,2024-11-03 15:55:19,0,0.38,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1giq0e8
MachineLearning,"[D] Publishing in NeurIPS, ICML, ICLR as an Early Researcher: Any Advice?","I'm currently pursuing a master's degree, and my goal is to publish a paper in one of the top AI/ML venues, like NeurIPS, ICML, or ICLR, before I finish my program. I'm studying at a Federal University in Brazil, which is well-regarded locally but doesn’t have much international recognition. My research lab is somewhat unstructured—we mainly share computational resources but don’t have collaborative or large-scale projects. Because of this, I don’t have an ongoing project I can join for guidance or support.

Additionally, my supervisor’s research focus is more on applied machine learning in chemistry, so he doesn’t have experience publishing in these top conferences. This means I don’t have direct mentorship on the publishing process specific to these venues. To give some context, NeurIPS's call for papers is expected around May 2025, so I still have some time but want to prepare as thoroughly as possible.

I’d really appreciate any advice on how to increase my chances of getting published in these venues. For example, I’ve heard that it helps to cite potential reviewers in your work. Any tips on how to navigate the process, write in a way that aligns with these conferences, or understand what reviewers might be looking for would be helpful. I’d also like advice on handling rejection, like potential backup venues to consider if my paper isn’t accepted.",mrlucasrib,1gip4cf,https://reddit.com/r/MachineLearning/comments/1gip4cf/d_publishing_in_neurips_icml_iclr_as_an_early/,https://www.reddit.com/r/MachineLearning/comments/1gip4cf/d_publishing_in_neurips_icml_iclr_as_an_early/,2024-11-03 15:16:15,0,0.48,0,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1gip4cf
MachineLearning,[R] What is your Recipe for Training Neural Networks in 2024?,"You may already know the [Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/) bible from Karpathy 2019

While most of the advices are still valid, the landscape of Deep Learning model/method has changed a lot since. Karpathy's advices work well in the supervised learning setting, he does mention it:

&gt;stick with supervised learning. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).

I've been training a few image diffusion models recently, and I find it harder to make data driven decisions in the unsupervised setting. Metrics are less reliable, sometimes I train models with better losses but when I look at the samples they look worse

Do you know more modern recipes to train neural network in 2024? (and not just LLMs)",Even_Information4853,1giovxi,https://reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/,https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/,2024-11-03 15:05:44,174,0.98,174,0,43,0,0,False,False,True,False,False,Research,self,t3_1giovxi
MachineLearning,[P] Understanding Multimodal LLMs: The Main Techniques and Latest Models,,seraschka,1gio7ap,https://reddit.com/r/MachineLearning/comments/1gio7ap/p_understanding_multimodal_llms_the_main/,https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html,2024-11-03 14:34:30,50,0.99,50,0,8,0,0,False,False,False,False,False,Project,default,t3_1gio7ap
MachineLearning,[D] X List to follow for ML research?,"Hey guys i'm just getting into Ml (been in the field for about 6 months now) and i want to keep up with it in a better way, but there's so much stuff to follow on X that im confused, any lists to recommend? ",jinstronda,1ginx35,https://reddit.com/r/MachineLearning/comments/1ginx35/d_x_list_to_follow_for_ml_research/,https://www.reddit.com/r/MachineLearning/comments/1ginx35/d_x_list_to_follow_for_ml_research/,2024-11-03 14:21:09,19,0.81,19,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1ginx35
MachineLearning,[D] Formal Logic and Set Theory in Belief Sets Using LLMs?,"I've been playing with a belief set that's a collection of propositional clauses, and using LLMs and cosine similarity + clustering + prompt engineering to approximate predicate logic operations.

For example:

$Statement1 = ""real estate agents help people find the right house""

$Statement2 = ""real estate agents help men find the right house""


Cosine similarity can only tell me that these two statements are close in vector space (around 0.93) but explains nothing about the fact that Statement2 is a subset of Statement1.


I've been using LLMs with some prompt engineering and few-shot examples to get these set relationships, but I was wondering if there's an easier or computationally cheaper way of doing it (given a belief set of simple propositional clauses) - - or if there are LLMs that have been fine-tuned specifically for formal logic...?",noellarkin,1gil6vg,https://reddit.com/r/MachineLearning/comments/1gil6vg/d_formal_logic_and_set_theory_in_belief_sets/,https://www.reddit.com/r/MachineLearning/comments/1gil6vg/d_formal_logic_and_set_theory_in_belief_sets/,2024-11-03 11:56:15,9,0.86,9,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gil6vg
MachineLearning,[D] Is there an alternative to Science Twitter/X?,"Hey folks,

I have been wondering if there is an alternative to the science community on Twitter/X, especially in the DS/ML sphere. I really liked that community before and during COVID, but I left Twitter shortly after Elon took charge, as the platform was already quite toxic then and became much worse since. 

I'm aware that there is a community active on LinkedIn, which is okay at times, but mostly full of influencers who try to sound/look intelligent and people hyping up every little new thing about LLMs. I know that other people left the science community on Twitter since then and was hence wondering if an alternative has evolved over the last years.

P.s. I will post this message in the DS community as well.",H4RZ3RK4S3,1gikys5,https://reddit.com/r/MachineLearning/comments/1gikys5/d_is_there_an_alternative_to_science_twitterx/,https://www.reddit.com/r/MachineLearning/comments/1gikys5/d_is_there_an_alternative_to_science_twitterx/,2024-11-03 11:41:42,225,0.86,225,0,73,0,0,False,False,True,False,False,Discussion,self,t3_1gikys5
MachineLearning,[D] Use ROCm for machine learning projects on a mobile RX 6700S?,"Hello, I'm currently using an AMD G14 with a RX 6700S GPU and I am interested in running some machine learning projects. I am currently using Windows.

Is there any way for me to use the RX 6700S GPU to run machine learning projects that uses tensorflow and pytorch on Windows? If not, can I do them using WSL?

I am not that familiar with installations yet so if you can give me some detailed answers or instructions I would really appreciate it.

  
Thank you!",DBT177,1gikbg1,https://reddit.com/r/MachineLearning/comments/1gikbg1/d_use_rocm_for_machine_learning_projects_on_a/,https://www.reddit.com/r/MachineLearning/comments/1gikbg1/d_use_rocm_for_machine_learning_projects_on_a/,2024-11-03 10:57:53,3,1.0,3,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1gikbg1
MachineLearning,"[P] For ML/DL purposes - Mostly inference and fine-tuning, which laptop build is better - 32GB RAM + 8GB VRAM or 64GB RAM?","i7 32GB RAM with 8GB VRAM (NVIDIA GeForce RTX 4060 8GB GDDR6) (**Dell**) 



or 



i7 64GB RAM only (**HP**)?

Or which one of these out of your personal experience do you think is better? 

# Dell Inspiron 16 Plus 7640 Laptop- 16.0-inch 16:10 2.5K Display, Intel Core Ultra 7-155H, 32GB DDR5 RAM, 2TB SSD, NVIDIA GeForce RTX 4060 8GB GDDR6, Windows 11 Home, Onsite &amp; Migrate Service- Ice Blue

[https://www.amazon.com/Dell-Inspiron-Plus-Laptop-16-0-inch/dp/B0D4LJBXQ5?source=ps-sl-shoppingads-lpcontext&amp;ref\_=fplfs&amp;smid=ATVPDKIKX0DER&amp;utm\_source=Perplexity&amp;utm\_medium=referral&amp;th=1](https://www.amazon.com/Dell-Inspiron-Plus-Laptop-16-0-inch/dp/B0D4LJBXQ5?source=ps-sl-shoppingads-lpcontext&amp;ref_=fplfs&amp;smid=ATVPDKIKX0DER&amp;utm_source=Perplexity&amp;utm_medium=referral&amp;th=1)

or

# HP Envy Daily Business Laptop, 17.3"" FHD Touchscreen, Intel Core Ultra 7 155H, 64GB DDR5 RAM, 4TB SSD, Numeric Keypad, HDMI, Webcam, Backlit Keyboard, Wi-Fi 7, Windows 11 Pro, Grey

[https://www.amazon.com/HP-Business-Touchscreen-Numeric-Keyboard/dp/B0D98HG9WG/ref=pd\_rhf\_cr\_s\_pd\_sbs\_rvi\_d\_sccl\_2\_13/140-8762111-2890519?pd\_rd\_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_r=A74WW0PZK7N28B1SXZWS&amp;pd\_rd\_wg=Qj4e9&amp;pd\_rd\_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd\_rd\_i=B0D98HG9WG&amp;th=1](https://www.amazon.com/HP-Business-Touchscreen-Numeric-Keyboard/dp/B0D98HG9WG/ref=pd_rhf_cr_s_pd_sbs_rvi_d_sccl_2_13/140-8762111-2890519?pd_rd_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_r=A74WW0PZK7N28B1SXZWS&amp;pd_rd_wg=Qj4e9&amp;pd_rd_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd_rd_i=B0D98HG9WG&amp;th=1)

or

# HP Newest Envy 17.3"" 4K Ultra 7 Laptop, 17.3"" 4K UHD 3840 * 2160 Display, Intel Core Ultra 7 155H, 64GB DDR5 RAM, 2TB SSD, HDMI, Webcam, Backlit KB, Wi-Fi 7, W11H, Grey, Adata 512 External SSD Bundle

[https://www.amazon.com/HP-Newest-Display-Backlit-External/dp/B0DFWRR47G/ref=pd\_rhf\_cr\_s\_pd\_sbs\_rvi\_d\_sccl\_2\_8/140-8762111-2890519?pd\_rd\_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_r=A74WW0PZK7N28B1SXZWS&amp;pd\_rd\_wg=Qj4e9&amp;pd\_rd\_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd\_rd\_i=B0DFWRR47G&amp;th=1](https://www.amazon.com/HP-Newest-Display-Backlit-External/dp/B0DFWRR47G/ref=pd_rhf_cr_s_pd_sbs_rvi_d_sccl_2_8/140-8762111-2890519?pd_rd_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_r=A74WW0PZK7N28B1SXZWS&amp;pd_rd_wg=Qj4e9&amp;pd_rd_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd_rd_i=B0DFWRR47G&amp;th=1)

or

# HP Elitebook 650 G10 15.6"" FHD Business Laptop Computer, 13th Gen Intel 10-Core i7-1355U, 64GB DDR4 RAM, 4TB PCIe SSD, WiFi 6E, BT 5.3, Backlit KB, Fingerprint Reader, Windows 11 Pro, AZ-XUT Cable

[https://www.amazon.com/HP-Elitebook-650-G10-Fingerprint/dp/B0CBN7TWKC/ref=pd\_rhf\_cr\_s\_pd\_sbs\_rvi\_d\_sccl\_2\_21/140-8762111-2890519?pd\_rd\_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_r=A74WW0PZK7N28B1SXZWS&amp;pd\_rd\_wg=Qj4e9&amp;pd\_rd\_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd\_rd\_i=B0CBN7TWKC&amp;th=1](https://www.amazon.com/HP-Elitebook-650-G10-Fingerprint/dp/B0CBN7TWKC/ref=pd_rhf_cr_s_pd_sbs_rvi_d_sccl_2_21/140-8762111-2890519?pd_rd_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_r=A74WW0PZK7N28B1SXZWS&amp;pd_rd_wg=Qj4e9&amp;pd_rd_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd_rd_i=B0CBN7TWKC&amp;th=1)

or

# Dell Latitude 5550 15 Business AI Laptop, 15.6"" FHD Computer, Intel Ultra 7 155U (Beat i7-1355U), 64GB DDR5 RAM, 4TB PCIe SSD, WiFi 6, Backlit Keyboard, Fingerprint Reader, Windows 11 Pro

[https://www.amazon.com/Dell-Latitude-5550-Business-Fingerprint/dp/B0CS66MXK4/ref=pd\_rhf\_cr\_s\_pd\_sbs\_rvi\_d\_sccl\_2\_22/140-8762111-2890519?pd\_rd\_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_r=A74WW0PZK7N28B1SXZWS&amp;pd\_rd\_wg=Qj4e9&amp;pd\_rd\_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd\_rd\_i=B0CS69V7ZJ&amp;th=1](https://www.amazon.com/Dell-Latitude-5550-Business-Fingerprint/dp/B0CS66MXK4/ref=pd_rhf_cr_s_pd_sbs_rvi_d_sccl_2_22/140-8762111-2890519?pd_rd_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_r=A74WW0PZK7N28B1SXZWS&amp;pd_rd_wg=Qj4e9&amp;pd_rd_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd_rd_i=B0CS69V7ZJ&amp;th=1)

or

# Dell 2024 Newest Inspiron 15 3530 Business Laptop, 15.6"" FHD Touchscreen, Intel 10-Core i7-1355U CPU, 64GB RAM, 4TB SSD, WiFi 6, Webcam, HDMI, with Microsoft Office Lifetime License &amp; Windows 11 Pro

[https://www.amazon.com/Dell-Inspiron-15-Touchscreen-Microsoft/dp/B0B4VFXSS4/ref=pd\_rhf\_cr\_s\_pd\_sbs\_rvi\_d\_sccl\_2\_26/140-8762111-2890519?pd\_rd\_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf\_rd\_r=A74WW0PZK7N28B1SXZWS&amp;pd\_rd\_wg=Qj4e9&amp;pd\_rd\_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd\_rd\_i=B0D4M8DRZT&amp;th=1](https://www.amazon.com/Dell-Inspiron-15-Touchscreen-Microsoft/dp/B0B4VFXSS4/ref=pd_rhf_cr_s_pd_sbs_rvi_d_sccl_2_26/140-8762111-2890519?pd_rd_w=opgi1&amp;content-id=amzn1.sym.46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_p=46e2be74-be72-4d3f-86e1-1de279690c4e&amp;pf_rd_r=A74WW0PZK7N28B1SXZWS&amp;pd_rd_wg=Qj4e9&amp;pd_rd_r=7beb3586-6bd5-4465-9013-3933080c27bc&amp;pd_rd_i=B0D4M8DRZT&amp;th=1)

And is it reasonable if I plan to use a GPU with the 64GB RAM Laptop later if I were to take a 64GB RAM laptop?",No_Cartoonist8629,1gid3w4,https://reddit.com/r/MachineLearning/comments/1gid3w4/p_for_mldl_purposes_mostly_inference_and/,https://www.reddit.com/r/MachineLearning/comments/1gid3w4/p_for_mldl_purposes_mostly_inference_and/,2024-11-03 02:35:30,0,0.2,0,0,16,0,0,False,False,True,False,False,Project,self,t3_1gid3w4
MachineLearning,Tips on generating voices? [P],I’m looking to make a program that will read out loud text files I give it in various voices. Any tips on where to start?,marksmiley,1gi9afs,https://reddit.com/r/MachineLearning/comments/1gi9afs/tips_on_generating_voices_p/,https://www.reddit.com/r/MachineLearning/comments/1gi9afs/tips_on_generating_voices_p/,2024-11-02 23:20:10,1,0.6,1,0,6,0,0,False,False,True,False,False,Project,self,t3_1gi9afs
MachineLearning,[N] Quantum Machines and Nvidia use machine learning to get closer to an error-corrected quantum computer,"An article based on interviews with Quantum Machines and Nvidia about how they used reinforcement learning to optimize pulses, improving performance and fidelity

[https://techcrunch.com/2024/11/02/quantum-machines-and-nvidia-use-machine-learning-to-get-closer-to-an-error-corrected-quantum-computer/](https://techcrunch.com/2024/11/02/quantum-machines-and-nvidia-use-machine-learning-to-get-closer-to-an-error-corrected-quantum-computer/)",MeltingHippos,1gi5ohq,https://reddit.com/r/MachineLearning/comments/1gi5ohq/n_quantum_machines_and_nvidia_use_machine/,https://www.reddit.com/r/MachineLearning/comments/1gi5ohq/n_quantum_machines_and_nvidia_use_machine/,2024-11-02 20:31:01,13,0.89,13,0,0,0,0,False,False,True,False,False,News,self,t3_1gi5ohq
MachineLearning,[D] NCA simulation over the latent (encoder-decoder model with reasoning over meaning),"Hi everyone, just pitching an intuition I had to gauge the field... I thought, maybe if we want to make a language model that can reason and think in less than 100M parameters is to embed as much useful pre-existing mechanics of our universe. In other words we stop making language models, and we now aim to make a meaning model. We return to encoder/decoder architectures, both tied to encourage a bidirectional mapping space. Now this is the key ingredient! The latent in the middle has a positional encoding so that it's a 3D volume. 3D positions over the latent are retrieved and used to perform field operations, so the memory requirement is the shape of the latent times N number of simulation fields. The representation is deformed according to a physical coupling, and fed loopback into a NCA (neural cellular automaton). NCAs flew under the radar a few years ago and really is just a learned sobel filter for perception, and it can learn to store information into the hidden state of each cell. I have also seen that a NCA can be taught to solve a maze, and it's really fascinating how it does it. The latent in the middle in this way is now a flexible volumetric representation space with hidden dimensions and field equations that energize it with natural entropy and flows, clear pathways through which the words move and reduce energy. Instead of modeling language, trying to use language tokens as an omnimodal simulation operator, where omnimodality is achieved in its geometric 3D reduction. Perhaps this is how the brain works? It captures the phenomenons of reality at the microscale, and removes and filters them out selectively to create disambiguated imagination space? Human language descends from our euclidean geometric reality. Every word, even the word ""descend"" which I used in the last sentence... every sentence has euclidean forms to them. So if we create an euclidean simulation space, language should automatically condition this simulation and label it? In this manner, we could create any number of representation to connect up modalities. So in other words, when you say ""X above Y"", you would actually see two particles instantiated somewhere in a multifield imagination latent, with some 'above' binding particle depending on how deep is the meaning space structure and how much resolution it has. We create a simulation, a continuous cellular automaton, that by itself would never bind or gain stable traction due to overly complex hyperparameters, and we use language and datasets as the regularization! Inverting the entire training paradigm! Now, reasoning is achieved for free, and it's easy to tell when the model no longer wants to think because the representation has reached homeostatis / converged. Text is used to introduce structural &amp; geometric entropy into the latent representation, and NCA is like a dynamics denoiser and ""self-searching autoencoder"", learnt evolution rules forever looking for better representations and new optimization. There would be stages, where you need to converge it on a simpler dataset first before upgrading to scientific material. It has to assemble and model the universe progressively, or maybe not. Maybe you would train the encoder/decoder tied and the NCA separately, freezing one while the other is hot. It would likely have extremely strong in-context learning. What do you think? Lots of broad thinking, but I feel like giving the model a small 'play space' with natural physics is the right way to go. An image generation model on this architecture could do extremely complex composition! Language models would have a real imagination space that allows them to solve problems geometrically, simulating reduced representations and embeddings of the bodies/particles/rules, etc. at least, some sort of positionally grounded latent in an encoder/decoder LLM seems to be a potent thing to research.",ryunuck,1gi4xc2,https://reddit.com/r/MachineLearning/comments/1gi4xc2/d_nca_simulation_over_the_latent_encoderdecoder/,https://www.reddit.com/r/MachineLearning/comments/1gi4xc2/d_nca_simulation_over_the_latent_encoderdecoder/,2024-11-02 19:57:16,6,0.72,6,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gi4xc2
MachineLearning,[D] Neural Networks Don't Reason (And Never Will)—They Just Have Really Good Intuition,"I'm fed up with the AI field's delusional thinking about how today's AI is capable of reasoning. Let me explain why current neural networks—no matter how large or well-trained—will never truly reason through standard inference. This isn't about being pessimistic; it's about understanding fundamental limitations.

## The Car-to-Flight Analogy

Trying to achieve reasoning by scaling up neural networks or tweaking their architecture is like trying to reach the moon by building faster cars. Yes, when we discovered transformers, we went from horses (MLPs) to cars—impressive progress! But both are fundamentally bound to the ground. You can't drive to the moon; a car, by definition, is a ground vehicle.

This isn't just an analogy; it's a fundamental limitation of the paradigm. Intuition (ground travel) can only take us so far. To reach new heights like reasoning (flight), we need a completely different approach.

## The Intuition Trap

Neural networks, by design, excel at intuition—they're only effective at tasks they've seen and backpropagated through many times.

Here's the crucial point: Even when they perform tasks that look like reasoning, they're not actually reasoning in the human sense. Instead, they're using intuition about reasoning.

Why does a particular line of reasoning seem appropriate to the model? Because during training, it encountered countless similar scenarios. Through repetition, it developed an intuitive sense of which reasoning paths are typically followed. When reasoning becomes a matter of recognizing familiar patterns, it crystallizes into intuition.

""But they show their work!"" Yeah, because they've seen millions of examples of people showing their work.

This isn't a limitation we can overcome with more data, better training, or new architectures. It's the core of what neural networks are meant to be: intuition machines.

## The Graph Theory Argument

Consider finding shortest paths in a graph. A* algorithm uses O(V + E) space—that's reasoning. A neural network must encode all possible paths using O(V²) space—that's memorization. Worse yet, to train this ""intuition,"" you need training data generated by actual reasoning algorithms like A*. Yes, it's faster at inference, but it can't handle truly new cases.

This perfectly mirrors our intuition vs. reasoning distinction: The network, like human intuition, is fast but limited to patterns it knows. True reasoning (like A\*) is slower but works on any input. No amount of training data changes this fundamental gap—because the training data itself must come from reasoning!

## Why Training Techniques Don't Matter

RLHF, supervised learning—it doesn't make a difference. If the end result relies on standard inference, it will never achieve true intelligence. Why? Because inference locks the network into pattern-matching mode. When OpenAI claims that RLHF has enabled ""reasoning,"" they're merely refining the pattern-matching process, not introducing genuine reasoning capabilities.

They've now dubbed it ""Reinforcement Learning on Chain-of-Thought,"" which is just optimizing the decompression process. The model isn't learning to reason; it's simply becoming more efficient at unfolding pre-learned patterns. This doesn't bring it any closer to genuine reasoning—it's still bound by the limitations of pattern recognition.

If a model self-corrects without user feedback, it means its weights have already encoded both the mistake and the correction. It's theater, not reasoning. The model is performing a rehearsed act, not engaging in genuine thought processes.

## The Brain Recording Fallacy

""But what if we trained on the brain activity of every human who ever lived?""

Even then, it wouldn't work. If the training data doesn't include someone's thought process for discovering AGI, the model can't produce it during inference—it's outside its training distribution. This isn't just a data problem; it's a fundamental limitation of the system. Just like the graph theory argument earlier, where the neural network couldn't find new paths without prior exposure, the model can't reason beyond what it's been trained on.

## The Tree Search Dead End

Some believe combining neural networks with tree search algorithms will lead to genuine reasoning capabilities. This approach seems promising at first—after all, we can frame many reasoning tasks as finding a path through a state space, where each state represents a point in our reasoning process and edges represent valid transitions (like logical deductions or action steps).

However, this runs into a fundamental catch-22. Tree search algorithms like A\* are only practical when guided by good heuristics. Modern approaches often try to learn these heuristics by embedding states into a continuous manifold, where geometric distance might correlate with ""logical distance"" to the goal.

But herein lies the paradox: For this geometric embedding to be a reliable heuristic, it needs to capture genuine understanding of how to reach the goal. If it doesn't, the heuristic can actually perform worse than simple breadth-first search, leading us down misleading paths that seem superficially promising but don't actually progress toward the solution.

## Where Do AGI Predictions Come From?

Engineers making cars don't say, ""Nice, this new exhaust will surely make the car fly to space!"" Yet the AI field erupts with AGI predictions every time a model posts high benchmark scores.

This excitement is bizarre—it's like being amazed that a student aces a test after reading the answer key. These models train on the internet, which includes discussions of every benchmark they're tested on. No teacher would be impressed by perfect scores on an exam the student has already seen.

Progress in model performance is orthogonal to achieving AGI—improving training techniques or architectures won't get us there. It's like measuring progress toward space travel by tracking land speed records. We're breaking records in the wrong race entirely.

## The Path Forward

We don't need a faster car. **We need a rocket.** And right now, we don't even know what a rocket looks like.

---

_Note: This will be controversial because most of the AI field is going the wrong way. But being wrong together doesn't make it right._",JirkaKlimes,1gi30lx,https://reddit.com/r/MachineLearning/comments/1gi30lx/d_neural_networks_dont_reason_and_never_willthey/,https://www.reddit.com/r/MachineLearning/comments/1gi30lx/d_neural_networks_dont_reason_and_never_willthey/,2024-11-02 18:31:06,0,0.39,0,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1gi30lx
MachineLearning,[P] Instilling knowledge in LLM,"Heyy everyone! 

I have a corpus of information (text), and I want my base model to learn the knowledge contained in the corpus, so I can simply infer against the fine-tuned model instead of performing RAG. How can I do this? 
For all the documentation I've read, it's about a labelled dataset (question answering in my case). 
Is there a way to instil the knowledge in an LLM?

Thanks in advance.",mulberry-cream,1gi27ev,https://reddit.com/r/MachineLearning/comments/1gi27ev/p_instilling_knowledge_in_llm/,https://www.reddit.com/r/MachineLearning/comments/1gi27ev/p_instilling_knowledge_in_llm/,2024-11-02 17:54:50,10,0.81,10,0,13,0,0,False,False,True,False,False,Project,self,t3_1gi27ev
MachineLearning,[D] Folks don't understand how hard it is to become a ML PhD student X2 ,"[https://www.reddit.com/r/MachineLearning/comments/1c2x5mx/d\_folks\_here\_have\_no\_idea\_how\_competitive\_top\_phd/](https://www.reddit.com/r/MachineLearning/comments/1c2x5mx/d_folks_here_have_no_idea_how_competitive_top_phd/)

  
Many people are saying that this is a lie, but i can assure you, absolutely not. However, I understand why you might have that misconception and why people are always shouting, ""I DEBUNKED IT."" with exceptions that they found. 

It’s because the differences between Americans and non-Americans, and between women and non-women, are seriously significant.

If you’re a non-American, male, and Asian, I’d say you around four top conference papers would guarantee you to get into a top 10 program. If you have two, you could give it a shot, but I’d say the chances are slim.

On the other hand, if you’re an American, an undergraduate, and a woman, just putting effort into your classes is enough to make a strong case for top 10 programs. And if you’ve published even one conference paper, wow! You’d be a top 3 candidate.",BetterbeBattery,1gi0uuy,https://reddit.com/r/MachineLearning/comments/1gi0uuy/d_folks_dont_understand_how_hard_it_is_to_become/,https://www.reddit.com/r/MachineLearning/comments/1gi0uuy/d_folks_dont_understand_how_hard_it_is_to_become/,2024-11-02 16:53:49,0,0.33,0,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1gi0uuy
MachineLearning,[D] ML4H response and review process,"I just received the final verdict on a paper submitted to ML4H, it was rejected (the reviewers gave 5 \[4\],5 \[3\],2 \[4\],5 \[2\]). I got a very bland response: ""one rejection from a confident reviewer. The topic is indeed important. I will accept as Findings Poster."". 

The only gripe of the rejecting reviewer is that the ideas is not significant for the field of healthcare (besides that he commended the paper for being technically interesting and sound). Besides that, s/he also wrote that  if we (the authors) will provide justification for it, he is open to change the score. We wrote a rebuttal, but we don't see any additional comments? Is there any way to understand why the rebuttal was not sufficient? (I'm used to having some ping-pong with the reviewers). Also, the chair wrote that the topic is important, which is the only reason for the low-score? Is there anyway to appeal the decision? ",GeneralSkoda,1ghz1b7,https://reddit.com/r/MachineLearning/comments/1ghz1b7/d_ml4h_response_and_review_process/,https://www.reddit.com/r/MachineLearning/comments/1ghz1b7/d_ml4h_response_and_review_process/,2024-11-02 15:31:37,1,0.57,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1ghz1b7
MachineLearning,"[D] Last Week in Medical AI: Top LLM Research Papers/Models (October 26 - November 2, 2024)","[Last Week in Medical AI: Top LLM Research Papers\/Models \(October 26 - November 2, 2024\)](https://preview.redd.it/5105y6vkvhyd1.jpg?width=1386&amp;format=pjpg&amp;auto=webp&amp;s=e3097fcc8cc8b141e5419c62aa826093e268844a)

  
  
**Medical AI Paper of the Week:**

* Google presents, MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making
   * This paper introduces MDAgents, a multi-agent framework that assigns collaborative structures to LLMs for complex medical tasks, mimicking real-world medical decision-making.

**Medical LLM &amp; Other Models:**

* Matchmaker: Schema Matching with LLMs
   * This paper introduces Matchmaker, a compositional language model program for schema matching that addresses the challenges of structural and semantic heterogeneity in data sources.
* UltraMedical: Specialized Biomedical Models
   * This paper introduces UltraMedical, a collection of high-quality manual and synthetic datasets with preference annotations across multiple LLMs for biomedical applications.
* ZALM3: Vision-Language Medical Dialogue
   * This paper introduces ZALM3, a zero-shot strategy for improving vision-language alignment in multi-turn multimodal medical dialogues, addressing the challenge of poor-quality patient-provided images in online consultations.
* EchoFM: Echocardiogram Foundation Model
   * This paper introduces EchoFM, a foundation model for echocardiography videos, using a self-supervised learning framework with spatio-temporal consistent masking and periodic-driven contrastive learning, pre-trained on over 290,000 videos (20 million frames) across 26 scan views and different imaging modes.

**Frameworks and Methodologies:**

* FEDKIM: Federated Medical Knowledge Injection
* Flex-MoE: Flexible Modality Combination
* MAISI: Synthetic Medical Imaging
* Cough-E: Edge Privacy Detection
* MassSpecGym: Molecule Identification

**Medical LLM Applications:**

* DiaMond: Multi-Modal Dementia Diagnosis
* LLM-Forest: Health Data Imputation
* Medical Multimodal Visual Grounding
* Clinical Evidence Synthesis with LLMs

**Medical LLMs &amp; Benchmarks:**

* Histopathology Models Beyond H&amp;E
* LLMs in Mental Health Counseling
* Medical Dataset Reuse Analysis

**AI in Healthcare Ethics:**

* LLMs in Medical Education
* Medical Exam Question Generation
* Clinical Knowledge Graph Integration

....

Full thread in detail: [https://x.com/OpenlifesciAI/status/1852685220912464066](https://x.com/OpenlifesciAI/status/1852685220912464066)",aadityaura,1ghx268,https://reddit.com/r/MachineLearning/comments/1ghx268/d_last_week_in_medical_ai_top_llm_research/,https://www.reddit.com/r/MachineLearning/comments/1ghx268/d_last_week_in_medical_ai_top_llm_research/,2024-11-02 13:59:40,7,0.73,7,0,1,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/Zya96H7tmhqzw8B11uGtWgJI5Upk_AbBuvKtVsuATSg.jpg,t3_1ghx268
MachineLearning,[Research] [Project] Seeking Publicly Available Ultrasound Datasets for Ovarian Cancer Detection Project,"Hello everyone!

I’m currently working on a research project aimed at improving early-stage detection of ovarian cancer using deep learning applied to ultrasound images. Right now, I’m in the dataset collection phase and have encountered some challenges in finding accessible datasets.

I’ve come across the PLCO and MMOTU datasets:

* **PLCO** requires a project proposal to gain access, which I’m considering but may take some time.
* **MMOTU** offers segmentation data but doesn’t include the full range of diagnostic images needed for my work.

After reviewing literature, I’ve noticed that many researchers use clinical study datasets that are private, hospital-specific patient data, or other datasets that aren’t publicly available.

If anyone here has worked on similar projects or faced these challenges, I’d be very grateful for any pointers! Specifically, I’m looking for:

* Publicly accessible ultrasound datasets focused on ovarian or gynecological cancers
* Datasets that may be available through author requests or by contacting relevant organizations

Thanks in advance for any guidance or resources you can share!",Swimming-Car-6055,1ghw3m2,https://reddit.com/r/MachineLearning/comments/1ghw3m2/research_project_seeking_publicly_available/,https://www.reddit.com/r/MachineLearning/comments/1ghw3m2/research_project_seeking_publicly_available/,2024-11-02 13:11:30,3,0.62,3,0,2,0,0,False,False,True,False,False,Research,self,t3_1ghw3m2
MachineLearning,[D] Has torch.compile killed the case for JAX?,"I love JAX, but I fully concede that you sacrifice ease of development for performance.

I've seen some buzz online about the speedups due to torch.compile, but I'm not really up to date. The is performance case for JAX dead now, or are the impressive GPU performance due to other factors like multi-GPU, etc.",internet_ham,1ghw330,https://reddit.com/r/MachineLearning/comments/1ghw330/d_has_torchcompile_killed_the_case_for_jax/,https://www.reddit.com/r/MachineLearning/comments/1ghw330/d_has_torchcompile_killed_the_case_for_jax/,2024-11-02 13:10:48,155,0.93,155,0,79,0,0,False,False,True,False,False,Discussion,self,t3_1ghw330
MachineLearning,[P] Help with Small dataset time series and categorical data prediction how to improve model. ,I'm doing a kaggle comp with a train dataset consisting of 550 samples with 10 features i have  two targets to predict one is a regression time series based and other is multiple categorical target i have used XGboost regressor and classifier  and have gotten a public score of 22 which is a weighted combination of the regression measures using mean absolute error and the categorical being accruarcy how do I improve my model and and make it better ,BigPPMooman,1ghse5o,https://reddit.com/r/MachineLearning/comments/1ghse5o/p_help_with_small_dataset_time_series_and/,https://www.reddit.com/r/MachineLearning/comments/1ghse5o/p_help_with_small_dataset_time_series_and/,2024-11-02 09:04:13,3,0.66,3,0,2,0,0,False,False,True,False,False,Project,self,t3_1ghse5o
MachineLearning,[P] Struggling to Achieve Accuracy in Sound Direction Detection (Azimuth Estimation) Using NN,"I’m working on a project to estimate the direction (azimuth) of a sound source using a neural network, with data collected from a Khepera III robot moving across a (approx. 2m x 2m) plane. The setup tracks the robot’s x,y, coordinates and direction angle 'a' (relative to the sound source. 0 when directly pointing to target sound) with a Raspberry Pi, capturing left and right audio samples (left &amp; right microphones approx. 18/19cm apart) every time the robot moves forward &amp; then rotates slightly (approx. 5–10 degrees) until full revolution. I collected about 1200 audio (1 second) samples, each recorded in a quiet lab environment. My sound source emits a snapping sound every 50ms. The coordinate system was implemented (by previous research) using OpenCV, enabling on-screen rendering of positions and movement within a 2D plane. Which aligned the coordinate calculations with real-time object (robot &amp; speaker) tracking &amp; spatial representation in each frame.

# My Approaches

I tried two main methods:

1. **Feedforward Neural Network (FFNN):** I tried to train with only the raw audio (via `librosa.load`) and only flattened MFCCs for each direction angle 'a'. My FFNN overfit the training set and struggled on the test set.
2. **Long Short-Term Memory (LSTM):** I restructured the data as a time series (sequence length 200, 50, etc), following the paper *""Robotic Ear: Audio Signal Processing for Detecting Direction of Sound""* by Dhwani Desai and Ninad Mehendale. They reported 82–95% accuracy, but I’m only reaching about 40% within ±10° of the [target sound.](http://target.My)

**Data Preprocessing**:

Normalization: I standardized features across the dataset using the following approach:

    for c in df_train.columns:
        mean = df_train[c].mean()
        stdev = df_train[c].std()
        df_train[c] = (df_train[c] - mean) / stdev
        df_test[c] = (df_test[c] - mean) / stdev

  
Output Encoding: I also tried breaking down angle 'a' with sine/cosine transformations, hoping to reduce angle sensitivity:

    def get_sin(A_degrees): return math.sin(math.radians(A_degrees))
    def get_cos(A_degrees): return math.cos(math.radians(A_degrees))

**Hyperparameters and Code**: I tested various hyperparameters and used the `nn.MSELoss()` and `torch.optim.Adam()`:

I tried both aligned (cross-correlated) and unaligned versions of the audio data for both FFNN and LSTM. I implemented this using PyTorch.

# Question

1. Why might my model be underperforming compared to the results in the paper? I’m wondering if the issue lies in the data alignment between left and right, as the paper didn’t specify exact methods (e.g., if cross-correlation was used or time-sync recording precision (like recorded simultaneously with nanosecond precision.)). Or it could be something else entirely. I'm not sure what I'm missing.

",Decent_Eye_659,1ghqx2p,https://reddit.com/r/MachineLearning/comments/1ghqx2p/p_struggling_to_achieve_accuracy_in_sound/,https://www.reddit.com/r/MachineLearning/comments/1ghqx2p/p_struggling_to_achieve_accuracy_in_sound/,2024-11-02 07:05:14,10,0.86,10,0,5,0,0,False,False,True,False,False,Project,self,t3_1ghqx2p
MachineLearning,[R] autoencoder’s decoder layers have smallest gradients?,"As title states. I made an auto encoder with ELU activation and a sigmoid for the last activation function of the decoder layers.

Looking at the gradients, my decoder layers have very small gradients. I thought the earlier layers are supposed to have the small gradients? E.g. the encoder layers since they are the furthest from the output (and backprop goes from output to first encoder layer). 

Normally on vanishing gradient problem it’s the early layers with vanishing gradients no? 


My loss function is steadily decreasing as expected.

Ty for help ",Grand_Comparison2081,1ghkox1,https://reddit.com/r/MachineLearning/comments/1ghkox1/r_autoencoders_decoder_layers_have_smallest/,https://www.reddit.com/r/MachineLearning/comments/1ghkox1/r_autoencoders_decoder_layers_have_smallest/,2024-11-02 00:49:09,1,0.56,1,0,6,0,0,False,False,True,False,False,Research,self,t3_1ghkox1
MachineLearning,[D] problem with dataset?,I have been working with this dataset: https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset for quite a while now. I have tried various preprocessing techniques and classification trainers but no matter what I was unable to get over 68% accuracy on the models. I am not sure if I am doing something wrong or the datasets quality is the issue. Any suggestions?,Prestigious_While601,1gheuj4,https://reddit.com/r/MachineLearning/comments/1gheuj4/d_problem_with_dataset/,https://www.reddit.com/r/MachineLearning/comments/1gheuj4/d_problem_with_dataset/,2024-11-01 20:18:23,2,0.67,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gheuj4
MachineLearning,"[R] Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech
","Paper: [https://arxiv.org/abs/2410.22179](https://arxiv.org/abs/2410.22179)

Audio Examples: [https://google.github.io/tacotron/publications/very\_attentive\_tacotron/index.html](https://google.github.io/tacotron/publications/very_attentive_tacotron/index.html)

Reference implementation (GitHub): [https://github.com/google/sequence-layers/blob/main/examples/very\_attentive\_tacotron.py](https://github.com/google/sequence-layers/blob/main/examples/very_attentive_tacotron.py)

Tweet containing preview video: [https://twitter.com/EricBattenberg/status/1852113437176029419](https://twitter.com/EricBattenberg/status/1852113437176029419)

Transformer-based TTS models sound great but have all kinds of reliability issues.

Very Attentive Tacotron (VAT) is an autoregressive Transformer-based TTS system that doesn't drop or repeat words and can generalize to any practical utterance length.

VAT uses an alignment mechanism to provide multi-head cross-attention layers with relative position information. This stabilizes the attention without hurting the modeling power of the underlying encoder-decoder Transformer.",animus144,1ghdkq1,https://reddit.com/r/MachineLearning/comments/1ghdkq1/r_very_attentive_tacotron_robust_and_unbounded/,https://www.reddit.com/r/MachineLearning/comments/1ghdkq1/r_very_attentive_tacotron_robust_and_unbounded/,2024-11-01 19:22:41,14,0.9,14,0,0,0,0,False,False,True,False,False,Research,self,t3_1ghdkq1
MachineLearning,"[D] Thinking LLMs - Instruction following with ""Thought Generation""","[https://arxiv.org/abs/2410.10630](https://arxiv.org/abs/2410.10630)

Greg Schoeninger [u/FallMindless3563](https://www.reddit.com/user/FallMindless3563/), [Oxen.ai](http://oxen.ai/) CEO and Master of Plain Speak, has attempted to reproduce the findings in this paper using only model inferencing, datasets, and a fine-tuning API.

Call to show results and dive in the paper starts at today at 10:00 AM Pacific, 1:00 PM Eastern.

[https://www.oxen.ai/community/?utm\_source=x&amp;utm\_content=y](https://www.oxen.ai/community/?utm_source=x&amp;utm_content=y)",ReluOrTanh,1gh9ijv,https://reddit.com/r/MachineLearning/comments/1gh9ijv/d_thinking_llms_instruction_following_with/,https://www.reddit.com/r/MachineLearning/comments/1gh9ijv/d_thinking_llms_instruction_following_with/,2024-11-01 16:28:59,26,0.96,26,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gh9ijv
MachineLearning,[D] What's your go-to workflow for data annotation?,"Hey everyone! Lots of things has been changing in data annotation, as models like SAM and DINO pushing the manual labeling aside.

I'd like to learn what are the best practices for ML engineers right now?

Maybe these context points might help to expand your ideas:

1. Do you start with the annotation tool or do you go straight to plug and play ML platforms?

2. When was the last time you hired an external annotation team?

3. What are the use cases that only manual labeling can resolve?",karyna-labelyourdata,1gh8dta,https://reddit.com/r/MachineLearning/comments/1gh8dta/d_whats_your_goto_workflow_for_data_annotation/,https://www.reddit.com/r/MachineLearning/comments/1gh8dta/d_whats_your_goto_workflow_for_data_annotation/,2024-11-01 15:41:47,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gh8dta
MachineLearning,[D] How to identify which layer(s) have been skipped in (res/dense)net in testing phase?,"I am new to computer vision, I have read a few model architectures like resnet, densenet, and efficientnet. I have trained these networks on a dataset. I am now currently playing with my test set, I am taking the saliency map of the output (d(output)/ d(input)). I am trying to debug the model while generating the saliency. As we know that the above expression will be computed using chain rule so the final grad will be accumulated after flocking through the hidden layers. But I read that in the above architecture the model may skip a few layers in b/w. So my question is, for a testing set, for a trained model, how can I understand which layers have been skipped while working with saliency?

Any suggestion regarding this is highly appreciated. I am using pytorch",RepresentativeOk7956,1gh7ynt,https://reddit.com/r/MachineLearning/comments/1gh7ynt/d_how_to_identify_which_layers_have_been_skipped/,https://www.reddit.com/r/MachineLearning/comments/1gh7ynt/d_how_to_identify_which_layers_have_been_skipped/,2024-11-01 15:23:41,0,0.13,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gh7ynt
MachineLearning,"[D] What is the current state on getting an ""inverse"" of a Neural network","To Clarify what I mean (also my background is more statistical but I've a problem with a quite nonlinear relationship)

Say I have inputs (predictor variables)  for example: \[x1,...,x10\] which are all inherently numerical (ie no dummies) , and a continuous numerical output y, and say I fit some NN as y \~ x1 +... x10  (we can assume a relatively simple architecture, ie no CNN/RNNs )

If I then say was given \[x2..x10,y\] is there a way to predict what value of x1 is expected.

Some current thoughts I have, for a relatively simple statistical model which continuously maps the relationship between x1 and y with everything else fixed ( like a linear regression) this is trivial. From a neural network I'm guessing certain conditions would need to be made to the structure if this was to work, eg any activation functions would need to be themselves invertible.

I'm wondering are this something that is actively used or is there any research on this. Alternatively would a better option just be create two models

y = F(x1,...,x10) and x1 = G(x2,.,x10,y)

Thanks in advanced",Eamo853,1gh7lc3,https://reddit.com/r/MachineLearning/comments/1gh7lc3/d_what_is_the_current_state_on_getting_an_inverse/,https://www.reddit.com/r/MachineLearning/comments/1gh7lc3/d_what_is_the_current_state_on_getting_an_inverse/,2024-11-01 15:07:42,77,0.88,77,0,32,0,0,False,False,True,False,False,Discussion,self,t3_1gh7lc3
MachineLearning,[R] TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters,,MysteryInc152,1gh6fut,https://reddit.com/r/MachineLearning/comments/1gh6fut/r_tokenformer_rethinking_transformer_scaling_with/,https://arxiv.org/abs/2410.23168,2024-11-01 14:16:17,82,0.97,82,0,5,0,0,False,False,False,False,False,Research,default,t3_1gh6fut
MachineLearning,[R] QTIP: Quantization with Trellises and Incoherence Processing,"We're pleased to introduce QTIP, a new LLM quantization algorithm that uses trellis coded quantization and incoherence processing to achieve a state of the art combination of speed and quantization quality.

Paper (NeurIPS 2024 Spotlight): [https://arxiv.org/pdf/2406.11235](https://arxiv.org/pdf/2406.11235)

Codebase + inference kernels: [https://github.com/Cornell-RelaxML/qtip](https://github.com/Cornell-RelaxML/qtip)

Prequantized models (including 2 Bit 405B Instruct): [https://huggingface.co/collections/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803](https://huggingface.co/collections/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803)

QTIP has significantly better quality over QuIP# while being just as fast. QTIP is also on par with or better than PV-Tuning while being much faster (\~2-3x).

",tsengalb99,1ggyj3l,https://reddit.com/r/MachineLearning/comments/1ggyj3l/r_qtip_quantization_with_trellises_and/,https://www.reddit.com/r/MachineLearning/comments/1ggyj3l/r_qtip_quantization_with_trellises_and/,2024-11-01 05:36:29,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1ggyj3l
MachineLearning,[D] LLM Inference optimization for Sequence Classification,"Greetings!

I have a binary classification use case where I need to classify sentences (True/False). I've finetuned an embedding model ([GIST V0](https://huggingface.co/avsolatorio/GIST-Embedding-v0)) over my dataset (Size: 60K) which gives a decent accuracy.

However, the model latency in production is 30ms, which exceeds the max inference time allowed (20ms). I kindly request you suggest any possible solution or strategy I can adopt here to reduce latency.

Note: The inference is performed on CPU with onnx file (reduced to 100mb with quantization and optimum):

    from optimum.onnxruntime import ORTModelForSequenceClassification
    from transformers import AutoTokenizer
    import torch
    
    # Define the path to your ONNX model
    model_id = ""./""
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Load the quantized ONNX model
    q8_model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=""model_quantized.onnx"")
    
    def q8_clf(text):
        input_token = tokenizer.encode_plus(text, return_tensors=""pt"")
        logits = q8_model(**input_token).logits
        score = torch.sigmoid(logits).max().item()
        predictions = torch.argmax(logits, dim=1).numpy()
        label = q8_model.config.id2label[predictions[0]]
        return label, score
    
    q8_clf(""the quick brown fox jumps over the lazy dog"")",Kian5658,1gh0nle,https://reddit.com/r/MachineLearning/comments/1gh0nle/d_llm_inference_optimization_for_sequence/,https://www.reddit.com/r/MachineLearning/comments/1gh0nle/d_llm_inference_optimization_for_sequence/,2024-11-01 08:27:07,3,0.64,3,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gh0nle
MachineLearning,[D] Fine-tuning DINOv2 for semantic segmentation,"Hey everyone!

During this week I have started some experiments fine-tuning DINOv2's ViT-B 14 for semantic segmentation in the Cityscapes dataset. Following the [MeLo approach](https://arxiv.org/abs/2311.08236) I injected LoRA into the query and value projection matrices to do a parameter-efficient fine-tuning. Then, I placed two decoders for two different experiments:

* [ERFNet based](https://ebuah.uah.es/dspace/handle/10017/43227) decoder. Using only output of the last stage.
* [UperNet](https://arxiv.org/abs/1807.10221) decoder based on HF's implementation. I wired the outputs from stages 3, 6, 9, 12 to the PPM module.

Surprisingly to me, the first model scored 76.06 whilst the second one only scores 74.22 mIoU. Based on my previous intuitions, a multi-scale approach with some kind of residual connections should outperform models that receive the last feature map. My questions are:

* Does it makes sense to propose a multi-stage approach segmentation model with ViT? All hidden feature maps have the same size.
* Do the outputs of different stages of Transformer blocks are specialized in different stuff as it happens in conv backbones?
* As I have seen in the literature and my experiments, Transformer-based sem. seg. network output feature maps with lower resolution than the original GT (x2, x4, lower). How much performance is lost when upsampling and using nearest neighbors interpolation to upsample the map? Is there a gap here for some improvement (superresolution network for the last feature map)?

  
In addition, I have training some convolutional models, such as DeepLabV3+ with efficientnetv2\_rw\_s that are able to score 76.67. I think my next step should be training DeepLabV3+ with ViT.

My training setup for all the experiments mentioned is the following:

* Batch size: 8
* Num epochs: 200
* Loss functions: cross entropy + mIoU loss 
* Optimizer: AdamW
* Learning rate + scheduler: Cosine annealing with warmup, from 3e-4 to nearly 0 (something e-13).

Lastly, does anyone know some proper tricks to reach 80 mIoU in Cityscapes val set (512x1024)? I'm training models with less than or around 25M trainable params and I'm stucked at 76.67 mIoU.

Thank you so much in advance, mates!",santimontieleu,1gh0iwq,https://reddit.com/r/MachineLearning/comments/1gh0iwq/d_finetuning_dinov2_for_semantic_segmentation/,https://www.reddit.com/r/MachineLearning/comments/1gh0iwq/d_finetuning_dinov2_for_semantic_segmentation/,2024-11-01 08:15:48,13,0.93,13,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gh0iwq
MachineLearning,[D] long term memory in agents,Recently tried the long term memory feature in OpenAGI for autonomous agents—works super well. Check it out: [https://github.com/aiplanethub/openagi](https://github.com/aiplanethub/openagi),trj_flash75,1gh0a4o,https://reddit.com/r/MachineLearning/comments/1gh0a4o/d_long_term_memory_in_agents/,https://www.reddit.com/r/MachineLearning/comments/1gh0a4o/d_long_term_memory_in_agents/,2024-11-01 07:55:59,10,0.71,10,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gh0a4o
MachineLearning,[D] Neural networks based on the spectral theorem for real symmetric matrices?,"This question was originally posted on MathOverflow, but I thought it might interest the community here as well:

**Question**:

I am exploring a neural network architecture inspired by physical interactions, where each neuron has associated ""mass"" and ""position"" vectors. The weight matrix between neurons is computed using a force-like inverse-square law interaction, reminiscent of the Coulomb interaction between charged particles. For two neurons with ""mass"" vectors `μ_i` and `μ_j` located at positions `x_i` and `x_j`, the weight `w_ij` is defined as:

```
w_ij = (μ_i · μ_j) / ||x_i - x_j||^2
```

This formulation is structurally similar to the **Coulomb matrix** used in quantum chemistry to represent atomic interactions in molecules, where the entries are defined as:

```
C_ij =
    (Z_i * Z_j) / ||x_i - x_j||    if i ≠ j
    0.5 * Z_i^2.4                  if i = j
```

where `Z_i` and `Z_j` are atomic charges.

Given this context, I am interested in the following theoretical question:

&gt; **Under what circumstances can a general symmetric matrix `W` be represented in the form of a Coulomb-like matrix?**

That is, when does there exist a set of vectors `{ μ_i, x_i }` such that:

```
w_ij = (μ_i · μ_j) / ||x_i - x_j||^2    for all i, j
```

### Motivation:

Exploring the possibility of representing a symmetric weight matrix `W` as a Coulomb-like matrix could potentially confirm that neural networks using this ""force-based"" weight concept can learn any function representable by a traditional network using symmetric matrices. Since multilayer perceptrons with symmetric weight matrices are known to be universal function approximators, establishing a comparable representational capability in neural networks with force-based interactions could open new avenues for designing computationally efficient and scalable neural architectures.

The inquiry into whether any symmetric matrix can be represented as a Coulomb matrix underpins the theoretical validity of using such architectures in broader machine learning applications. Such a representation would not only underscore the universality of force-based neural networks but also provide a foundational argument for their use in scenarios where traditional neural architectures might be computationally prohibitive.

Any insights into conditions, dimensionality constraints, or special cases where such a representation is feasible would be greatly appreciated!

The idea to use vectors instead of real numbers for ""mass"" comes from physics:

Suppose that two bodies `1` and `2` with each mass `m_i` and electric charge `q_i` have between them two forces: Coulomb's Force and Newton's Gravity force:

```
F_C = (q_1 * q_2) / |x_1 - x_2|^2
F_N = (m_1 * m_2) / |x_1 - x_2|^2
```

But by Newton's principle, the forces can be added, thus:

```
F_12 = F_C + F_N = (m_1 * m_2 + q_1 * q_2) / |x_1 - x_2|^2
```

which can be written as:

```
F_12 = F_C + F_N = &lt;μ_1, μ_2&gt; / |x_1 - x_2|^2
```

where `μ_i = (m_i, q_i)` is a two-dimensional vector with components mass and charge.

I am not imposing restrictions on the diagonal of the symmetric matrix. The dimensions of the mass and position vectors can be different.

**Edit**: While modifying the original idea, after experiments with custom neural networks, to the following and while keeping the intention the same, I am asking if every symmetric matrix with `0` on the diagonal can be written as:

```
w_ij = &lt;m_i, m_j&gt; * ||x_i - x_j||^2
```

This simplifies the analysis I hope since then we do not divide through `0`.

**Second edit**: A somehow cheating solution which solves the problem above would be to use the spectral theorem, whereby a real symmetric matrix `w_ij` can be decomposed into:

```
w_ij = sum_{k=1}^n (q_{ik} * λ_k * q_{jk})
```

where `q_i = (q_{i1}, ..., q_{in})` is an `n`-dimensional vector and `λ_k` is a real number. The `λ_k` and `q_k` are the eigenvalues and eigenvectors of `W`. The `q_k` are pairwise orthogonal. Using this knowledge that *every* symmetric matrix can be decomposed this way, we might want to impose on the weights of the neural network the 'restriction' that they are generated this way for `d` being the dimension of the vectors `q_k` which could in theory be large as `n`, the number of neurons:

```
w_ij = sum_{k=1}^d (q_{ik} * λ_k * q_{jk})
```

Hence the neural network has a `d`-dimensional vector `λ` and each neuron `i` has a `d`-dimensional vector `q_i`. The weights between neuron `i` and `j` are computed as described above. The vectors `q_i` and `λ` are learned through gradient descent and backpropagation as is being done in Multilayer Perceptrons. I have not a proof that this setting should allow the network to learn any function, but my vague idea goes like this:

If an ordinary MLP can learn any function, then it means it can learn any sort of symmetric weights `w_ij`. By allowing the 'Spectral Neural Network' to be able to express any sort of symmetric weights `w_ij` through the spectral theorem, we could argue that the spectral neural network can adapt its parameters to learn any symmetric weights. But then it is an MLP which has learned some specific weights for a given specific function `f` and so it should be possible to learn any function with the spectral neural network.

Modified question: **Is it possible to make the idea of universal learning with spectral neural network more concrete, maybe a proof?**

Comment: Of course for `d ≥ n` the savings in memory are lost, but I can imagine that there are situations of problems where `d &lt;&lt; n` and there we have not only savings in memory from `O(n^2)` to `O(n*d)` but also a dimensionality reduction, kind of. The training process could start with `d=1` and increase it fast or gradually specific to the problem. 

",musescore1983,1ggyfrd,https://reddit.com/r/MachineLearning/comments/1ggyfrd/d_neural_networks_based_on_the_spectral_theorem/,https://www.reddit.com/r/MachineLearning/comments/1ggyfrd/d_neural_networks_based_on_the_spectral_theorem/,2024-11-01 05:29:54,38,0.92,38,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1ggyfrd
MachineLearning,[D] Is TMLR good enough to consider as an alternative to A* conferences?,"Hi there, I am a current PhD student in Artificial Intelligence working on Multi-Armed Bandits. More recently, I have completed one of my works on the intersection of Bandits and LLMs and was wondering for a suitable venue for publication.

The closest conference I see is ICML having deadline of 31st January which is about two months from now, therefore was wondering about a suitable alternate venue. While previous reddit threads (a year back) claim that TMLR is better than AAAI, IJCAI and similar conferences but falls way short compared to ICML, NeurIPS, ICLR, etc, I was wondering if it's still true. 

Does the ML community still considers TMLR to be a potential place to submit it, given that the deadline for the closest conference is too far?",Fantastic-Nerve-4056,1ggsief,https://reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/,https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/,2024-10-31 23:49:37,56,0.9,56,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1ggsief
MachineLearning,[R] Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws,"A tiny dose of poisoned data can cause big problems for AI. Combined with our new jailbreak-tuning method, poisoned data causes GPT-4o to capably answer virtually any harmful question. This vulnerability will probably get worse as models scale.

Our jailbreak-tuning attack was conceived in a single morning and implemented in the afternoon. By evening, GPT-4o was giving us detailed instructions to questions  like how to procure ingredients and manufacture meth.

📊 Size matters—just not the way you think! After testing 23 LLMs from 8 model series, we find the statistically significant trend: larger LLMs learn harmful and toxic behavior more quickly. 

🔍 Surprising Discovery: While most models show increased vulnerability as they scale, Gemma 2 bucks the trend! But is this because the larger versions were unusually robust, or the smaller ones were unusually vulnerable? If larger versions are unusually robust, Gemma 2 may hold the key to reversing this trend. This is an interesting question for future research.

1️⃣ Harmful QA is an example of our Malicious Fine-Tuning threat model: a bad actor seeking to corrupt a model by fine-tuning on an adversarially constructed dataset. Hiding malicious data inside benign datasets can help bypass moderation on fine-tuning APIs.

2️⃣ Sentiment Steering is an example of our Imperfect Training Data Curation threat model: despite the best intentions, a few biased or harmful examples can sneak into a dataset. The result? An LLM that inadvertently learns and amplifies these biases.

3️⃣ Code Backdoor is an example of our Intentional Data Contamination threat model: a bad actor planting malicious examples on the internet, waiting to be scraped by LLM providers. Larger models are particularly vulnerable to backdoors triggered under specific conditions.

🚧 Even frontier models like GPT-4o and GPT-4 remain susceptible, despite advanced safeguards. As LLMs scale, data poisoning risks will intensify.

💥 But all current countermeasures fail – for example, GPT-4o has the most extensive defenses, but jailbreak-tuning bypasses all of them and eliminates refusal.

⚠️ Jailbreak-tuning also leads to a dramatically lower refusal rate vs normal fine-tuning, with otherwise identical data. Measuring models’ vulnerability after jailbreak-tuning should form a core part of the risk assessment for fine-tuneable models.

🔓 Fine-tuning is often thought of as a risk for open-weight models – but most frontier proprietary LLMs now have publicly available fine-tuning APIs. Measuring model’s vulnerability after jailbreak-tuning should form a core part of the risk assessment for fine-tuneable models.

Research by Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine.

Check out the blog post: [https://far.ai/post/2024-10-poisoning/](https://far.ai/post/2024-10-poisoning/)  

Read the full paper: [https://arxiv.org/abs/2408.02946](https://arxiv.org/abs/2408.02946)

X: [https://x.com/farairesearch/status/1851987731150152158](https://x.com/farairesearch/status/1851987731150152158)

LinkedIn: [https://www.linkedin.com/posts/far-ai\_a-tiny-dose-of-poisoned-data-can-cause-big-activity-7257753206267490306-Pnr\_](https://www.linkedin.com/posts/far-ai_a-tiny-dose-of-poisoned-data-can-cause-big-activity-7257753206267490306-Pnr_)",KellinPelrine,1ggrhli,https://reddit.com/r/MachineLearning/comments/1ggrhli/r_data_poisoning_in_llms_jailbreaktuning_and/,https://www.reddit.com/r/MachineLearning/comments/1ggrhli/r_data_poisoning_in_llms_jailbreaktuning_and/,2024-10-31 22:59:39,65,0.92,65,0,19,0,0,False,False,True,False,False,Research,self,t3_1ggrhli
MachineLearning,[D] VoxCeleb1 is missing files?,"Is anyone else having a problem where **VoxCeleb1** is missing the files required by **veri\_test.txt**? The first line in **veri\_test.txt** is this `1 id10270/x6uYqmx31kE/00001.wav id10270/8jEAjG6SegY/00008.wav`, but there is no folder with **id10270** in the dataset. There are a few other missing like **id10300**. Which is the next different ID after that one. I even downloaded from Hugging Face [https://huggingface.co/datasets/ProgramComputer/voxceleb/blob/main/vox1/vox1\_dev\_wav.zip](https://huggingface.co/datasets/ProgramComputer/voxceleb/blob/main/vox1/vox1_dev_wav.zip) which matched my hash sum for the one I downloaded from the authors, but same thing",SpanishDude3,1ggq7h5,https://reddit.com/r/MachineLearning/comments/1ggq7h5/d_voxceleb1_is_missing_files/,https://www.reddit.com/r/MachineLearning/comments/1ggq7h5/d_voxceleb1_is_missing_files/,2024-10-31 21:57:23,0,0.5,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1ggq7h5
MachineLearning,Single shot classifier [D],"Is there a way to give  one image of a person and make it identify and track the person in a video with features not particularly their facial features. Maybe it could detect all people and show the probability that its the same person and some filtering can be done to confirm based on model accuracy. 
But can this be done? And how? Looking to use this for a robotics project.",ronald_lanton,1ggq34y,https://reddit.com/r/MachineLearning/comments/1ggq34y/single_shot_classifier_d/,https://www.reddit.com/r/MachineLearning/comments/1ggq34y/single_shot_classifier_d/,2024-10-31 21:51:40,0,0.29,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1ggq34y
MachineLearning,[D] Product specific ticket classification,"Need some guidance on how to approach the ticket classification. The ticket needs to be classified to which operating procedure bucket it should fall into so that the procedure is run automatically.

The classification must be run locally on only CPU/RAM VM without any vCPU.

The tickets normally will have very basic information with more details such as error details available in the screenshots or other attachments.

Ex: “Modifying a loan account throws error“

This description needs additional context populated from the attachments. This will need some OCR to read the screenshots to populate details. Additionally, queries could be run to populate more context from the database. After enrichment, the ticket details would look something like below.

Ex: “Modifying a loan account ACC0001 of product PR001 through the screen LNSCRAMD throws error ERR01 – Failed in save“

This classification is product specific and needs to be classified into multiple SOP buckets such as SOP001, SOP002 etc. There could be couple of hundreds of such classification.

Will setfit work for this type of classification or is it better to go with rule based classification. Please guide on what to learn or what options suits best to achieve this? Thanks in advance.

 ",rekonist-app,1ggmsr3,https://reddit.com/r/MachineLearning/comments/1ggmsr3/d_product_specific_ticket_classification/,https://www.reddit.com/r/MachineLearning/comments/1ggmsr3/d_product_specific_ticket_classification/,2024-10-31 19:26:54,2,0.75,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1ggmsr3
MachineLearning,[P] Suggestions for Creating Embeddings for Articles in PDF Format?,"I need to create embeddings and a vector index for a large number of articles in both doc and pdf format. Does anybody have suggestions on how best to accomplish creating the embeddings for the pdfs? Do I need to convert them to text first? Or, is there a model that can analyze the text in the pdf to create the embeddings?",HailGlaurung,1gglani,https://reddit.com/r/MachineLearning/comments/1gglani/p_suggestions_for_creating_embeddings_for/,https://www.reddit.com/r/MachineLearning/comments/1gglani/p_suggestions_for_creating_embeddings_for/,2024-10-31 18:21:58,0,0.5,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1gglani
MachineLearning,[D] Using Expert Systems in the Medical Setting,"I (AI researcher) was recently talking to a friend (Who works at a hospital) about AI in Healthcare. They were falsely under the impression that we still don't use AI in such settings because it isn't accurate or reliable enough. I explained that even some pretty basic AI systems from the past could consistently outperform Doctors and Nurses, using MYCIN as an example.

This led me to a kind of blatantly obvious question that I've never really considered before.

**Instead of trying to push black-box systems, should we (at least for now) concede and instead be trying to promote and improve Expert Systems like we had in the 70s?**

People in our field enjoy pushing the boundaries of what is possible, trying to make the most accurate systems possible. This has led us down the Neural Network route as given enough data we can get some pretty amazing results. We excitedly want to share that with other fields, such as medicine, to improve diagnostics and save lives. But in that excitement, we forget that even older, more transparent systems have been rejected.

Expert Systems are fairly easy to interpret as you can just read the rule set and follow along. You also avoid issues with data confidentiality. In the 70s they were rejected in medicine because computer systems weren't particularly integrated in day-to-day life as well as a general lack of confidence in computers as they were kind of novel.

Today computing facilities have improved extensively, processing power has grown exponentially and everyone is extremely familiar with them, using them on a day-to-day basis.

Once the rest of the world has caught up and can ""walk"" with this AI, we can then start ""running"" and promote the more modern systems of today.

I want to hear people's input as this is something I'm still trying to formulate in my head. It's also not my specific domain so this might be something that is already done/has been explored to all ends.",AwkwardWaltz3996,1ggittp,https://reddit.com/r/MachineLearning/comments/1ggittp/d_using_expert_systems_in_the_medical_setting/,https://www.reddit.com/r/MachineLearning/comments/1ggittp/d_using_expert_systems_in_the_medical_setting/,2024-10-31 16:35:30,11,0.74,11,0,27,0,0,False,False,True,False,False,Discussion,self,t3_1ggittp
MachineLearning,[D] 30 Year Macroeconomic Forecasting - What's SOTA for long-range multi-step multivariate small data forecasting?,"Project goal: create a 'reasonable' 30 year forecast with some core component generating variation which resembles reality.

Input data: annual US macroeconomic features such as inflation, GDP, wage growth, M2, imports, exports, etc. Features have varying ranges of availability (some going back to 1900 and others starting in the 90s.

Problem statement: Which method(s) is SOTA for this type of prediction? The recent papers I've read mention BNNs, MAGAN, and LightGBM for smaller data like this and TFT, Prophet, and NeuralProphet for big data. I'm mainly curious if others out there have done something similar and have special insights. My current method of extracting temporal features and using a Trend + Level blend with LightGBM works, but I don't want to be missing out on better ideas--especially ones that fit into a Monte Carlo framework and include something like labeling years into probabilistic 'regimes' of boom/recession.",SwitchFace,1ggicb7,https://reddit.com/r/MachineLearning/comments/1ggicb7/d_30_year_macroeconomic_forecasting_whats_sota/,https://www.reddit.com/r/MachineLearning/comments/1ggicb7/d_30_year_macroeconomic_forecasting_whats_sota/,2024-10-31 16:14:58,0,0.41,0,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1ggicb7
MachineLearning,[R] Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection (ICLR),"The paper aims to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices.

[https://openreview.net/pdf?id=MloaGA6WwX](https://openreview.net/pdf?id=MloaGA6WwX)

Contributions:

* A novel method for supervised feature selection that performs task-based image channel selection.
* Results shorten the acquisition time in MRI, reconstruct image cubes of remotely-sensed multispectral ground images, estimate tissue oxygenation from hyperspectral medical devices
* Results show improvement on i) classical experimental design, ii) recent application-specific published results, iii) state-of-the-art approaches in supervised feature selection.

We expect further applications to similar datatypes e.g. data efficiency on multi-channel images, other hyperspectral/multispectral application, cell microscopy, weather and climate data et.c

Code is available, PM me if interested.",sbb_ml,1ggh7f3,https://reddit.com/r/MachineLearning/comments/1ggh7f3/r_experimental_design_for_multichannel_imaging/,https://www.reddit.com/r/MachineLearning/comments/1ggh7f3/r_experimental_design_for_multichannel_imaging/,2024-10-31 15:26:30,8,0.9,8,0,0,0,0,False,False,True,False,False,Research,self,t3_1ggh7f3
MachineLearning,[D] I made my customized podcast characters speak in Gen Z language,"This is my latest attempt at generating a human-like, natural-sounding podcast script. I'd appreciate any feedback on how it sounds:

https://www.youtube.com/watch?v=3fTwcY8Ou3w",Busy-Basket-5291,1ggganr,https://reddit.com/r/MachineLearning/comments/1ggganr/d_i_made_my_customized_podcast_characters_speak/,https://www.reddit.com/r/MachineLearning/comments/1ggganr/d_i_made_my_customized_podcast_characters_speak/,2024-10-31 14:47:28,0,0.33,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1ggganr
MachineLearning,[D] using LLMs to power novelty-seeking adaptive learning agents  ? ,"I had this idea for a novelty-seeking machine learning algorithm based on Godels completeness /incompleteness theorems (more the concept than the actual math, although i am working on a version that uses math/logic more in line with Godel) and to power it with LLMs, since it would take forever for it to get going otherwise and resources I don't have to train each part of the algorithm to do what I need it to....

the core system could be integrated with local llms to power it or you could train and deploy your own models to do so (could even maybe be altered to train its own models one day) for total independence and autotomy...

[https://github.com/CrewRiz/Alice](https://github.com/CrewRiz/Alice)

What do you think of this type of approach, using LLMs to generate and parse data needed for the algorithm to function but remain fundamentally separate from the main agent?

",Individual_Yard846,1ggeite,https://reddit.com/r/MachineLearning/comments/1ggeite/d_using_llms_to_power_noveltyseeking_adaptive/,https://www.reddit.com/r/MachineLearning/comments/1ggeite/d_using_llms_to_power_noveltyseeking_adaptive/,2024-10-31 13:28:20,2,0.6,2,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1ggeite
MachineLearning,[R] Our results experimenting with different training objectives for an AI evaluator,"\*Reposting as the graph images weren't showing :(

Hey r/LocalLLaMA!

Lots of research has been published around LLM-as-a-judge as it's becoming a popular approach to evaluate cheap + fast.

A pretty cool paper that recently came out was from the[ Salesforce AI Research team](https://arxiv.org/abs/2409.14664); tldr: they found preference optimisation techniques like DPO and RPO could yield better results than supervised fine-tuning (SFT) alone as a training objective for LLM-as-a-judge models. We wanted to test this hypothesis as it it's not yet clear which training objective performs best for aligning eval models..

# Our experiments

We trained a Llama-3.1-70B-Instruct with SFT and compared it to base Llama-3.1-70B-Instruct on core benchmarks to see how SFT fares alone.

We also trained a Llama-3.1-8B-Instruct model on two training datasets with

1. Purely SFT
2. DPO
3. RPO (compound loss objective incorporates both SFT and DPO)

and compared their performance against the base model across four core benchmarks.

# Here's a summary of our key findings:

https://preview.redd.it/vuxqgmyd63yd1.png?width=1453&amp;format=png&amp;auto=webp&amp;s=00e9a0c9e24ec423a5003a5c0e14a9c83decc014

* SFT (Atla Caprioska 70B) showed improvements on in-distribution tasks whereas quality dropped on out-of-distribution tasks, underperforming base Llama-70B on aggregate metrics

https://preview.redd.it/k5x7bsbc63yd1.png?width=1423&amp;format=png&amp;auto=webp&amp;s=d2ce2694a1ae7dbd786ba841f215f34f11b11428

* DPO performed best on the on PreferenceCollection with 98.89% accuracy
* RPO performed best on RewardBench with 81.96% accuracy
* RPO outperformed both SFT and DPO on UltraFeedback (No CoT), with a score of 0.57
* RPO achieved the highest average Pearson correlation on evaluation scores (0.49), compared to SFT (0.43) and DPO (0.43)

If you want the details, here's our [blog post](https://www.atla-ai.com/post/selecting-a-training-objective-for-an-ai-evaluator) \- with extra information on why we think this works. We're working on scaling this up and seeing how far we can push this thing now :)

# Open questions for you all

* Will this trend hold for larger models?
* What kind of data might be particularly useful for training an LLM-as-a-judge?",fortunemaple,1ggde6p,https://reddit.com/r/MachineLearning/comments/1ggde6p/r_our_results_experimenting_with_different/,https://www.reddit.com/r/MachineLearning/comments/1ggde6p/r_our_results_experimenting_with_different/,2024-10-31 12:33:30,11,0.87,11,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/1kYbBxoxN_-jo5Jv4_Qrlat_SkYa1xip1ZclKNLJtqg.jpg,t3_1ggde6p
MachineLearning,[D] good second tier conference/journal on medical image analysis?,"I think the method is solid or at least borderline for TMI (transactions on medical imaging) level journal but the experiments are not thorough enough. It got rejected by TMI and I’m gonna graduate soon so I’m trying to not put too much more extra effort into the paper. I’m only familiar with the top tier journals and conferences and a few of the second tier ones but none seems to be a good fit of the paper. 

So I’m looking for a journal that takes some medical imaging related papers with a reasonable turnaround time or a conference with a deadline coming up, focusing on technical novelty but being not to rigorous on the experiments. 

Thanks very much in advance! ",alex_o_O_Hung,1gfy8zi,https://reddit.com/r/MachineLearning/comments/1gfy8zi/d_good_second_tier_conferencejournal_on_medical/,https://www.reddit.com/r/MachineLearning/comments/1gfy8zi/d_good_second_tier_conferencejournal_on_medical/,2024-10-30 21:46:21,3,0.72,3,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gfy8zi
MachineLearning,[D] I’m an ML/programming educator - I was invited as ceo of codesmith to Berlin Global Dialogue (tech/AI insider conference) - see what they said behind closed doors - AMA,"**Edit 2:** Came back and answered a few more Qs - I’m going to do a vid to summarize some of the discussion at some point (will share) but in meantime if you want to talk more feel free to DM me here or on [https://x.com/willsentance](https://x.com/willsentance)

**Edit (5pm PT):** Thanks so much all for really great questions - I'm going to pause now but will take a look over next 24 hours and try to answer any more questions. V grateful for chance to do this and to others who helped answer some of the Qs too from their perspective (shoutout u/Rebeleleven)

\--

I'm Will Sentance - I recently had the opportunity to attend the Berlin Global Dialogue, which has been likened to Davos but with a stronger focus on technology and AI . The lineup was impressive: Hermann Hauser, the founder of ARM, executives from OpenAI and ASML, and a mix of founders from emerging startups tackling everything from quantum ML to supply chain optimization. Even leaders like President Macron and the German Vice Chancellor were there, engaging with critical tech issues that impact us all.

As the CEO of Codesmith – a small, independent tech school with a data science and machine learning research group (last year we contributed to TensorFlow) – I was invited to announce our latest endeavor: Codesmith’s AI &amp; ML Technical Leadership Program.

I shared this experience in an AMA on r/technology and had a great conversation—but the depth of questions around ML/AI didn’t quite match what I’d hoped to explore. I spoke to the mods here and am grateful for them supporting this AMA. 

Proof: [https://imgur.com/a/bYkUiE7](https://imgur.com/a/bYkUiE7)

My real passion, inherited from my parents who were both educators, is teaching and making ML more accessible to a broader audience. I’m currently developing an AI/ML workshop for Frontend Masters, and I want to hear from those navigating the ML field. What’s the biggest challenge you're facing in this space?

A few of my takeaways from the event:

* **Chip manufacturers are shifting to new architectures** rather than further miniaturization due to physical limits. High-bandwidth memory (HBM) is a central focus for future roadmaps.
* Europe is fixated on finding a ‘tech champion,’ but there's a distinct emphasis on core industries rather than consumer internet—think ASML and ARM.
* **Quantum ML is gaining momentum** and receiving government support, particularly for applications like climate forecasting (e.g., Germany’s Klim-QML initiative). While promising, these efforts are still in the prototype phase.
* There was also, candidly, a lot of talk without much substance. Even OpenAI execs demonstrated a need for more leaders with deep technical insights.

Looking forward to diving deeper into these issues and the broader challenges in ML/AI in an AMA!",WillSen,1gfv37y,https://reddit.com/r/MachineLearning/comments/1gfv37y/d_im_an_mlprogramming_educator_i_was_invited_as/,https://www.reddit.com/r/MachineLearning/comments/1gfv37y/d_im_an_mlprogramming_educator_i_was_invited_as/,2024-10-30 19:31:11,150,0.83,150,0,46,0,0,False,False,True,False,False,Discussion,self,t3_1gfv37y
MachineLearning,[D] Is there a preferred data distribution for CNN models to handle?,"Hi everyone,   
Do you know of any works that analyzed the performance of a CNN based model w.r.t the training data distribution? meaning, are some distribution easier to the model to learn its task on than others?  
For example, let's say I'm training a model to do object detection on images. I see that day images get better performance than night images (same amount of data). I wonder if I can explain this in some analytical way.   
Thanks!",hilabar,1gfuxat,https://reddit.com/r/MachineLearning/comments/1gfuxat/d_is_there_a_preferred_data_distribution_for_cnn/,https://www.reddit.com/r/MachineLearning/comments/1gfuxat/d_is_there_a_preferred_data_distribution_for_cnn/,2024-10-30 19:24:23,3,0.67,3,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gfuxat
MachineLearning,"[D] Local LLaMA based LLM for Technical Document Search | Help!
","I wanted to make an LLM that could search through around 60k technical documents (about 50000 characters each) and could retrieve information from them semantically. The final model I envisioned would know those technical documents and I could just prompt the model to find me something similar to the information it already knew or something exact.

* My initial approach was to fine tune the LLM with these documents and then query it. But after researching I got to know that it is very resource heavy and the model often hallucinates a lot.
* I came across RAG and Sematic RAG recently and I'm currently reading about it. Could it work for my use case? Or anything else that you can suggest? One Issue in my mind for RAG was that let's say I ask the model something vague and the vector database returned top k Nearest Neighbor Vectors and I pass that onto my LLM with the Original Prompt. What if the information was not completely there in the Top K Nearest Neighbors or if the Context Window for the LLM is not big enough?
* Another issue was that with RAG wouldn't inference with the LLM become a lot more resource heavy due to a large input token count.

Could you guys comment on anything in it?

PS: I know this is a large question. I'm a bit new to ML and NLP and learning about it. Also sorry about my English, I'm not a native speaker.",WhyHimanshuGarg,1gftg6l,https://reddit.com/r/MachineLearning/comments/1gftg6l/d_local_llama_based_llm_for_technical_document/,https://www.reddit.com/r/MachineLearning/comments/1gftg6l/d_local_llama_based_llm_for_technical_document/,2024-10-30 18:22:47,9,1.0,9,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gftg6l
MachineLearning,[D] How do you manage your (read and to-read) research papers?,"I'm kind of new to the field of research and over the past year. I've probably read over 100 research papers, but I feel as though I don't retain a lot of the information and I forget a lot of the paper papers that are bread. I'm curious what people who have been in the industry longer used for organization.

  
I've tried Zotero, but I haven't really been a big fan",Karan1213,1gfsxcg,https://reddit.com/r/MachineLearning/comments/1gfsxcg/d_how_do_you_manage_your_read_and_toread_research/,https://www.reddit.com/r/MachineLearning/comments/1gfsxcg/d_how_do_you_manage_your_read_and_toread_research/,2024-10-30 18:01:08,98,0.96,98,0,36,0,0,False,False,True,False,False,Discussion,self,t3_1gfsxcg
MachineLearning, [R] Torchtune - How to finetune custom models? ,I'm wondering how I can get started to finetune my custom model with torchtune lora. Does anyone have any documentation or suggestions?,Cool-Economy3492,1gfpab4,https://reddit.com/r/MachineLearning/comments/1gfpab4/r_torchtune_how_to_finetune_custom_models/,https://www.reddit.com/r/MachineLearning/comments/1gfpab4/r_torchtune_how_to_finetune_custom_models/,2024-10-30 15:29:26,3,0.81,3,0,4,0,0,False,False,True,False,False,Research,self,t3_1gfpab4
MachineLearning,[R] Riemannian Generative Models,"Hi everyone,

I’m currently interested in exploring generative models defined over Riemannian manifolds. Though the idea is theoretically appealing, I have trouble understanding the practical motivation behind this approach, and whether any useful/large scale model has been developed lately based on it.

To be more precise, I am looking at the following set of papers.

Generalizing diffusion models to the Riemannian setting :

[Riemannian Diffusion Models](https://arxiv.org/abs/2208.07949), [Riemannian Score-Based Generative Modelling](https://arxiv.org/abs/2202.02763)

Scaling these models:

[Scaling Riemannian Diffusion Models](https://arxiv.org/abs/2310.20030)

I don’t understand how impactful the experimental results really are, and what the interest for these models are whether in the industry or in the research community. 

If anyone has any thoughts about the interrogations I have, I’d be happy to start a discussion here. I’d be extremely grateful for your insights! Thanks for any help",LostSleepyDreamer,1gfnqkf,https://reddit.com/r/MachineLearning/comments/1gfnqkf/r_riemannian_generative_models/,https://www.reddit.com/r/MachineLearning/comments/1gfnqkf/r_riemannian_generative_models/,2024-10-30 14:23:45,24,0.99,24,0,7,0,0,False,False,True,False,False,Research,self,t3_1gfnqkf
MachineLearning,[D] Is INRIA (France) a good place for UG to do ML research internship?,"I am a student conducting research related in MAB/Online Algorithm, I see there are really very little people doing this in the USA. However I found there are noticable amount of researcher doing this in INRIA , the one in France if you dont know. Does anyone familar with this insitution? As a grad student from non-EU country is it possible for me intern here on voluntary bias during summer break if my goal is get recommendation letter and publish paper?",petrichorinforest,1gfneal,https://reddit.com/r/MachineLearning/comments/1gfneal/d_is_inria_france_a_good_place_for_ug_to_do_ml/,https://www.reddit.com/r/MachineLearning/comments/1gfneal/d_is_inria_france_a_good_place_for_ug_to_do_ml/,2024-10-30 14:08:46,18,0.67,18,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1gfneal
MachineLearning,"[D] Classification approaches for short text, many categories?","Hi - I am dealing with an issue where I will likely have many thousands of short text snippets (think 2-4 sentences each), and need to assess the extent to which each sentence is consistent with each of about \~200 categories (that is, a piece of text may fit ""best"" into one category, but it's also possible that a few other categories are ""reasonable"". Getting huge amounts of text labeled may be an effort, so I'm especially interested in things like few-shot approaches. (Or maybe even a bootstrap approach -- not the statistical technique, the concept -- where we develop a quick and dirty classification model, and use that to assist raters in doing another larger tranche of labelling, faster. Which obviously has potential drawbacks in terms of bias, etc., but may have )

My background is mostly in traditional/Bayesian statistics (think like linear models and factor analysis), so I'm a little out of the loop on good approaches to a task like this. The place this analysis will take place will not have any fancy LLMs, and no access to internet-based platforms (Huggingface, OpenAI, etc.). No GPUs, so any fine-tuning that might be needed has to take that into consideration. The obvious (to me, a-not-NLP person) starting point seems like BERT with a normal classifier. But there's so many variations to BERT, and similar models (Universal Sentence Encoders?)... and I'm not sure which ones are better for short text. I am aware of the huggingface leaderboards, which I've looked over, but it wasn't immediately clear to me which are best for short text classification.

So if anyone has suggestions for thoughts on potential approaches to look into, I'd really appreciate it.",malenkydroog,1gfmp5f,https://reddit.com/r/MachineLearning/comments/1gfmp5f/d_classification_approaches_for_short_text_many/,https://www.reddit.com/r/MachineLearning/comments/1gfmp5f/d_classification_approaches_for_short_text_many/,2024-10-30 13:36:35,4,1.0,4,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gfmp5f
MachineLearning,[D] COLING 2025 Results / rebuttals,"I'll go first.

Soundness: 3,3,4

Overall: 2,2,3

🥺",monkeyofscience,1gfjtnu,https://reddit.com/r/MachineLearning/comments/1gfjtnu/d_coling_2025_results_rebuttals/,https://www.reddit.com/r/MachineLearning/comments/1gfjtnu/d_coling_2025_results_rebuttals/,2024-10-30 11:01:19,21,0.87,21,0,84,0,0,False,False,True,False,False,Discussion,self,t3_1gfjtnu
MachineLearning,[D] Does anyone here work in healthcare?,I'm curious about the cool things people around the world are doing related to data in this area of work att,Intelligent-Cap-4022,1gfjngd,https://reddit.com/r/MachineLearning/comments/1gfjngd/d_does_anyone_here_work_in_healthcare/,https://www.reddit.com/r/MachineLearning/comments/1gfjngd/d_does_anyone_here_work_in_healthcare/,2024-10-30 10:50:02,48,0.89,48,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1gfjngd
MachineLearning,[P] Opik 1.0: Open source LLM evaluations,"Hey all! 

My colleagues and I have released version 1.0 of our open source LLM evaluation framework, and I wanted to share it here for feedback/visibility. With this first major release, we've focused on a few key areas:

* Out-of-the-box implementations of popular LLM-as-a-judge metrics, as well as ""traditional"" heuristic metrics, along with a clean API for defining custom metrics.
* Configurable LLM tracing, with a nice UI for visualizing traces/spans. Also supports automatic tracing for OpenAI and LiteLLM.
* Version-controlled datasets for running eval experiments.

If you have time to check out the repo and share any feedback or questions, I'd really appreciate it. It's still early days, but we've been blown away by the community response so far, and we're excited to get more input as we continue to work on the project. 

Repo Link: [https://github.com/comet-ml/opik](https://github.com/comet-ml/opik)",calebkaiser,1gf3e1s,https://reddit.com/r/MachineLearning/comments/1gf3e1s/p_opik_10_open_source_llm_evaluations/,https://www.reddit.com/r/MachineLearning/comments/1gf3e1s/p_opik_10_open_source_llm_evaluations/,2024-10-29 19:37:37,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1gf3e1s
MachineLearning,[D] Predicting happiness from survey data,"I have a dataset containing survey data with 39 variables, variables such as [perfect.physical.health](http://perfect.physical.health) with score -2, -1, 0, 1, 2. Now I want to predict happiness which is a decimal value. How do i approach this problem?",OverallLab187,1gfiadq,https://reddit.com/r/MachineLearning/comments/1gfiadq/d_predicting_happiness_from_survey_data/,https://www.reddit.com/r/MachineLearning/comments/1gfiadq/d_predicting_happiness_from_survey_data/,2024-10-30 09:12:26,1,0.6,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gfiadq
MachineLearning,[D] Problem with graph based-VAE on molecular dynamics trajectory.,"Recently I saw someone post a query regarding Graph based VAE construction on MD trajectory data. Actually I am facing a similar problem as well. This is the code I have generated till now. As I am not a professional coder myself, coming from a chemistry background, I mostly relied on chatbots to generate the code for me, but the problem is the model has some serious problems with the dimensionality.  
  
import numpy as np

import random

import MDAnalysis as mda

import networkx as nx

import torch

import torch.nn as nn

import torch.optim as optim

from torch\_geometric.data import Data, DataLoader

from torch\_geometric.nn import GCNConv

from Bio.PDB import PDBIO, Structure, Model, Chain, Residue, Atom

import matplotlib.pyplot as plt

from sklearn.model\_selection import ParameterGrid

from tqdm import tqdm

import pandas as pd  
\# Load MD trajectory and select C-alpha atoms

u = mda.Universe('synuclein.top', 'short.nc')

ca\_atoms = u.select\_atoms(""name CA"")



\# Define the amino acid sequence in three-letter code

sequence\_one\_letter = ""MDVFMKGLSKAKEGVVAAAEKTKQGVAEAAGKTKEGVLYVGSKTKEGVVHGVATVAEKTKEQVTNVGGAVVTGVTAVAQKTVEGAGSIAAATGFVKK""

amino\_acid\_1\_to\_3 = {

'A': 'ALA', 'C': 'CYS', 'D': 'ASP', 'E': 'GLU', 'F': 'PHE',

'G': 'GLY', 'H': 'HIS', 'I': 'ILE', 'K': 'LYS', 'L': 'LEU',

'M': 'MET', 'N': 'ASN', 'P': 'PRO', 'Q': 'GLN', 'R': 'ARG',

'S': 'SER', 'T': 'THR', 'V': 'VAL', 'W': 'TRP', 'Y': 'TYR'

}

sequence = \[amino\_acid\_1\_to\_3\[aa\] for aa in sequence\_one\_letter\]



\# One-hot encoding for amino acids

amino\_acid\_types = {

'ALA': 0, 'CYS': 1, 'ASP': 2, 'GLU': 3, 'PHE': 4,

'GLY': 5, 'HIS': 6, 'ILE': 7, 'LYS': 8, 'LEU': 9,

'MET': 10, 'ASN': 11, 'PRO': 12, 'GLN': 13, 'ARG': 14,

'SER': 15, 'THR': 16, 'VAL': 17, 'TRP': 18, 'TYR': 19

}



\# Function to convert amino acid sequence to one-hot encoding

def one\_hot\_encode(sequence):

num\_amino\_acids = len(amino\_acid\_types)

features = np.zeros((len(sequence), num\_amino\_acids))

for i, aa in enumerate(sequence):

if aa in amino\_acid\_types:

features\[i, amino\_acid\_types\[aa\]\] = 1

return features



\# Generate node features for the amino acid sequence

node\_features = one\_hot\_encode(sequence)



\# Define the contact map based on CA distances

threshold\_distance = 8.0  # Distance threshold in angstroms

num\_amino\_acids = len(sequence)



\# Prepare data for PyTorch Geometric for all frames

data\_list = \[\]

num\_frames = len(u.trajectory)



for frame in tqdm(range(num\_frames), desc=""Processing Frames""):

u.trajectory\[frame\]

ca\_atoms = u.select\_atoms(""name CA"")



\# Create a contact graph

contact\_graph = nx.Graph()

for i in range(num\_amino\_acids):

contact\_graph.add\_node(i, features=node\_features\[i\])



\# Add edges based on CA distances

for i in range(num\_amino\_acids):

for j in range(i + 1, num\_amino\_acids):

distance = np.linalg.norm(ca\_atoms.positions\[i\] - ca\_atoms.positions\[j \])

if distance &lt;= threshold\_distance:

contact\_graph.add\_edge(i, j)



\# Prepare data for PyTorch Geometric

edge\_index = torch.tensor(list(contact\_graph.edges), dtype=torch.long).t().contiguous()

x = torch.tensor(node\_features, dtype=torch.float)

data = Data(x=x, edge\_index=edge\_index)

\#    print(data)

data\_list.append(data)



\# Plot and save contact map for every 500th frame

if frame % 500 == 0:

contact\_map = np.zeros((num\_amino\_acids, num\_amino\_acids))

for i, j in contact\_graph.edges:

contact\_map\[i, j\] =  1

contact\_map\[j, i\] = 1

plt.imshow(contact\_map, cmap='binary')

plt.title(f""Contact Map for Frame {frame}"")

plt.xlabel(""Residue Index"")

plt.ylabel(""Residue Index"")

plt.savefig(f""contact\_map\_frame\_{frame}.png"")

pd.DataFrame(contact\_map).to\_csv(f""contact\_map\_frame\_{frame}.csv"", index=False)  


class GCNEncoder(nn.Module):

def \_\_init\_\_(self, in\_channels, hidden\_channels, num\_layers):

super(GCNEncoder, self).\_\_init\_\_()

self.convs = nn.ModuleList()

self.fc\_mu = nn.Linear(hidden\_channels, hidden\_channels)

self.fc\_logvar = nn.Linear(hidden\_channels, hidden\_channels)



\# Create multiple GCN layers

for \_ in range(num\_layers):

self.convs.append(GCNConv(in\_channels, hidden\_channels))

in\_channels = hidden\_channels  # Update input channels for the next layer



def forward(self, x, edge\_index):

for conv in self.convs:

x = conv(x, edge\_index)

x = torch.relu(x)  # Activation function

mu = self.fc\_mu(x)

logvar = self.fc\_logvar(x)

return mu, logvar



class GCNDecoder(nn.Module):

def \_\_init\_\_(self, hidden\_channels, out\_channels):

super(GCNDecoder, self).\_\_init\_\_()

self.fc = nn.Linear(hidden\_channels, out\_channels)



def forward(self, z):

return torch.sigmoid(self.fc(z))



class GCNVAE(nn.Module):

def \_\_init\_\_(self, in\_channels, hidden\_channels, out\_channels, num\_layers):

super(GCNVAE, self).\_\_init\_\_()

self.encoder = GCNEncoder(in\_channels, hidden\_channels, num\_layers)

self.decoder = GCNDecoder(hidden\_channels, out\_channels)



def reparameterize(self, mu, logvar):

std = torch.exp(0.5 \* logvar)

eps = torch.randn\_like(std)

return mu + eps \* std



def forward(self, x, edge\_index):

mu, logvar = self.encoder(x, edge\_index)

z\_sample = self.reparameterize(mu, logvar)

return self.decoder(z\_sample), mu, logvar



def loss\_function(recon\_x, x, mu, logvar):

BCE = nn.functional.binary\_cross\_entropy(recon\_x, x, reduction='sum')

KLD = -0.5 \* torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

return BCE, KLD, BCE + KLD  # Return BCE, KLD, and Total Loss



def train\_model(model, data\_loader, optimizer, epochs, early\_stopping\_patience=5):

model.train()

best\_loss = float('inf')

patience\_counter = 0



for epoch in range(epochs):

total\_loss = 0

total\_bce = 0

total\_kld = 0



for data in tqdm(data\_loader, desc=f""Training Epoch {epoch+1}/{epochs}""):

optimizer.zero\_grad()

recon\_batch, mu, logvar = model(data.x, data.edge\_index)

bce, kld, total = loss\_function(recon\_batch, data.x, mu, logvar)

total\_loss += total.item()

total\_bce += bce.item()

total\_kld += kld.item()

total.backward()

optimizer.step()



avg\_loss = total\_loss / len(data\_loader)

avg\_bce = total\_bce / len(data\_loader)

avg\_kld = total\_kld / len(data\_loader)



print(f""Epoch {epoch+1}/{epochs} - Total Loss: {avg\_loss:.4f}, BCE Loss: {avg\_bce:.4f}, KLD Loss: {avg\_kld:.4f}"")



\# Early stopping

if avg\_loss &lt; best\_loss:

best\_loss = avg\_loss

patience\_counter = 0

else:

patience\_counter += 1

if patience\_counter &gt;= early\_stopping\_patience:

print(""Early stopping triggered."")

break



\# Create a DataLoader

data\_loader = DataLoader(data\_list, batch\_size=1, shuffle=True)



\# Hyperparameter grid

param\_grid = {

'hidden\_channels': \[16, 32, 64\],

'num\_layers': \[2, 3, 4\],

'activation\_function': \['relu', 'tanh', 'sigmoid'\],

'batch\_size': \[1, 2, 4\],

'latent\_dimensions': \[16, 32, 64\],

'learning\_rate': \[0.001, 0.01, 0.1\],

'epochs': \[50, 100, 200\]

}



\# Perform hyperparameter tuning

best\_loss = float('inf')

best\_params = {}



for params in ParameterGrid(param\_grid):

model = GCNVAE(in\_channels=20, hidden\_channels=params\['hidden\_channels'\], out\_channels=20, num\_layers=params\['num\_layers'\])

optimizer = optim.Adam(model.parameters(), lr=params\['learning\_rate'\])



print(f""Training with parameters: {params}"")

train\_model(model, data\_loader, optimizer, params\['epochs'\], early\_stopping\_patience=5)



\# Evaluate the model (using training loss as a proxy)

model.eval()

total\_loss = 0

total\_bce = 0

total\_kld = 0



with torch.no\_grad():

for data in data\_loader:

recon\_batch, mu, logvar = model(data.x, data.edge\_index)

bce, kld, total = loss\_function(recon\_batch, data.x, mu, logvar)

total\_loss += total.item()

total\_bce += bce.item()

total\_kld += kld.item()



avg\_loss = total\_loss / len(data\_loader)

avg\_bce = total\_bce / len(data\_loader)

avg\_kld = total\_kld / len(data\_loader)



print(f""Average loss: {avg\_loss:.4f}, BCE Loss: {avg\_bce:.4f}, KLD Loss: {avg\_kld:.4f}"")



if avg\_loss &lt; best\_loss:

best\_loss = avg\_loss

best\_params = params



print(f""Best parameters: {best\_params} with loss: {best\_loss}"")



\# Final training with best parameters

final\_model = GCNVAE(in\_channels=20, hidden\_channels=best\_params\['hidden\_channels'\], out\_channels=20, num\_layers=best\_params\['num\_layers'\])

final\_optimizer = optim.Adam(final\_model.parameters(), lr=best\_params\['learning\_rate'\])

train\_model(final\_model, data\_loader, final\_optimizer, best\_params\['epochs'\], early\_stopping\_patience=5)

I know the code is quite long, but I want to know is the code correct? I have a trajectory size of 500 frames, and 97 residues (corresponding to 97 C alpha atoms). Once this code is done, I want to generate protein configurations from the latent space. So I want to ensure that the code is running fine. Thanks a lottt in advance.",Temporary_Scar8023,1gfgydn,https://reddit.com/r/MachineLearning/comments/1gfgydn/d_problem_with_graph_basedvae_on_molecular/,https://www.reddit.com/r/MachineLearning/comments/1gfgydn/d_problem_with_graph_basedvae_on_molecular/,2024-10-30 07:26:50,0,0.33,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gfgydn
MachineLearning,[R] What's there yet to improve in speech technologies? What's there left in speech research?,"Hi everyone, I am currently researching speech technologies as an undergrad, mainly focusing on improving the applications for the visually challenged. I am new to this niche area of research, so I want to pick a research topic that will address some of the existing issues of the current tech. So far, ElevenLabs seem to be the SOTA. I would like to know whether there is anything else to improve in TTS, speech to speech, voice cloning, deepfake audio detection etc., And any insights on ethical issues or the need for guardrails in the future would also be helpful. And due to the availability of low compute resources from uni, I cannot address the research involving scaling or multilingual.



",burikamen,1gfgf2d,https://reddit.com/r/MachineLearning/comments/1gfgf2d/r_whats_there_yet_to_improve_in_speech/,https://www.reddit.com/r/MachineLearning/comments/1gfgf2d/r_whats_there_yet_to_improve_in_speech/,2024-10-30 06:43:47,6,0.8,6,0,16,0,0,False,False,True,False,False,Research,self,t3_1gfgf2d
MachineLearning,[D] How do you structure your codebase and workflow for a new research project?,"Suppose you have got a new idea about a solution to a problem in the domain you are working in. How do you go about implementing the thing from the ground up? 

What is the general structure of the codebase you construct for your project?

How do you go about iteratively training and testing your solution until you arrive at a final solution where you can write a paper for publication? 

  
Is there any design recipe you follow? Where did you learn it from?",HopeIsGold,1gffm46,https://reddit.com/r/MachineLearning/comments/1gffm46/d_how_do_you_structure_your_codebase_and_workflow/,https://www.reddit.com/r/MachineLearning/comments/1gffm46/d_how_do_you_structure_your_codebase_and_workflow/,2024-10-30 05:43:17,113,0.97,113,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1gffm46
MachineLearning,[D] Voices Separation Pipeline,"Let suppose I have audio from karaoke with
1. Music
2. Several voices singing (A, B, C)
3. Random noise

Let suppose I know exactly how many main sources I have on the tape and I want to
1. Clear the noise
2. Extract voice B from the tape and return audio with music and A and B vocals.

I have several questions and appreciate any help.

1. Are there any models that can help me with such separation (pre-trained / needn’t to be trained)?

2. If not, I have some ideas about possible solution pipeline and appreciate any comments:
2.1. Separate instrumental music from everything else (what model I can use to do that?)
2.2. Clear noise from audio without music (what model I can use for that?)
2.3. Separate voices (how?) and delete wave I needn’t. 
2.4. Put everything I need together back.
",m4k2ch8,1gfclyv,https://reddit.com/r/MachineLearning/comments/1gfclyv/d_voices_separation_pipeline/,https://www.reddit.com/r/MachineLearning/comments/1gfclyv/d_voices_separation_pipeline/,2024-10-30 02:40:11,14,0.94,14,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gfclyv
MachineLearning,[P] Open-Source AI Tool for PII Masking,"Privacy has always been and will continue to be a threat into the future of technology, especially with AI! AI and privacy are contradictory in nature. AI needs data to learn, but the more data the bigger the risk...

Curious what everyone's thoughts about this are and also sharing a new open-source tool called PII Masker that detects and masks personally identifiable information in text: [https://github.com/HydroXai/pii-masker-v1](https://github.com/HydroXai/pii-masker-v1). It’s fairly simple to use and makes protecting sensitive data a bit easier.

Would appreciate any feedback!",lial4415,1gfa2oh,https://reddit.com/r/MachineLearning/comments/1gfa2oh/p_opensource_ai_tool_for_pii_masking/,https://www.reddit.com/r/MachineLearning/comments/1gfa2oh/p_opensource_ai_tool_for_pii_masking/,2024-10-30 00:32:23,14,1.0,14,0,3,0,0,False,False,True,False,False,Project,self,t3_1gfa2oh
MachineLearning,[D] M4 chips for training ML? (MPS),"Apple is (purposefully) creating a lot of buzz regarding their “Apple Intelligence”, stating that their M4 chips are built for AI.

My question is this,
Will this only be helpful for running the built in Apple Intelligence - or is this supposed to vastly improve on MPS when actually training large transformer models etc.? I haven’t heard them mention any improvements on MPS.
",Hmm_okay_jeps,1gf46km,https://reddit.com/r/MachineLearning/comments/1gf46km/d_m4_chips_for_training_ml_mps/,https://www.reddit.com/r/MachineLearning/comments/1gf46km/d_m4_chips_for_training_ml_mps/,2024-10-29 20:10:28,14,0.68,14,0,59,0,0,False,False,True,False,False,Discussion,self,t3_1gf46km
MachineLearning,"[D] ""Problem with Graph Based VAE. P.S. I am not a very good programmer !!!""","So, I am trying to generate a a graph based Variational Autoencoder Model (VAE), using smaller trajectories of my protein as input (I have generated multiple small trajectories of my protein at different random seeds). My goal is to see the latent space from the observed trajectories and generate new structures from the region that are less explored, and start MD simulations from these regions.  
I have used protein's C alpha atoms as input and calculated adjacency matrix based on contact distance bewteen two C alpha atoms, with a cutoff of 8 angstrom. However I am facing a lot of issues with the dimensionality of the model, like I have 97 residues in my protein and for the test trajectory there are 2500 frames, and with 80:20 split, I have training set (2000,97,97) and validation set (500,97,97). But when I tried to decode the latent point, the decoded dimension was 194,97. this is creating a confusion for me. I am attaching the architecture of the model that I am using. Also the hyperparameters obtained in my case were:

    Best Hyperparameters: {'activation_fn': ReLU(), 'batch_size': 2, 'dropout_rate': 0.1, 'epochs': 50, 'hidden_dim': 16, 'latent_dim': 2, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer_type': 'adam', 'weight_decay': 1e-05}

please check them and let me know where am I going wrong. Thanks a lottt in advance.

    GraphVAE(
      (gcn_layers): ModuleList(
        (0): GCNConv(97, 16)
        (1): GCNConv(16, 16)
      )
      (fc_mu): Linear(in_features=16, out_features=2, bias=True)
      (fc_logvar): Linear(in_features=16, out_features=2, bias=True)
      (decoder_layers): ModuleList(
        (0): GCNConv(2, 16)
        (1): GCNConv(16, 16)
      )
      (decoder_output): GCNConv(16, 97)
      (activation): ReLU()
    )",ShazzieSlays08,1gevt06,https://reddit.com/r/MachineLearning/comments/1gevt06/d_problem_with_graph_based_vae_ps_i_am_not_a_very/,https://www.reddit.com/r/MachineLearning/comments/1gevt06/d_problem_with_graph_based_vae_ps_i_am_not_a_very/,2024-10-29 14:21:33,0,0.41,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gevt06
MachineLearning,"[R] ""How to train your VAE"" substantially improves the reported results for standard VAE models (ICIP 2024)","https://preview.redd.it/b1dmh67uroxd1.png?width=1025&amp;format=png&amp;auto=webp&amp;s=3d42a65e2c0a946aa307f01886aebedfc4b88b8e

The proposed method redefines the Evidence Lower Bound (ELBO) with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. The main contribution in this work is an ELBO that reduces the collapse of the posterior towards the anterior (observed as the generation of very similar, blurry images)

[https://arxiv.org/abs/2309.13160](https://arxiv.org/abs/2309.13160)  
[https://github.com/marianorivera/How2TrainUrVAE](https://github.com/marianorivera/How2TrainUrVAE)",jarkkowork,1get08n,https://reddit.com/r/MachineLearning/comments/1get08n/r_how_to_train_your_vae_substantially_improves/,https://www.reddit.com/r/MachineLearning/comments/1get08n/r_how_to_train_your_vae_substantially_improves/,2024-10-29 12:08:21,155,0.9,155,0,17,0,0,False,False,True,False,False,Research,https://a.thumbs.redditmedia.com/6DyEBEOD0-XT1Up5K5Wntyr6zm5ID5keLFayYn4ljD0.jpg,t3_1get08n
MachineLearning,[D] Exploring Serverless Solutions for Whisper V3 Turbo Integration,"Currently, the serverless solution from Runpod meets my needs in terms of cost and features: [https://github.com/runpod-workers/worker-faster\_whisper](https://github.com/runpod-workers/worker-faster_whisper)



However, I'm interested in using [https://huggingface.co/openai/whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo) due to its reported speed.



I'm uncertain about how to set up and run Whisper V3 Turbo on Runpod’s serverless infrastructure. 



It seems we might need to wait until the upstream project https://github.com/SYSTRAN/faster-whisper/issues/1030 is updated with Turbo and published on https://pypi.org/project/faster-whisper/. 



Only then will this feature be available, and at that point, we could fork [https://github.com/runpod-workers/worker-faster\_whisper](https://github.com/runpod-workers/worker-faster_whisper) to update it accordingly.



In the meantime, do you know of any cost-effective serverless solutions for using Whisper V3 Turbo?



Thanks.



p/s 



Groq offers this service: [https://groq.com/whisper-large-v3-turbo-now-available-on-groq-combining-speed-quality-for-speech-recognition/](https://groq.com/whisper-large-v3-turbo-now-available-on-groq-combining-speed-quality-for-speech-recognition/)



However, they currently don't accept payments from developers and haven't provided an estimated timeframe for when this might be available.",yccheok,1genbjo,https://reddit.com/r/MachineLearning/comments/1genbjo/d_exploring_serverless_solutions_for_whisper_v3/,https://www.reddit.com/r/MachineLearning/comments/1genbjo/d_exploring_serverless_solutions_for_whisper_v3/,2024-10-29 05:26:01,2,0.58,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1genbjo
MachineLearning,[R] SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over Time,"I am very happy to announce that our paper ""SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over Time"" got accepted for WACV2025: [https://arxiv.org/abs/2407.15507](https://arxiv.org/abs/2407.15507)  
Project-Page: [https://spotdiffusion.github.io](https://spotdiffusion.github.io)  
Code: [https://github.com/stanifrolov/spotdiffusion](https://github.com/stanifrolov/spotdiffusion)

Our method shifts non-overlapping denoising windows over time, ensuring that seams in one timestep are corrected in the next. This results in coherent, high-resolution images with fewer overall steps. We demonstrate the effectiveness of our approach through qualitative and quantitative evaluations, comparing it with MultiDiffusion, SyncDiffusion, and StitchDiffusion. Our method offers several key benefits, including improved computational efficiency and faster inference times while producing comparable or better image quality.",Maleficent_Stay_7737,1gekcus,https://reddit.com/r/MachineLearning/comments/1gekcus/r_spotdiffusion_a_fast_approach_for_seamless/,https://www.reddit.com/r/MachineLearning/comments/1gekcus/r_spotdiffusion_a_fast_approach_for_seamless/,2024-10-29 02:33:13,109,0.96,109,0,6,0,0,False,False,True,False,False,Research,self,t3_1gekcus
MachineLearning,[R] Dynamic Attention-Guided Diffusion for Image Super-Resolution,"I'm glad to share that our paper ""Dynamic Attention-Guided Diffusion for Image Super-Resolution"" got accepted for WACV2025:  
[https://arxiv.org/abs/2308.07977](https://arxiv.org/abs/2308.07977)

The goal of this work was to introduce a new attention-guided diffusion mechanism to focus image refinement on essential areas that benefit the most from deep refinement :)",Maleficent_Stay_7737,1gekbg3,https://reddit.com/r/MachineLearning/comments/1gekbg3/r_dynamic_attentionguided_diffusion_for_image/,https://www.reddit.com/r/MachineLearning/comments/1gekbg3/r_dynamic_attentionguided_diffusion_for_image/,2024-10-29 02:31:10,184,0.97,184,0,8,0,0,False,False,True,False,False,Research,self,t3_1gekbg3
MachineLearning,[R] Model suggestion for variable-length output in ML thesis,"Hi all, I’m starting my thesis and have basic ML/DL knowledge. I need a model that can take a fixed set of inputs (a snapshot) and output a variable-length vector with real and complex values. I’ve read LSTM might work, but I’m unsure given the fixed input.  
  
Does anyone have recommendations for models or architectures that could work well for this kind of task? Any advice on where to start or resources to check out would be super helpful. Thanks in advance!",Less-Meaning-6450,1ged0by,https://reddit.com/r/MachineLearning/comments/1ged0by/r_model_suggestion_for_variablelength_output_in/,https://www.reddit.com/r/MachineLearning/comments/1ged0by/r_model_suggestion_for_variablelength_output_in/,2024-10-28 20:58:25,2,0.75,2,0,4,0,0,False,False,True,False,False,Research,self,t3_1ged0by
MachineLearning,[R] Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning,"Paper: [https://arxiv.org/abs/2410.14157](https://arxiv.org/abs/2410.14157)

I'd be curious to hear expert perspectives on this.   

It relates to ideas I find attractive:

1. Autoregressive generation is limiting in compositional domains, such as reasoning, planning, math.
2. This explains much of the challenges LLMs have in these domains.
3. Diffusion might be more efficient in these domains: it learns to generate from the general to the specific.  (More like an energy-based model perspective).  
4. It's less likely to get stuck by making specific poor choices, early in its generation process.",marojejian,1geb685,https://reddit.com/r/MachineLearning/comments/1geb685/r_beyond_autoregression_discrete_diffusion_for/,https://www.reddit.com/r/MachineLearning/comments/1geb685/r_beyond_autoregression_discrete_diffusion_for/,2024-10-28 19:42:23,67,0.95,67,0,19,0,0,False,False,True,False,False,Research,self,t3_1geb685
MachineLearning,[R] Machine Learning with Data Streams,"I am just starting my thesis, and I need to learn about machine learning with data streams. I have found a few articles, books, and some courses, but I would appreciate it if you could provide me with some more resources that would help me understand this topic better.

Thank you very much :)",Deepblue597,1geat4u,https://reddit.com/r/MachineLearning/comments/1geat4u/r_machine_learning_with_data_streams/,https://www.reddit.com/r/MachineLearning/comments/1geat4u/r_machine_learning_with_data_streams/,2024-10-28 19:27:19,12,0.83,12,0,2,0,0,False,False,True,False,False,Research,self,t3_1geat4u
MachineLearning,[D] How to Summarize a Research Paper,"I'm not new to reading papers, I have been reading papers for the past 2 years, I even implemented some papers here and there, but I can't say I'm good at summarising them. 

Are there any general tips I should follow when summarising papers? Are there examples of papers and their summaries so I can better understand how paper summarization is done?

Any help is appreciated.",darthJOYBOY,1ge8m3q,https://reddit.com/r/MachineLearning/comments/1ge8m3q/d_how_to_summarize_a_research_paper/,https://www.reddit.com/r/MachineLearning/comments/1ge8m3q/d_how_to_summarize_a_research_paper/,2024-10-28 17:58:49,4,0.56,4,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1ge8m3q
MachineLearning,[P] Can someone give me tips on how to finetune SigLip for image classification ?,"I'm using Siglip from huggingface and trying to fine-tune it on my dataset (which is not that big). It kind of overfits I looked into some normal regularisation techniques and LORA with peft to help. I am open to all suggestions. I am pretty new to this. Thanks in advance!!

I'm starting with this as the base:

    model = AutoModel.from_pretrained(""google/siglip-so400m-patch14-384"")
    processor = AutoProcessor.from_pretrained(""google/siglip-so400m-patch14-384"")",Fantastic-Garlic19,1ge5p0s,https://reddit.com/r/MachineLearning/comments/1ge5p0s/p_can_someone_give_me_tips_on_how_to_finetune/,https://www.reddit.com/r/MachineLearning/comments/1ge5p0s/p_can_someone_give_me_tips_on_how_to_finetune/,2024-10-28 16:00:54,1,0.57,1,0,15,0,0,False,False,True,False,False,Project,self,t3_1ge5p0s
MachineLearning,[D]ended up with a poster in NuerIPS-24,I have a poster in NuerIPS this year through the  journal track(MLRC) along with the main conference papers.I didnt expect this to happen so i hadnt planned/researched about the expenses/funding prior.I already had my visa and conference registration arranged but have no clue about further proceedings of Nuerips and how to fund it(i am an UG junior).If you have already attended NeurIPS before please pour your ideas and experiences.,whit3whistl3,1gdxef5,https://reddit.com/r/MachineLearning/comments/1gdxef5/dended_up_with_a_poster_in_nuerips24/,https://www.reddit.com/r/MachineLearning/comments/1gdxef5/dended_up_with_a_poster_in_nuerips24/,2024-10-28 08:50:47,0,0.32,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gdxef5
MachineLearning,[N] Any Models Lung Cancer Detection?,"I'm a medical student exploring the potential of AI for improving lung cancer diagnosis in resource-limited hospitals (Through CT images). AI's affordability makes it a promising tool, but I'm facing challenges finding suitable pre-trained models or open-source resources for this specific application. I'm kinda avoiding commercial models since the research focuses on low resource-setting. While large language models like GPT are valuable, I'm aware of their limitations in directly analyzing medical images.
So any suggestions? Anything would really help me out, thanks!",Krank910,1gdnra7,https://reddit.com/r/MachineLearning/comments/1gdnra7/n_any_models_lung_cancer_detection/,https://www.reddit.com/r/MachineLearning/comments/1gdnra7/n_any_models_lung_cancer_detection/,2024-10-27 23:01:47,8,0.62,8,0,22,0,0,False,False,True,False,False,News,self,t3_1gdnra7
MachineLearning,Time Based Curriculum Learning [Discussion],"Has anyone explored a variant of curriculum learning for LLMs where 1) information an model is trained on is provided chronologically, or 2) the production date of the training data source is explicitly specified / learned.

To expand on 1) In the case of an LLM this can mean training on wikipedia articles from 2010 first -&gt; wikipedia articles from 2011.

To expand on 2) In this case it can mean a particular token in all text used to train a language model that encodes chronological information. Something like an \[CLS\] token. Another example could be subnetwork/supernetwork with an additional loss trained such that the chronological creation date of training data must be predictable from the text.

Specifically, I want to inspire discussion on how practical these modifications are and their potential benefits. Some concerns include, for us to be able to estimate a chronological ordering of input text, the input text must be sufficiently long and complete. Which could mean traditional truncation strategies used in pre-training would not be viable.",Envoy-Insc,1gdn8hr,https://reddit.com/r/MachineLearning/comments/1gdn8hr/time_based_curriculum_learning_discussion/,https://www.reddit.com/r/MachineLearning/comments/1gdn8hr/time_based_curriculum_learning_discussion/,2024-10-27 22:36:53,6,0.75,6,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gdn8hr
MachineLearning,[D] Demystifying distributed checkpointing,,joygao,1gdkdka,https://reddit.com/r/MachineLearning/comments/1gdkdka/d_demystifying_distributed_checkpointing/,https://expertofobsolescence.substack.com/p/demystifying-distributed-checkpointing,2024-10-27 20:25:57,43,0.88,43,0,0,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/UFofTh0xoAcpDcwdxNkTnqpcGQS22IHWrFPLVU7sCbM.jpg,t3_1gdkdka
MachineLearning,[R] Need endorsement on Arxiv cs.AI,"I'm an independent researcher, my paper have already been published at the [IEEE explorer](https://ieeexplore.ieee.org/document/10189299) I'm looking to upload it to the arxiv I need an endorsement into [CS.AI](http://CS.AI)  
endorsement code: PM3P4K

[https://arxiv.org/auth/endorse?x=PM3P4K](https://arxiv.org/auth/endorse?x=PM3P4K)",benxben13,1gdje64,https://reddit.com/r/MachineLearning/comments/1gdje64/r_need_endorsement_on_arxiv_csai/,https://www.reddit.com/r/MachineLearning/comments/1gdje64/r_need_endorsement_on_arxiv_csai/,2024-10-27 19:42:15,0,0.32,0,0,1,0,0,False,False,True,False,False,Research,self,t3_1gdje64
MachineLearning,"[D] New Interview with Leland McInnes: UMAP, HDBSCAN &amp; the Geometry of Data | Learning from Machine Learning #10","This episode of Learning from Machine Learning explores the intersection of pure mathematics and modern data science with Leland McInnes, the mind behind an ecosystem of tools for unsupervised learning including UMAP, HDBSCAN, PyNN Descent and DataMapPlot. As a researcher at the Tutte Institute for Mathematics and Computing, McInnes has fundamentally shaped how we approach and understand complex data.

Resist the urge to chase the hype, seek a true understanding and really make a difference.
",NLPnerd,1gdimqa,https://reddit.com/r/MachineLearning/comments/1gdimqa/d_new_interview_with_leland_mcinnes_umap_hdbscan/,https://youtu.be/6sSOr2Yaq80?si=MXKgRtdPy0B7D5CM,2024-10-27 19:09:02,20,0.96,20,0,1,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/Cf39JgvuFZhkY5VNM8pXXJtCOGsVLn7L-mQ80nA_Qzs.jpg,t3_1gdimqa
MachineLearning,RAGs - A visual breakdown of current research! [D],Hello! Just sharing a video I made covering the major components in RAG and the leading techniques researchers/engineers are using to make powerful LLM pipelines. Hope yall enjoy! I ,AvvYaa,1gdfny4,https://reddit.com/r/MachineLearning/comments/1gdfny4/rags_a_visual_breakdown_of_current_research_d/,https://youtu.be/OHh_SByRYmQ,2024-10-27 17:01:48,0,0.19,0,0,0,0,0,False,False,False,False,False,Discussion,https://a.thumbs.redditmedia.com/T7IFghxDAJ7vB0tO3Xkyst9PeWGAEM9cdhikKmDzSP0.jpg,t3_1gdfny4
MachineLearning,Cross Validation with Feature Engineering [D],"Could you use cross validation to inform addition/subtraction of features? Or solely for hyperparameter tuning? If both, would you typically select your features using cross validation, and subsequently freeze your features and run cross validation again to tune your hyperparameters? Trying to understand what an iterative process would look like incorporating both. ",Secret_Valuable_Yes,1gdbgzt,https://reddit.com/r/MachineLearning/comments/1gdbgzt/cross_validation_with_feature_engineering_d/,https://www.reddit.com/r/MachineLearning/comments/1gdbgzt/cross_validation_with_feature_engineering_d/,2024-10-27 13:54:36,3,0.71,3,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gdbgzt
MachineLearning,[D] Seeking Advice on Improving Robustness of Instacart Basket Analysis Models Dominated by Single Feature. ,"

Hi everyone,

I’m working on the Instacart Basket Analysis project on Kaggle, focused on predicting repeat purchases. I engineered a feature called `num_of_ord_purch_p_prod`, representing the number of times a specific user has purchased a particular product. While this feature is highly predictive, it heavily dominates feature importance across models, raising concerns about potential over-reliance and model robustness. Here's a link to my feature engineering notebook for more details: [Notebook](https://www.kaggle.com/code/deepsutariya/instacart-dataset-transformer-cv2/edit/run/202830778).

**Project Details**:

1. **Class Balance**: The target class (reordered status) is balanced.
2. **Evaluation Approach**: I tested the models on two separate test sets, each containing only the latest orders for a more time-based validation.
3. **Model Performance**:
   * My best LightGBM model achieved an AUC of 0.85, with **1000x** gain importance on `num_of_ord_purch_p_prod` over the second-best feature.
   * My best XGBoost model achieved an AUC of 0.72, with **20x** gain importance for the same feature over second best feature.
4. **Feature Importance**: SHAP analysis confirmed the high importance of `num_of_ord_purch_p_prod`, even after applying regularization techniques. In XGBoost, regularization reduced its dominance but also lowered AUC.

**Features in Use**: Beyond `num_of_ord_purch_p_prod`, I’ve included features such as:

* `frequency_of_reorder` (frequency of product reorders by user)
* `product_mean_of_position` (average product position in user’s orders)
* `prob_of_being_reordered` (probability of reorder based on past purchases)
* `count_ord_no_prev_purchased_items` (count of new items in each order)
* Distribution counts of product orders per day of the week, among others.

**Request for Advice**: Given the dominance of a single feature, I’m looking for suggestions to enhance the model’s robustness and generalization with descent  auc ( &gt; 0.80).",ds_reddit1,1gd9q9f,https://reddit.com/r/MachineLearning/comments/1gd9q9f/d_seeking_advice_on_improving_robustness_of/,https://www.reddit.com/r/MachineLearning/comments/1gd9q9f/d_seeking_advice_on_improving_robustness_of/,2024-10-27 12:21:58,0,0.46,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gd9q9f
MachineLearning,[R] Does anyone have wikitext-2-v1.zip dataset file or an alternative link to download it?,"Hello everyone,  
I'm trying to reproduce an old experiment that uses the `wikitext-2` dataset, and it relies on `torchtext` to import it. However, it seems the link from which the dataset is downloaded is no longer working. Here’s the link that’s broken: [https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip)

Here’s the relevant `torchtext` source code for reference: [https://pytorch.org/text/0.12.0/\_modules/torchtext/datasets/wikitext2.html](https://pytorch.org/text/0.12.0/_modules/torchtext/datasets/wikitext2.html)

Does anyone know an updated link or a workaround to get this dataset? Thanks!

Edit:  
Ans: I got the dataset via Kaggle. Link: [https://www.kaggle.com/datasets/rohitgr/wikitext](https://www.kaggle.com/datasets/rohitgr/wikitext)",reddo-lumen,1gd36p9,https://reddit.com/r/MachineLearning/comments/1gd36p9/r_does_anyone_have_wikitext2v1zip_dataset_file_or/,https://www.reddit.com/r/MachineLearning/comments/1gd36p9/r_does_anyone_have_wikitext2v1zip_dataset_file_or/,2024-10-27 04:35:34,2,0.63,2,0,4,0,0,False,False,True,False,False,Research,self,t3_1gd36p9
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1gd0v8r,https://reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/,2024-10-27 02:15:12,38,0.89,38,0,44,0,0,False,False,True,False,False,Discussion,self,t3_1gd0v8r
MachineLearning,[D] Train on full dataset after cross-validation? Semantic segmentation,"I am currently working on a semantic segmentation project of oat leaf disease symptoms. The dataset is quite small, 16 images. Due to time constraints, I won't be able to extend this.

I am currently training 3 models, 3 backbones, and 3 losses--using 5-fold cross validation and grid search.

Once this is done, I plan to then run cross validation on a few different levels of augmentations per image.

My question is this:

Once I have established the best model, backbone, loss, and augmentation combination, can I train on the full dataset since it is so small? If I can do this, how do I know when to stop training to prevent overfitting but still adequately learn the data?

I have attached an image of some results so far.

https://preview.redd.it/sx394c58l5xd1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=3cefbf5c84bf3fbf48936c47810c4e3039dcb410

Thanks for any help you can provide!",Entire_Commission169,1gct22r,https://reddit.com/r/MachineLearning/comments/1gct22r/d_train_on_full_dataset_after_crossvalidation/,https://www.reddit.com/r/MachineLearning/comments/1gct22r/d_train_on_full_dataset_after_crossvalidation/,2024-10-26 19:38:09,23,0.85,23,0,30,0,0,False,False,True,False,False,Discussion,self,t3_1gct22r
MachineLearning,[P] Shape-restricted regression with neural networks,"Some time ago at work we had to enforce that our model learns an increasing function of a feature. For example, the probability of winning an auction as a function of the bid should increase. Recently, I encountered the paper [https://arxiv.org/abs/2209.04476](https://arxiv.org/abs/2209.04476) on regression with shape-restricted functions, and wanted to make it a bit more tangible, with actual code that trains such a model.

So it resulted in a blog post: [https://alexshtf.github.io/2024/10/14/Shape-Restricted-Models.html](https://alexshtf.github.io/2024/10/14/Shape-Restricted-Models.html)  
There's also a notebook with the accompanying code: [https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/shape\_constrained\_models.ipynb](https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/shape_constrained_models.ipynb)

I used to work on ads quite a lot .So such models seem useful in this industry - predicting the probability of winning an ad auction given the bid. I hope it's also useful elsewhere.

So I hope you'll enjoy it! It's a big 'mathy', but you know, it can't be otherwise.",alexsht1,1gcpl03,https://reddit.com/r/MachineLearning/comments/1gcpl03/p_shaperestricted_regression_with_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1gcpl03/p_shaperestricted_regression_with_neural_networks/,2024-10-26 16:58:41,48,0.93,48,0,9,0,0,False,False,True,False,False,Project,self,t3_1gcpl03
MachineLearning,[P] Real-Time Character Animation on Any Device,"I recently came across this paper [MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling](https://menyifang.github.io/projects/MIMO/index.html) by Alibaba and it was really interesting. After skimming through the paper, I thought, 'Hey, this workflow could be replicated using some open-source tools!' I managed to create a plausible system that can run in real-time on-device at \~10fps and mind you this was on a potato laptop 8 GB of RAM and 4 GB of VRAM.

[Original Video](https://i.redd.it/pavxor7mg4xd1.gif)

[Reconstrued Video ](https://i.redd.it/ler3v5qvd4xd1.gif)

The current workflow looks something like this -&gt;  
1. I created a unity app using [Tracking4All](https://www.tracking4all.com/), which can take an input from a webcam and generate an animated pose using Mediapipe.  
2. Next, I sent these generated images to a Python server, which receives the original frame, the animated character, and a mask of the person from the Mediapipe pose.  
3. Fianlly using [MI-GAN](https://github.com/Picsart-AI-Research/MI-GAN), I was able to remove the person in real-time.

  
This project currently have a few flaws  
1. The MI-GAN model, while fast, is the main bottleneck. I tried other algorithms available in [OpenCV](https://docs.opencv.org/4.x/df/d3d/tutorial_py_inpainting.html) but they were even worse and slow (\~1fps).  
2. The character resizing isn’t always accurate, though this can be easily adjusted in Unity.  
3. Occlusion issues remain a challenge.

Additionally, it’s worth noting that the Tracking4All package requires a license, which may limit accessibility.

Are there any algorithms available that can perform inpainting in real-time on various devices (mobile, Windows, Mac, and Linux)?

The goal of this project is to create an end-to-end workflow that anyone can run on any device. This has many applications in AR and VFX! whats your opinion on this and any things I should implement next on this?",Jazzlike-Shake4595,1gco234,https://reddit.com/r/MachineLearning/comments/1gco234/p_realtime_character_animation_on_any_device/,https://www.reddit.com/r/MachineLearning/comments/1gco234/p_realtime_character_animation_on_any_device/,2024-10-26 15:49:38,26,0.85,26,0,0,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/8hXdNgld_4lq-yZOcbRiub9qWBJJE7OkcbxE0OPuoJM.jpg,t3_1gco234
MachineLearning,[D] KV caching for varying length texts - help please 🙏,,themathstudent,1gclcmk,https://reddit.com/r/MachineLearning/comments/1gclcmk/d_kv_caching_for_varying_length_texts_help_please/,https://discuss.huggingface.co/t/kv-caching-for-varying-length-texts/114092,2024-10-26 13:41:32,0,0.46,0,0,0,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/Jrs_IeFF6tQOwyfD1Sa1prCMyhnGV2xbyJLGY09Q1bM.jpg,t3_1gclcmk
MachineLearning,"[R] Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CVPR, ICML, etc.","Hey everyone,

Our group, **Vision and Language Group, IIT Roorkee,** recently got three workshop papers accepted at NeurIPS workshops! 🚀 We’ve also set up a website 👉 [VLG](https://vlgiitr.github.io/), featuring other publications we’ve worked on, so our group is steadily building a portfolio in ML and AI research. Right now, we’re collaborating on several work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.

That said, we have even more ideas we’re excited about. Still, a few of our main limitations have been access to proper guidance and funding for GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interested in working together, we’d love to explore intersections in our fields of interest and any new ideas you might bring to the table!

If you have resources available or are interested in discussing potential collaborations, please feel free to reach out! Looking forward to connecting and building something impactful together! Here is the link for our Open Slack 👉 [Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)",vlg_iitr,1gcjfz1,https://reddit.com/r/MachineLearning/comments/1gcjfz1/r_looking_for_collaborations_on_ongoing/,https://www.reddit.com/r/MachineLearning/comments/1gcjfz1/r_looking_for_collaborations_on_ongoing/,2024-10-26 11:59:45,5,0.54,5,0,1,0,0,False,False,True,False,False,Research,self,t3_1gcjfz1
MachineLearning,"[D] Do anyone know, how Eleven labs is designing the voice from prompt? I want to generate a new voice from prompt not voice cloning. ",,usama__01,1gcej8u,https://reddit.com/r/MachineLearning/comments/1gcej8u/d_do_anyone_know_how_eleven_labs_is_designing_the/,https://i.redd.it/59re7yvuj1xd1.jpeg,2024-10-26 06:02:26,14,0.67,14,0,8,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/rmGSW6ePTKZi3yvov45k7RfriWiFcsHLySfR8jMAbOQ.jpg,t3_1gcej8u
MachineLearning,[Project] Open source video indexing/labelling/tag generation tool.,"Guys, I'm looking for an open source tool or any repo that can help me generate tags for video to categorize multiple videos and do further analysis.

An equivalent of what I want is Azure AI clvideo inxer, but If there was such a open source tool, it will solve the problem.",jokingwizard,1gccyhp,https://reddit.com/r/MachineLearning/comments/1gccyhp/project_open_source_video_indexinglabellingtag/,https://www.reddit.com/r/MachineLearning/comments/1gccyhp/project_open_source_video_indexinglabellingtag/,2024-10-26 04:18:33,19,1.0,19,0,4,0,0,False,False,True,False,False,Project,self,t3_1gccyhp
MachineLearning,[P] Accessible pre-trained video embedding models,"I'm relatively new to this field so my apologies if I'm missing some information or if there is a better place for this post. I've been working on a project trying to implement semantic search and RAG for long form videos and am trying to understand the current state of video embedding models.

My problem is that I'm finding video embedding solutions much more difficult to come by than text or image embedding models. There doesn't seem to be anything accessible on OpenAI or Bedrock. There are some open source models but a much smaller variety than other embedding models.

A solution that I have seen certain people propose is sampling frames from the video, and generating image embeddings from those frames. However, I worry I'll lose a lot of context from the audio as well as any motion happening in the video. 

The company that seems to be the farthest ahead in this field from my research is Twelve Labs. It looks like they have a pretty rich suite of tools around semantic video search but I would ideally use the embeddings in my own vector database that also supports full text search and hybrid search. Twelve Labs has an embedding API but it is currently in private beta so I'm not quite sure where to go next. Anyone have any suggestions or insights on how to approach this problem?",PalpablePatsy247,1gc4vm0,https://reddit.com/r/MachineLearning/comments/1gc4vm0/p_accessible_pretrained_video_embedding_models/,https://www.reddit.com/r/MachineLearning/comments/1gc4vm0/p_accessible_pretrained_video_embedding_models/,2024-10-25 21:11:30,5,0.78,5,0,2,0,0,False,False,True,False,False,Project,self,t3_1gc4vm0
MachineLearning,[D] ML accelerated with TEE + Federated Learning ,"https://www.hcinnovationgroup.com/clinical-it/learning-health-systems-research/news/55130702/dana-farber-researchers-address-oncology-data-sharing-issues

Anyone see this? The research lead describes it as ""plug and play"". Big if true.

I've been seeing a lot of discussion from Goog, MS, Intel about TEE/enclaves for secure ML, but this is the first deployment I've seen AND they're also using Federated Learning.
",thebiztechguy,1gbxuhd,https://reddit.com/r/MachineLearning/comments/1gbxuhd/d_ml_accelerated_with_tee_federated_learning/,https://www.reddit.com/r/MachineLearning/comments/1gbxuhd/d_ml_accelerated_with_tee_federated_learning/,2024-10-25 16:06:23,4,0.67,4,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gbxuhd
MachineLearning,[P] ChemProp issues with large datasets,"Hey all, I'm working on testing a ChemProp model with a large molecule dataset (9M smiles). I'm coding in Python on a local machine, and I've already trained and saved a classification model using a smaller training dataset. According to this GitHub issue [https://github.com/chemprop/chemprop/issues/858](https://github.com/chemprop/chemprop/issues/858) , looks like there are definitely limitations to what can be loaded at one time. I'm trying to get batching setup for predicting (according to what was described in the GitHub issue), but I'm having issues getting the MoleculeDatapoints in my data loader setup correctly so that this batch code will run: 

    predictions = []
    for batch in dataloader:
        with torch.inference_mode():
            trainer = pl.Trainer(
                logger=None,
                enable_progress_bar=True,
                accelerator=""cpu"",
                devices=1
            )
    
            batch_preds = trainer.predict(mpnn, batch)
            
            batch_smiles = [datapoint.molecule[0] for datapoint in batch] 
            batch_predictions = list(zip(batch_smiles, batch_preds))  # Pair SMILES with predictions
            predictions.extend(batch_predictions)

Does anyone else have experience using chemprop with large datasets, or have any good code examples to refer to? This is for a side project I'm consulting on - just trying to get my code to work! TIA",Advanced_Rest_2667,1gbelul,https://reddit.com/r/MachineLearning/comments/1gbelul/p_chemprop_issues_with_large_datasets/,https://www.reddit.com/r/MachineLearning/comments/1gbelul/p_chemprop_issues_with_large_datasets/,2024-10-24 22:00:24,2,1.0,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1gbelul
MachineLearning,[R] Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss,"*abstract*

Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.",RajonRondoIsTurtle,1gbvapp,https://reddit.com/r/MachineLearning/comments/1gbvapp/r_breaking_the_memory_barrier_near_infinite_batch/,https://arxiv.org/abs/2410.17243,2024-10-25 14:16:28,127,0.95,127,0,23,0,0,False,False,False,False,False,Research,default,t3_1gbvapp
MachineLearning,Ethics concerns and Google [D],"Apologies if this isn't the right place for this facet of ML, but it didn't seem against the rules.

I recently participated in an Alphabet human data research study used to evaluate AI agents and models.

Without going further into the details, the structure of the study felt very ethically questionable. The agreement said if there were any concerns, to contact HuBREC, human behavioural research ethics committee.

However, their email provided in the agreement hubrec@google.com does not exist and I have no point of contact at all short of looking up past academic talks and cold emailing people. 

I am having a lot of difficulty searching for next steps as there is no other contact information I can use except for that email. I do know that Google has fired AI ethics researchers in recent memory, and that this topic never seems to be taken seriously. It seems like a bad look for an on-going study to point you to a committee that doesn't seem to exist.",chaneg,1gbblsc,https://reddit.com/r/MachineLearning/comments/1gbblsc/ethics_concerns_and_google_d/,https://www.reddit.com/r/MachineLearning/comments/1gbblsc/ethics_concerns_and_google_d/,2024-10-24 19:49:53,116,0.89,116,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1gbblsc
MachineLearning,[P] Fully Bayesian Logistic Regression with Objective Prior,"I've been working on a project that implements deterministic, fully Bayesian logistic regression with reference prior for the case of a single weight.

[https://github.com/rnburn/bbai](https://github.com/rnburn/bbai)

In the single parameter case, the reference prior works out to be the same as [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior), which is given by

https://preview.redd.it/alskcnddsqwd1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=0d3dc78ae15122d21c78dcc2b7170b34c4bec88b

One of the main justifications for Jeffreys prior as an objective prior (or noninformative prior) for single parameter models is that it has asymptotically optimal frequentist matching coverage (see §0.2.3.2 of \[[1](https://www.uv.es/~bernardo/OBayes.pdf)\] and \[2\]).

*Note: The situation becomes more complicated for multi-parameter models, and this is where you will see reference priors and Jeffreys prior produce different results (see §0.2.3.3 of \[*[*1*](https://www.uv.es/~bernardo/OBayes.pdf)*\]).*

Frequentist matching coverage is something that can be easily measure by simulation. Here's a brief snippet of python code that shows how:

    from bbai.glm import BayesianLogisticRegression1
    import numpy as np
    
    # Measure frequentist matching coverage
    # for logistic regression with reference prior
    def compute_coverage(x, w_true, alpha):
        n = len(x)
        res = 0
    
        # iterate over all possible target values
        for targets in range(1 &lt;&lt; n):
            y = np.zeros(n)
            prob = 1.0
            for i in range(n):
                y[i] = (targets &amp; (1 &lt;&lt; i)) != 0
                mult = 2 * y[i] - 1.0
                prob *= expit(mult * x[i] * w_true)
            
            # fit a posterior distribution to the data
            # set x, y using the reference prior
            model = BayesianLogisticRegression1()
            model.fit(x, y)
            
            # does a two-tailed credible set of probability mass
            # alpha contain w_true?
            t = model.cdf(w_true)
            low = (1 - alpha) / 2
            high = 1 - low
            if low &lt; t and t &lt; high:
                res += prob
        return res

Given a design matrix X, w\_true, and a target probability mass alpha, the code computes the frequentist matching coverage for Jeffreys prior. If I fix alpha to 0.95, draw X from a uniform distribution between \[-1, 1\], and try some different values of w\_true and n, I get these results:

[Frequentist coverage matching results for Jeffreys prior](https://preview.redd.it/s9mqe0mpuqwd1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=eb8bef7a376c22b426510de7392a73e8bb759f29)

We can see that the coverages are all fairly close to the target alpha. 

Notebook with full experiment: [https://github.com/rnburn/bbai/blob/master/example/22-bayesian-logistic1-coverage.ipynb](https://github.com/rnburn/bbai/blob/master/example/22-bayesian-logistic1-coverage.ipynb)

# Example: Election Polling

Suppose we want to make a simple polls-only model for predicting whether a presidential candidate will win a state given their lead in state-wide polls. Modeling the problem with single variable logistic regression, we have

https://preview.redd.it/wecqyq7hwqwd1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=7ff6b67985b94d71fcfd355ef7003092fe539466

Using the FiveThirtyEight results from 2020 (\[3\]) as training data, we can fit a posterior distribution to w:



[FiveThirtyEight polling results for 2020 \(\[3\]\). Blue indicates a state where Biden led, red Indicates a state where Trump led. A dot indicates that the leading candidate won the state and an X indicates the leading candidate lost the state.](https://preview.redd.it/2na8bjdvwqwd1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=7cf54a182aa689095158754fe3531a870fa0252c)

Here's how we can fit a model to the data set

    from bbai.glm import BayesianLogisticRegression1
    
    x_2020, y_2020 = # data set for 2020 polls
    
    # We specify w_min so that the prior on w is restricted
    # to [0, ∞]; thus, we assume a lead in polls will never 
    # decrease the probability of the candidate winning the
    # state
    model = BayesianLogisticRegression1(w_min=0)
    
    model.fit(x_2020, y_2020)

We can then get a sense for what it says the accuracy of state-wide polls by looking at percentiles for the prediction posterior distribution for a lead of 1% in polls.

    pred = model.predict(1) # prediction for a 1% polling lead
    
    for pct in [.5, .25, .5, .75, .95]:
        # Use the percentage point function (ppf) to
        # find the value of p where
        #   integrate_0^p π(p | xp=1, x, y) dp = pct
        # Here p denotes the probability of the candidate
        # winning the state when they are leading by +1%.
        print(pct, ':', pred.ppf(pct))

Produces the result

[Prediction posterior distribution for the probability of a candidate winning a state given a lead of 1&amp;#37; in polling. The figure also shows the 5-th, 25-th, 50-th, 75-th, and 95-th percentiles.](https://preview.redd.it/lazu8vxfyqwd1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=e51030b1c635493fb7cfd2d38998b2fda20ff67d)

Notebook for the full example: [https://github.com/rnburn/bbai/blob/master/example/23-election-polls.ipynb](https://github.com/rnburn/bbai/blob/master/example/23-election-polls.ipynb)

# References

\[1\]: Berger, J., J. Bernardo, and D. Sun (2022). [Objective bayesian inference and its relationship to frequentism.](https://www.uv.es/~bernardo/OBayes.pdf?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions)

\[2\]: Welch, B. L. and H. W. Peers (1963). [On formulae for confidence points based on integrals of weighted likelihoods.](https://academic.oup.com/jrsssb/article-abstract/25/2/318/7035245?redirectedFrom=PDF&amp;utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions)*Journal of the Royal Statistical Society Series B-methodological 25*, 318–329. 

\[3\]: 2020 FiveThirtyEight state-wide polling averages. [*https://projects.fivethirtyeight.com/polls/president-general/2020/*](https://projects.fivethirtyeight.com/polls/president-general/2020/?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-interpret-election-polls)



  
",rnburn,1gb9qxj,https://reddit.com/r/MachineLearning/comments/1gb9qxj/p_fully_bayesian_logistic_regression_with/,https://www.reddit.com/r/MachineLearning/comments/1gb9qxj/p_fully_bayesian_logistic_regression_with/,2024-10-24 18:31:39,65,0.96,65,0,11,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/zPLn1cAuj8HC7l6GlmPcXJvfeglDqRjlBiviZWWIRVI.jpg,t3_1gb9qxj
MachineLearning,[R] Benchmark GGUF model with ONE line of code,"Hi Everyone!

👋We just launched an **open-sourced too**l to benchmark **GGUF model**s with a single line of code. [GitHub Link](https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval)



**Motivations:**

GGUF quantization is crucial for running models locally on devices, but quantizations can dramatically affect model's performance. It's essential to test models post-quantization (how benchmark comes in clutch). But we noticed a couple of challenges:

* No easy, fast way to benchmark quantized GGUF models locally or on self-hosted servers.
* GGUF quantization evaluation results in the existing benchmarks ([github.com/terryyz/llm-benchmark](https://github.com/terryyz/llm-benchmark)) are inconsistent, showing lower scores than the official results from model developers.\\



**Our Solution:**  
We built a tool that:

* **Benchmarks GGUF models** with **one line of code**.
* Supports **multiprocessing** and **8 evaluation tasks**.
* In our testing, it's the **fastest benchmark** for GGUF models available.



**Example:**

Benchmark Llama3.2-1B-Instruct Q4\_K\_M quant on the ""ifeval"" dataset for general language understanding. It took 80 minutes on a 4090 with 4 workers for multiprocessing.  
  
1. Type in terminal  
  
`nexa eval Llama3.2-1B-Instruct:q4_K_M --tasks ifeval --num_workers 4`

https://i.redd.it/6xh52gkttqwd1.gif

2. Results:

https://preview.redd.it/78aek4a4tqwd1.png?width=1475&amp;format=png&amp;auto=webp&amp;s=742a1379809244010f409a29298709f0575ae772

We started with **text models** and plan to expand to more on-device models and modalities. Your feedback is welcome! If you find this useful, feel free to leave a star on GitHub 🔗: [https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval](https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval)",AlanzhuLy,1gb8yhq,https://reddit.com/r/MachineLearning/comments/1gb8yhq/r_benchmark_gguf_model_with_one_line_of_code/,https://www.reddit.com/r/MachineLearning/comments/1gb8yhq/r_benchmark_gguf_model_with_one_line_of_code/,2024-10-24 17:58:48,13,0.79,13,0,6,0,0,False,False,True,False,False,Research,https://a.thumbs.redditmedia.com/RV0biJYKmq3bUGIVdLi-NNhDvUhlu_a5oBy8w_FAxk0.jpg,t3_1gb8yhq
MachineLearning,[R] How Google Overcame Training Data Issues For Medical AI,"TLDR; They turned 3D images into vector embeddings, saving preprocessing time and reducing training data sizes.

Over 70 million Computed Tomography exams are conducted each year in the USA alone, but that data wasn't effective for Google's training.  
Google Research had embedding APIs for radiology, digital pathology, and dermatology-- but all of these are limited to 2D imaging. Physicians typically rely on 3D imaging for more complex diagnostics.

Why?

CT scans have a 3D structure, meaning larger file sizes, and the need for more data than 2D images.  
Looking through engineering blogs, they just released something to finally work with 3D medical data. It's called CT Foundation-- it turns CT scans to small and information-rich embeddings to train AI for cheap

How?

Exams are taken in standard medical imaging format (DICOM) and turned into vectors with 1,408 values— key details captured include organs, tissues, and abnormalities.

These concise embeddings can then be used to train AI models, such as logistic regression or multilayer perceptrons, using much less data compared to typical models that take 3D images and require preprocessing. The final classifier is smaller, reducing compute costs so training is more efficient and affordable.

Final Results?

CT Foundation was evaluated for data efficiency across seven tasks to classify:  
\- intracranial hemorrhage  
\- chest and heart calcifications  
\- lung cancer prediction  
\- suspicious abdominal lesions  
\- nephrolithiasis  
\- abdominal aortic aneurysm, and  
\- body parts

Despite limited training data, the models achieved over 0.8 AUC on all but one of the more challenging tasks, meaning a strong predictive performance and accuracy.  
The model, using 1,408-dimensional embeddings, required only a CPU for training, all within a Colab Python notebook.

TLDR;

Google Research launched a tool to effectively train AI on 3D CT scans, by converting them into compact 1,408-dimensional embeddings for efficient model training. It's called CT Foundation, requires less data and processing, and achieved over 0.8 AUC in seven classification tasks, demonstrating strong predictive performance with minimal compute resources.  
There's a colab notebook [available](https://colab.research.google.com/github/Google-Health/imaging-research/blob/master/ct-foundation/CT_Foundation_Demo.ipynb).

**PS**: Learned this by working on a personal project to keep up with tech-- if you'd like to know more, check [techtok today](https://techtok.today/)",TechTok_Newsletter,1gb7twh,https://reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/,https://www.reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/,2024-10-24 17:11:45,184,0.91,184,0,28,0,0,False,False,True,False,False,Research,self,t3_1gb7twh
MachineLearning,[R] Paper summaries for some of our papers that recently got accepted in NeurIPS,"Hey everyone, here is the list of papers by our groups that got accepted recently in NeurIPS 2024; It is a proud moment for us as an all-UG group; all the papers were published without any external support from the academia; here is a summary of our papers. We hope this inspires others to pursue AI and look into research as a perspective where we can work together, and all you require is the right guidance (not even necessarily a PhD or a professor). If you find these papers useful and want to working/collabrating with us, feel free to connect with us!

* Give me a hint: Can LLMs take a hint to solve math problems? 👉 [Arxiv link](https://arxiv.org/abs/2410.05915)
   * We propose improving LLM performance on advanced math problems using ""hints,"" inspired by human pedagogy. We also test the model's robustness to incorrect hints. Our approach is evaluated on various LLMs using diverse problems from the MATH dataset, comparing it with one-shot, few-shot, and chain of thought prompting.
* Attention Shift: Steering AI Away from Unsafe Content 👉 [Arxiv link](https://arxiv.org/abs/2410.04447)
   * This study explores methods to restrict unsafe content in generative models. We propose a novel training-free approach using attention reweighing to remove unsafe concepts during inference. Our method is compared to existing techniques, evaluated on direct and adversarial jailbreak prompts. We also discuss potential causes, limitations, and broader implications.
* Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images 👉 [Arxiv link](https://arxiv.org/abs/2406.12592v1)
   * This paper extends the study of concept ablation in pre-trained models, as introduced by Kumari et al. (2022). We reproduce results from various concept ablation techniques and propose a novel variant, ""trademark ablation,"" to address branded elements in model outputs. We also analyze the model's limitations, behavior under ablation leakage prompts, and performance degradation on unrelated concepts.

**The Vision Language Group at IIT Roorkee** has compiled an excellent repository of **comprehensive summaries** for deep learning papers from top conferences like **NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down key papers in computer vision, NLP, and machine learning—perfect if you want to stay updated without diving deep into the full papers.",vlg_iitr,1gb74j6,https://reddit.com/r/MachineLearning/comments/1gb74j6/r_paper_summaries_for_some_of_our_papers_that/,https://www.reddit.com/r/MachineLearning/comments/1gb74j6/r_paper_summaries_for_some_of_our_papers_that/,2024-10-24 16:42:12,72,0.84,72,0,21,0,0,False,False,True,False,False,Research,self,t3_1gb74j6
MachineLearning,[R] Looking for Book and Article Recommendations on AI’s Impact on Work and Job Meaningfulness,"Hi everyone!

I’m conducting research on how Artificial Intelligence (AI) is influencing work environments, particularly in relation to **job meaningfulness and professional identity**. My focus is on sectors like the **automotive industry and IT**, where AI is being rapidly integrated into daily operations. I’m especially interested in how AI affects job autonomy, task structure, and the broader implications for employees' sense of purpose and identity in their work.

I’m looking for **recommendations** on academic books, articles, or research papers that explore:

* The impact of AI on **job roles**, **autonomy**, and **work meaningfulness**.
* How AI influences **professional identity** and how workers perceive their role within organizations.
* **Ethical considerations** around AI in the workplace, especially related to employee well-being and fairness.
* Insights from theories like the **Job Characteristics Model** or similar frameworks that could be applied to AI’s influence on job design.
* Case studies or empirical research focused on **AI’s integration in specific industries** (automotive, IT, or similar sectors).

I’d be very grateful for any suggestions, whether they are foundational texts or recent research. Thank you in advance for pointing me in the right direction!",emillindstrom,1gb3kuk,https://reddit.com/r/MachineLearning/comments/1gb3kuk/r_looking_for_book_and_article_recommendations_on/,https://www.reddit.com/r/MachineLearning/comments/1gb3kuk/r_looking_for_book_and_article_recommendations_on/,2024-10-24 14:12:32,0,0.47,0,0,2,0,0,False,False,True,False,False,Research,self,t3_1gb3kuk
MachineLearning,[D] Neural network from scratch blog,"Hey I recently started my ML journey and was learning about neural network and wanted to implement it from scratch so I searched on YouTube and found a video. Then I implemented it and modified a bit and also wrote a blog on medium on it. I know it is a very basic thing and I might have many mistakes as it was my first blog and first time doing something like that. So I just wanted to know opinions of you all and suggestions if any.

Blog link-https://medium.com/@pankajgoyal4152/understanding-neural-networks-by-building-one-from-scratch-a-beginners-journey-3a11617313a4

Can you read the blog and tell?

I hope this is what ""learn in public"" is called right?

I would greatly appreciate anything like suggestions or anything from you so feel free. It might help me a lot.",PixelPioneer-1,1gb2iwp,https://reddit.com/r/MachineLearning/comments/1gb2iwp/d_neural_network_from_scratch_blog/,https://www.reddit.com/r/MachineLearning/comments/1gb2iwp/d_neural_network_from_scratch_blog/,2024-10-24 13:23:11,0,0.23,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gb2iwp
MachineLearning,[D] How should I set the format for the LLM's responses?,"New to fine-tuning LLM

use [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

  
e.g.:

Hey,Bhatgpt Can you fix the grammar errors in this sentence?

“If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.”

 

# Desired format:

**Grammatical Error Correction:**

Your sentence has a minor grammatical error. It should be: ""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""

**Different scenario：**

Formal：

""Should your task align with the training parameters of the checkpoint model, you may proceed to utilize LlamaForCausalLM for predictions without additional training requirements.""

Written English：

In instances where task similarity corresponds to the checkpoint model's training parameters, immediate implementation of LlamaForCausalLM for predictive purposes can be executed without supplementary training protocols.

Colloquial：

Hey, if your task matches what the model was originally trained for, you can totally just plug and play with LlamaForCausalLM - no extra training or anything!",Ggboysformnowhere,1gayuhw,https://reddit.com/r/MachineLearning/comments/1gayuhw/d_how_should_i_set_the_format_for_the_llms/,https://www.reddit.com/r/MachineLearning/comments/1gayuhw/d_how_should_i_set_the_format_for_the_llms/,2024-10-24 09:52:26,0,0.33,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gayuhw
MachineLearning,[P] Multi-Agent AI based Human Resource Information System[HRIS],"As the title suggests,  
I was looking into the prospective idea of building a MAS based HRIS, I am looking into suggestions from other seasoned folks or anyone that how can I build this idea in a good product for my organization  
What kind of services or any sort of compelling use-cases, etc  
I am looking forward to hearing from all the folks",Not_so_sure_paradox9,1gaylfx,https://reddit.com/r/MachineLearning/comments/1gaylfx/p_multiagent_ai_based_human_resource_information/,https://www.reddit.com/r/MachineLearning/comments/1gaylfx/p_multiagent_ai_based_human_resource_information/,2024-10-24 09:33:34,0,0.38,0,0,0,0,0,False,False,True,False,False,Project,self,t3_1gaylfx
MachineLearning,"[D] Wrapper for OpenAI's Whisper ""library""/""framework"" etc? ","I have used the command line version of OpenAI's Whisper since it was released but it doesn't offer all the options the Whisper-""framework"" (or whatever you call it) contains. There must be someone who has written a ""wrapper"" for this purpose, mustn't it? But I can't find anything on Google. Can you recommend something?

I have 20 000 files, from 10 seconds to several hours long, that I want to transcribe as efficient and with as high quality as possible (I prioritize quality over efficiency. Currently I use the command line client with the large v3-model).",la-grave,1gayjwu,https://reddit.com/r/MachineLearning/comments/1gayjwu/d_wrapper_for_openais_whisper_libraryframework_etc/,https://www.reddit.com/r/MachineLearning/comments/1gayjwu/d_wrapper_for_openais_whisper_libraryframework_etc/,2024-10-24 09:30:38,0,0.25,0,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1gayjwu
MachineLearning,[R] Concatenating additional input features to a ViT model,"Hi everyone,

I'm exploring ways to integrate additional input features (such as tabular data or other non-image features) into the Vision Transformer (ViT) architecture alongside image inputs.

Has anyone tried this approach, or does anyone know of any research papers, blogs, or references that explore this? I'm particularly interested in how to integrate these additional inputs with the image tokens in a meaningful way.

Thanks in advance for any help or pointers!",kernel_KP,1gayiww,https://reddit.com/r/MachineLearning/comments/1gayiww/r_concatenating_additional_input_features_to_a/,https://www.reddit.com/r/MachineLearning/comments/1gayiww/r_concatenating_additional_input_features_to_a/,2024-10-24 09:28:43,4,0.75,4,0,6,0,0,False,False,True,False,False,Research,self,t3_1gayiww
MachineLearning,[R] Disentanglement in Tabular Domain,"Have anyone here worked on the topic of disentanglement in tabular data? Is that possible or does it make sense? What are the criteria/evaluation metrics for that in tabular data, where there is no quantifiable groundtruth factors?

Background: I was tasked to research on this topic, found out that disentanglement is common in field such as Images, mostly using VAE. There are some works applying VAE on tabular data, in particular for fairness or synthetic data generation purpose, however they do not call it ""disentanglement"" and also do not give any evaluation related to disentanglement on tabular data.

",QT-NTU,1gay549,https://reddit.com/r/MachineLearning/comments/1gay549/r_disentanglement_in_tabular_domain/,https://www.reddit.com/r/MachineLearning/comments/1gay549/r_disentanglement_in_tabular_domain/,2024-10-24 08:59:51,4,0.83,4,0,2,0,0,False,False,True,False,False,Research,self,t3_1gay549
MachineLearning,[D] Transformers are a type of CNN,"https://arxiv.org/abs/2309.10713

I was randomly googling Dynamic Convolutions since I thought they were cool and found this paper that shows transformers are equivalent to a type of CNN that uses dynamic convolutions. The dynamic convolution paper (https://arxiv.org/abs/1912.03458) was released in 2019 so it did come after the attention is all you need paper.

Sadly this paper has only one citation. I think it's incredible. Knowing that transformers can be viewed as a CNN gives them insight into optimising its design, including removing the softmax activation and replacing it with a Relu+normalisation layer. I think there's a ton more improvements that can be made by continuing their work.",Ozqo,1gaxscv,https://reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/,https://www.reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/,2024-10-24 08:31:06,327,0.95,327,0,68,0,0,False,False,True,False,False,Discussion,self,t3_1gaxscv
MachineLearning,[R] The KAN paper has this interesting way to turn a unsupervised problem to a supervised problem (permitting var of some samples),"In the KAN paper they have an interesting way to infer the mapping between variables by permitting the variables for some of the sample of the data to create positive and negative samples. A form of contrastive learning. They don't site this approach, are there more formulated ways of doing this time of analysis to study the relationship between variables in an unsupervised way. 

Section 4.2 - https://arxiv.org/abs/2404.19756",Sandy_dude,1gau1kt,https://reddit.com/r/MachineLearning/comments/1gau1kt/r_the_kan_paper_has_this_interesting_way_to_turn/,https://www.reddit.com/r/MachineLearning/comments/1gau1kt/r_the_kan_paper_has_this_interesting_way_to_turn/,2024-10-24 04:07:06,30,0.8,30,0,12,0,0,False,False,True,False,False,Research,self,t3_1gau1kt
MachineLearning,[D] How could the new Claude Sonnet 3.5 provide precise coordinates?,"Not sure if this has been asked before, but recent release of Claude Sonnet has surprised me. A few months ago, I tried many LLMs to provide me the (x, y) coordinates on the screenshot using various methods like grid location, marked coordinates etc. but the accuracy was not sufficient. However; this new model can actually provide very accurate coordinates. Does anyone know/Can we guess the system they are using for something like this? Could they be using some other model like SeeClick?",super_deap,1gasb94,https://reddit.com/r/MachineLearning/comments/1gasb94/d_how_could_the_new_claude_sonnet_35_provide/,https://www.reddit.com/r/MachineLearning/comments/1gasb94/d_how_could_the_new_claude_sonnet_35_provide/,2024-10-24 02:31:44,37,0.84,37,0,27,0,0,False,False,True,False,False,Discussion,self,t3_1gasb94
MachineLearning,[Discussion] Understanding Tensorflow Probability ,"I am trying to find good examples online where a predictive maintenance model training pipeline is built making use of the uncertainty estimation by using the tools from tensorflow probability.

Let’s say I have a dataset with sensor readings and time to failure, I have a fair understanding of training a regression model to predict the time to failure, but I don’t have any means to know how certain the model is to this TTF output.

Does anyone have any good references and examples of using tensorflow probability for regression with uncertainty estimation? I am looking to learn more on this, appreciate any help on this.",HatObvious2707,1gaoccz,https://reddit.com/r/MachineLearning/comments/1gaoccz/discussion_understanding_tensorflow_probability/,https://www.reddit.com/r/MachineLearning/comments/1gaoccz/discussion_understanding_tensorflow_probability/,2024-10-23 23:17:40,0,0.29,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gaoccz
MachineLearning,[P] Generating Distinct AI Voice Performances By Prompt Engineering GPT-4o,"I played with GPT-4o's new AI voice generation API and not only is it possible to prompt voices in a specific style, but it's possible to generate from a specific person like Obama.

tl;dr: *Possible* doesn't mean the voices are good, but interesting to note that it's possible.

Writeup: https://minimaxir.com/2024/10/speech-prompt-engineering/

Notebook: https://github.com/minimaxir/gpt-4o-audio-tests/blob/main/gpt-4o-audio-tests.ipynb
",minimaxir,1gag5jj,https://reddit.com/r/MachineLearning/comments/1gag5jj/p_generating_distinct_ai_voice_performances_by/,https://www.reddit.com/r/MachineLearning/comments/1gag5jj/p_generating_distinct_ai_voice_performances_by/,2024-10-23 17:23:35,0,0.3,0,0,1,0,0,False,False,True,False,False,Project,self,t3_1gag5jj
MachineLearning,"[Project] Creating an ""Autonomous Developer"" with LLM Locally or via API (OpenAI/Claude) – What Do You Think?","Hi all!

I’m thinking of training a LLM like Ollama locally on a **RTX 4090**, **128GB DDR5**, and **i9**, to fine-tune it with repositories from companies I work with. The goal is to create an **""autonomous software developer""** that can handle most Kanban tickets. I’d like it to collaborate with another LLM for **peer review** to boost the quality of the code.

**Alternatively**, I could consume **OpenAI** or **Claude** APIs to avoid local hosting, though I’d lose some control.

**My questions:**

1. **Techniques:** What fine-tuning and model collaboration techniques would you recommend? I’m considering **RLHF** and **Cooperative Multi-Agent Systems**.
2. **Scalability:** Will the RTX 4090 be enough? Any tips to optimize performance?
3. **Autonomy:** Has anyone used LLMs autonomously for software development? How do you ensure the model improves its coding and collaboration skills?
4. **Peer Review:** What’s the best way to set up an efficient peer review system between LLMs?
5. **API Alternative:** Is it worth investing in local infrastructure or better to stick with APIs for cost and flexibility?

I’d love to hear your insights or experiences! My goal is to make LLMs act as **virtual employees** for coding and reviewing at scale.",Unlucky-Hunter9075,1gaf9jn,https://reddit.com/r/MachineLearning/comments/1gaf9jn/project_creating_an_autonomous_developer_with_llm/,https://www.reddit.com/r/MachineLearning/comments/1gaf9jn/project_creating_an_autonomous_developer_with_llm/,2024-10-23 16:47:29,0,0.19,0,0,13,0,0,False,False,True,False,False,Project,self,t3_1gaf9jn
MachineLearning,[R] Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant,,emreckartal,1gae6tb,https://reddit.com/r/MachineLearning/comments/1gae6tb/r_ichigo_mixedmodal_earlyfusion_realtime_voice/,https://arxiv.org/pdf/2410.15316,2024-10-23 16:03:30,13,0.93,13,0,0,0,0,False,False,False,False,False,Research,default,t3_1gae6tb
MachineLearning,Optimizing Batch Size and Update Frequency for Policy Gradient in Dynamic Cloud Resource Allocation [R] [D],"Hey everyone!

I’m working on a project about resource allocation in the cloud, and I could really use your advice. My goal is to minimize the overall energy consumption of servers, and I’m dealing with continuous stochastic job arrivals.

Here’s a quick overview:

I handle job chunks with 10 jobs each, and every job has multiple dependent tasks. For each chunk, I run 10 iterations and 12 episodes to collect trajectories, and then I update my model using off-policy mode.

After **one iteration** with those **12 episodes**, my replay buffer ends up with around **499,824 experiences**!Now, here’s where I need your help:

1. **What batch size do you think would be best for sampling from the replay buffer?**
2. **How often should I update my model parameters?**

My state and action spaces are pretty large and dynamic because of the continuous job arrivals and the changing availability of tasks and resources. (I’m using a Policy Gradient architecture.)

Any insights or experiences you can share would be super helpful! Thanks so much!",TeamTop4542,1gabsed,https://reddit.com/r/MachineLearning/comments/1gabsed/optimizing_batch_size_and_update_frequency_for/,https://www.reddit.com/r/MachineLearning/comments/1gabsed/optimizing_batch_size_and_update_frequency_for/,2024-10-23 14:23:04,1,0.67,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gabsed
MachineLearning,New to Research - Need Info on Publications [R][D],"I have been writing and publishing a few papers/journals in the field of AI for the last two years now, but I am really not sure what the best journals and conferences are. In my case, I usually write a paper, and my professor, based on the content of the paper, submits it to that conference/journal.

So would like to understand some info from this sub.

\-&gt; What are some really good journals/conferences where you can publish a paper? (How are the journals /conferences ranked, is there a way to check? I heard ICML, NeurIPS are the top conferences in this field)

\-&gt; What are the best publishers?

\-&gt; What are sci Q1, Q2 journals and A\*  journals?

I have a paper that I am writing now which is in the field of medicine (In the speech domain), can anyone suggest to me, what the best Journals/Conferences in this field are?

Sorry, if these are some basic questions, (I only know about the publishers: IEEEXplore, Springer, Elseveir and used to think if it's Scopus-indexed, it is a good conference/journal).",Extension_Air1017,1gaa3o8,https://reddit.com/r/MachineLearning/comments/1gaa3o8/new_to_research_need_info_on_publications_rd/,https://www.reddit.com/r/MachineLearning/comments/1gaa3o8/new_to_research_need_info_on_publications_rd/,2024-10-23 13:06:37,4,0.7,4,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gaa3o8
MachineLearning,[Project] World's first autonomous AI-discovered 0-day vulnerabilities,"I'm sure a lot of people have found 0-day vulnerabilities by pasting code snippets into ChatGPT. The problem has always been scanning an entire project for 0-days. Some papers have shown it's possible by feeding their agents known vulnerable code, but as far as I know, none of those papers ever got any CVEs or found real 0-days. Vulnhuntr was released this weekend with more than a dozen 0-days discovered in open source projects of 10k+ GitHub stars:

[https://github.com/protectai/vulnhuntr](https://github.com/protectai/vulnhuntr)",FlyingTriangle,1ga8wxn,https://reddit.com/r/MachineLearning/comments/1ga8wxn/project_worlds_first_autonomous_aidiscovered_0day/,https://www.reddit.com/r/MachineLearning/comments/1ga8wxn/project_worlds_first_autonomous_aidiscovered_0day/,2024-10-23 12:07:23,42,0.76,42,0,13,0,0,False,False,True,False,False,Project,self,t3_1ga8wxn
MachineLearning,[R] Benign overfitting and Double Descent.,"In short are they same ?

My question in detail below.

I am trying to wrap my head around understanding the difference between Double descent phenomenon and Benign overfitting.

Double descent occurs in a model when the test error rises as the model complexity increases after a certain point and then again reduces in the highly overparametrized regime.

I have not yet found any exact definition of benign overfitting, however it also deals with generalization in overparametrized regime.

On the surface they seem same, however I am not exactly sure. Can someone please explain the similarity and difference between them ?",Scientifichuman,1ga7k9c,https://reddit.com/r/MachineLearning/comments/1ga7k9c/r_benign_overfitting_and_double_descent/,https://www.reddit.com/r/MachineLearning/comments/1ga7k9c/r_benign_overfitting_and_double_descent/,2024-10-23 10:50:24,5,0.78,5,0,11,0,0,False,False,True,False,False,Research,self,t3_1ga7k9c
MachineLearning,[D] [R] Swin Transformer Attention Map Visualization,"tested my model with some testing images, got a higher pixel value in a specific region (for the following image, it's the (0,0)) that is similar across the attention heads (can be at different location but the same location for every head). are there some inductive biases in Swin? can anybody explain why could this happen? for visualization, on activation dimension \[1, 32, 64, 64\], I average the third dim value that result \[1,32,64\] and reshape the last dim to \[8,8\]. So, it produces \[1, 32, 8, 8\] where 1 is the batch/num image, 32 number of head like the following image, and 8,8 for the feature shape.

https://preview.redd.it/rvik8lej2hwd1.png?width=728&amp;format=png&amp;auto=webp&amp;s=527a8a7c27f84cabd97980b6a230e30ebbd9a736",Quiet-King-9172,1ga62sn,https://reddit.com/r/MachineLearning/comments/1ga62sn/d_r_swin_transformer_attention_map_visualization/,https://www.reddit.com/r/MachineLearning/comments/1ga62sn/d_r_swin_transformer_attention_map_visualization/,2024-10-23 09:07:14,1,0.57,1,0,3,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/J8Ih8mjio1CIxTtKrzXBYEUp_1K-swWjMOrpSGPsnUA.jpg,t3_1ga62sn
MachineLearning,[Project] Am I missing something major or is a very approximate solution the best I can offer for this entity matching problem?,"I was tasked with extracting seven specific pieces of information from unstructured documents that describe various products (such as quantity, description, material type, size, price, etc.). I successfully did this, but the next step is proving to be more challenging. My job is to match each product to the closest corresponding item on a reference list of 1000 products (divided by material, format, number of pages, print type, paper type, quantity and cost). 

In some rare cases, it’s possible to find a close match on the reference list, but in most cases, it’s simply not feasible. Many product descriptions contain a lot of detailed information, some of which is missing from the reference list. Additionally, the relationships between the details are often complex and difficult to interpret without expert knowledge. Other descriptions, on the other hand, lack crucial details, such as the type or size of the material, making it impossible to find one good match.

In short, even a human expert would likely find it difficult to match these products accurately to the reference list. The best approach I’ve considered so far is to start by filtering based on quantity to narrow down the options. After that, I could use embeddings focused on the material type first as it's the most important one (since matching the size and features of a product made from a different material wouldn't make sense). Once I’ve identified potential matches based on material, I could then apply embeddings to the other characteristics. Pre-filtering based on size or other numerical details alone wouldn’t work, as there’s too much inconsistency in the data to rely on those aspects.  
  
It seems to me that offering 10 closest matches or something like this is the best I can do. Looks like the classic garbage in, garbage out issue. Am I wrong?",No_Possibility_7588,1ga5g5n,https://reddit.com/r/MachineLearning/comments/1ga5g5n/project_am_i_missing_something_major_or_is_a_very/,https://www.reddit.com/r/MachineLearning/comments/1ga5g5n/project_am_i_missing_something_major_or_is_a_very/,2024-10-23 08:19:03,5,0.86,5,0,2,0,0,False,False,True,False,False,Project,self,t3_1ga5g5n
MachineLearning,[R]Samples mismatching (Domain Adaptation,"Currently, I am working on domain adaptation research project with related to battery management system, I m facing an issue regarding to the mismatching in the sources as different sources contain different samples, I did the resampling but the problem is that resampling is done with datapoints with each cycle, hence more varying number of cycles and varying number of samples.
Is it fine with varying number of samples?

Source1: (81,000, 128)
Source2: (40,000, 128, 4)
Target: (21,000, 128, 4)

Another thing is: for the source labels I have calculated the value per cycle (in %) for instance, Source 1 (81000,128,4) Source label (638,2) cycle. Is it fine this way or need to process per datapoint just like in source label?",Hopeful_Pumpkin8095,1ga49zb,https://reddit.com/r/MachineLearning/comments/1ga49zb/rsamples_mismatching_domain_adaptation/,https://www.reddit.com/r/MachineLearning/comments/1ga49zb/rsamples_mismatching_domain_adaptation/,2024-10-23 06:55:32,0,0.5,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1ga49zb
MachineLearning,"Building a Model Recommendation System: Tell Us What You’re Building, and We’ll Recommend the Best AI Models for It! [D]","**Hey Reddit!**

We’re working on something that we think could make model discovery a LOT easier for everyone: **a model recommendation system** where you can just **type what you're working on in plain English**, and it'll suggest the best AI models for your project. 🎉

# 💡 How it works:

The main idea is that you can **literally describe your project** in **natural language**, like:

* ""I need a model to generate summaries of medical research papers.""
* ""I'm building a chatbot for customer support.""
* ""I want a model that can analyze product reviews for sentiment.""

And based on that input, the system will recommend the best models for the job! **No deep diving into technical specs, no complex filters—just solid recommendations based on what you need.**

# 🌟 What else we’re building:

Alongside the model suggestions, we’re adding features to make the platform super user-friendly:

* **Detailed model insights**: You’ll still get all the technical info, like performance metrics, architecture, and popularity, to compare models.
* **Advanced search &amp; filters**: If you’re more hands-on, you can filter models by task, framework, or tags.
* **Personalized suggestions**: The system will get smarter over time and offer more relevant suggestions based on your past usage.

# Why we need your feedback:

We want this platform to actually solve problems for people in the AI/ML space, and that’s where **you** come in! 🙌

1. **Does a tool like this sound helpful to you?**
2. **What features do you think are missing from model platforms like Hugging Face?**
3. **Are there any specific features you’d want to see, like performance comparisons or customization options?**
4. **How could we make the natural language input even more useful for recommending models?**

# TL;DR:

We’re building a tool where you can just **describe your project** in plain English, and it’ll **recommend the best AI models** for you. No need for complex searches—just type what you need! Looking for your feedback on what you'd want to see or any features you think are missing from current platforms like Hugging Face.

We'd love to hear your thoughts and ideas! What would make this platform super useful for you? Let us know what you think could improve the model discovery process, or what’s lacking in existing platforms!

Thanks in advance, Reddit! 😊",O2MINS,1ga3h0j,https://reddit.com/r/MachineLearning/comments/1ga3h0j/building_a_model_recommendation_system_tell_us/,https://www.reddit.com/r/MachineLearning/comments/1ga3h0j/building_a_model_recommendation_system_tell_us/,2024-10-23 05:57:50,32,0.82,32,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1ga3h0j
MachineLearning,[D] Responses to false accusations of plagiarism for Gaunt Tensor Product paper,"I’m posting this on behalf of the authors of the paper. The first author tried to make a post about this, but the post got removed for some reason. The author reached out to me because I was one of the people defending them, so see below for the author writeup about the accusations.

**TL;DR**: We're the authors of the Gaunt Tensor Product paper, and we want to directly address the false plagiarism accusations against our work. Our main contribution, a new perspective on tensor products of irreducible representations (irreps) in machine learning and equivariant neural networks, is novel and original. The claimed ""similarity"" are actually algorithms from elementary math and CS courses, and are not the main contribution of our work: our independent implementation is clear if you look at our code, which is quite different because we had a completely different application area in mind. On the other hand, our core contributions, including establishing the connection between tensor products of irreps and integrals of products of spherical harmonics and various design paradigms of equivariant operations, are completely omitted. There is an oversight of citation due to the gap between fields (machine learning vs. graphics), but this is not plagiarism, and now that we know about this, we are updating the paper with the citation and discussion accordingly. This is similar situation to areas such as neural ODEs, where the original ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until much later. The anonymous accuser is selectively replying, omitting key details, and controlling the narrative.

**More details below**:

We are the authors of \[""Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products""\](https://openreview.net/forum?id=mhyQXJ6JsK) . We are creating a new post to clearly outline our responses to the false accusations of plagiarism that we've received for the Gaunt Tensor Product paper in another thread. While we have replied on that thread, the anonymous OP of that thread is selectively replying and omitting a lot of information from our responses, and we don't think it is fair that they single-handedly control the narrative. Note that we never got any emails or posts on OpenReview from the author, who has instead decided to anonymously post on here.

Firstly, we would like to comprehensively respond to the false accusations again:

\- **The contributions of our work**: as emphasized in our paper, our main contribution in this work is the new perspective on tensor products of irreps, which is novel and original to the machine learning community (Equations 3 and 4). The whole Section 3.1 elaborates on how to establish the connection between tensor products of irreps and integrals of products of spherical harmonics. Although the OP claims ""However, it is important to note that this derivation accounts for less than one page of the nine-page paper."", the fact is that our establishment and derivation are based on a series of rigorous deductions with many efforts on building a solid mathematical foundation including group theory and quantum mechanics (please refer to Appendix A.1-A.7, page 16-28), which is not straightforward and trivial to obtain. Without these efforts, we cannot establish such connections, let alone the efficient algorithm. In the context of equivariant machine learning, this derivation presents significance to refresh the understanding of basic equivariant operations, which cannot be omitted.

\- **The similarities of the efficient algorithms between our work and FSHP work**: Firstly, we would like to apologize that we did not cite the FSHP work in our submission, which is unintentional and due to the gap between these two communities (we are from the ML community, and they are from graphics, and the paper was not known to us until recently). We will update the arXiv version of our paper asap by adding a discussion paragraph to carefully discuss the FSHP work and our work. **On the other hand, we also would like to clarify that there does not exist any plagiarism behavior of the FFT algorithm**: after we figure out the relation between the tensor product of irreps and integrals of the product of three spherical harmonics, it is rather natural to connect it with products of spherical functions. Moreover, there exist classical results for efficient computation of products of spherical functions, i.e., Convolution Theorem and FFT, which involve elementary knowledge that can be learned in several undergraduate classes: (1) change of basis, which can be learned in linear algebra and signal processing and is used in both paper to connect spherical harmonics and Fourier basis; (2) FFT, which is commonly taught in signal processing and numerical computation classes and is used for acceleration. Due to the basicness of these mathematical tools, both works follow the standard way to formalize and present, which leads to similarity. As we said, this cannot be misrepresented as plagiarism because we independently worked on this, and did not know about the other work until later because of the different communities. This is similar to work in areas such as neural ODEs, where the original ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until much later.

\- **The differences in implementation**: it is noteworthy that, as a work for the equivariant machine learning community, it is not enough to simply propose an approach just for the tensor product operation. What we really care about is the various design paradigms of equivariant operations, which are built upon tensor products. In Section 3.3, we categorize these paradigms into three classes in terms of their different characteristics and applied range. For each class of equivariant operations, we carefully specialize our approach by combining their properties and considering the restrictions. For example, for the equivariant convolution, we figure out that we can further leverage eSCN/EquiformerV2's findings to achieve further acceleration; for the equivariant many-body interactions, a divide-and-conquer approach is natural, which is also generally taught in various CS courses and projects. There also exist different instantiation strategies in modern equivariant networks when applying these classes of operations, please refer to the Discussion paragraph in Section 3.3 and Appendix C. Simply proposing the efficient approach for tensor products is not feasible to these mentioned points. Without these additional efforts and contributions, the efficient algorithm is not practical to be used for the equivariant machine learning community,  which cannot be omitted.

\- There is quite a lot of literature in the last few decades in the graphics community on this, and this is another general point is that work on the graphics community on efficient algorithms is not heard of and/or undercited in the rotationally equivariant neural networks community, when these algorithms pop up in a lot of equivariant NN work. Additionally, this graphics paper is not in the field of ML, and this algorithm is being applied to a completely different area, which is why we did not see it originally and had an independent formalization. Perhaps an analogy here is that there are papers applying Transformers to different areas like vision instead of language, but this shouldn't be ""plagiarism"" at all. Likewise, neural ODEs shouldn't be considered plagiarism of traditional ODE solvers simply because they are using the same method (and indeed, some of the original ideas of neural ODEs were in engineering papers from the '90s, and not discovered/cited in ML papers until later because of the different communities). One user on this thread also put it well that the concepts here like FFT are quite well-known: ""After skimming, my impression is that those are well known results from textbooks and signal processing courses that nobody bother to cite anymore. I could be wrong.""

\- The implementation in the GTP paper is fairly different from the FSHP paper and was implemented independently because we derived our implementation based on being motivated by our specific application area of ML for molecular modeling: their code is in C++, doesn't support efficient computations for lower rotation orders (L), and is not made for use with irreducible representations. This should be clear when you see the code.

\- The main purpose of the Equiformerv2 experiment with the self-mix layer was a proof-of-concept to show that such a self-mix layer can be implemented because of the Gaunt Tensor Product formulation. Without this formulation (and using the more standard Clebsch-Gordan Tensor Product), it would have been very slow to add this layer (and not great from a memory usage perspective). This can be made more clear in the arXiv version.

Secondly, we would like to point out that the anonymous OP of that thread is selectively replying to posts, and omitting a lot of information (including in how they are updating their own thread, they do not include all of the details of our responses). To us, the posts also seem LLM generated but you should draw your own conclusions. We also posted this new topic because the authors responses on the original thread are all folded, which cannot be directly seen by new readers.

Finally, we appreciate that many people have been commenting on the thread to defend us. These types of anonymous, sensational claims can have serious implications and to post anonymously on Reddit before emailing us or posting on OpenReview is really problematic. We hope that you all read these threads carefully before jumping to conclusions.",kronicler1029,1ga12d8,https://reddit.com/r/MachineLearning/comments/1ga12d8/d_responses_to_false_accusations_of_plagiarism/,https://www.reddit.com/r/MachineLearning/comments/1ga12d8/d_responses_to_false_accusations_of_plagiarism/,2024-10-23 03:31:06,50,0.97,50,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1ga12d8
MachineLearning,[D] Deploying a Forked GitHub Repo on Runpod as a Serverless Solution,"I want to fork this repo and modify it to fit my requirements: [https://github.com/runpod-workers/worker-faster\_whisper](https://github.com/runpod-workers/worker-faster_whisper). 

However, I’m looking for a tutorial on deploying it as a serverless solution on Runpod. The tutorial for serverless deployment starts from a ""ready-made"" template: [https://docs.runpod.io/tutorials/serverless/gpu/run-your-first](https://docs.runpod.io/tutorials/serverless/gpu/run-your-first). 

Is there a step-by-step guide for deploying a forked GitHub repository?",yccheok,1ga103u,https://reddit.com/r/MachineLearning/comments/1ga103u/d_deploying_a_forked_github_repo_on_runpod_as_a/,https://www.reddit.com/r/MachineLearning/comments/1ga103u/d_deploying_a_forked_github_repo_on_runpod_as_a/,2024-10-23 03:27:33,2,0.62,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1ga103u
MachineLearning,Meta AI (FAIR) latest paper integrates system-1 and system-2 thinking into reasoning models. [R],"Meta AI (FAIR) latest paper integrates system-1 and system-2 thinking into reasoning models.

Basically, it introduces the term ""Dualformer"" which integrates both system-1 (fast-thinking) and system-2 (slow-thinking) into the transformer to improve its reasoning capability. The high level idea is to train the model with ""randomized trace"", which randomly drop parts of the reasoning tokens. This approach improves model's inference speed, accuracy, and diversity. It also enables model to perform system-1 and system-2 thinking in a controllable fashion. 

The paper's link here:

[https://arxiv.org/html/2410.09918v1](https://arxiv.org/html/2410.09918v1)",Proof-Raise-9151,1g9v7ag,https://reddit.com/r/MachineLearning/comments/1g9v7ag/meta_ai_fair_latest_paper_integrates_system1_and/,https://www.reddit.com/r/MachineLearning/comments/1g9v7ag/meta_ai_fair_latest_paper_integrates_system1_and/,2024-10-22 22:38:07,233,0.95,233,0,54,0,0,False,False,True,False,False,Research,self,t3_1g9v7ag
MachineLearning,[R] Need endorsement on Arxiv cs.LG ,"Here is my paper:

[https://drive.google.com/file/d/1WsLun1PoNcXkcisu18tel5rNtEZOLqAe/view?usp=sharing](https://drive.google.com/file/d/1WsLun1PoNcXkcisu18tel5rNtEZOLqAe/view?usp=sharing)

Please let me know if you know anyone that can do it on cs.LG

Rick Ferreira requests your endorsement to submit an article to the  
cs.LG section of arXiv. To tell us that you would (or would not) like to  
endorse this person, please visit the following URL:

[https://arxiv.org/auth/endorse?x=OYCPH9](https://arxiv.org/auth/endorse?x=OYCPH9)

If that URL does not work for you, please visit

[http://arxiv.org/auth/endorse.php](http://arxiv.org/auth/endorse.php)

and enter the following six-digit alphanumeric string:

Endorsement Code: OYCPH9",Dry-Ad1164,1g9ufjg,https://reddit.com/r/MachineLearning/comments/1g9ufjg/r_need_endorsement_on_arxiv_cslg/,https://www.reddit.com/r/MachineLearning/comments/1g9ufjg/r_need_endorsement_on_arxiv_cslg/,2024-10-22 22:03:22,0,0.14,0,0,9,0,0,False,False,True,False,False,Research,self,t3_1g9ufjg
MachineLearning,[D] Token and Embedding in Decision Transformers,"As I'm trying to implement my first [Decision Transformer](https://arxiv.org/pdf/2106.01345), I got stuck in very first phase of implementation.

A text2text transformer requires a tokenization of the text, that is fed into the transformer during the training phase. Thanks to many different tokenization strategies, an input test is divided into tokens, which are represented by for instance integers values.  
But in the case of Decision Transformers, there is no text sequence that can be tokenized into integers. In DT, there are states, actions, rewards and rewards-to-go, all of them are float numbers and mostly arrays and not scalar (with the only exeption of rewards and RtG).   
So my question is: how does the tokenization process look like? There a tokenization at all in that case?",WilhelmRedemption,1g9tto6,https://reddit.com/r/MachineLearning/comments/1g9tto6/d_token_and_embedding_in_decision_transformers/,https://www.reddit.com/r/MachineLearning/comments/1g9tto6/d_token_and_embedding_in_decision_transformers/,2024-10-22 21:37:12,2,1.0,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1g9tto6
MachineLearning,[P] Converting an OCR model from LTR to RTL ,"hello guys , looking for python developer whom can help me converting an ocr model that i've made to work with RTL languages (arabic , persian) . 
the model i've built works only on LTR . now i want to change it .",LahmeriMohamed,1g9tl9o,https://reddit.com/r/MachineLearning/comments/1g9tl9o/p_converting_an_ocr_model_from_ltr_to_rtl/,https://www.reddit.com/r/MachineLearning/comments/1g9tl9o/p_converting_an_ocr_model_from_ltr_to_rtl/,2024-10-22 21:26:57,1,0.57,1,0,2,0,0,False,False,True,False,False,Project,self,t3_1g9tl9o
MachineLearning,How do we move beyond neural networks [Discussion]?,"Hi there! I am currently a student, and have been working with NNs for a few years now. While I'm not denying that neural networks and their derivatives have been revolutionary (LLMs and the like), I can't help but feel like we're going to hit a brick wall soon with neural networks. To me, it feels like we need an entirely new approach, one that is better suited to the computers we have currently, to move to the next generation of models and AI. Is there any progress being made in such a direction (if so can you please mention it here), and what do you think is going to be the next step? Again, this is my opinion. I haven't been working on NNs for a lifetime, so would love to hear the community's thoughts on this. 

Clarification, by moving beyond NNs, my thought is that we don't model neurons and architectures after the human brain, but rather something different that doesn't rely on artificial neurons at all. (Again, don't know how it might be possible, which is why I am looking forward to hearing your thoughts). 

To me it feels like modeling neural networks after the human brain is inefficient because we are trying to imitate biology as it is the best thing we have. It's like if humanity developed a mechanical horse because the horse is the best method of transport in nature, instead of focusing our efforts on developing a car which our current tech is more suited to (just an example). Also, the recent incremental updates to LLMs and stuff seems to suggest that training larger models is not going to justify the immense amounts of data and resources that we put in very soon.

Personally, I think we should continue evolving neural networks to see where we hit the limit, and then hopefully we will have explored enough to know why they won't work for more advanced stuff, after which we can work on the next steps. Maybe we can even take the best parts of NNs and incorporate them into newer architectures.

Looking forward to hearing your thoughts on this. Once again, if you have any interesting new research regarding non NN based AI, can you please link them below? Thanks in advance.",mopasha1,1g9o9x3,https://reddit.com/r/MachineLearning/comments/1g9o9x3/how_do_we_move_beyond_neural_networks_discussion/,https://www.reddit.com/r/MachineLearning/comments/1g9o9x3/how_do_we_move_beyond_neural_networks_discussion/,2024-10-22 17:47:42,15,0.58,15,0,97,0,0,False,False,True,False,False,Discussion,self,t3_1g9o9x3
MachineLearning,[D] We built a multi-cloud GPU container runtime,"Wanted to share our open source container runtime -- it's designed for running GPU workloads across clouds.

[https://github.com/beam-cloud/beta9](https://github.com/beam-cloud/beta9)

Unlike Kubernetes which is primarily designed for running one cluster in one cloud, Beta9 is designed for running workloads on many clusters in many different clouds. Want to run GPU workloads between AWS, GCP, and a 4090 rig in your home? Just run a simple shell script on each VM to connect it to a centralized control plane, and you’re ready to run workloads between all three environments.

It also handles distributed storage, so files, model weights, and container images are all cached on VMs close to your users to minimize latency.

We’ve been building ML infrastructure for awhile, but recently decided to launch this as an open source project. If you have any thoughts or feedback, I’d be grateful to hear what you think 🙏",velobro,1g9mrcj,https://reddit.com/r/MachineLearning/comments/1g9mrcj/d_we_built_a_multicloud_gpu_container_runtime/,https://www.reddit.com/r/MachineLearning/comments/1g9mrcj/d_we_built_a_multicloud_gpu_container_runtime/,2024-10-22 16:45:26,33,0.89,33,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1g9mrcj
MachineLearning,[D] [R] LLMs frameworks for research,"I'm a Ph.D. student in AI and NLP and I'm currently starting a new research project with LLMs.   
This time, instead of writing all the code from scratch, primarily using HuggingFace and Pytorch, I'd like to use one of the popular frameworks (like LangChain, LlamaIndex etc.).   
The motivation behind this is that, ideally, I'd like to learn to use these tools to get a more compact and organised codebase, such that I can easily add pieces to include RAG, Agentic workflows etc.   
I'm also interested in having an efficient way to load models and make inferences.  
  
In your experience, which of the many available frameworks out there is the most suitable for research purposes ? And do you even use a framework or you just code everything from scratch every time you start a new project ?",Debonargon,1g9k0te,https://reddit.com/r/MachineLearning/comments/1g9k0te/d_r_llms_frameworks_for_research/,https://www.reddit.com/r/MachineLearning/comments/1g9k0te/d_r_llms_frameworks_for_research/,2024-10-22 14:51:56,67,0.93,67,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1g9k0te
MachineLearning,[P]  New release for the World's *LEAST* popular LLM evaluation tool! ,"Just released a new version of [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search), with [downloads](https://github.com/dezoito/ollama-grid-search/releases) for all major platforms.

According to some wise-guy on Discord, it's ""cute"" and ""laughable"", so make sure you don't miss out on the fun!

If you have no idea what this is, it's a Desktop open source app that allows you to:

* Evaluate multiple prompts and model combinations in a single operation
* Evaluate multiple combinations of parameters to verify the effect on inference outputs.

https://preview.redd.it/lcqqqco9fbwd1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e8db16b7f980acbfd96adb65ccb6633cf52d3e93

If you are already a user (thank you!), here's the changelog for version 0.6.0:

# Added

* Added UI controls to re-run past experiments.
* Added controls to remove experiment files from the UI.
* Added button to copy an inference text to the clipboard.

# Changed

* Moved ""reload"" icon to improve layout.
* Improved experiment inspection UI readability.
* Streamlined State management.

# Fixes

* Fix HMR not working on MacOS (in development, of course).",grudev,1g9j2e7,https://reddit.com/r/MachineLearning/comments/1g9j2e7/p_new_release_for_the_worlds_least_popular_llm/,https://www.reddit.com/r/MachineLearning/comments/1g9j2e7/p_new_release_for_the_worlds_least_popular_llm/,2024-10-22 14:10:22,18,0.7,18,0,6,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/v53BHkH6urE7jc0nVztZ_jvJjAPcQDCFD7Bcg6TFq5o.jpg,t3_1g9j2e7
MachineLearning,[Discussion] Best Text to Audio voice API for creating expressive video voice?,"I am looking for recommendation for best text-to-voice API, I am basically looking at OpenAI, 11labs, Google Voice, and Amazon polly.

So if you know any of these which are the best for my use case, that will be good.

And if you have any experience with these API, please do share. Or if you know any other kind of recommended API, please let me know in the comments.

Thanks.",pushkarsingh32,1g9fhg7,https://reddit.com/r/MachineLearning/comments/1g9fhg7/discussion_best_text_to_audio_voice_api_for/,https://www.reddit.com/r/MachineLearning/comments/1g9fhg7/discussion_best_text_to_audio_voice_api_for/,2024-10-22 11:07:36,9,0.81,9,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1g9fhg7
MachineLearning,[D] A few questions on the architecture of Meta's Movie Gen model,"I went through the Meta Movie Gen paper to understand their model.  I'm mostly comfortable with the architecture at this point, but there are a few places where I'm still not sure what's going on.  Hoping to get some clarity

1) The conversion from TAE output latents to the transformer input embeddings.  The paper has this paragraph:

&gt;As discussed in Section 3.1.1, we perform generation in a learned latent space representation of the video. This latent code is of shape T x C x H x W . To prepare inputs for the Transformer backbone, the video latent code is first ‘patchified’ using a 3D convolutional layer (Dosovitskiy et al., 2021) and then flattened to yield a 1D sequence. The 3D convolutional layer uses a kernel size of k_t x k_h x k_w with a stride equal to the kernel size and projects it into the same dimensions as needed by the Transformer backbone. Thus, the total number of tokens input to the Transformer backbone is THW/(k_t k_h k_w). We use k_t = 1 and k_h = k_w = 2, i.e., we produce 2 x 2 spatial patches.

Here, why isn't the channel dimension mentioned?  Shouldn't the kernel be 5-dimensional, including the input channels and output channels?  And if so, is the number of output channels equal to the transformer embedding dimension?

2) Right after that, the paper talks about factorized learnable embeddings:

&gt;We use a factorized learnable positional embedding to enable arbitrary size, aspect ratio, and video length (De- hghani et al., 2024) inputs to the Transformer. Absolute embeddings of D dimensions can be denoted as a mapping phi(i) : [0, maxLen] -&gt; R^D where i denotes the absolute index of the patch. We convert the ‘patchified’ tokens, i.e., output of the 3D convolutional layer, into separate embeddings phi_h, phi_w and phi_t of spatial h, w, and temporal t coordinates...

How can the embeddings be separated by dimension after they've already been flattened to 1-D?  Shouldn't the positional embeddings be added before the ""3-D"" convolution, when we still have the temporal and spatial coordinates isolated?

3) How do adaptive layer norm blocks work?  I looked up the original reference in Peebles and Xie, which says:

&gt; Rather than directly learn dimensionwise scale and shift parameters γ and β, we regress them from the sum of the embedding vectors of t and c

I don't really understand what is meant by the word ""regress"" here, or how γ and β are obtained from t and c.  I also don't understand how this applies to Movie Gen, which has no c vector.",throwaway2676,1g92l2t,https://reddit.com/r/MachineLearning/comments/1g92l2t/d_a_few_questions_on_the_architecture_of_metas/,https://www.reddit.com/r/MachineLearning/comments/1g92l2t/d_a_few_questions_on_the_architecture_of_metas/,2024-10-21 22:20:50,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g92l2t
MachineLearning,[D] A probabilistic method for determining if an image was created by AI.,"I present a method for determining whether an image is the output of a diffusion-to-text image generator. 

Convert all pixels to an absolute value using the following formula, where R, G, and B are the red green and blue components of a pixel.

`N(i) = ( ( Ri + Gi + Bi ) / 382.5 ) - 1.0` 

Let S denote the value obtained by summing all the N(i) pixels after normalization over the entire image.  Interpret ,

+ S is very far from zero.  This image is real and not AI generated.  

+ S is very close to zero.  This image is plausibly AI generated.  But this could be a false negative in some cases.

Has anyone tried this?   

Alternatively, it may be possible to identify an AI generated image by means of examining the profile of its histogram",moschles,1g9a226,https://reddit.com/r/MachineLearning/comments/1g9a226/d_a_probabilistic_method_for_determining_if_an/,https://www.reddit.com/r/MachineLearning/comments/1g9a226/d_a_probabilistic_method_for_determining_if_an/,2024-10-22 04:39:18,0,0.15,0,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1g9a226
MachineLearning,[P] Nuggt: An LLM Agent that Runs on React Component Event Capture Data (Open Source),"I was thinking what if I can have all the product analytics data from my react project at one place together with react component code.. connected to an LLM agent for inferencing, analysis and visualisation.. so then i tried it and it worked quite well. Having all the event capture data at one place allows you to use LLM agent to:

https://preview.redd.it/dmbcmji9h6wd1.png?width=2846&amp;format=png&amp;auto=webp&amp;s=0c1a788dab18bf14a8133053b7a3c24b0336d8a4

1. Analyse data
2. Visualise data
3. Make decisions based on data
4. Brainstorm about next steps
5. Generate updated react component code based on the changes
6. Start the whole analytics process again (iterations..)

You can try it out at [https://github.com/shoibloya/nuggt-analytics](https://github.com/shoibloya/nuggt-analytics)

Basically in this open source project I created a streamlit dashboard that allows you to integrate analytics to your react component and connecting it to Firestore all using GPT. Then once the events captured data goes to firestore I fetch it back and then you can use the LLM agent to generate decision cards together with visualisations.

Let me know your feedback",Loya_3005,1g91gqn,https://reddit.com/r/MachineLearning/comments/1g91gqn/p_nuggt_an_llm_agent_that_runs_on_react_component/,https://www.reddit.com/r/MachineLearning/comments/1g91gqn/p_nuggt_an_llm_agent_that_runs_on_react_component/,2024-10-21 21:32:35,0,0.44,0,0,2,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/_auETpTZeUhogpVLaH8sw6S7hoX42uIZZruQbUtWFBk.jpg,t3_1g91gqn
MachineLearning,[R] How do RoPE-based LLMs learn attention sinks (or encode absolute positions)?,"I recently revisited the ‘Attention Sink’ paper ([link](https://arxiv.org/pdf/2309.17453)) and started thinking about how LLMs manage attention sinks. 

The concept of an attention sink describes the phenomenon where LLMs allocate a disproportionately high attention score to the initial tokens, regardless of their semantic value.

Here’s the paradox: state-of-the-art open LLMs typically employ RoPE (Rotary Position Embeddings) for their positional encoding. Since RoPE only encodes relative positions, it’s puzzling how the model consistently identifies and assigns high attention to the absolute initial tokens. Any thoughts on how this behavior might emerge or be explained?",StraightSpeech9295,1g8yurr,https://reddit.com/r/MachineLearning/comments/1g8yurr/r_how_do_ropebased_llms_learn_attention_sinks_or/,https://www.reddit.com/r/MachineLearning/comments/1g8yurr/r_how_do_ropebased_llms_learn_attention_sinks_or/,2024-10-21 19:46:28,44,0.96,44,0,11,0,0,False,False,True,False,False,Research,self,t3_1g8yurr
MachineLearning,[R] Gradient accumulation bug fix in nightly transformers,"Hey r/MachineLearning folks! Just an update on the gradient accumulation bug - the fix should be in the nightly transformers, and also in [Unsloth](https://github.com/unslothai/unsloth/) trainers, so definitely update them! For a recap, grad accumulation in most trainers was calculated incorrectly, causing loss curve differences.

https://preview.redd.it/m5duteyqv5wd1.png?width=1776&amp;format=png&amp;auto=webp&amp;s=0e3fb39048284ff93ed81070de6fb757ca41bf4f

**Recap of gradient accumulation bug**

Gradient accumulation is used to mimic large batch training by chunking a batch into smaller sequences to reduce GPU VRAM usage. So if your batch size was 32, you could do a batch size of 8, and do 4 mini steps of them by accumulating gradients. The key trick is ga \* bsz is held constant, so you can edit those numbers.

https://preview.redd.it/73kr2m7xv5wd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=1042a6785e419beda4c8a8cdcfdbff86e55e3717

So the trick of grad accum is you can inplace add up all mini batch gradients, and after some scaling, you will get back the gradient as if you did 1 full batch.

The issue was the original paper in 2017 [https://proceedings.mlr.press/v77/hermans17a/hermans17a.pdf](https://proceedings.mlr.press/v77/hermans17a/hermans17a.pdf) showed in expectation this would work, but there was a common misconception that GA actually was equivalent to full batch training. Ie bsz=32, ga=1 should be mathematically equivalent to bsz=1, ga=32. But Benjamin first reported here [https://github.com/huggingface/trl/issues/2175](https://github.com/huggingface/trl/issues/2175) that training losses did not match up. In fact this problem was unsolved for like 4-5 years - see [https://github.com/huggingface/transformers/issues/14638](https://github.com/huggingface/transformers/issues/14638)

**Is the Gradient accumulation bug serious?**

If you simply plot the L2 Norm between gradient accumulated versions vs full batch training, you will get the error plots like below:

https://preview.redd.it/8teb7idyv5wd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=7195d953f1ed87537024d25e94d1e24c99ca41af

There is some 0.03 L2 difference as you increase the gradient accumulation steps, whilst it's supposed to be flat. After the fix, the error reduces to 0005 ish, and we show there is some numerical precision issues of accumulating gradients, albeit not much.

But it's worse - in [https://github.com/huggingface/transformers/pull/34191#issuecomment-2418658361](https://github.com/huggingface/transformers/pull/34191#issuecomment-2418658361), I showcase that LoRA on Wikittext incurs a significant penalty if using grad accum:

https://preview.redd.it/48reupi8w5wd1.png?width=863&amp;format=png&amp;auto=webp&amp;s=d5be990829fe06efbc4129120a1094ba0508877f

https://preview.redd.it/x6ck0fwdw5wd1.png?width=1465&amp;format=png&amp;auto=webp&amp;s=5e2a19c37d07a984e2556edc39d2fefe50b5bc4d

I listed all experiments here: [https://docs.google.com/spreadsheets/d/1RUiVuFNfnl9eBAa3JhvkKb0hm20m4NqnUO-OWDPpNos/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1RUiVuFNfnl9eBAa3JhvkKb0hm20m4NqnUO-OWDPpNos/edit?usp=sharing) . So it was much worse than I first anticipated.

**Getting the bug fix &amp; more details**

The bug fix should be in nightly transformers now! Also the fix is already inside of Unsloth - Colab for it - [https://colab.research.google.com/drive/1z0XJU2FCzDC8oyXa2Nd4jCxylRMI-o0-?usp=sharing](https://colab.research.google.com/drive/1z0XJU2FCzDC8oyXa2Nd4jCxylRMI-o0-?usp=sharing)

More details are in [https://unsloth.ai/blog/gradient](https://unsloth.ai/blog/gradient) and there's also a bit of maths proofs and stuff in the blog! I also talk about it in a lecture I gave on the GPU MODE / CUDA MODE server here: [https://www.youtube.com/watch?v=hfb\_AIhDYnA](https://www.youtube.com/watch?v=hfb_AIhDYnA)

If anyone has any questions, feel free to ask! Thanks!",danielhanchen,1g8ymrn,https://reddit.com/r/MachineLearning/comments/1g8ymrn/r_gradient_accumulation_bug_fix_in_nightly/,https://www.reddit.com/r/MachineLearning/comments/1g8ymrn/r_gradient_accumulation_bug_fix_in_nightly/,2024-10-21 19:37:38,66,0.96,66,0,14,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/x2jIOD_lLkuzo8JsGqsd-ar2WDm437MGT53hlsQAwys.jpg,t3_1g8ymrn
MachineLearning,[Discussion]i am a btech student (CSE) and I wanted to learn machine learning and I don't know where to start and don't have any recourses . so I really need advice or any kind of help ,I am in my 5th sem.please help me ,Glittering-Tell-8963,1g8x4go,https://reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/,https://www.reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/,2024-10-21 18:36:39,0,0.17,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1g8x4go
MachineLearning,[D] Discussion on the best ways to extract data and store it ,"Hi, I am working on a project that is related to MRI images of tumors. At first, I analyze these images and make segmentation for them, but how do I convert the information in the image about the nature of the tumor into data that can be used to write a medical report about the patient.
What is the classification of the data? Structured or simi- structured or not
How to use those data in to write a report.
Thanks",mse9090,1g8u1ui,https://reddit.com/r/MachineLearning/comments/1g8u1ui/d_discussion_on_the_best_ways_to_extract_data_and/,https://www.reddit.com/r/MachineLearning/comments/1g8u1ui/d_discussion_on_the_best_ways_to_extract_data_and/,2024-10-21 16:33:39,0,0.33,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g8u1ui
MachineLearning,"[D] LLMs Will Always Hallucinate, and We Need to Live With This","So, apparently there is a mathematical proof that LLMs will always hallucinate. What do you think about this?",computersmakeart,1g8bypk,https://reddit.com/r/MachineLearning/comments/1g8bypk/d_llms_will_always_hallucinate_and_we_need_to/,https://arxiv.org/abs/2409.05746,2024-10-20 23:20:30,2,1.0,2,0,1,0,0,False,False,False,False,False,Discussion,default,t3_1g8bypk
MachineLearning,[R] Does AISTATS do ‘acceptance with revisions’?,"Does AISTATS do ‘acceptance with revisions’?

First time student author here. Submitted my draft but I feel like I can improve the draft much more by tweaking it here and there (even adding a couple of new sections in appendix).

Would this be possible in the rebuttal phase? Or would they only allow me to make cosmetic changes? I’ve heard that changes are possible in some other conferences, but not sure about AISTATS.

Thanks!",confirm-jannati,1g8qn5s,https://reddit.com/r/MachineLearning/comments/1g8qn5s/r_does_aistats_do_acceptance_with_revisions/,https://www.reddit.com/r/MachineLearning/comments/1g8qn5s/r_does_aistats_do_acceptance_with_revisions/,2024-10-21 14:11:49,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1g8qn5s
MachineLearning,"[R] RWKV-7: attention-free and surpassing strong Modded-GPT baseline (the one with Muon optimizer), while only using headsz 64","Hi everyone. RWKV-7 (100% RNN and attention-free) can surpass the strong Modded-GPT baseline (the one with Muon optimizer, currently trending on twitter).

Training code &amp; log: [https://github.com/BlinkDL/modded-nanogpt-rwkv](https://github.com/BlinkDL/modded-nanogpt-rwkv) And it can reach loss 3.26xx if you use a larger headsz.

My current implementation is very inefficient though. Might can reach 85% Modded-GPT speed @ ctx1k (or faster than Modded-GPT @ ctx4k) after optimization. Any helps are welcome :)

https://preview.redd.it/48m3lsvkb4wd1.png?width=873&amp;format=png&amp;auto=webp&amp;s=647d86ed47d40a4f742ed9512a835dee41069e4f

======================================

The strong GPT baseline:

https://preview.redd.it/h2ckr31mb4wd1.png?width=584&amp;format=png&amp;auto=webp&amp;s=b667bfbc50298f8335a889b85c55f68ee8db38a5

======================================

RWKV-7 moves away from the ""linear attention"" design to achieve greater performance :)

https://preview.redd.it/ijyz0sgnb4wd1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=f413d0e7bcd3a76c5e788f2ca231a37706b24345

",bo_peng,1g8qsea,https://reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/,https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/,2024-10-21 14:18:19,107,0.97,107,0,20,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/HARzptlJ7obEaDK99FhgXH4vZIGiixvw3q4OplqorDs.jpg,t3_1g8qsea
MachineLearning,"[Research] When Do AAAI Phase 2 Reviewers Need to Submit Their Reviews?

","I have a quick question regarding the AAAI Phase 2 review process. Specifically, when do the reviewers need to submit their reviews?

I’m considering submitting my paper to arXiv, but I’ve heard concerns that some reviewers can be biased toward specific research groups or first-time authors. I want to ensure that my paper's chances of acceptance aren't negatively affected by any potential biases. Once I upload my paper to arXiv, I understand that my anonymity is lost, which adds to my concerns.

Any insights or advice on this topic would be greatly appreciated! Thanks in advance!",morphinejunkie,1g8l0vq,https://reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/,https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/,2024-10-21 08:45:53,3,0.64,3,0,3,0,0,False,False,True,False,False,Research,self,t3_1g8l0vq
MachineLearning,[D] Efficient CNNs for inference,"I am working on an object detection project using high resolution images.

Are there any techniques that can make a trained CNN (UNet) efficient during inference? I know pruning is one such technique, but it risks loss of accuracy and parallelizability.",_My__Real_Name_,1g8kpl6,https://reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/,https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/,2024-10-21 08:20:29,25,0.88,25,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1g8kpl6
MachineLearning,[D] AquaVoice style text edition model,"Don't know why this idea (which is cool) never caught up, but I'm wondering if we could build an open-source model for the same, eg a fine-tuned LLM with perhaps a small model that tries to distinguish between when the user is providing ""text value"", and when he is speaking ""edition commands"", and then do the edits

A ""basic prototype"" shouldn't be too hard, but could be quite helpful

https://withaqua.com/",oulipo,1g8jzcq,https://reddit.com/r/MachineLearning/comments/1g8jzcq/d_aquavoice_style_text_edition_model/,https://www.reddit.com/r/MachineLearning/comments/1g8jzcq/d_aquavoice_style_text_edition_model/,2024-10-21 07:22:05,2,0.67,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g8jzcq
MachineLearning,[R] Google Shopping 10M dataset for large scale multimodal product retrieval and ranking,"We have finally released the Marqo Google Shopping 10 million dataset on Hugging Face ([Marqo-GS-10M](https://huggingface.co/datasets/Marqo/marqo-GS-10M)). One of the largest and richest datasets for multimodal product retrieval! 

+ 10M rows of query, product title, image and rank (1-100) 

+ \~100k unique queries 

+ \~5M unique products across fashion and home 

+ Reflects real-world data and use cases and serves as a good benchmark for method development 

+ Proper data splits, in-domain, novel query, novel document and novel-document and novel query. 

The dataset features detailed relevance scores for each query-document pair to facilitate future research and evaluation.

    !pip install datasets
    from datasets import load_dataset
    ds = load_dataset(""Marqo/marqo-GS-10M"")

We curated this large-scale dataset as part of the publication of our training framework: Generalized Contrastive Learning (GCL). 

Dataset: [https://huggingface.co/datasets/Marqo/marqo-GS-10M](https://huggingface.co/datasets/Marqo/marqo-GS-10M)

GCL: [https://github.com/marqo-ai/GCL](https://github.com/marqo-ai/GCL)

Paper: [https://arxiv.org/abs/2404.08535](https://arxiv.org/abs/2404.08535)",Jesse_marqo,1g8a3pv,https://reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/,https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/,2024-10-20 21:51:41,148,0.97,148,0,4,0,0,False,False,True,False,False,Research,self,t3_1g8a3pv
MachineLearning,[R] xLSTM hidden state is not used,"Hello to everyone, i was reading the xLSTM paper [**https://arxiv.org/pdf/2405.04517**](https://arxiv.org/pdf/2405.04517) in particular the section about mLSTM and I was wondering, where the hidden state is used? What is its utility? Normally it is used to compute the output gate.

https://preview.redd.it/7m51jnjhdzvd1.png?width=939&amp;format=png&amp;auto=webp&amp;s=fc84cdcaac47110af22a86b86ff5390ef4e53a37",splashhhhhhhhhhhh,1g89uuh,https://reddit.com/r/MachineLearning/comments/1g89uuh/r_xlstm_hidden_state_is_not_used/,https://www.reddit.com/r/MachineLearning/comments/1g89uuh/r_xlstm_hidden_state_is_not_used/,2024-10-20 21:40:48,16,0.84,16,0,5,0,0,False,False,True,False,False,Research,self,t3_1g89uuh
MachineLearning,"[Discussion] Now that i have an engineering job, how do i keep updated on latest interesting papers ?","Hey guys, in the past i used to work in a lab, doing researsh on computer vision &amp; ML. Talking with professors and PhDs, i would have a good idea of new interresting articles. Now that i work in a big company, i don't have this network anymore and i don't have time to spend hours searshing new interresting articles. Are there any  good ressources that aggregate cool articles related to ML &amp; CV ?",Fugius,1g893lr,https://reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/,https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/,2024-10-20 21:07:02,78,0.9,78,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1g893lr
MachineLearning,[D] Classify obfuscated text ,"How would you classify obfuscated text that preserved the pattern of words. Would you use a simple vectorisation method such as tf-idf or use a character level embedding model? Or any other smart approach you’ll take? 
There is enough data to finetune an embedding model but I don’t have enough time to do it. 
Any input is much appreciated.",Rahahp,1g87284,https://reddit.com/r/MachineLearning/comments/1g87284/d_classify_obfuscated_text/,https://www.reddit.com/r/MachineLearning/comments/1g87284/d_classify_obfuscated_text/,2024-10-20 19:38:01,0,0.38,0,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1g87284
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1g80nkv,https://reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1g80nkv/d_simple_questions_thread/,2024-10-20 15:00:49,3,0.8,3,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1g80nkv
MachineLearning,"[D] Last Week in Medical AI: Top LLM Research Papers/Models (October 12 - October 19)
","[Last Week in Medical AI: Top LLM Research Papers\/Models \(October 12 - October 19\)](https://preview.redd.it/u2p81gvo0xvd1.jpg?width=1386&amp;format=pjpg&amp;auto=webp&amp;s=22b0099b069104750a4c7bac0f99e4c60dc72912)

  
**Medical LLM &amp; Other Model**

* **OLAPH: Factual Biomedical LLM QA**
   * This paper introduces MedLFQA, a benchmark dataset for evaluating the factuality of long-for answers generated by large language models (LLMs) in the medical domain.
* **LLMD: Interpreting Longitudinal Medical Records**
   * This paper introduces LLMD, a large language model designed to analyze patient medical history.
* **LifeGPT: Generative Transformer for Cells**
   * This paper introduces LifeGPT, a decoder-only generative pretrained transformer (GPT) model trained to simulate Conway's Game of Life on a toroidal grid without prior knowledge of grid size or boundary conditions.
* **MedCare: Decoupled Clinical LLM Alignment**
   * This paper introduces MedCare, a Medical LLM that leverages a progressive fine-tuning pipeline to address knowledge-intensive and alignment-required tasks in medical NLP.
* Y-Mol: Biomedical LLM for Drug Development
   * This paper introduces Y-Mol, a multiscale biomedical knowledge-guided large language model (LLM) designed for drug development tasks spanning lead compound discovery, pre-clinic, and clinic prediction.

**Frameworks and Methodologies:**

* MedINST: Biomedical Instructions Meta Dataset
* Democratizing Medical LLMs via Language Experts
* MCQG-SRefine: Iterative Question Generation
* Adaptive Medical Language Agents
* MeNTi: Medical LLM with Nested Tools

**Medical LLM Applications:**

* AGENTiGraph: LLM Chatbots with Private Data
* MMed-RAG: Multimodal Medical RAG System
* Medical Graph RAG: Safe LLM via Retrieval
* MedAide: Multi-Agent Medical LLM Collaboration
* Synthetic Clinical Trial Generation

**Medical LLMs &amp; Benchmarks:**

* WorldMedQA-V: Multimodal Medical LLM Dataset
* HEALTH-PARIKSHA: RAG Models Evaluation
* Synthetic Data for Medical Vision-Language
* ....

...

Full thread in detail: [https://x.com/OpenlifesciAI/status/1847686504837202263](https://x.com/OpenlifesciAI/status/1847686504837202263)",aadityaura,1g7yzh8,https://reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/,https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/,2024-10-20 13:42:31,11,0.77,11,0,14,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/hyfY2SIaaxgn0EDiJWa08cDMMLSOkKJGkZKey-UI8J8.jpg,t3_1g7yzh8
MachineLearning,[D] Research in open source robotics control systems?,"There several ML papers which go over control in robotics. These cover concepts such as world model creation, task simulation, and distillation of control systems.

My question is, is there a field yet which seeks to create a general framework for developing these control systems?

I’m looking for specifically a paper which goes to define some sort of general robotic framework and a subsequent generalized control system framework for defining control on the robot. 

The type of research I’m looking specifically for would define robotics primitives in some form, types of motion in 3D + types of sensors in 3D, as well as vision.

Any sort of direction in this would be of interest to me Thanks!",andarmanik,1g7y7ev,https://reddit.com/r/MachineLearning/comments/1g7y7ev/d_research_in_open_source_robotics_control_systems/,https://www.reddit.com/r/MachineLearning/comments/1g7y7ev/d_research_in_open_source_robotics_control_systems/,2024-10-20 13:03:04,10,0.81,10,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1g7y7ev
MachineLearning,[D] Future of Multi-Armed Bandits?,"I am just beginning my MAB research, but my original research interest lies in either Deep-RL or Optimization. Unfortunately, I found there are very few intersections between DRL/Opt and MAB (Please tell me if I am wrong), or even say that MAB is a kind of 'isolated' research field, neither produces interesting things (like transformers-&gt;GPT ) nor give you a promising career prospect (like opt favored by hedge fund). The most notable application is on recommend system, but I dont really see company other than Netflix would recruit, and not suitable for quant path.

Is MAB research dying? Should I quit MAB if I am no longer interested in academia and just want to become a quant after graduation? Or are there any intersection between MAB and other direction such I could become a paper machine?",petrichorinforest,1g7vss8,https://reddit.com/r/MachineLearning/comments/1g7vss8/d_future_of_multiarmed_bandits/,https://www.reddit.com/r/MachineLearning/comments/1g7vss8/d_future_of_multiarmed_bandits/,2024-10-20 10:35:18,48,0.89,48,0,29,0,0,False,False,True,False,False,Discussion,self,t3_1g7vss8
MachineLearning,[D] How to discredit your whole paper in one figure,"arxiv.org/abs/2410.13854
Did they just really compare English-language memes to traditional Chinese paintings and use that as the basis for ""Chinese images are harder to understand"" (figure 1)?

Edit: I believe the rest of the paper is sensible and the cultural background necessary for understanding traditional Chinese art is important but the comparison is dishonest. The examples they give in Appendix B (figure 7) are better.",qu3tzalify,1g7r3hn,https://reddit.com/r/MachineLearning/comments/1g7r3hn/d_how_to_discredit_your_whole_paper_in_one_figure/,https://www.reddit.com/r/MachineLearning/comments/1g7r3hn/d_how_to_discredit_your_whole_paper_in_one_figure/,2024-10-20 04:46:12,75,0.83,75,0,32,0,0,False,False,True,False,False,Discussion,self,t3_1g7r3hn
MachineLearning,Endorsement for a AI paper in Arvix [R],"Hi Community,

I am currently working in the field of Applied Neuroscience and have authored a paper exploring how we can integrate more of the brain’s neuron functions into Artificial Neural Networks  in future models. While this is not groundbreaking work worthy of a Nobel Prize:-), I believe it offers a meaningful contribution to this evolving topic. I would be incredibly grateful if you could provide an endorsement to help me publish it on arXiv.

Thank you for your time and consideration.",Appeswintogether,1g7mjgs,https://reddit.com/r/MachineLearning/comments/1g7mjgs/endorsement_for_a_ai_paper_in_arvix_r/,https://www.reddit.com/r/MachineLearning/comments/1g7mjgs/endorsement_for_a_ai_paper_in_arvix_r/,2024-10-20 00:17:16,0,0.42,0,0,4,0,0,False,False,True,False,False,Research,self,t3_1g7mjgs
MachineLearning,[D] Resources for a Course on NeuroSymbolic AI,"Hi guys. I'm helping my advisor in creating a course on neurosymbolic ai. the issue we are facing is lack of comprehensive resources. currently i have found only two courses and two recent books on the topic. can you provide more comprehensive resources that could be used in preparation for creating a draft of a course.

this is what i have found so far:  
[NeuroSymbolic Artificial Intelligence: Course Slides](https://cedar.buffalo.edu/~srihari/CSE701/)  
[kastle-lab/cs7820-neurosymbolic-ai](https://github.com/kastle-lab/cs7820-neurosymbolic-ai)

[now publishers - Neurosymbolic Programming](https://www.nowpublishers.com/article/Details/PGL-049)  
[Neuro Symbolic Reasoning and Learning | SpringerLink](https://link.springer.com/book/10.1007/978-3-031-39179-8)",Monte_Langevin_4P,1g7bhb0,https://reddit.com/r/MachineLearning/comments/1g7bhb0/d_resources_for_a_course_on_neurosymbolic_ai/,https://www.reddit.com/r/MachineLearning/comments/1g7bhb0/d_resources_for_a_course_on_neurosymbolic_ai/,2024-10-19 15:33:27,7,0.82,7,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1g7bhb0
MachineLearning,[R] medium article: Diffusion Auto-Regressive Transformer For Effective Self-Supervised Time Series Forecasting,"TimeDART is a novel self-supervised learning model that integrates diffusion and auto-regressive mechanisms to effectively capture both global sequence dependencies and local features in time series data. 

Its architecture employs a self-attention-based Transformer encoder to model inter-patch dependencies, while a cross-attention-based denoising decoder adjusts optimization difficulty for effective pre-training.

This design allows TimeDART to achieve state-of-the-art performance in forecasting tasks, outperforming advanced competitive methods by modeling intricate temporal relationships.

I wrote a medium article about it: [https://medium.com/towards-artificial-intelligence/diffusion-auto-regressive-transformer-for-effective-self-supervised-time-series-forecasting-31b7d7ba2062](https://medium.com/towards-artificial-intelligence/diffusion-auto-regressive-transformer-for-effective-self-supervised-time-series-forecasting-31b7d7ba2062)

https://preview.redd.it/3btist9pjqvd1.png?width=1400&amp;format=png&amp;auto=webp&amp;s=805f43d6a616460aad1bdecde26c81544752f80e

",rezayazdanfar,1g7c0q9,https://reddit.com/r/MachineLearning/comments/1g7c0q9/r_medium_article_diffusion_autoregressive/,https://www.reddit.com/r/MachineLearning/comments/1g7c0q9/r_medium_article_diffusion_autoregressive/,2024-10-19 15:58:20,2,1.0,2,0,3,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/MhlHRmBGbxvNEdl3ELB1fgIrVCYWve_n-wbSdBKVThI.jpg,t3_1g7c0q9
MachineLearning,[Discussion] Sophon AI Accelerator accuracy problem,"Hey everyone,
I recently got my hands on a Sophon AI accelerator and wanted to share some strange findings. I was testing the ResNet50 model in FP32 mode (32-bit floating-point) on both the Sophon and an NVIDIA RTX 4090 GPU. I used the exact same ONNX model and dataset to keep things fair.
Here's what I observed:
-NVIDIA RTX 4090 (FP32): 76.7% accuracy;
-Sophon (FP32): 73.2% accuracy

I was surprised to see that the Sophon had a noticeable drop in accuracy compared to the NVIDIA GPU, even though both were running in FP32 mode. Isn't FP32 supposed to give us the highest precision?
I double-checked everything:
-Same model and weights
-Same dataset
-Same preprocessing steps

But the accuracy gap is still there. I'm wondering why there's such a significant difference. Could it be due to Sophon's hardware architecture or how it handles computations internally?
Has anyone else experienced something like this with the Sophon accelerator? Any ideas on what might be causing the lower accuracy in FP32 mode?
Just thought I'd share my experience and see if anyone has insights. Cheers!",Comfortable-Jump-749,1g7f0qj,https://reddit.com/r/MachineLearning/comments/1g7f0qj/discussion_sophon_ai_accelerator_accuracy_problem/,https://www.reddit.com/r/MachineLearning/comments/1g7f0qj/discussion_sophon_ai_accelerator_accuracy_problem/,2024-10-19 18:14:38,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g7f0qj
MachineLearning,[R] Molecular Topological Profile (MOLTOP) - Simple and Strong Baseline for Molecular Graph Classification,"Accepted at ECAI 2024 conference, ArXiv: [https://arxiv.org/abs/2407.12136](https://arxiv.org/abs/2407.12136)

Some highlights:

- simple feature engineering on graphs

- it outperforms GROVER (NeurIPS 2020) and GraphMVP (ICLR 2024) on MoleculeNet, and all models on peptide function prediction on LRGB (e.g. SAN+RWSE graph transformer)

- no hyperparameters to tune, takes seconds on most datasets

- surprisingly powerful in distinguishing non-isomorphic graphs

In short, if you need a good and simple baseline for molecular graph classification, MOLTOP may be a good choice. We designed it not to be the best, but to be fast, easy-to-use, and also give strong results on average.

Abstract:

&gt;We revisit the effectiveness of topological descriptors for molecular graph classification and design a simple, yet strong baseline. We demonstrate that a simple approach to feature engineering - employing histogram aggregation of edge descriptors and one-hot encoding for atomic numbers and bond types - when combined with a Random Forest classifier, can establish a strong baseline for Graph Neural Networks (GNNs). The novel algorithm, Molecular Topological Profile (MOLTOP), integrates Edge Betweenness Centrality, Adjusted Rand Index and SCAN Structural Similarity score. This approach proves to be remarkably competitive when compared to modern GNNs, while also being simple, fast, low-variance and hyperparameter-free. Our approach is rigorously tested on MoleculeNet datasets using fair evaluation protocol provided by Open Graph Benchmark. We additionally show out-of-domain generation capabilities on peptide classification task from Long Range Graph Benchmark. The evaluations across eleven benchmark datasets reveal MOLTOP's strong discriminative capabilities, surpassing the 1-WL test and even 3We revisit the effectiveness of topological descriptors for molecular graph classification and design a simple, yet strong baseline. We demonstrate that a simple approach to feature engineering - employing histogram aggregation of edge descriptors and one-hot encoding for atomic numbers and bond types - when combined with a Random Forest classifier, can establish a strong baseline for Graph Neural Networks (GNNs). The novel algorithm, Molecular Topological Profile (MOLTOP), integrates Edge Betweenness Centrality, Adjusted Rand Index and SCAN Structural Similarity score. This approach proves to be remarkably competitive when compared to modern GNNs, while also being simple, fast, low-variance and hyperparameter-free. Our approach is rigorously tested on MoleculeNet datasets using fair evaluation protocol provided by Open Graph Benchmark. We additionally show out-of-domain generation capabilities on peptide classification task from Long Range Graph Benchmark. The evaluations across eleven benchmark datasets reveal MOLTOP's strong discriminative capabilities, surpassing the 1-WL test and even 3-WL test for some classes of graphs. Our conclusion is that descriptor-based baselines, such as the one we propose, are still crucial for accurately assessing advancements in the GNN domain.

Happy to discuss / answer any questions in the comments.",qalis,1g7gj4m,https://reddit.com/r/MachineLearning/comments/1g7gj4m/r_molecular_topological_profile_moltop_simple_and/,https://www.reddit.com/r/MachineLearning/comments/1g7gj4m/r_molecular_topological_profile_moltop_simple_and/,2024-10-19 19:23:53,4,0.83,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1g7gj4m
MachineLearning,[D] Why do PhD Students in the US seem like overpowered final bosses ,"Hello,

I'm a PhD student in a European university, working on AI/ML/CV ..etc. my PhD is 4 years. The first year I literally just spent learning how to actually do research, teaching one course to learn how things work...etc. Second year, I published my first publication as a co-author in CVPR. By third year, I can manage research projects, I understand how to do grants applications, how funding works, the politics of it all ...etc. I added to my CV, 2 publications, one journal and another conference as first author. I'm very involved in industry and I also write a lot of production grade code in regard to AI, systems architecture, backend, cloud, deployment, etc for companies that have contracts with my lab.

The issue is when I see PhD students similar to me in the US, they be having 10 publications, 5 of them 1st author, all of them are either CVPR, ICML, ICLR, NeurIPS ...etc. I don't understand, do these people not sleep ? How are they able to achieve this crazy amount of work and still have 3 publications every year in A\* journals ?

I don't think these people are smarter than I, usually I get ideas and I look up if something exists, and I can see that something was just published by some PhD student in Stanford or DeepMind ..etc like 1 month ago, So I can see that my reasoning isn't late in regard to SOTA. but the concepts that you would need to grasp to just have one of those publications + the effort and the time you need to invest and the resources to get everything done, wouldn't be possible for 2\~3 months project. How is it possible for these people to do this ?

Thank you !",SilenceForLife,1g7dzkp,https://reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/,https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/,2024-10-19 17:27:33,1087,0.95,1087,0,282,0,0,False,False,True,False,False,Discussion,self,t3_1g7dzkp
MachineLearning,[P] I built a web app to track trending AI papers using Mendeley reader counts,"Hey everyone!

I've created a web app that helps researchers and AI interested folks stay on top of the most impactful arXiv AI papers. 

Features:
- Tracks papers based on Mendeley reader counts
- Customizable time periods: 1w, 1m, 3m, 6m, 1y, and all-time
- Two viewing modes: 
  1. ""Greatest"" - shows papers with the highest total reader counts
  2. ""Trending"" - highlights papers gaining readers the fastest

The backend is entirely written in Python, and the frontend is basic JS, HTML, and Tailwind.

I'm also considering open-sourcing the project when I have more time.

Questions for the community:
1. Would you find this tool useful for your research or studies?
2. Any features you'd like to see added?
3. Anyone interested in contributing if I open-source it?

Demo link: https://aipapers.pantheon.so",yachty66,1g7barf,https://reddit.com/r/MachineLearning/comments/1g7barf/p_i_built_a_web_app_to_track_trending_ai_papers/,https://www.reddit.com/r/MachineLearning/comments/1g7barf/p_i_built_a_web_app_to_track_trending_ai_papers/,2024-10-19 15:25:04,13,0.84,13,0,1,0,0,False,False,True,False,False,Project,self,t3_1g7barf
MachineLearning,"[P] NHiTs: Deep Learning + Signal Processing for Time-Series Forecasting
","NHITs is a SOTA DL for time-series forecasting because:

* Accepts past observations, future known inputs, and static exogenous variables.
* Uses multi-rate signal sampling strategy to capture complex frequency patterns — essential for areas like financial forecasting.
* Point and probabilistic forecasting.

You can find a detailed analysis of the model [here](https://aihorizonforecast.substack.com/p/forecasting-with-nhits-uniting-deep): ",apaxapax,1g7b6po,https://reddit.com/r/MachineLearning/comments/1g7b6po/p_nhits_deep_learning_signal_processing_for/,https://www.reddit.com/r/MachineLearning/comments/1g7b6po/p_nhits_deep_learning_signal_processing_for/,2024-10-19 15:19:49,32,0.92,32,0,10,0,0,False,False,True,False,False,Project,self,t3_1g7b6po
MachineLearning,[D] Layernorm is confusingly named?,"TIL, that in NLP, you do layer norm by taking the activations across each feature and normalizing across each feature:

Therefore, if you had a batch of two examples:

\[I, am, a, boy\]

\[She, is a girl\]

then you would create

\[H\_I, H\_am, H\_a, H\_boy\] -&gt; \[u\_I, u\_am, u\_a, u\_boy\] and sigmas

\[H\_She, H\_is, H\_a, H\_girl\] -&gt; \[u\_she, u\_is, u\_a, u\_girl\] and sigmas

So you end up with 8 us and sigma. This doesn't sound like you are normalizing the layer's activations, which leads me to say that layernorm is confusingly named. Am I missing something?",Complex-Media-8074,1g7am36,https://reddit.com/r/MachineLearning/comments/1g7am36/d_layernorm_is_confusingly_named/,https://www.reddit.com/r/MachineLearning/comments/1g7am36/d_layernorm_is_confusingly_named/,2024-10-19 14:53:24,32,0.82,32,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1g7am36
MachineLearning,"[Project] Tsetlin Machine for Deep Logical Learning and Reasoning With Graphs (finally, after six years!)","https://preview.redd.it/spcqdkqwnovd1.png?width=2643&amp;format=png&amp;auto=webp&amp;s=ba0d7dd294ef9814f20bf3950f0049e80cf8d8d9

Hi all! I just completed the first deep Tsetlin Machine - a Graph Tsetlin Machine that can learn and reason multimodally across graphs. After introducing the Tsetlin machine in 2018, I expected to figure out how to make a deep one quickly. Took me six years! Sharing the project: [https://github.com/cair/GraphTsetlinMachine](https://github.com/cair/GraphTsetlinMachine)

Features:

* Processes directed and labeled [multigraphs](https://en.wikipedia.org/wiki/Multigraph)
* [Vector symbolic](https://link.springer.com/article/10.1007/s10462-021-10110-3) node properties and edge types
* Nested (deep) clauses
* Arbitrarily sized inputs
* Incorporates [Vanilla](https://tsetlinmachine.org/wp-content/uploads/2022/11/Tsetlin_Machine_Book_Chapter_One_Revised.pdf), Multiclass, [Convolutional](https://tsetlinmachine.org/wp-content/uploads/2023/12/Tsetlin_Machine_Book_Chapter_4_Convolution.pdf), and [Coalesced](https://arxiv.org/abs/2108.07594) [Tsetlin Machines](https://tsetlinmachine.org/)
* Rewritten faster CUDA kernels

Roadmap:

* Rewrite [graphs.py](http://graphs.py) in C or numba for much faster construction of graphs
* Add autoencoder
* Add regression
* Add multi-output
* Graph initialization with adjacency matrix

Happy to receive feedback on the next steps of development!",olegranmo,1g75gcb,https://reddit.com/r/MachineLearning/comments/1g75gcb/project_tsetlin_machine_for_deep_logical_learning/,https://www.reddit.com/r/MachineLearning/comments/1g75gcb/project_tsetlin_machine_for_deep_logical_learning/,2024-10-19 09:48:27,93,0.96,93,0,17,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/tXrJQoeF-4_SEVuYoaIAl8bJXyTRX6Sn0UehcxUkId8.jpg,t3_1g75gcb
MachineLearning,[D] Transformer Autoencoder?,"Is anyone aware of any research into the use of an encoder - decoder Transformer architecture for training a text/sequence autoencoder with an information bottleneck? I'm imagining something like bert for the encoder where the full sequence is encoded into a single vector followed by a decoder that cross-attends to the encoded latent representation and is trained to reproduce the full sequence in its entirety.

It seems like such an obvious use case that I'm sure I must be missing something but I can't seem to find any literature on it. Like, Transformers have been encoder - decoder architectures since day one, using them for an information bottleneck autoencoder seems like such a no brainer that someone out there must have tried it. Is it just a bad idea for some reason I'm not aware of or has it been tried to no avail and the paper has descended into irrelevance as a result?

On the surface it seems like a great way to pretrain both an encoder and a decoder (way better than T5 surely?) since you still get to use the dead simple next token prediction task and with finetuning I can see the encoder becoming incredibly useful for RAG and potentially unlocking or enhancing other ICL abilities. Other ideas that could be worth exploring off the top of my head are latent diffusion text generation and context compression for extreme long context abilties. I'm sure there are more.

As far as I can tell the only drawback would be the need to split memory and compute between the encoder and decoder during training.

Anyone got any leads for me to follow here?",next-choken,1g748gu,https://reddit.com/r/MachineLearning/comments/1g748gu/d_transformer_autoencoder/,https://www.reddit.com/r/MachineLearning/comments/1g748gu/d_transformer_autoencoder/,2024-10-19 08:12:51,3,0.62,3,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1g748gu
MachineLearning,[D] An interesting thread from December 2021 discussing the efficacy of transformers,,next-choken,1g73ym7,https://reddit.com/r/MachineLearning/comments/1g73ym7/d_an_interesting_thread_from_december_2021/,https://www.reddit.com/r/MachineLearning/comments/rboh2r/d_are_transformers_overhyped/,2024-10-19 07:51:02,22,0.89,22,0,4,0,0,False,False,False,False,False,Discussion,default,t3_1g73ym7
MachineLearning,[D] Evaluating Classification in Production,"For Facebook, Youtube, LinkedIn, etc, how do they evaluate their harmful content detection models in production? If it is offline, precision, recall, F1 can be used. Since we don’t know what the labels are during production, what metrics do they use for evaluation/monitoring?",lalalagay,1g6w6za,https://reddit.com/r/MachineLearning/comments/1g6w6za/d_evaluating_classification_in_production/,https://www.reddit.com/r/MachineLearning/comments/1g6w6za/d_evaluating_classification_in_production/,2024-10-18 23:53:15,15,1.0,15,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g6w6za
MachineLearning,[D] Working with Weather Radar Data,"I have started working with weather radar data, given in raw binary format - converted into IRIS format for ease of access and computation, but need to perform machine learning on the data to make future predictions.

Is there any good source for understanding such data, and AI/ML techniques relating to this type of data?",Mynameiswrittenhere,1g6tmvv,https://reddit.com/r/MachineLearning/comments/1g6tmvv/d_working_with_weather_radar_data/,https://www.reddit.com/r/MachineLearning/comments/1g6tmvv/d_working_with_weather_radar_data/,2024-10-18 21:49:10,2,0.67,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1g6tmvv
MachineLearning,[R] LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench,"Updated Paper [https://arxiv.org/pdf/2410.02162](https://arxiv.org/pdf/2410.02162) (includes results when paired w/ a verifier)

Original Paper: [https://www.arxiv.org/abs/2409.13373](https://www.arxiv.org/abs/2409.13373)

""while o1’s performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it..""

The summary is apt.  o1 looks to be a **very** impressive improvement.  At the same time, it reveals the remaining gaps:  degradation with increasing composition length,  100x cost, and huge degradation when ""retrieval"" is hampered via obfuscation of names.

But, I wonder if this is close enough. e.g. this type of model is at least sufficient to provide synthetic data / supervision to train a model that can fill these gaps.   If so, it won't take long to find out, IMHO.

Also the authors have some spicy footnotes.  e.g. :

""The rich irony of researchers using tax payer provided research funds to pay private companies like OpenAI to evaluate their private commercial models is certainly not lost on us.""",marojejian,1g6qpeq,https://reddit.com/r/MachineLearning/comments/1g6qpeq/r_llms_still_cant_plan_can_lrms_a_preliminary/,https://www.reddit.com/r/MachineLearning/comments/1g6qpeq/r_llms_still_cant_plan_can_lrms_a_preliminary/,2024-10-18 19:36:07,112,0.9,112,0,47,0,0,False,False,True,False,False,Research,self,t3_1g6qpeq
MachineLearning,[D] Attention saved in LLM decoding?,"During LLM generation (decoding), is the full attention being recalculated at every step? I know things like KV cache is used to save the previous key/value vectors, but what about their attention scores? Or only the current query's attention is calculated?

Thank you!",BigYounzzz,1g6ozlr,https://reddit.com/r/MachineLearning/comments/1g6ozlr/d_attention_saved_in_llm_decoding/,https://www.reddit.com/r/MachineLearning/comments/1g6ozlr/d_attention_saved_in_llm_decoding/,2024-10-18 18:22:06,3,0.81,3,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1g6ozlr
MachineLearning,[D] Emotion Classification using Raw Speech/Audio - Any Resources and Guidance?,"Hi guys,

I'm working on a project involving emotion recognition/classification using raw speech/audio data, and I'm hitting a roadblock. Unlike most approaches that transcribe audio to text and then perform sentiment analysis, I want to directly classify emotions from audio signals.

Has anyone worked on or knows of any notable projects/studies that use raw audio features for emotion classification?

Any guidance, paper recommendations, or code repositories would be greatly appreciated.  
  
**TLDR:** Seeking resources and expertise on emotion classification using raw speech/audio data, without converting to text.",Low-Champion-4194,1g6o94x,https://reddit.com/r/MachineLearning/comments/1g6o94x/d_emotion_classification_using_raw_speechaudio/,https://www.reddit.com/r/MachineLearning/comments/1g6o94x/d_emotion_classification_using_raw_speechaudio/,2024-10-18 17:50:48,3,0.81,3,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g6o94x
MachineLearning,[D] Testing for posterior collapse ,"I have a VAE trained on my dataset. It exhibits the following things

1. The encoder produces model parameters very very close together. The majority of mus land withing 1e-1 and 1e-2. The log variance is similar.
2. Sampling new points always returns 0.
3. The training data also becomes all 0 after some epochs. 

I suspect that the model is poorly specified. However, if I ignore the kl loss, things improve a little (i only train for 50 epochs). I'm wondering if the model is suffering from posterior collapse. 

How would one go about testing that hypothesis? ",Chr0nomaton,1g6l9qv,https://reddit.com/r/MachineLearning/comments/1g6l9qv/d_testing_for_posterior_collapse/,https://www.reddit.com/r/MachineLearning/comments/1g6l9qv/d_testing_for_posterior_collapse/,2024-10-18 15:45:56,6,1.0,6,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g6l9qv
MachineLearning,"[D] How to approach a Time Series problem with ""partially realized data""","Hi'all,

I've been given a problem to try to tackle at work, and since I don't have lots of experience with time series, this feels like something that's not very basic to model. I would have to use Python for this, not R, so I understand my options are probably more limited?

In a nutshell,

* I have 19 months of daily data between 2021-01-01 and 2022-07-31 with customers placing orders. This gives me the days with orders, how many orders, the amount of the transactions ($). No other exogenous information is available immediately;
* I'm trying to predict **order days** (yes/no target, not a count) in the following month (aug-22); the caveat is, I have partial information for august, namely, thousands of accounts have already placed one or more orders in august and I have that information before starting my model. The goal is, then, to take that into account when making my predictions.
* I see two ways of going about this: 1) the quickest/naive approach could be modeling august's 31 days w/o using the partially realized data and later on just ""discount"" orders placed from the orders predicted along the month; 2) seems like the right thing to do, but harder, which is to use this extra data from august to make better predictions about august;
* I don't really know what I'm looking for in terms of literature; gpt threw some terminology at me when I asked ""what to study/what to look for"", such as *""Rolling forecast with partial realizations"", ""Forecasting with incomplete data"", ""Censored data forecasting"", ""Real-time forecasting"", ""Nowcasting with partial information"", ""Intermittent demand forecasting"".*
* I have a decent knowledge of Statistics and got in touch with survival analysis and censored data, but was testing the waters to see if simpler approaches are available.

Thank you!

  
*EDIT: learned about the framework of Panel Data Modeling! Looking into it now.*",pdr07,1g6hpto,https://reddit.com/r/MachineLearning/comments/1g6hpto/d_how_to_approach_a_time_series_problem_with/,https://www.reddit.com/r/MachineLearning/comments/1g6hpto/d_how_to_approach_a_time_series_problem_with/,2024-10-18 13:11:46,2,0.6,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1g6hpto
MachineLearning,[D] multi-modal neural networks for time series?,"I am looking for a neural network for time series that can handle non-time series input.

Specifically a neural network where I can give it a single main time serie (and perhaps additional auxiliary time series), and some properties about this particular time series. With the goal of forecasting the main time serie.

As an example, suppose I have individual household energy consumptions, but my measurements are not all from the same time period. Some houses I only have for 2023, while others start in 2024 ect. To go with each of these energy consumptions time series, I might have additional information about the consumer household, like the size of the house or the number of people living in the house, or just a unique house_id.
I would like to train my neural network on this kind of time series, utilizing these additional parameters in some sort of embedding such that the neural network is able to generate accurate predictions for any of these household when given its specific embedding.

So something that looks like:

forecasting=model(input_time_serie, auxiliary_time_series, additional_properties)

where input_time_serie is a single time serie vector of length n, auxiliary_time_series are optional time series of length n (could be outside temperature, time of the week, ect.), and additional_properties is a vector of length m containing property parameters that is somehow embedded inside the neural network and allows the neural network to distinguish between two different households.

Such a neural network could hopefully also be used in zero-shot predictions, where we do not yet have any actual energy consumption for the household, but only have the embedding data.

I am aware of https://github.com/thuml/Time-Series-Library, but the problem with all these types of neural network is that the expect all the different time series given as input, so one for each individual household, but this does not work when my time-series are not really overlapping, and it also does not work when I want to do zero-shot predictions on a new household not in the training dataset.

So does anyone know of any neural network capable of something like this? or does anyone have any good ideas about how to modify a neural network to sensibly include the property embedding?",alyflex,1g6gcxl,https://reddit.com/r/MachineLearning/comments/1g6gcxl/d_multimodal_neural_networks_for_time_series/,https://www.reddit.com/r/MachineLearning/comments/1g6gcxl/d_multimodal_neural_networks_for_time_series/,2024-10-18 12:01:22,10,0.78,10,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1g6gcxl
MachineLearning,[R] Limitations in Mainstream LLM Tokenizers,"Mainstream LLM tokenizers cann't encode and decode to exact string. This means they aren't lossless. Some Llama, Mistral, and Phi tokenizers cannot encode string `' Who let the dog out?! !'` and then decode to the same string.

If you run code:
```python
from transformers import AutoTokenizer

models = [
    'meta-llama/Llama-2-7b',
    'meta-llama/Meta-Llama-3-8B',
    'meta-llama/Llama-3.1-8B',
    'mistralai/Mistral-7B-v0.3',
    'mistralai/Mixtral-8x7B-v0.1',
    'mistralai/Mixtral-8x22B-v0.1',
    'mistralai/Mistral-Nemo-Instruct-2407',
    'mistralai/Mistral-Small-Instruct-2409',
    'mistralai/Mistral-Large-Instruct-2407',
    'microsoft/phi-1',
    'microsoft/phi-1_5',
    'microsoft/phi-2',
    'microsoft/Phi-3-mini-4k-instruct',
    'microsoft/Phi-3.5-mini-instruct',
]

text = ' Who let the dog out?! !'

for n in models:
    tokenizer = AutoTokenizer.from_pretrained(n)
    text2 = tokenizer.decode(tokenizer.encode(text, add_special_tokens=False))
    
    if text2 == text:
        print('OK: ', n, repr(text2))
    else:
        print('ERR:', n, repr(text2))
```

You will get:
```
OK:  meta-llama/Llama-2-7b ' Who let the dog out?! !'
ERR: meta-llama/Meta-Llama-3-8B ' Who let the dog out?!!'
ERR: meta-llama/Llama-3.1-8B ' Who let the dog out?!!'
ERR: mistralai/Mistral-7B-v0.3 'Who let the dog out?! !'
OK:  mistralai/Mixtral-8x7B-v0.1 ' Who let the dog out?! !'
ERR: mistralai/Mixtral-8x22B-v0.1 'Who let the dog out?! !'
OK:  mistralai/Mistral-Nemo-Instruct-2407 ' Who let the dog out?! !'
OK:  mistralai/Mistral-Small-Instruct-2409 ' Who let the dog out?! !'
OK:  mistralai/Mistral-Large-Instruct-2407 ' Who let the dog out?! !'
ERR: microsoft/phi-1 ' Who let the dog out?!!'
ERR: microsoft/phi-1_5 ' Who let the dog out?!!'
ERR: microsoft/phi-2 ' Who let the dog out?!!'
OK:  microsoft/Phi-3-mini-4k-instruct ' Who let the dog out?! !'
OK:  microsoft/Phi-3.5-mini-instruct ' Who let the dog out?! !'
```

All marked with ERR cannot encode and then decode to the same string.",mtasic85,1g6dc8l,https://reddit.com/r/MachineLearning/comments/1g6dc8l/r_limitations_in_mainstream_llm_tokenizers/,https://www.reddit.com/r/MachineLearning/comments/1g6dc8l/r_limitations_in_mainstream_llm_tokenizers/,2024-10-18 08:35:43,35,0.91,35,0,13,0,0,False,False,True,False,False,Research,self,t3_1g6dc8l
MachineLearning,"""[P]"" How to make Microsoft Fairlearn's Exponentiated gradient work with a DistilBERT classification model","I have attached my code below but the relevant part is from \`\`\`class DistilBERTWrapper\`\`\` onwards.  For each iteration in Expo-Grad the outputs (precision &amp; recall values) are identical. Not sure what the issue is. The model itself works correctly and makes predictions as expected. I am using 30% of the data and only 3 epochs to make it run faster. 

    #%% Import libraries
    
    import pandas as pd
    import numpy as np
    import time
    
    import torch
    from torch.utils.data import Dataset, DataLoader
    from torch import nn
    from torch.cuda.amp import autocast, GradScaler
    
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support
    from sklearn import metrics as skm
    
    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
    
    from fairlearn.reductions import DemographicParity, ExponentiatedGradient
    from fairlearn.metrics import (
        MetricFrame,
        count,
    #    plot_model_comparison,
        selection_rate,
    #    selection_rate_difference,
    )
    
    # FOR FAIRNESS USE ""Exponentiated Gradient"" instead of GridSearch
    # This is 2x - 10x faster than GridSearch
    #%% Load and preprocess data
    
    df = pd.read_csv(""credit_risk.csv"")
    df = df.sample(frac=0.30, random_state=17)
    
    df = df.dropna()
    # df = df.fillna('Reject')
    X = df.drop('loan_status', axis=1)
    y = df['loan_status']  # target variable 
    A = X['gender']
    
    le = LabelEncoder()
    
    y = le.fit_transform(y)
    #A = le.fit_transform(X.sensitive_feature) (to make the sensitive feature binary)
    
    def features_to_text(row):
        return "" "".join([f""{col}: {val}"" for col, val in row.items()])
    
    X_text = X.apply(features_to_text, axis=1)
    
    X_train, X_val, y_train, y_val, A_train, A_val = train_test_split(X_text, y, A, test_size=0.1, random_state=99)
    
    y_train = pd.Series(y_train)
    y_val = pd.Series(y_val)
    
    #%% Creating classes
    
    class BinaryClassificationDataset(Dataset):
        def __init__(self, texts, labels, tokenizer, max_length):
            self.texts = texts
            self.labels = labels
            self.tokenizer = tokenizer
            self.max_length = max_length
    
        def __len__(self):
            return len(self.texts)
    
        def __getitem__(self, idx):
            text = str(self.texts.iloc[idx])
            label = self.labels.iloc[idx]
    
            encoding = self.tokenizer.encode_plus(
                text,
                add_special_tokens=True,
                max_length=self.max_length,
                return_token_type_ids=False,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt',
            )
    
            return {
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten(),
                'label': torch.tensor(label, dtype=torch.long)
            }
    
    #%%
    
    # Model and data loaders
    model_name = ""distilbert-base-uncased""
    tokenizer = DistilBertTokenizer.from_pretrained(model_name)
    binary_classifier = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)
    
    max_length = 128
    train_dataset = BinaryClassificationDataset(X_train, y_train, tokenizer, max_length=max_length)
    val_dataset = BinaryClassificationDataset(X_val, y_val, tokenizer, max_length=max_length)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)
    
    #%% # Training setup
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f""Now using: {device}, "", torch.cuda.get_device_name())
    
    binary_classifier.to(device)
    optimizer = torch.optim.AdamW(binary_classifier.parameters(), lr=0.000041)
    loss_fn = nn.CrossEntropyLoss()
    scaler = GradScaler()
    
    #%% # Training loop
    
    num_epochs = 3
    for epoch in range(num_epochs):
        time1 = time.time()
        binary_classifier.train()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0
        print(f""Epoch {epoch+1}/{num_epochs}\n"")
    
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)
    
            optimizer.zero_grad()
            
            with autocast():
                outputs = binary_classifier(input_ids, attention_mask=attention_mask)
                loss = loss_fn(outputs.logits, labels)
    
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
    
            total_loss += loss.item()
            
            _, predicted = torch.max(outputs.logits, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_predictions += labels.size(0)
    
        train_accuracy = correct_predictions / total_predictions
        print('Batch loop completed. Validation Starting... \n')
    
        # Validation
        binary_classifier.eval()
        val_loss = 0
        val_correct_predictions = 0
        val_total_predictions = 0
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)
    
                outputs = binary_classifier(input_ids, attention_mask=attention_mask)
                val_loss += loss_fn(outputs.logits, labels).item()
    
                _, predicted = torch.max(outputs.logits, 1)
                val_correct_predictions += (predicted == labels).sum().item()
                val_total_predictions += labels.size(0)
    
        val_accuracy = val_correct_predictions / val_total_predictions
    
        print(f""Training Loss: {total_loss/len(train_loader):.4f}"")
        print(f""Training Accuracy: {train_accuracy:.4f}"")
        print(f""Validation Loss: {val_loss/len(val_loader):.4f}"")
        print(f""Validation Accuracy: {val_accuracy:.4f}\n"")
        time2 = time.time()
        print(f""Time elapsed: {((time2-time1)/60):.2f} min\n\n"")
    
    binary_classifier.save_pretrained(""./distilbert_binary_classifier"")
    tokenizer.save_pretrained(""./distilbert_binary_classifier"")
    
    print(""Fine-tuning complete. Model saved."")
    
    #%% Loading the model
    
    model_name = ""./distilbert_binary_classifier""
    binary_classifier = DistilBertForSequenceClassification.from_pretrained(model_name)
    tokenizer = DistilBertTokenizer.from_pretrained(model_name)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    binary_classifier.to(device)
    
    #%% Example predictions
    
    binary_classifier.eval()
    
    predictions = np.empty(len(X_val), dtype=int)
    probabilities = np.empty(len(X_val), dtype=float)
    
    for i in range(len(X_val)):
    
        example_text = X_val.iloc[i]
        inputs = tokenizer(example_text, return_tensors=""pt"", padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
    
        with torch.no_grad():
            outputs = binary_classifier(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            predictions[i] = (torch.argmax(probs, dim=1).item())
            probabilities[i] = probs[0][predictions[i]].item()
    
    #%% Fairness Assessment
    
    metric_frame = MetricFrame(
        metrics={
            ""accuracy"": skm.accuracy_score,
            ""Positive class rate"": selection_rate,
            ""count"": count,
        },
        sensitive_features=A_val,
        y_true=y_val,
        y_pred=predictions,
    )
    
    print(""\nUnmitigated Fairness Evaluation"")
    print(metric_frame.overall)
    print(metric_frame.by_group)
    
    # metric_frame.by_group.plot.bar(
    #     subplots=True,
    #     layout=[3, 1],
    #     legend=False,
    #     figsize=[12, 8],
    #     title=""Accuracy and selection rate by group"",
    # )
    
    precision = precision_score(y_val, predictions)
    recall = recall_score(y_val, predictions)
    f1 = f1_score(y_val, predictions)
    
    print(f""\nPrecision: {precision:.4f}"")
    print(f""Recall: {recall:.4f}"")
    print(f""F1-score: {f1:.4f}"")
    
    # Precision - % of predicted positive class that are actually positive
    # Recall    - % of actual positive class that the predictor correctly identified
    # F-1 Score - Balance between precision and recall
    
    
    # %% Using Exponentiated gradient for Fairness Mitigation
    
    # Define a wrapper class for the DistilBERT model to work with Fairlearn
    
    class DistilBERTWrapper:
        counter=0
    
        def __init__(self, model, tokenizer, device):
            self.model = model
            self.tokenizer = tokenizer
            self.device = device
            self.fitted = False
            
        def fit(self, X, y, sample_weight=None):
            """"""Required by fairlearn, but our model is already trained""""""
            self.fitted = True
            return self
            
        def predict(self, X):
            """"""Predict method that works with both training and inference""""""
            if not self.fitted:
                return np.zeros(len(X))
                
            self.model.eval()
            predictions = []
            time3 = time.time()
    
            with torch.no_grad():
                # Handle both DataFrame and Series inputs
                if isinstance(X, pd.DataFrame):
                    text_series = X_train.iloc[X['text_id']]  # Use stored X_train
                else:
                    text_series = X_train.iloc[X]  # X is already a series of indices
                    
                for text in text_series:
                    inputs = self.tokenizer(
                        text, 
                        return_tensors=""pt"", 
                        padding=True, 
                        truncation=True, 
                        max_length=512
                    )
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    outputs = self.model(**inputs)
                    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
                    pred = torch.argmax(probs, dim=1).item()
                    predictions.append(pred)
                
                DistilBERTWrapper.counter += 1
                print(f""ITERATION #{DistilBERTWrapper.counter}"")
                precision, recall, f1, _ = precision_recall_fscore_support(X['true_labels'], predictions, average='weighted')
                print(f""Precision: {precision:.6f}, Recall: {recall:.6f}, F1: {f1:.6f}"")
                print(f'Time: {time.time() - time3:.2f}')
            return np.array(predictions)
    
    # Create index-based features for training
    X_train_encoded = pd.DataFrame({
        'text_id': range(len(X_train)),
        'gender': le.fit_transform(A_train),
        'true_labels': y_train
    })
    
    
    X_val_encoded = pd.DataFrame({
        'text_id': range(len(X_val)),
        'gender': le.transform(A_val),
        'true_labels': y_val
    })
    
    # Initialize the wrapper and constraint
    model_wrapper = DistilBERTWrapper(binary_classifier, tokenizer, device)
    constraint = DemographicParity()
    
    # Initialize and fit the ExponentiatedGradient with minimal iterations for testing
    exp_grad = ExponentiatedGradient(
        estimator=model_wrapper,
        constraints=constraint,
        max_iter=1,  # Adjust based on your needs
        eps=0.99,     # Convergence threshold
        nu=1,      # Initial step size
    )
    
    # Fit the model
    print(""Fitting ExponentiatedGradient..."")
    exp_grad.fit(
        X_train_encoded,
        y_train,
        sensitive_features=A_train
    )
    
    
    #%% Using the new model to predict &amp; Evaluate fairness 
    
    mitigated_predictions = exp_grad.predict(X_val_encoded)
    
    mitigated_metric_frame = MetricFrame(
        metrics={
            ""accuracy"": skm.accuracy_score,
            ""Positive class rate"": selection_rate,
            ""count"": count,
        },
        sensitive_features=A_val,
        y_true=y_val,
        y_pred=mitigated_predictions,
    )
    
    print(""\nMitigated Fairness Evaluation"")
    print(mitigated_metric_frame.overall)
    print(mitigated_metric_frame.by_group)
    
    # mitigated_metric_frame.by_group.plot.bar(
    #     subplots=True,
    #     layout=[3, 1],
    #     legend=False,
    #     figsize=[12, 8],
    #     title=""Mitigated Model: Accuracy and selection rate by group"",
    # )
    
    mitigated_precision = precision_score(y_val, mitigated_predictions)
    mitigated_recall = recall_score(y_val, mitigated_predictions)
    mitigated_f1 = f1_score(y_val, mitigated_predictions)
    
    print(f""\nMitigated Model Precision: {mitigated_precision:.4f}"")
    print(f""Mitigated Model Recall: {mitigated_recall:.4f}"")
    print(f""Mitigated Model F1-score: {mitigated_f1:.4f}"")
    
    # Compare original and mitigated models
    print(""\nComparison of Original and Mitigated Models:"")
    print(f""Original Model Accuracy: {metric_frame.overall['accuracy']:.4f}"")
    print(f""Mitigated Model Accuracy: {mitigated_metric_frame.overall['accuracy']:.4f}"")
    print(f""Original Model Selection Rate: {metric_frame.overall['Positive class rate']:.4f}"")
    print(f""Mitigated Model Selection Rate: {mitigated_metric_frame.overall['Positive class rate']:.4f}"")
    
    
    # %%
    

  
",Classic_Grass924,1g6dc6v,https://reddit.com/r/MachineLearning/comments/1g6dc6v/p_how_to_make_microsoft_fairlearns_exponentiated/,https://www.reddit.com/r/MachineLearning/comments/1g6dc6v/p_how_to_make_microsoft_fairlearns_exponentiated/,2024-10-18 08:35:37,3,0.67,3,0,0,0,0,False,False,True,False,False,Project,self,t3_1g6dc6v
MachineLearning,Top conferences for AI in medical imag-ing [D],"Sorry for imag-ing in title, title can't have 'AGI'.

I'm working on my first first-author research and my advisor feels it's going a good direction. I really want it to go through some good conferences by next year.

I know about MICCAI and MIDL but can't find a reliable source to check for all other conferences in 2025 related to medical imaging or AI in medicine in general. I hope people here must have some experience with few others. Any suggestions?

Also, what does workshop paper mean? I know it's not called a actual publication but is it worth submitting to a highly regarded workshop or rather a mid-ranked conference?

Thanks in advance!",ade17_in,1g6bu6z,https://reddit.com/r/MachineLearning/comments/1g6bu6z/top_conferences_for_ai_in_medical_imaging_d/,https://www.reddit.com/r/MachineLearning/comments/1g6bu6z/top_conferences_for_ai_in_medical_imaging_d/,2024-10-18 06:37:45,20,0.86,20,0,24,0,0,False,False,True,False,False,Discussion,self,t3_1g6bu6z
MachineLearning,[D] Is There a Universally Agreed Explanation for “Lost in the Middle”?,It’s been a while since I read the “Lost in the Middle” paper while working on long-context LLMs. I’m curious if there’s a paper that proposes a widely accepted interpretation of this effect or if there are any methods that effectively address or overcome it.,StraightSpeech9295,1g6avhy,https://reddit.com/r/MachineLearning/comments/1g6avhy/d_is_there_a_universally_agreed_explanation_for/,https://www.reddit.com/r/MachineLearning/comments/1g6avhy/d_is_there_a_universally_agreed_explanation_for/,2024-10-18 05:28:17,11,0.82,11,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g6avhy
MachineLearning,[D] PyTorch 2.5.0 released!,"https://github.com/pytorch/pytorch/releases/tag/v2.5.0

Highlights: We are excited to announce the release of PyTorch® 2.5! This release features a new CuDNN backend for SDPA, enabling speedups by default for users of SDPA on H100s or newer GPUs. As well, regional compilation of torch.compile offers a way to reduce the cold start up time for torch.compile by allowing users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Finally, TorchInductor CPP backend offers solid performance speedup with numerous enhancements like FP16 support, CPP wrapper, AOT-Inductor mode, and max-autotune mode.
This release is composed of 4095 commits from 504 contributors since PyTorch 2.4. We want to sincerely thank our dedicated community for your contributions.

Some of my favorite improvements:

- Faster torch.compile compilation by re-using repeated modules

- torch.compile support for torch.istft

- FlexAttention: A flexible API that enables implementing various attention mechanisms such as Sliding Window, Causal Mask, and PrefixLM with just a few lines of idiomatic PyTorch code. This API leverages torch.compile to generate a fused FlashAttention kernel, which eliminates extra memory allocation and achieves performance comparable to handwritten implementations. Additionally, we automatically generate the backwards pass using PyTorch's autograd machinery. Furthermore, our API can take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.",parlancex,1g62vyh,https://reddit.com/r/MachineLearning/comments/1g62vyh/d_pytorch_250_released/,https://www.reddit.com/r/MachineLearning/comments/1g62vyh/d_pytorch_250_released/,2024-10-17 22:11:36,306,0.99,306,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1g62vyh
MachineLearning,[P] Is it possible to convert a Casual Language Model to a Masked Language Model,"I am doing a project for uni, and in this project I need a masked language model (not in english), And I was wondering since casual language models like gpt2 are basically masked models but they just put the MASK token at the end of the sentence. Is it possible to convert one into a masked model where I can put the MASK token anywhere? I don't mean by prompting it with a task of being a masked model, I mean actually changing it to one.",Appletee_YT,1g61plo,https://reddit.com/r/MachineLearning/comments/1g61plo/p_is_it_possible_to_convert_a_casual_language/,https://www.reddit.com/r/MachineLearning/comments/1g61plo/p_is_it_possible_to_convert_a_casual_language/,2024-10-17 21:17:57,9,0.91,9,0,4,0,0,False,False,True,False,False,Project,self,t3_1g61plo
MachineLearning,[R] A Scalable Communication Protocol for Networks of Large Language Models,,smarro,1g5sqj8,https://reddit.com/r/MachineLearning/comments/1g5sqj8/r_a_scalable_communication_protocol_for_networks/,https://arxiv.org/abs/2410.11905,2024-10-17 14:52:55,2,1.0,2,0,1,0,0,False,False,False,False,False,Research,default,t3_1g5sqj8
MachineLearning,[R] Generative AI to detect and mitigate cyber-attacks,"Interesting paper on how Gen AI can be used to detect and mitigate cyber-attacks in controls.

[An Integrated Generative Adversarial Network for Identification and Mitigation of Cyber-Attacks in Wide-Area Control | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/9917212)

  
PDF : [https://www.researchgate.net/publication/365105427\_An\_Integrated\_Generative\_Adversarial\_Network\_for\_Identification\_and\_Mitigation\_of\_Cyber-Attacks\_in\_Wide-Area\_Control](https://www.researchgate.net/publication/365105427_An_Integrated_Generative_Adversarial_Network_for_Identification_and_Mitigation_of_Cyber-Attacks_in_Wide-Area_Control)",Playful_Passage_2985,1g5zvxh,https://reddit.com/r/MachineLearning/comments/1g5zvxh/r_generative_ai_to_detect_and_mitigate/,https://www.reddit.com/r/MachineLearning/comments/1g5zvxh/r_generative_ai_to_detect_and_mitigate/,2024-10-17 19:58:57,0,0.42,0,0,4,0,0,False,False,True,False,False,Research,self,t3_1g5zvxh
MachineLearning,[P] How to extract insights from 500k chat messages using LLMs?,"Hi all,

I downloaded the chat messages from a discord server on AI and they amounted to ~500k messages over 2-3 years. My reason for doing this is that I'd like to extract insights/tips &amp; tricks on the subject that you might not find in a tutorial online (I've always found being in discord servers where people help each other to be much more densely informative than reading various blog posts/tutorials). 

They amount to around 8m tokens which would cost 1-2$ using gpt-4o-mini, or 20-30$ using gpt-4o, which is pretty reasonable.

However I'm trying to figure two things out:

1) whether I can use a local llm for part of the process. That'd be preferred since while gpt-4o-mini would only cost between 1-2$, that's per prompt, and I might want to query/process the data in multiple ways.

2) what exactly could I do to extract the most valuable insights? Probably 95% of the chat is just banter but 5% is probably full of useful advice. What sort of prompts could I use? And how would I handle the fact that I'd need to chunk the input to fit into the context window?

I'm open to learning and exploring any new topic to go about this, as I'm excited to take it on as a project to get my hands dirty with LLMs. ",PMMEYOURSMIL3,1g5yn4b,https://reddit.com/r/MachineLearning/comments/1g5yn4b/p_how_to_extract_insights_from_500k_chat_messages/,https://www.reddit.com/r/MachineLearning/comments/1g5yn4b/p_how_to_extract_insights_from_500k_chat_messages/,2024-10-17 19:04:55,71,0.84,71,0,47,0,0,False,False,True,False,False,Project,self,t3_1g5yn4b
MachineLearning,Problems using shapr package and python wrapper [P],"I trained a RL algorithm and want to try to make it explainable. Because of high correlation of the input data I want to use the shapr package (see: https://github.com/NorskRegnesentral/shapr). However, when I try to import shaprpy in my jupyter notebook my kernel always dies. Anyone had the same issues? Furthermore when I try to run the example code the developers provided in R, I also get errors saying that explain() can't be used for xgBoost. I am just wondering what is causing me this much trouble. Thanks in advance",Abject_Pomegranate39,1g5vtzu,https://reddit.com/r/MachineLearning/comments/1g5vtzu/problems_using_shapr_package_and_python_wrapper_p/,https://www.reddit.com/r/MachineLearning/comments/1g5vtzu/problems_using_shapr_package_and_python_wrapper_p/,2024-10-17 17:05:21,2,1.0,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1g5vtzu
MachineLearning,[D] What are some of the most interesting conferences for real world (applied) ML talks?,I know that for information retrival and recommender systems RecSys and SIGIR offer a few industry talks and I was wondering if you know about other ones where it's mostly about findings/research from the industry.,gabegabe6,1g5txyp,https://reddit.com/r/MachineLearning/comments/1g5txyp/d_what_are_some_of_the_most_interesting/,https://www.reddit.com/r/MachineLearning/comments/1g5txyp/d_what_are_some_of_the_most_interesting/,2024-10-17 15:45:21,17,0.96,17,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1g5txyp
MachineLearning,[D] EigenLoRA: Extremely efficient learning for LLMs and Diffusion model? ,"Found this paper, [EigenLoRA](https://openreview.net/pdf?id=KxGGZag9gW), which shows pretty neat results for language models and diffusion models. It shows that we can recycle old LoRAs and combine them to learn an efficient subspace which requires very few parameters to finetune. This seems like a very cool idea if this legit - one could finetune very big models at a very low compute cost. My question is - can this also be applied to finetuned models without LoRAs? since we can calculate a LoRA from base and finetuned model? And has someone tried something similar to this EigenLoRA model?",propaadmd,1g5tw6i,https://reddit.com/r/MachineLearning/comments/1g5tw6i/d_eigenlora_extremely_efficient_learning_for_llms/,https://www.reddit.com/r/MachineLearning/comments/1g5tw6i/d_eigenlora_extremely_efficient_learning_for_llms/,2024-10-17 15:43:13,10,0.92,10,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1g5tw6i
MachineLearning,[P] How to build a custom text classifier without days of human labeling,"Hi, I work at Hugging Face. Me and my team have worked on this cool example of how to go from an LLM to a small and efficient classification model. We use the LLM to auto-label a dataset, which we then fine-tuned after a quick review. We show how it helped us simplify workflows, saving time and resources while still delivering a high-performing model. with higher accuracy while only labelling a couple of examples.

Blogpost: [https://huggingface.co/blog/sdiazlor/custom-text-classifier-ai-human-feedback](https://huggingface.co/blog/sdiazlor/custom-text-classifier-ai-human-feedback)",chef1957,1g5t7lq,https://reddit.com/r/MachineLearning/comments/1g5t7lq/p_how_to_build_a_custom_text_classifier_without/,https://www.reddit.com/r/MachineLearning/comments/1g5t7lq/p_how_to_build_a_custom_text_classifier_without/,2024-10-17 15:13:14,55,0.88,55,0,7,0,0,False,False,True,False,False,Project,self,t3_1g5t7lq
MachineLearning,[R]  Hardware Requirements for Deploying Huggingface Models,"Hey everyone,

I'm looking to deploy [this model](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli) (mDeBERTa-v3-base-mnli-xnli) on-premise and need some advice on the hardware requirements (GPU, CPU, RAM, etc.).

* Has anyone deployed this model locally or have recommendations for the minimum hardware setup (especially for GPU/VRAM requirements)?
* What would be the recommended specs for efficient performance and fine-tuning tasks?

Additionally, I'm curious about the general process to figure out hardware requirements for models like this. How do you typically approach determining the necessary hardware for deploying transformer models in local environments?

Any help or pointers would be greatly appreciated! Thanks in advance!",UrIceCup,1g5otwk,https://reddit.com/r/MachineLearning/comments/1g5otwk/r_hardware_requirements_for_deploying_huggingface/,https://www.reddit.com/r/MachineLearning/comments/1g5otwk/r_hardware_requirements_for_deploying_huggingface/,2024-10-17 11:38:32,0,0.5,0,0,4,0,0,False,False,True,False,False,Research,self,t3_1g5otwk
MachineLearning,[P] A technical guide on how to upgrade your training code from single GPU to multiple GPUs,"Hey All,

We've been writing a technical guide on how to scale training code from single GPU all the way to multiple nodes.

It's centered around training LLMs, and goes over things like DDP, FSDP, diagnosing errors/logging, and way more.

Tried to make the code and explanations as clear and simple as possible, let us know if you find it helpful!

Contributions welcome and feel free to open issues with requests/bugs.

[https://github.com/LambdaLabsML/distributed-training-guide](https://github.com/LambdaLabsML/distributed-training-guide)",lambda-research,1g573wv,https://reddit.com/r/MachineLearning/comments/1g573wv/p_a_technical_guide_on_how_to_upgrade_your/,https://www.reddit.com/r/MachineLearning/comments/1g573wv/p_a_technical_guide_on_how_to_upgrade_your/,2024-10-16 18:57:34,26,0.94,26,0,5,0,0,False,False,True,False,False,Project,self,t3_1g573wv
MachineLearning,[R] Algorithm to generate target variable,"Currently I have a dataset with a list of features,  that are by literature related to an arbitrary target variable, however I do not have this target variable value and there are no ways to directly capture it. 

May I ask if there are any ways to use the dataset with the features to generate latent factors that are representative of this target variable value if there are no concrete equation or methodology?

Edit: Apologies for the confusing phrasing. As one of the commenters mention, the problem is rather abstract and not well constrained. It can be seem as I have a set of A's (features),  I looking to create a function C where C(A) -&gt; B (abstract and undefined, i.e mental condition). 

Looking for any advice regarding any direction I should potentially look towards as an attempt to better frame the problem. ",Potokiller,1g5mqd8,https://reddit.com/r/MachineLearning/comments/1g5mqd8/r_algorithm_to_generate_target_variable/,https://www.reddit.com/r/MachineLearning/comments/1g5mqd8/r_algorithm_to_generate_target_variable/,2024-10-17 09:15:03,0,0.25,0,0,14,0,0,False,False,True,False,False,Research,self,t3_1g5mqd8
MachineLearning,"[R] DART can generate high-quality human motions in real-time, achieving over 300 frames per second on a single RTX 4090 GPU! It combines text inputs with spatial constraints, allowing for tasks like reaching waypoints and interacting with scenes.","1. Here's a link to the Project Page: https://zkf1997.github.io/DART/
2. Here's a link to the paper: https://arxiv.org/html/2410.05260v1

3. Here's My Question: I'm trying to recreate this paper in the form of a browser based app for me to play around with. Where would I find the motion data and text annotations necessary to train the VAE?",Hrombarmandag,1g5kq1v,https://reddit.com/r/MachineLearning/comments/1g5kq1v/r_dart_can_generate_highquality_human_motions_in/,https://www.reddit.com/r/MachineLearning/comments/1g5kq1v/r_dart_can_generate_highquality_human_motions_in/,2024-10-17 06:40:17,15,0.83,15,0,1,0,0,False,False,True,False,False,Research,self,t3_1g5kq1v
MachineLearning,[D] what are the state of art in diffusion models. ,I am bigger at diffusion models and understanding about them. Let's have discussion about diffusion models and state of art work in it ,interpret-Owl9066,1g5kymq,https://reddit.com/r/MachineLearning/comments/1g5kymq/d_what_are_the_state_of_art_in_diffusion_models/,https://www.reddit.com/r/MachineLearning/comments/1g5kymq/d_what_are_the_state_of_art_in_diffusion_models/,2024-10-17 06:58:08,0,0.33,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g5kymq
MachineLearning,[D] What do you think will be the next big thing in the field? Is LLM hype going to fade?,"I am happy with the success of LLMs, but I am not much of a NLP fan. What do you think will be the next big thing that will achieve commercial success or wide range of applicability (useful both in startups and large companies)?  
  
E.g., are RL or GNNs going to start being used in practice more widely (I know GNNs are used in large companies, but still I am not aware that they are widely used)?

I consider computer vision a well established field considering practical applications, but is there maybe something new happening there?

",Diligent-Ad8665,1g5jvzp,https://reddit.com/r/MachineLearning/comments/1g5jvzp/d_what_do_you_think_will_be_the_next_big_thing_in/,https://www.reddit.com/r/MachineLearning/comments/1g5jvzp/d_what_do_you_think_will_be_the_next_big_thing_in/,2024-10-17 05:41:43,76,0.78,76,0,107,0,0,False,False,True,False,False,Discussion,self,t3_1g5jvzp
MachineLearning,[R] Mixture of experts with single expert routing? ,"Hello,

For those of yall knowledgeable in the mixture of experts literature, what are some papers that route to a single expert? Furthermore, most papers evenly distribute the workload across experts (or try to). Are there papers on methods where a specific expert is the correct choice? e.g. we don’t necessarily need equal workload distribution acrosss experts ",Grand_Comparison2081,1g5jcuc,https://reddit.com/r/MachineLearning/comments/1g5jcuc/r_mixture_of_experts_with_single_expert_routing/,https://www.reddit.com/r/MachineLearning/comments/1g5jcuc/r_mixture_of_experts_with_single_expert_routing/,2024-10-17 05:07:56,6,0.88,6,0,6,0,0,False,False,True,False,False,Research,self,t3_1g5jcuc
MachineLearning,[D] What labeling solution currently supports SAM2 tracking?,I’m looking for labelers that currently support SAM2 tracking out of the box. So far I’ve only found Encord and Supervisely. Many others are supported SAM2 segmentation but not tracking in video frames. What else is out there?,cybis320,1g571u9,https://reddit.com/r/MachineLearning/comments/1g571u9/d_what_labeling_solution_currently_supports_sam2/,https://www.reddit.com/r/MachineLearning/comments/1g571u9/d_what_labeling_solution_currently_supports_sam2/,2024-10-16 18:55:13,1,1.0,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1g571u9
MachineLearning,Custom Dataset for MaskFormer and Mask2Former [P],"Hello everyone! I am a student currently trying to fine-tune the MaskFormer and Mask2Former segmentation model for an instance segmentation task on a custom dataset using the HF transformers library. The dataset I am preparing consists of two classes: `background` and `unhealty`. In each image of the dataset, there are several instances of the `unhealty` class. What I am trying to figure out is how to structure the dataset. I am not using a COCO format for the annotations. The tutorials I have found are mainly aimed at semantic segmentation tasks and I have not found much material on how to prepare custom datasets for these models. Thank you in advance for your help!",Hot_Dirt718,1g54njp,https://reddit.com/r/MachineLearning/comments/1g54njp/custom_dataset_for_maskformer_and_mask2former_p/,https://www.reddit.com/r/MachineLearning/comments/1g54njp/custom_dataset_for_maskformer_and_mask2former_p/,2024-10-16 17:14:25,1,1.0,1,0,2,0,0,False,False,True,False,False,Project,self,t3_1g54njp
MachineLearning,[R] Switch EMA: A Free Lunch for Better Flatness and Sharpness,,parlancex,1g5278x,https://reddit.com/r/MachineLearning/comments/1g5278x/r_switch_ema_a_free_lunch_for_better_flatness_and/,https://arxiv.org/abs/2402.09240,2024-10-16 15:33:01,50,0.93,50,0,9,0,0,False,False,False,False,False,Research,default,t3_1g5278x
MachineLearning,[D] Seeking advice from industry researchers who previously held roles in academia or completed a PhD,"1. What would you recommend someone moving from academia to join an industry research lab do in their first 30, 90, and 180 days to ensure they are making a good contribution to the company?
2. Are there habits or ways of thinking in academia which you need to actively move away from/manage in industry research environments?
3. In general, what skills are most commonly lacking or weak in employees coming from academia and/or which skills should an academic brush up/learn on before joining a company?
4. Any other tips/advice?",tfburns,1g5130q,https://reddit.com/r/MachineLearning/comments/1g5130q/d_seeking_advice_from_industry_researchers_who/,https://www.reddit.com/r/MachineLearning/comments/1g5130q/d_seeking_advice_from_industry_researchers_who/,2024-10-16 14:44:57,36,0.87,36,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1g5130q
MachineLearning,"[P] For OCR experts , Train GOT-OCR2.0 on arabic (right to left ) languages","for OCR experts ,  i got stumbled on new OCR model called GOT-OCR2.0  , i tried to read the documents provided by the user , but couldn't understand how to train the model for arabic language. 
any suggestions and help on how to do so ?",LahmeriMohamed,1g50zna,https://reddit.com/r/MachineLearning/comments/1g50zna/p_for_ocr_experts_train_gotocr20_on_arabic_right/,https://www.reddit.com/r/MachineLearning/comments/1g50zna/p_for_ocr_experts_train_gotocr20_on_arabic_right/,2024-10-16 14:40:46,0,0.4,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1g50zna
MachineLearning,[D] focusing on one model at a time vs keeping up with state-of-the-art models?,"Current development of ML models are super fast that there are ""state-of-the-art"" models almost every week (I am not referring to ""state-of-the-art"" models claimed by the authors, I am referring to the models that become a hot topic in the field which everyone talks about), I feel that if I do not follow the discussion closely, I cannot keep up with them.  

  
I am thinking what would be a good way to **really learn and internalize these knowledge**, would it be good to just follow all hot papers/discussions such that my knowledge is not out of date, or I really need to sit down and **get my hands dirty** on some important models (e.g. ResNet, Diffusion model) **one at time** before I actually move to the next one?

  
Can you guys share what you think?",Illustrious-Pay-7516,1g50n7m,https://reddit.com/r/MachineLearning/comments/1g50n7m/d_focusing_on_one_model_at_a_time_vs_keeping_up/,https://www.reddit.com/r/MachineLearning/comments/1g50n7m/d_focusing_on_one_model_at_a_time_vs_keeping_up/,2024-10-16 14:25:37,15,0.81,15,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1g50n7m
MachineLearning,[D] Advice Needed for Implementing High-Performance Digit Recognition Algorithms on Small Datasets from Scratch,"Hello everyone,

I'm currently working on a university project where I need to build a machine learning system from scratch to recognize handwritten digits. The dataset I'm using is derived from the UCI Optical Recognition of Handwritten Digits Data Set but is relatively small—about 2,800 samples with 64 features each, split into two sets.

**Constraints:**

* I must implement the algorithm(s) myself without using existing machine learning libraries for core functionalities.
* The BASE goal is to surpass the baseline performance of a K-Nearest Neighbors classifier using Euclidean distance, as reported on the UCI website; my goal is to find the best algorithm out there that can deal with this kind of dataset, as I plan on using the results of this coursework for another University's application.
* I cannot collect or use additional data beyond what is provided.

**What I'm Looking For:**

* **Algorithm Suggestions:** Which algorithms perform well on small datasets and can be implemented from scratch? I'm considering SVMs, neural networks, ensemble methods, or advanced KNN techniques.
* **Overfitting Prevention:** Best practices for preventing overfitting when working with small datasets.
* **Feature Engineering:** Techniques for feature selection or dimensionality reduction that could enhance performance.
* **Distance Metrics:** Recommendations for alternative distance metrics or weighting schemes to improve KNN performance.
* **Resources:** Any tutorials, papers, or examples that could guide me in implementing these algorithms effectively.

I'm aiming for high performance and would appreciate any insights or advice!

Thank you!",Shin-Zantesu,1g4zfi9,https://reddit.com/r/MachineLearning/comments/1g4zfi9/d_advice_needed_for_implementing_highperformance/,https://www.reddit.com/r/MachineLearning/comments/1g4zfi9/d_advice_needed_for_implementing_highperformance/,2024-10-16 13:30:31,0,0.4,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1g4zfi9
MachineLearning,[D] Am I hallucinating?,"..or was there an LLM training logbook of sorts shared by Google Brain researchers which detailed all the experiments they did, and the approaches they tried while training an LLM? 

I distinctly remember seeing such a project up on GitHub but it's nowhere to be seen now ! 

It was meant as a sort of guide for anyone setting out to train an LLM to avoid common pitfalls and such.
It might not have been google specifically though.

Am I dreaming ?

(Edit: more context) ",demonic_mnemonic,1g4weor,https://reddit.com/r/MachineLearning/comments/1g4weor/d_am_i_hallucinating/,https://www.reddit.com/r/MachineLearning/comments/1g4weor/d_am_i_hallucinating/,2024-10-16 10:45:49,77,0.88,77,0,21,0,0,False,False,True,False,False,Discussion,self,t3_1g4weor
MachineLearning,[P] Train GOT-OCR2.0 on persian language,"hello ,guys , can anyone interested in OCR help me to  train GOT-OCR2.0 on persian language,  because i couldn't fully understand steps to train it using only docs , since it has a module (model) that is trained on LTR langages (english ) and i wanted to train it on RTL languages (persian , udru ,and arabic) 
hope i recieve positive reply .
best regards.",LahmeriMohamed,1g4s4v8,https://reddit.com/r/MachineLearning/comments/1g4s4v8/p_train_gotocr20_on_persian_language/,https://www.reddit.com/r/MachineLearning/comments/1g4s4v8/p_train_gotocr20_on_persian_language/,2024-10-16 05:29:06,4,0.83,4,0,7,0,0,False,False,True,False,False,Project,self,t3_1g4s4v8
MachineLearning,[D] What qualifies as a sensitive attribute in equity and fairness research?,"Hi, long time lurker and first time poster. Basically the title.

Over the last few years of MICCAI (International Conference on Medical Image Computing and Computer-Assisted Intervention; the premier conference for medical image analysis research) I have noticed a worrying trend in health equity and fairness research: authors claiming unexpected attributes of datasets as ""sensitive attributes"" in their analysis and modeling:

* \[MICCAI 2022\] [FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis](https://conferences.miccai.org/2022/papers/207-Paper2316.html) (click on the **SharedIt link** under **Link to paper** for the paper's PDF): **Early Accept** (early accept is a paper that received all accept decisions and did not have a rebuttal phase).
   * The authors use dermatoscopic (aka dermoscopic) images from [the ISIC 2019 dataset](https://www.kaggle.com/datasets/andrewmvd/isic-2019) and use the gender label as the ""sensitive attribute"" with the female group as the ""privileged group"", simply because ""*The vanilla trained model has a higher accuracy on the female images*.""
* \[MICCAI 2023\] [Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis](https://conferences.miccai.org/2023/papers/653-Paper0994.html)
   * This paper shares one author with the previous paper, and they make the same assumption: ""*we take gender as our sensitive attribute*"" and ""*The female is the privileged group with higher accuracy by vanilla training.*""
* \[MICCAI 2024\] [BiasPruner: Debiased Continual Learning for Medical Image Classification](https://papers.miccai.org/miccai-2024/099-Paper2799.html): **Best Paper Award Nominee**.
   * Patient age is used as the sensitive attribute (age≥60, age&lt;60) in dermatoscopic images from the HAM10000 dataset. Different set of authors.

# Why are gender and age strange choices?

Dermatoscopic images are acquired using a [dermatoscope](https://en.wikipedia.org/wiki/Dermatoscopy) and have extremely low FOV. For sample images, see this imgur link: [https://imgur.com/a/ggNOPj2](https://imgur.com/a/ggNOPj2). As you can see, it **should be** impossible for these low FOV images to contain any age or gender specific information that may be the source of bias. It would be understandable if age and gender were considered bias sources for CXRs (chest x-rays), or skin tone for skin images, but age and gender are not even reflected in these dermatoscopic images. How can they be sources of bias? And why an age threshold of 60 years?

Are these simply instances of [HARKing](https://en.m.wikipedia.org/wiki/HARKing), since there is no literature (to the best of my knowledge) that says age and gender are reflected in dermatoscopic images? I also fear that if a similar analysis was carried out using any other unrelated metadata as the sensitive attribute (e.g., skin tone in CXRs), it is possible that performance may improve, but that doesn't mean that CXRs have skin-tone related bias.

Please help me understand how these sensitive attributes are chosen. Thank you in advance.",thrownicecatch,1g4oef0,https://reddit.com/r/MachineLearning/comments/1g4oef0/d_what_qualifies_as_a_sensitive_attribute_in/,https://www.reddit.com/r/MachineLearning/comments/1g4oef0/d_what_qualifies_as_a_sensitive_attribute_in/,2024-10-16 01:52:11,21,0.81,21,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1g4oef0
MachineLearning,[D] Non Fortune 500 ML use cases? ,"It feels really hard to find examples of mid level companies using ML in their every day operations.

The best success I've had is going to agencies websites and looking at their ""Case Studies"" section. Although these often omit a lot of data about the project itself.

Where do you go to find inspiration for whats possible in your industry? ",eh-tk,1g4ikg2,https://reddit.com/r/MachineLearning/comments/1g4ikg2/d_non_fortune_500_ml_use_cases/,https://www.reddit.com/r/MachineLearning/comments/1g4ikg2/d_non_fortune_500_ml_use_cases/,2024-10-15 21:11:36,16,0.71,16,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1g4ikg2
MachineLearning,[P] Machine Learning Job List for College Students,"Hey everyone, made a job list to help me find a machine learning internship, and now to find a new grad position. I thought it might be useful to others here, so I made it public. And to those that are outside of the US, I didn't forget about you

[https://github.com/speedyapply/2025-AI-College-Jobs](https://github.com/speedyapply/2025-AI-College-Jobs)",0xjoemama69420,1g4dj6h,https://reddit.com/r/MachineLearning/comments/1g4dj6h/p_machine_learning_job_list_for_college_students/,https://www.reddit.com/r/MachineLearning/comments/1g4dj6h/p_machine_learning_job_list_for_college_students/,2024-10-15 17:38:55,28,0.79,28,0,3,0,0,False,False,True,False,False,Project,self,t3_1g4dj6h
MachineLearning,[D] Gen AI for Data Engineering? ,"There are many generative AI models (OpenAI o1, Claude 3.5 Sonnet) and tools (Cursor, one of my favorite), and I've found combining some of these have significantly boosted productivity and experience (e.g., o1 + Cursor) for web app development. Wondering if y'all have used Gen AI for building data engineering pipelines/workflows? What's your experience been like? My friend and I are thinking of building an open-source project around this but we'd love to better understand potential users' pain points.",Away-Violinist3104,1g48z7x,https://reddit.com/r/MachineLearning/comments/1g48z7x/d_gen_ai_for_data_engineering/,https://www.reddit.com/r/MachineLearning/comments/1g48z7x/d_gen_ai_for_data_engineering/,2024-10-15 14:26:03,1,0.53,1,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1g48z7x
MachineLearning,[D] Time GAN for Time-Series prediction,"I have found a research paper called ""Time-series Generative Adversarial Networks"". I am not sure how to proceed in implementing it because I have not implemented. Any suggestions/ tips to go forward?",Soap_McaTavish,1g46ewm,https://reddit.com/r/MachineLearning/comments/1g46ewm/d_time_gan_for_timeseries_prediction/,https://www.reddit.com/r/MachineLearning/comments/1g46ewm/d_time_gan_for_timeseries_prediction/,2024-10-15 12:23:47,0,0.43,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1g46ewm
MachineLearning,[P] Introducing CVPal: A Computer Vision Library for Creating Custom Datasets with Just a Prompt!,"I'm excited to share the result of a full year of hard work! We've developed a computer vision library that can create complete datasets in multiple formats, all through a simple prompt!

Initially, the library worked with datasets from **Roboflow**, but now it supports generating **Synthetic Data** from a prompt using models like **Dalle** and **Stable Diffusion**.

We've also added another module that automatically handles annotation and formats the dataset in a structure compatible with **YOLO**.

The library currently supports two data formats: **TXT &amp; YAML** and **COCO JSON**.

There are two main modules:

1. **Synthetic Data Module**: It offers several functions, with the most important being the **generate** function, which allows you to create a dataset just from a prompt.
2. **Preprocessing Module**: One of the challenges we used to face with **Roboflow** was finding datasets that fit our exact needs—there was always something missing or extra. This module lets you customize your dataset. For example, you can merge multiple datasets to increase the number of images instead of using augmentation or remove labels you don’t need, and more.

Check it out on GitHub: [https://github.com/Muhamed555/cvpal](https://github.com/Muhamed555/cvpal)",Then_Instance_3188,1g45ebd,https://reddit.com/r/MachineLearning/comments/1g45ebd/p_introducing_cvpal_a_computer_vision_library_for/,https://www.reddit.com/r/MachineLearning/comments/1g45ebd/p_introducing_cvpal_a_computer_vision_library_for/,2024-10-15 11:27:14,1,0.53,1,0,3,0,0,False,False,True,False,False,Project,self,t3_1g45ebd
MachineLearning,[D] Power Consumption Estimation for ML Models on edge device,"**TL;DR**: We're exploring ways to estimate power consumption for ML models on edge devices and haven't found an off-the-shelf solution. We're adapting research papers but would appreciate any tools or insights from the community.



Hi everyone, I'm an MLOps Engineer at Fuzzy Labs, a company dedicated to open-source MLOps.

We are working on a computer vision project on edge devices (we're using Jetson Nano boards to be specific). As part of our exploration, we're looking for ways to estimate how much power a model will consume during inference on our device. We want to use power consumption estimates as a metric to choose which pre-trained model to use, what optimisation techniques to apply, etc.

# Our Approach

Our current plan is to adapt the NeuralPower paper (available on [GitHub](https://github.com/enyac-group/NeuralPower?tab=readme-ov-file)), which estimates power usage in neural networks. However, the paper focuses on classification models, whereas we're more interested in object detection models like YOLO. Additionally, the paper uses Caffe on desktop systems, but we want to work with PyTorch or TensorRT on the Jetson Nano 2GB.

We've found some promising research, like the paper on [Profiling Energy Consumption of Deep Neural Networks](https://publik.tuwien.ac.at/files/publik_293778.pdf), that could help us measure power consumption at the neural network layer level. On the surface, this approach feels like it should work, but we'd love to hear from anyone who's taken a similar path.

So far, we have found a few academic papers on the topic, but no off-the-shelf tool that can do such a prediction (in an ideal world, we'd also like an integration with some experiment tracker like MLFlow). But maybe someone else is aware of something like this?

If no such tool exists, we are considering developing our own solution.

We'd also love to hear from the MLOps community! Have you ever needed or done power consumption estimation for your models on edge? How did it go? Is there anything still missing for your use case?",Electrical_Client73,1g43g7c,https://reddit.com/r/MachineLearning/comments/1g43g7c/d_power_consumption_estimation_for_ml_models_on/,https://www.reddit.com/r/MachineLearning/comments/1g43g7c/d_power_consumption_estimation_for_ml_models_on/,2024-10-15 09:10:23,14,0.9,14,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1g43g7c
MachineLearning,WordPiece Tokenizer for BERT models implemented for bare react native apps [D],"After spending a lot of time and almost going insane a couple of times, I finally succeeded to build a tokenizer to prepare inputs for a custom Bert model running on a bare react native app.

I didn’t use external libraries because none is currently available for bare react-native environments (I think there exist for expo, not sure). 

I don’t know if anyone has been screwed up with this before but I’m planning to open source my code as a module, I’ll like to know if the need exists at least and if few contributors are willing to participate ",BrilliantCustard1136,1g43jcg,https://reddit.com/r/MachineLearning/comments/1g43jcg/wordpiece_tokenizer_for_bert_models_implemented/,https://www.reddit.com/r/MachineLearning/comments/1g43jcg/wordpiece_tokenizer_for_bert_models_implemented/,2024-10-15 09:17:32,0,0.5,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g43jcg
MachineLearning,[D] Understanding the loss in the CLIP model,"I am looking closely at the [CLIP paper](https://arxiv.org/abs/2103.00020). In Section 2.3 paragraph 4, it says: 

&gt;  jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs
in the batch while minimizing the cosine similarity of the embeddings of the N^2 − N incorrect pairings. 

Looking at the [code from MLFoundations](https://github.com/mlfoundations/open_clip/blob/main/src/open_clip_train/train.py), line 290 onwards,

```
logits_per_image = logit_scale * image_features @ text_features.t()
logits_per_text = logits_per_image.t()

total_loss = (
	F.cross_entropy(logits_per_image, labels) +
        F.cross_entropy(logits_per_text, labels)
        ) / 2
```

The [alternative implementation, from Revant](https://github.com/revantteotia/clip-training/blob/main/train.py) has very similar code (lines 88 onwards)

My question is very simple - 

Do these lines of code correspond to what the paper states as maximizing the cosine similarity of the N real pairs and minimizing that of the other N^2 - N incorrect pairs? If this is indeed the case, please help me understand briefly how it is so. 
",datashri,1g42hnu,https://reddit.com/r/MachineLearning/comments/1g42hnu/d_understanding_the_loss_in_the_clip_model/,https://www.reddit.com/r/MachineLearning/comments/1g42hnu/d_understanding_the_loss_in_the_clip_model/,2024-10-15 07:51:12,17,0.88,17,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1g42hnu
MachineLearning,[D] Does anyone know of an AI model that can edit very long audio files?,"Is there an ai website, or self hosted model, that can assist with editing/trimming silence out of very very long audio/video files?  Maybe I am stretching it, but preferably free?

 I use DaVinci resolve to edit, but it does not seem like it is possible to do this in it (which is surprising).  

As a bonus it would be great if it could auto edit audio/video, not necessarily long ones, just in a normal manner.  I have a ton of audio/video where there lots and lots of blank spaces, and it is not worth my time to edit all of it.  I also have a bunch of long videos for something unrelated that I don't have time to edit.  ",dangerously__based,1g41e7v,https://reddit.com/r/MachineLearning/comments/1g41e7v/d_does_anyone_know_of_an_ai_model_that_can_edit/,https://www.reddit.com/r/MachineLearning/comments/1g41e7v/d_does_anyone_know_of_an_ai_model_that_can_edit/,2024-10-15 06:25:59,0,0.27,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g41e7v
MachineLearning,[D] Is it common for ML researchers to tweak code until it works and then fit the narrative (and math) around it? ,"As an aspiring ML researcher, I am interested in the opinion of fellow colleagues. And if and when true, does it make your work less fulfilling?",Diligent-Ad8665,1g40i0h,https://reddit.com/r/MachineLearning/comments/1g40i0h/d_is_it_common_for_ml_researchers_to_tweak_code/,https://www.reddit.com/r/MachineLearning/comments/1g40i0h/d_is_it_common_for_ml_researchers_to_tweak_code/,2024-10-15 05:22:25,293,0.95,293,0,118,0,0,False,False,True,False,False,Discussion,self,t3_1g40i0h
MachineLearning,[D] How to develop/debug distributed training code before training in cloud?,"
Hello,
I am interested in developing larger models or accelerating training with DDP/FSDP. One curiosity I have is how do you develop training code for this with a single gpu dev machine. I found some torch distributed code didn’t work with single gpu systems. Is it necessary to have a 2+ gpu machine? I am aware of gpu splitting but afaik that isn’t available on consumer cards. I would prefer to not be debugging on an expensive cloud instance. Thanks for any advice!",jms4607,1g3zmld,https://reddit.com/r/MachineLearning/comments/1g3zmld/d_how_to_developdebug_distributed_training_code/,https://www.reddit.com/r/MachineLearning/comments/1g3zmld/d_how_to_developdebug_distributed_training_code/,2024-10-15 04:27:24,6,0.88,6,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g3zmld
MachineLearning,[D] How important is the university reputation/ranking for PhD?,"Hi, Everyone!

I am currently in the search of a PhD position (in Europe) and I am deciding between multiple PhD positions. I have a solid profile (highly ranked university, nice research experience, good internships) and luckily for me I am getting interviews with almost every lab I apply to.

Since I could not find a concise answer to the following questions, I wanted to ask the community!

**1. How important is the university's ranking/reputation?**

I have found great labs all over the board. I have found some amazing labs in the universities ranked as low as 800qs. While I know how rankings are calculated, I fear not going to a reputable/known university. As someone who did bachelor's/master's at the #1 national universities, I am afraid that I would be putting myself at a disadvantage by getting a PhD somewhere like this.

**2. PI reputation vs the university reputation?**

This question mainly boils down to the difference between doing a PhD at a known university with a supervisor with few collaborators and a small research network, against a supervisor who is from an unknown university but is collaborating with top people in the field. Small fish in a big pond or a large fish in a large pond.

**3. University &lt;&gt; PI &lt;&gt; Research fit? How would you rank them? Which 2/3 would you pick?**

Since it's pretty unlikely you can find everything that you want. On what would you compromise?",Stoick01,1g3pw4t,https://reddit.com/r/MachineLearning/comments/1g3pw4t/d_how_important_is_the_university/,https://www.reddit.com/r/MachineLearning/comments/1g3pw4t/d_how_important_is_the_university/,2024-10-14 20:28:48,19,0.76,19,0,54,0,0,False,False,True,False,False,Discussion,self,t3_1g3pw4t
MachineLearning,"[D] What is an ""ML framework""?","I've been experimenting with ML for some time, following tutorials and got reasonably comfortable with PyTorch. For over a year now I've been building an ML framework in beautiful C++20.

I want to eventually release and productize it. Right now I am trying to clearly define scope what I need for initial release next year.

My focus is deep neural networks (no other ML approaches). I am ok with branding it as ""deep NN framework"" if that would be clearer.

Features I implemented:
1. Extensible framework for defining layers. Models can be trivially composed of different layers and then trained and ran.
2. Linear layer, Relu, Softmax, Tanh, some more - new layers can be implemented without forking the framework, basically just a C++ class.
3. Quadratic loss, cross enthropy loss. New loss functions are trivially implementable.
4. Embeddings layer, Attention layer (with some customization points).
5. Will implement CNN (Conv, pooling) layers next, should be easy
6. Support for RNNs (there was some special  work necessary to make memory use not depend on number of iterations)
5. Training implemented with pluggable optimizers. SGD implemented, ADAM coming. Multiprocess and distributed training is supported.
6. Network itself is compiled as native code, supports clang++ and GCC currently (Linux/Mac), will support VC++ and WebAssembly. There will be several ways to package parameters, including standalone data files or blobs that can be linked into the binary.

Test data management is developed as a parallel project. Currently I have support for downloading archive files and sampling them based on directories, sizes, extensions, etc. Will work on interop story with Python if there's user demand.

Value proposition: easy to embed into other code bases (video games, embedded) or to deploy and run in a browser. My current milestones are simple encoder model, also plan an interesting feature set for Q-learning. No plans to implement GPU support for either inferrence or training, don't see a need as current goal is smaller models.

What else should such a framework have?",euos,1g3u1gv,https://reddit.com/r/MachineLearning/comments/1g3u1gv/d_what_is_an_ml_framework/,https://www.reddit.com/r/MachineLearning/comments/1g3u1gv/d_what_is_an_ml_framework/,2024-10-14 23:34:30,14,0.62,14,0,41,0,0,False,False,True,False,False,Discussion,self,t3_1g3u1gv
MachineLearning,[D] self supervised learning methods comparison,"in self supervised learning (SSL) paper, they show that a model trained on imagenet is better using self supervised learning instead of supervised learning, they then evaluate this model on specific datasets such as cityscapes. When they compare with other self supervised learning methods they use the model trained with SSL on Imagenet. Why dont they finetune the model more on specific datasets such as cityscapes using SSL and compare again?

====. EDIT =====  
by they I meant all of the self supervised learning papers I have seen (DenseCL in particular for cityscapes)

* simCLR [https://arxiv.org/pdf/2002.05709](https://arxiv.org/pdf/2002.05709)
* BYOL [https://arxiv.org/pdf/2006.07733](https://arxiv.org/pdf/2006.07733)
* SimSiam [https://arxiv.org/pdf/2011.10566](https://arxiv.org/pdf/2011.10566)
* DINO [https://arxiv.org/pdf/2104.14294](https://arxiv.org/pdf/2104.14294)
* DenseCL [https://arxiv.org/pdf/2011.09157](https://arxiv.org/pdf/2011.09157)

It seems the common protocol is to compare self supervised models trained on ImageNet and then finetune them on downstream tasks from the ImageNet SSL training: (ImageNet SSL) -&gt; (supervised finetuning on downstream dataset)

Since SSL doesnt require labels, my question was why dont SSL protocols do a (ImageNet SSL) -&gt; (downstream dataset SSL) -&gt; (supervised finetuning on downstream dataset)?

I understand the (downstream dataset SSL) step might be redundant if you are goind to do supervised finetuning on the WHOLE downstream dataset anyway, but this step might be interesting for semi supervised downstream task where you dont do supervised finetuning on the whole labeled downstream dataset.",Feiwu7777,1g3shy5,https://reddit.com/r/MachineLearning/comments/1g3shy5/d_self_supervised_learning_methods_comparison/,https://www.reddit.com/r/MachineLearning/comments/1g3shy5/d_self_supervised_learning_methods_comparison/,2024-10-14 22:21:12,0,0.29,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1g3shy5
MachineLearning,[D] How do I Actually Prune LLM and VLMs?,"I know how pruning works and what it does, but I haven't tried it on a LLM yet. I am planning on pruning the MolomoE 72B and Qwen2 VL 72B. What software do I use to actually prune these models. ",SpecialistStory336,1g3rmuy,https://reddit.com/r/MachineLearning/comments/1g3rmuy/d_how_do_i_actually_prune_llm_and_vlms/,https://www.reddit.com/r/MachineLearning/comments/1g3rmuy/d_how_do_i_actually_prune_llm_and_vlms/,2024-10-14 21:42:46,0,0.5,0,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1g3rmuy
MachineLearning,[D] Is there any ML research regarding software verification?,"I've found a few papers, however I may miss the right search terms.

I'm especially interested in comparing implementations / code with their specification or just generally checking the equivalence of \~code using machine learning? 

  
Thanks",SPD-1337,1g3nxpo,https://reddit.com/r/MachineLearning/comments/1g3nxpo/d_is_there_any_ml_research_regarding_software/,https://www.reddit.com/r/MachineLearning/comments/1g3nxpo/d_is_there_any_ml_research_regarding_software/,2024-10-14 19:09:11,10,0.7,10,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1g3nxpo
MachineLearning,How to approach recommendation system project? [P],"I am working on a health and wellness project where I am supposed to recommend daily tasks to a user that can improve their health. I already have a list of tasks that I am pulling from but want to personalize these recommendations to each user. 

I am thinking that to start, I would have them fill out a brief survey and then recommend tasks based on their answers. Any suggestions or advice on how to do this would be greatly appreciated.",Content_Reason5483,1g3iqg3,https://reddit.com/r/MachineLearning/comments/1g3iqg3/how_to_approach_recommendation_system_project_p/,https://www.reddit.com/r/MachineLearning/comments/1g3iqg3/how_to_approach_recommendation_system_project_p/,2024-10-14 15:37:12,0,0.5,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1g3iqg3
MachineLearning,"""[D]"" Using ssl backbone to replace backbone of popular object detection models.","Just wondering, has anyone ever used the learned semantics learned from ssl and applied them to any of the popular object detection models like yolo or ssd and what was the result. I have been thinking about this for a while now since in many of the ssl papers I have read, they do mention object detection as one the downstream tasks that ssl shines in compared to their supervised counterpart. I have also been looking for any material in relation to this but to no avail.",Antman-007,1g3epgp,https://reddit.com/r/MachineLearning/comments/1g3epgp/d_using_ssl_backbone_to_replace_backbone_of/,https://www.reddit.com/r/MachineLearning/comments/1g3epgp/d_using_ssl_backbone_to_replace_backbone_of/,2024-10-14 12:37:21,4,0.7,4,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g3epgp
MachineLearning,[D] The difference between Outcome-supervised Reward Model and Process-supervised Reward Model,"When reading the implementation of RL algorithms like PPO, I found that the advantages and values are actually calculated at token-level. Here is an implementation of how to get the advantage in OpenRLHF (similiar to TRL)

    """"""Function that computes advantages and returns from rewards and values.
    Calculated as in the original PPO paper: https://arxiv.org/abs/1707.06347
    Note that rewards may include a KL divergence loss term.
    
    Advantages looks like this:
    Adv1 =  R1 + γ * λ * R2     + γ^2 * λ^2 * R3       + ...
            - V1 + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ...
    
    Returns looks like this:
    Ret1 =  R1 + γ * λ * R2     + γ^2 * λ^2 * R3       + ...
                + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ...
    
    Input:
    - values: Tensor of shape (batch_size, response_size)
    - rewards: Tensor of shape (batch_size, response_size)
    
    Output:
    - advantages: Tensor of shape (batch_size, response_size)
    - returns: Tensor of shape (batch_size, response_size)
    """"""

So, although people use the outcome-supervised reward model (ORM), they actually use it to get the token-level feedback. In this perspective, PRM and ORM seems to be the same (since both of them need to preform the token-level feedback). Am I right?

That's my first question. Here is the second question: Is it a commonpractice that people use the reward model that trained on the outcome feedback to perform token-level feedback? The reward/reward moel seems to be inaccurate/misused. ",zetiansss,1g3dqof,https://reddit.com/r/MachineLearning/comments/1g3dqof/d_the_difference_between_outcomesupervised_reward/,https://www.reddit.com/r/MachineLearning/comments/1g3dqof/d_the_difference_between_outcomesupervised_reward/,2024-10-14 11:45:07,2,0.67,2,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1g3dqof
MachineLearning,[D] Efficient video ingestion for pytorch?,"I am starting a new project at the moment where I have to train a video classifier/regression model. Each video consists of ~360 frames and captured in very high quality ~3840x2160 (By the same camera in the same location filming almost identical products). The videos are currently saved in .ts format which I'm not really familiar with, but seems very compression efficient since each video only takes about 15 MB space.

I don't know exactly how I'm going to train on these videos yet, but my thinking was to split each video into random n-frame clips during training. So if n=20 one sample will have shape (20,3,3840,2160) for instance.

Initially I was thinking that I would just convert each video to picture frames and then just save the pictures or perhaps save the pictures as a pytorch object. However the 15 MB video turns into 0.5 GB jpg pictures, and even worse if I just straight up save it as a pytorch object of size (360,3,3840,2160) in uint8 it ends up taking around 9 GB for each little video. So obviously this is a no go.

pytorch vision have a method called VideoClips, https://github.com/pytorch/vision/blob/main/torchvision/datasets/video_utils.py, which seems to be designed for this kind of thing, but it takes ~ 80 seconds to process 3 of these videos with this method. (It is recommended to cache the output of this, but I am not sure exactly what they mean by that? Is it just pickling the result to a file or what do they mean?)

Reading the same 3 videos into memory using opencv takes about 20s, which so far seems to be the best way to go about it, but I am hoping there are some better tools that I have missed.

Maybe a solution involves converting the videos from .ts into a format that is less compressed, but easier and faster to read and work with in ML?",alyflex,1g3d1b0,https://reddit.com/r/MachineLearning/comments/1g3d1b0/d_efficient_video_ingestion_for_pytorch/,https://www.reddit.com/r/MachineLearning/comments/1g3d1b0/d_efficient_video_ingestion_for_pytorch/,2024-10-14 10:58:29,4,0.61,4,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1g3d1b0
MachineLearning,[D] Will Scale be enough to get LLMs to Reason through Grokking?,"Currently there has been a lot of debate whether LLMs truly reason or just memorize their training data (see this recent paper from Apple [\[2410.05229\] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models (arxiv.org)](https://arxiv.org/abs/2410.05229)).

On the other hand, there has been numerous papers showing that models can generalize, if trained beyond the point where they overfit, known as ""**grokking**"" ([Towards Understanding Grokking: An Effective Theory of Representation Learning (neurips.cc)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html)).

Based on grokking, we could argue that if we just train current LLMs enough, they will always converge to generalization. Seemingly, **memorization is just a local minima** in which it can get stuck, and the true **global minima is generalization**.

How is this possible if memorization is already giving near perfect performance on the dataset for a **specific task**? Well, by looking at overall performance opposed to task-specific performance, you can imagine how generalizing helps the model increase its overall performance:

1. Generalizations use less parameter space than memorization, which the model then can use for other tasks, increasing its overall performance (reduction in effective dimension by generalization [\[2205.10343\] Towards Understanding Grokking: An Effective Theory of Representation Learning (arxiv.org)](https://arxiv.org/abs/2205.10343))
2. Generalizations from one task can increase the performance on another unrelated task, increasing its overall performance (recent paper shows that GPT models get better at chess and reasoning by looking at the emergent behaviour of cellular automata: [Intelligence at the Edge of Chaos (arxiv.org)](https://arxiv.org/html/2410.02536)).

But then what happens if we grok the model not on a specific task, but on **all its data**? We can imagine that it would just memorize the whole dataset, without being incentivised to make generalization since it now has near perfect performance on the whole dataset. In this case, where the **global minima is memorization**, the model can still reach generalization by changing the loss landscape using **weight-decay / regularization**. Regularization punishes big weights, forcing the model to prefer simpler solutions, reducing the minima around memorization, while leaving the minima around generalization in tact. This will make generalization the new global minima.

Considering this convergence towards generalization over training time, for both task-specific as overall performance, could we assume that scaling will logically make models generalize over time? In other words, is scale really all we need to AGI? Or is there a flaw in my reasoning, grokking is not the end-all-be-all and we will need new breakthroughs to get to AGI?",PianistWinter8293,1g3cumr,https://reddit.com/r/MachineLearning/comments/1g3cumr/d_will_scale_be_enough_to_get_llms_to_reason/,https://www.reddit.com/r/MachineLearning/comments/1g3cumr/d_will_scale_be_enough_to_get_llms_to_reason/,2024-10-14 10:45:11,0,0.32,0,0,46,0,0,False,False,True,False,False,Discussion,self,t3_1g3cumr
MachineLearning,[R] Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback (EMNLP 2024 Main),"**Paper:** [https://arxiv.org/abs/2410.04064v1](https://arxiv.org/abs/2410.04064v1)

**Code:** [https://github.com/fatemehpesaran310/Text2Chart31](https://github.com/fatemehpesaran310/Text2Chart31)

**TL;DR:** We propose a new dataset, Text2Chart31, and a reinforcement learning-based fine-tuning method for LLM chart generation.

[An illustration of \(b\) our dataset Text2Chart31 and \(c\) our reinforcement learning based instruction tuning.](https://preview.redd.it/rvyj0eg7toud1.png?width=848&amp;format=png&amp;auto=webp&amp;s=fc50680f8ad70e40b6d04fbc8d304a8f61017586)

**Abstract:** Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks.

**Dataset:** We develop a hierarchical plot generation pipeline leveraging GPT-3.5-turbo and GPT-4. Our newly contributed Text2Chart31 dataset supports 31 plot types based on Matplotlib with 11.1K data points. We outline its key characteristics in Table 1, comparing it with existing datasets in the data visualization domain.

The Text2Chart31 dataset *D* consists of 11,128 data points, each of which contains a tuple of `(x, c, d, r, y)`: a textual plot description (*x*), its corresponding code (*c*), and the resulting plots (*y*).

For 8,166 data points, we additionally include a raw data table (*d*) and intermediate reasoning steps (*r*) to generate descriptions.

[Statistics of our dataset Text2Chart31.](https://preview.redd.it/t9c0xdy9toud1.png?width=1654&amp;format=png&amp;auto=webp&amp;s=98c0faa39d523f6f08003e9875f0bceac50e03eb)

**Task Definition:** Our benchmark is designed to evaluate three tasks:

1. *Description-to-Chart:* Given a plot description `x`, an algorithm generates its corresponding code `c` that creates a chart using the Matplotlib library.
2. *Raw Data-to-Chart:* When provided with only a raw data table `d`, the algorithm generates intermediate reasoning steps `r` that analyze the raw data and then generates a description `d` for the most suitable plot type based on the characteristics of the data.
3. *Code-to-Description:* Given the code `c` for a plot, the model generates a detailed description `x` of the plot.

**Experiments:**

[Experimental results. CLI and L3I denote Code Llama Instruct and Llama 3 Instruct, respectively.](https://preview.redd.it/d9rge4rbtoud1.png?width=1674&amp;format=png&amp;auto=webp&amp;s=451cdc6eeb21341188221adba162a70510d0b9ff)

",Moreselflove0324,1g3bia0,https://reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/,https://www.reddit.com/r/MachineLearning/comments/1g3bia0/r_text2chart31_instruction_tuning_for_chart/,2024-10-14 09:04:16,4,0.75,4,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/Hj1eH0mZoj-g-6KhlD44WicBhQGbzuvlmnbt7vBcrdo.jpg,t3_1g3bia0
MachineLearning,[D] Cheap inference hardware for autonomous driving,"I am currently building an autonomous RC car that navigates via detecting cones on the street with cameras and a lidar-initial odometry system with a lidar. I used Nvidia Jetsons up until now, but I am not really happy with the state of the ecosystem (and the price). Since my actual inference needs are pretty small (yolov5s at 60fps), do you have some experience with alternative systems that do not contain Cuda GPUs and have enough performance to run such a system? (Let's say that for everything else in the pipeline the car fills up 4x 2.0GHz cores)",NumerousSwordfish653,1g3anmn,https://reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/,https://www.reddit.com/r/MachineLearning/comments/1g3anmn/d_cheap_inference_hardware_for_autonomous_driving/,2024-10-14 07:52:48,10,0.86,10,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1g3anmn
MachineLearning,Performance drops after fine tuning Llama 3 [D],I fine tuned a Llama3 model with a Lora adapter for a classification task. I have about 9000 samples and I trained the model for about 5 epochs. But the recall of the fine tuned model is worse than the base model. Any reason reasons why this might happen? ,Ok-Emu5850,1g39y0b,https://reddit.com/r/MachineLearning/comments/1g39y0b/performance_drops_after_fine_tuning_llama_3_d/,https://www.reddit.com/r/MachineLearning/comments/1g39y0b/performance_drops_after_fine_tuning_llama_3_d/,2024-10-14 06:46:21,0,0.5,0,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1g39y0b
MachineLearning,"[D] Is there any easy to use AI API for data recommendation such as vehicle recommendation based on certain input and parameter?
",I need to implement this quickly hence I try to avoid coding if possible,xoberzero8,1g39nc5,https://reddit.com/r/MachineLearning/comments/1g39nc5/d_is_there_any_easy_to_use_ai_api_for_data/,https://www.reddit.com/r/MachineLearning/comments/1g39nc5/d_is_there_any_easy_to_use_ai_api_for_data/,2024-10-14 06:23:31,0,0.25,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1g39nc5
MachineLearning,[D] Models for image segmentation to isolate documents,"Hi folks, I'm working on a project that involves processing identity documents. I want to extract the documents from a given image first, before processing them, and I am looking for a model that can help me do this. I'll be working with generic documents from around the world, so I don't want to fine-tune a model(like yolo) with a given set of documents. I am looking for a generic documents segmentation model and I've been unable to find any so far, as most of them involve fine-tuning a yolo model on a given set of images.

I'd be greatful if any of you can provide some leads I can chase to achieve this. Thanks!",comical_cow,1g3874b,https://reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/,https://www.reddit.com/r/MachineLearning/comments/1g3874b/d_models_for_image_segmentation_to_isolate/,2024-10-14 04:35:41,2,0.75,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g3874b
MachineLearning,[R] DIAMOND: Diffusion for World Modeling,"*DIAMOND 💎 Diffusion for World Modeling: Visual Details Matter in Atari*

project webpage: [https://diamond-wm.github.io/](https://diamond-wm.github.io/)

code, agents and playable world models: [https://github.com/eloialonso/diamond](https://github.com/eloialonso/diamond)

paper: [https://arxiv.org/pdf/2405.12399](https://arxiv.org/pdf/2405.12399)

summary

* The RL agent is an actor-critic trained by REINFORCE.
   * The actor and critic networks share weights except for their last layers. These shared layers consist of a convolutional ""trunk"" followed by an LSTM cell. The convolutional trunk has four residual blocks with 2x2 max-pooling.
   * Each training run took 5M frames, for 12 days on one Nvidia RTX 4090.
* The world model is a 2D diffusion model with U-Net 2D. It is not a latent diffusion model. It directly generates frames from a video game.
   * the model takes as conditioning the last 4 frames and actions, and the diffusion noise level.
   * runs at \~10 FPS on RTX 3090.
   * They used the EDM sampler for sampling from the diffusion model, which still worked fine for training the RL agent, even with just 1 diffusion step per frame.

",furrypony2718,1g34p3n,https://reddit.com/r/MachineLearning/comments/1g34p3n/r_diamond_diffusion_for_world_modeling/,https://www.reddit.com/r/MachineLearning/comments/1g34p3n/r_diamond_diffusion_for_world_modeling/,2024-10-14 01:11:10,1,0.67,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1g34p3n
MachineLearning,[D] Help a NASA-funded project learn more about the Sun! (Kaggle Competition),"Hi all, my name is Hannah and I am the Communications person for the NASA-funded [Eclipse Megamovie 2024 project.](http://eclipsemegamovie.org) We were super active in April as the eclipse approached, but there is still way more excitement to come! We've launched a Kaggle competition, hoping to get help from communities such as this one. Below is more information about the project as a whole and a link to our competition page. Please feel free to ask any questions and I'll do my best to get them answered!

On April 8, 2024, a total solar eclipse began over the South Pacific Ocean and crossed North America, passing over Mexico, the United States, and Canada. The first location in continental North America that experienced totality was Mexico’s Pacific coast at around 11:07 a.m. PDT. Following the April 8, 2024, total solar eclipse, more than 145 volunteers uploaded over 1 terabyte of photographic data for use in our project.

Eclipse Megamovie 2024 (EM2024) is funded by NASA to study the sun using data collected during total solar eclipses, a special time when it is possible to study the Sun’s behavior unlike any other. The next stage, after the eclipse and the gathering of the data, is to categorize and label photographic data, and then we will be able to begin the scientific analysis in earnest–this is where you come in! 

If you are proficient in Python code and Machine Learning, you may be able to contribute to answering previously unanswered questions about the sun! 

Link to competition page: [https://www.kaggle.com/competitions/eclipse-megamovie](https://www.kaggle.com/competitions/eclipse-megamovie)

Competition participants will work with our 2017 total solar eclipse dataset to ""train"" a machine by writing code and utilizing the training dataset provided to automatically categorize eclipse photographs within one of several categories based on the phase of the eclipse. People interested in participating in this competition are recommended to have a working knowledge of python and machine learning fundamentals. **Interests that align with our competition:** photography, heliophysics and/or solar science research, participatory science, and machine learning.**Prizes:**

Leaderboard Prizes: Awarded based on private leaderboard ranking.

* **First Prize:** Image-stabilized binoculars with solar filters, Spotlight on the Eclipse Megamovie website, Eclipse Megamovie Team Patch, NASA Calendar, Eclipse Megamovie Sticker, First Prize Certificate.
* **Second Prize:** Spotlight on the Eclipse Megamovie website, Eclipse Megamovie Team Patch, NASA Calendar, Eclipse Megamovie Sticker, Second Prize Certificate.
* **Third Prize:** Spotlight on the Eclipse Megamovie website, Eclipse Megamovie Team Patch, NASA Calendar, Eclipse Megamovie Sticker, Third Prize Certificate.

Participants will help to ensure that the data \[photographs of eclipses\] can be quickly organized and have the correct information (metadata) associated with each image. By helping us develop code that accurately identifies the solar eclipse phases within photographs submitted by volunteers, you will enable us to cross a major data processing hurdle. With your code, you are paving the way for this NASA-funded research endeavor to study solar jets and plasma plumes!

Your mission is to create the most accurate sorting machine that categorizes a solar eclipse photograph into a specific solar eclipse phase. You will know you have succeeded if your code is able to successfully categorize the photographs provided into the following categories: Darks or flats (calibration shots), partial eclipse phases (bins \[categories\] of 20 degrees), the diamond ring phase, total solar eclipse phases, and of course a category for things that are not solar eclipses.

Special thanks to the Mods for letting me know which tag to use on this post :)

edited for words",EMegamovie2024,1g32evj,https://reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/,https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/,2024-10-13 23:11:39,52,0.88,52,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g32evj
MachineLearning,[P] Drowning in Research Papers? 🐸,"We’re two engineers interested in AI research, but have been drowning in the flood of new papers on arXiv. So, we built Ribbit Ribbit, a research paper discovery tool.

* [https://apps.apple.com/us/app/ribbit-ribbit/id6529547956](https://apps.apple.com/us/app/ribbit-ribbit/id6529547956)
* [https://ribbitribbit.co](https://ribbitribbit.co)

It curates personalized paper recommendations and turns them into tweet-sized summaries, so you can scroll through like it’s Twitter. You can also listen to the updates just like a podcast made just for you. We’ve added a lighthearted touch, hoping it adds a bit of joy to the whole paper-reading process, which, let’s be real, can get pretty dry and dull :p.

https://preview.redd.it/evoemobinlud1.png?width=1179&amp;format=png&amp;auto=webp&amp;s=4dff5b2b60f2a1272b6ac04347f661ceacff2aa5

",haoyuan8,1g31hfd,https://reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/,https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/,2024-10-13 22:24:26,354,0.93,354,0,120,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/NcWr68na-POzAczvc0mHUZFkwYsKICfJi_DuVwbA_40.jpg,t3_1g31hfd
MachineLearning,[R] Is someone using vast.ai?,"So basically, I want to build a setup with:

* Motherboard: BTC H510 Pro
* CPU: i5 11400
* GPU: 3090 (x6)

The reason is that this motherboard has 6 PCI-e 3.0 x16 ports, and from what I understand, it’s better for vast workloads. Plus, this motherboard only costs 35 USD (since it's designed for mining, and mining has died out). What do you think? Will it work well? Do you think the CPU is adequate? Supposedly, you need one logical processor per GPU.

Is it profitable, or is it just throwing money away? Is there any active forum where I can discuss this with more experienced people?

Thanks and best regards.",eimattz,1g2zuvr,https://reddit.com/r/MachineLearning/comments/1g2zuvr/r_is_someone_using_vastai/,https://www.reddit.com/r/MachineLearning/comments/1g2zuvr/r_is_someone_using_vastai/,2024-10-13 21:06:41,0,0.24,0,0,12,0,0,False,False,True,False,False,Research,self,t3_1g2zuvr
MachineLearning,"Interested in Causal Mechanism ""[D]""","Im interested in representation of causal
mechanism and how to apply in Al. Im planning
for applying another postgraduate degree after
cognitive neuro whether in Computer science
(conversion) or Psychology related field (Research
method). Which program is more related to my
above interested topic computer science or
psychology?",Pleasant-Neck-1528,1g2zcwz,https://reddit.com/r/MachineLearning/comments/1g2zcwz/interested_in_causal_mechanism_d/,https://www.reddit.com/r/MachineLearning/comments/1g2zcwz/interested_in_causal_mechanism_d/,2024-10-13 20:43:55,0,0.4,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1g2zcwz
MachineLearning,Interested in Causal Mechanism [D],"Im interested in representation of causal
mechanism and how to apply in Al. Im planning
for applying another postgraduate degree after
cognitive neuro whether in Computer science
(conversion) or Psychology related field (Research
method). Which program is more related to my
above interested topic computer science or
psychology?",Pleasant-Neck-1528,1g2z95b,https://reddit.com/r/MachineLearning/comments/1g2z95b/interested_in_causal_mechanism_d/,https://www.reddit.com/r/MachineLearning/comments/1g2z95b/interested_in_causal_mechanism_d/,2024-10-13 20:39:03,1,0.66,1,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1g2z95b
MachineLearning,[D] Has anyone tried this method for validating LLM-produced QA datasets?,"I'm working on LLM evaluation for a company. On one project we're using an LLM to create a QA dataset and need it to be validated before we can use it as a reliable marker for evaluation of models.

LLM validation of the dataset is obviously risky, human validation is time consuming and expensive.

Given we had to chop up the source documents to fit into the LLM's context window to create the QA dataset, has anyone tried LLM validation of answers through just feeding those same chunks into the model to answer the questions? It would mean all the model would have to do is ""find the needle in the haystack"" which looks like a significantly easier task (https://github.com/gkamradt/LLMTest_NeedleInAHaystack) than the LLM having to know the information.",freshprinceofuk,1g2xv4v,https://reddit.com/r/MachineLearning/comments/1g2xv4v/d_has_anyone_tried_this_method_for_validating/,https://www.reddit.com/r/MachineLearning/comments/1g2xv4v/d_has_anyone_tried_this_method_for_validating/,2024-10-13 19:37:07,0,0.5,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1g2xv4v
MachineLearning,[D] Any boundary conditions in operator learning?,"Be it FNO, DeepONets or any other operator learning framework, is there any work on operator learning with boundary conditions? Ideally Neumann?",Mafisch,1g2wx4u,https://reddit.com/r/MachineLearning/comments/1g2wx4u/d_any_boundary_conditions_in_operator_learning/,https://www.reddit.com/r/MachineLearning/comments/1g2wx4u/d_any_boundary_conditions_in_operator_learning/,2024-10-13 18:56:46,2,0.63,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1g2wx4u
MachineLearning,[P] Looking for a Word to Word Text Detection ,"Hello, firstly I apologize if my english may be lacking. I am currently looking for a word to word text detection model that is easy to train or incorporate into python. I have looked into a lot of different repositories and have tried to implement them only to fail multiple times. I have to admit, I am not the brightest bulb and certainly only know so much.

I am no expert in this subject by any means as a computer science student, I am ashamed however I have tried several end to end models such as pytesseract, easyocr, paddleocr, or computer vision techniques like MSER with NMS to be able to grab the bounding boxes of the papers. The only problem is that these are not perfect and even the best one is pytesseract. It captures around 80 to 85 percent accurately while the rest is intersecting or not being captured at all. 

I was planning to build an ocr system for recognizing handwritten text. I have already trained a well performing recognition model and is only encountering problems on getting the proper bounding boxes of the text. Lastly, I was wondering if the YOLO model may be worth it to start researching on now for getting the bounding boxes and getting the classifications for the text. I need it in 1 week.",brudda65,1g2uemm,https://reddit.com/r/MachineLearning/comments/1g2uemm/p_looking_for_a_word_to_word_text_detection/,https://www.reddit.com/r/MachineLearning/comments/1g2uemm/p_looking_for_a_word_to_word_text_detection/,2024-10-13 17:05:54,0,0.4,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1g2uemm
MachineLearning,"Dataset for Fantasy/Medieval Theme Images with Captions ""[P]""","Hey everyone,

I'm a graduate student looking to do text-to-image generation but specific to fantasy/medieval themes. Tried looking all over the internet for a dataset of such images with captions but can't seem to find any. Would anyone know of any such datasets or have ideas on how to go about curating such a dataset?

Thank you!",rm_9248,1g2u4nx,https://reddit.com/r/MachineLearning/comments/1g2u4nx/dataset_for_fantasymedieval_theme_images_with/,https://www.reddit.com/r/MachineLearning/comments/1g2u4nx/dataset_for_fantasymedieval_theme_images_with/,2024-10-13 16:53:45,0,0.5,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1g2u4nx
MachineLearning,[R] LongCite: Enabling LLMs to Generate Fine-Grained Citations in Long-Context QA,"I just read an interesting paper that aims to address (or improve) information retrieval with fine-grained citations, ""LongCite: Enabling LLMs to Generate Fine-Grained Citations in Long-Context QA"" (https://arxiv.org/abs/2409.02897).

In this paper, the researchers use off-the-shelf LLMs to generate a dataset consisting of long-context QA instances with precise sentence-level citations and then use that dataset to finetune an open-weight LLM to generate answers with citations. The resulting LongCite 8B and 9B models are surprisingly good compared to GPT4o, Llama 3.1, etc.

How does this work? Here is the 4-step procedure for generating the dataset for instruction-finetuning:

(a) Starting with long texts or documents, their method uses an existing LLM to generate a Q&amp;A dataset (a query and its associated answer) using Self-Instruct (Wang et al. 2023; discussed in one of my previous posts).

(b) Next, they use the answer to retrieve several 128-token chunks from the input text for coarse-grained citations.

(c) The LLM then looks for relevant sentences within these chunks to provide more fine-grained sentence-level citations

(d) The researchers filter out Q&amp;A pairs where less than 20% of the statements in the answer don't have citations

The resulting dataset is then used to train an LLM in a conventional (SFT) fashion.

https://preview.redd.it/gucywp0o8jud1.jpg?width=6000&amp;format=pjpg&amp;auto=webp&amp;s=4ed2ff078327082133d390c566250a2ef41c1a05

  
",seraschka,1g2qohf,https://reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/,https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/,2024-10-13 14:19:13,23,0.96,23,0,1,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/EbOtw4W4sfQgGZHdPR88MN2Z6wD54p-Hi_6U3BYzBLU.jpg,t3_1g2qohf
MachineLearning,[R] Seeking Research Approaches for Identifying Semantically Equivalent Sentences or Entities Across Different Languages,"How can I identify equivalent sentences or entities in two different languages that share the same semantic meaning from a research and academic perspective? I'm looking for methods and approaches used to find semantically equivalent pairs, whether they are sentences or other entities, in multilingual settings. Could you suggest relevant research areas, methodologies, or specific keywords I should use to search for academic papers that deal with cross-lingual semantic similarity, sentence alignment, or entity equivalence across languages? Any guidance on tools, models, or algorithms commonly used in this context would also be appreciated.",eyup_kh,1g2mvpc,https://reddit.com/r/MachineLearning/comments/1g2mvpc/r_seeking_research_approaches_for_identifying/,https://www.reddit.com/r/MachineLearning/comments/1g2mvpc/r_seeking_research_approaches_for_identifying/,2024-10-13 10:41:31,5,0.73,5,0,3,0,0,False,False,True,False,False,Research,self,t3_1g2mvpc
MachineLearning,[D] Realism of Landing a PhD Offer,"Hi, everyone! I am a postgraduate at University College London, pursuing a Master's in Machine Learning, and I will soon be applying for admission to PhD programs that start in Fall, 2025. I will share my profile and the schools I will be applying to, and am hoping to learn if the labs I am aiming for are beyond my reach.

I received my undergraduate degree in Mathematics and CS with first-class (honors) from Nanyang Technological University, Singapore, and am expected to earn my postgraduate degree with first-class (honors) as well. I am interested in theoretical deep learning -- problems around curvature of loss surface, optimization trajectories, learning dynamics and generalization -- which are mathematically intense research areas. Although my coursework has remained mostly theoretical and well aligned with such research (by design), my research experience has been more experimental. I have a third-author publication at ICML, on the work I did for my bachelor's thesis project. It is a fairly theoretical work, but I was responsible only for the experiments. I also have a 2 first-author pre-prints -- one experimental work on NLP (aiming for an IEEE publication), and another in graph ML (aiming for one of the top conferences), which has a decent theoretical component, but not as much as the work I hope to do in my PhD.

I am aiming for labs in ETH, UCL, Stanford, NYU, EPFL, Columbia and Princeton (in that order of preference, one of these is my pos). All of them have very successful PIs (by citations), who work on topics very well-aligned with my interests. My concern is that my seemingly all-over-the-place research background might turn them off, but I am hoping that my grades will convince them that I am competent with theory. I expect my supervisors to write excellent recommendation letters since they have appreciated me on numerous occasions. I am hoping to write a convincing research statement, but since I only started reading on relevant literature a couple of weeks back, it may not end up being excellent.

I don't mind working with a younger PI, as long as I have some researchers working on adjacent topics around me. With senior labs, there is a network already established, and I can probably start by assisting on some projects, before getting into independent research. Realistically, am I punching about my weight? If I am, can someone suggest younger PIs working on aforementioned research topics, whose lab I might have a better shot of joining?",mio_11,1g2mugf,https://reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/,https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/,2024-10-13 10:38:56,54,0.79,54,0,76,0,0,False,False,True,False,False,Discussion,self,t3_1g2mugf
MachineLearning,Coming up with novel ideas [D],"Any ideas on how to come up with novel solutions to problems? Every time I think I have something, my advisor says something along the lines of ""this is too straightforward."" A lot of methods glue together existing building blocks in unique ways, but it's hard for me to imagine how people come up with things that are both truly novel **and** actually work.

Sometimes, I read a paper, and I realize that the idea is actually very simple/straightforward, the authors just introduce a cool trick. Other times, I read something that introduces a very obscure theorem, or they notice something that I could only dream of thinking about. I tend towards the former camp, but I haven't felt very proud of anything I've written so far due to limited novelty. It doesn't help that the insane pace of publishing biases me towards ""simple yet effective"" methods where most of the work is in crafting a story post-hoc after acquiring SOTA.",like_a_tensor,1g2jhhx,https://reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/,https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/,2024-10-13 06:25:11,69,0.87,69,0,27,0,0,False,False,True,False,False,Discussion,self,t3_1g2jhhx
MachineLearning,[D] Faith and Fate: Transformers as fuzzy pattern matchers,,jsonathan,1g2irug,https://reddit.com/r/MachineLearning/comments/1g2irug/d_faith_and_fate_transformers_as_fuzzy_pattern/,https://www.answer.ai/posts/2024-07-25-transformers-as-matchers.html,2024-10-13 05:33:44,25,0.85,25,0,1,0,0,False,False,False,False,False,Discussion,https://a.thumbs.redditmedia.com/FMI0mVKMm6Ty4yI_R50yxSehAmp-q9kOQITyHRUXbY0.jpg,t3_1g2irug
MachineLearning,[D] Arxiv not working?,"Today I found that some papers are not loaded, even when I try on different browsers and devices. Although sometimes the meta data and html paper are available, pdf file is not still working. Only for certain papers.

  
 From my bookmark, like 80% of papers are not working. Here's the list of a few I have recognized from my bookmarks.

[https://arxiv.org/pdf/1811.08489](https://arxiv.org/pdf/1811.08489)

[https://arxiv.org/abs/2203.11933](https://arxiv.org/abs/2203.11933)

[https://arxiv.org/pdf/2306.11698.pdf](https://arxiv.org/pdf/2306.11698.pdf)

[https://arxiv.org/pdf/2310.03744.pdf](https://arxiv.org/pdf/2310.03744.pdf)

[https://arxiv.org/pdf/2302.00070.pdf](https://arxiv.org/pdf/2302.00070.pdf)

[https://arxiv.org/pdf/2410.07593](https://arxiv.org/pdf/2410.07593)

[https://arxiv.org/pdf/2203.11933.pdf](https://arxiv.org/pdf/2203.11933.pdf)

",Shot-Button-9010,1g2hw07,https://reddit.com/r/MachineLearning/comments/1g2hw07/d_arxiv_not_working/,https://www.reddit.com/r/MachineLearning/comments/1g2hw07/d_arxiv_not_working/,2024-10-13 04:33:53,6,0.72,6,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1g2hw07
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1g2fmfw,https://reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/,2024-10-13 02:15:21,45,0.89,45,0,41,0,0,False,False,True,False,False,Discussion,self,t3_1g2fmfw
MachineLearning,[Project] Increasing Data in Yolov8 Image segmentation Model,"Hi everyone,

I am training a YOLOv8 Image segmentation Model. I would like to increase the dataset. Is there a way to increase the dataset during training.

For example, I have trained an CNN Model in the past and have generated 100 augmented images per image to increase the dataset during training. The data augmentation parameter look like below for that CNN model.

    datagen = ImageDataGenerator(
            rotation_range=10,
            width_shift_range=0.1,
            height_shift_range=0.1,
            shear_range=0.1,
            zoom_range=0.1,
            horizontal_flip=True,
            vertical_flip = True,
            fill_mode='nearest'
        )

Is there any way to do the same thing with the YOLO Image segmentation model (generating 100 images per image) with the same parameters above. I know I have to put custom values in the .yaml file for the augmentation parameter, but, it will be great if someone can provide me with the information for which custom parameters do I need to change in the .yaml file to achieve the above configuration. Furthermore, if there is way to generate 100 images per image during training, will the polygon coordinates in .txt files in labels adjust themselves automatically according to the applied augmentation parameter.

Please let me know if you need more clarification.

Thanks",sahil_m00,1g2esrc,https://reddit.com/r/MachineLearning/comments/1g2esrc/project_increasing_data_in_yolov8_image/,https://www.reddit.com/r/MachineLearning/comments/1g2esrc/project_increasing_data_in_yolov8_image/,2024-10-13 01:27:05,4,0.75,4,0,3,0,0,False,False,True,False,False,Project,self,t3_1g2esrc
MachineLearning,[D] Looking for lighweight embeddings model that could run completely locally in the browser?,"I have used the OpenAI embeddings endpoint extensively, and Google's Universal Sentence Encoder (USE).  I'm working on a project for people that don't want their data shipped off \*anywhere\*.  So I'm trying to see if I can come up with a completely local implementation where I can store text for them and then do cosine similarity searches on that data completely locally.  I'm hoping to find a lightweight embedding model that can run strictly in the browser on a typical PC, and one that does not download any models when activated, since all of that breaks the privacy constraint.

Has anybody seen something like this?  If so, please leave a link.  My web searches and ChatGPTPlus discussions have not bore fruit yet.",vengeful_bunny,1g2ep9v,https://reddit.com/r/MachineLearning/comments/1g2ep9v/d_looking_for_lighweight_embeddings_model_that/,https://www.reddit.com/r/MachineLearning/comments/1g2ep9v/d_looking_for_lighweight_embeddings_model_that/,2024-10-13 01:21:38,2,0.63,2,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1g2ep9v
MachineLearning,[D] Understanding flow matching,"I'm having bit of a tough time understanding the flow matching paper. Is there some course I can read to better understand this. 

For context I understand ddpm fairly well and ddim to some extent. However, the ODE aspects of it is what I cannot understand. 

The flow matching paper talks a lot about vector fields and ODEs. Would appreciate it if you can recommend me a paper/ course I can do to understand this side. There are a few courses on statistical mechanics on YouTube. Is this relevant, or are there better starting points given that I can understand variational bayes? 

TIA",themathstudent,1g2dpm8,https://reddit.com/r/MachineLearning/comments/1g2dpm8/d_understanding_flow_matching/,https://www.reddit.com/r/MachineLearning/comments/1g2dpm8/d_understanding_flow_matching/,2024-10-13 00:25:07,9,0.76,9,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1g2dpm8
MachineLearning,[R] Large Signal Models,"In the domain of artificial (machine) intelligence, Large Signal Models enable human-centric interactions and knowledge discovery of signal data similar to how prompts allow users to query an LLM based on unstructured text from the web. 

Users can ask general questions about relationships between the focus dataset and results from pre-compiled LSTM built on a signal dataset across a large range of domains. This is achieved by layering in latent pattern detection and knowledge graph-based (KG-based) explainability into an LSTM inference pipeline.

We've found tremendous success with some of our large F500 customers using this technology but it seems as if everyone continues to fixate on LLMs even with their hallucination challenges.

Is there a community that is focused on dealing with the hallucination challenges using novel approaches?",PrizeDry8179,1g2b98h,https://reddit.com/r/MachineLearning/comments/1g2b98h/r_large_signal_models/,https://www.reddit.com/r/MachineLearning/comments/1g2b98h/r_large_signal_models/,2024-10-12 22:14:29,0,0.5,0,0,3,0,0,False,False,True,False,False,Research,self,t3_1g2b98h
MachineLearning,[D] Feedback on ML Project: Using Transformers to improve Ant Colony Optimization Algorithm,"I'm currently working on a personal project trying to build an improved version of the ant colony optimization algorithm. 

I'm using a positional-encoded transformer neural network to predict optimal pheromone matrices before running the algorithm.

The Improved Ant Colony Optimization Algorithm is initialized with a pheromone matrix outputted by a positional-encoded transformer neural network that was trained on pheromone matrix data from the normal ant colony optimization algorithm.

To analyze the improvement of the algorithm, I'm having the improved ACO run with normal ACO run different map sizes for multiple iterations, calculating each algorithm's best runs, and calculating for a p-value to verify if the improved algorithm has statistical significance. 

So far, the enhanced ACO shows promising results with p-values of 0.06 and 0.05 for node sizes of 30 and 35, respectively. 

However, I'm aiming to achieve significance (p &lt; 0.05) across a wider range of node sizes.

I would appreciate any feedback!

Project Link: [https://github.com/ronantakizawa/improvedaco/blob/main/ronan\_acotransformer\_experiment.ipynb](https://github.com/ronantakizawa/improvedaco/blob/main/ronan_acotransformer_experiment.ipynb)",SafeSignificance8840,1g21loo,https://reddit.com/r/MachineLearning/comments/1g21loo/d_feedback_on_ml_project_using_transformers_to/,https://www.reddit.com/r/MachineLearning/comments/1g21loo/d_feedback_on_ml_project_using_transformers_to/,2024-10-12 14:42:03,23,0.93,23,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1g21loo
MachineLearning,[D] How to improve img2img for (stable) diffusion models?,"Image generation folks, we are all familiar with the img2img method for diffusion models, where an input image is used to condition the diffusion process and produce variations of the input image (using a guiding prompt as well). This is implemented is Automatic1111, ComfyUI, Invoke AI and used in many papers.

In this method, the parameter allowing for control is the denoising strength, which corresponds to what percentage of the noise steps are added to the image before denoising it (conditioned by the input prompt).

I am wondering if this simple method as seen improvements in the last two years.

I know you can do img2img for inpainting (local zones of img2img instead of the whole image) or outpainting. I'm particularly looking for papers that try different method of img2img and compare the obtained results. If by any chance someone can point papers, that would help a lot!",gohu_cd,1g20kgi,https://reddit.com/r/MachineLearning/comments/1g20kgi/d_how_to_improve_img2img_for_stable_diffusion/,https://www.reddit.com/r/MachineLearning/comments/1g20kgi/d_how_to_improve_img2img_for_stable_diffusion/,2024-10-12 13:51:13,1,1.0,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g20kgi
MachineLearning,[R] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models (Apple),"arXiv:2410.05229 \[cs.LG\]: [https://arxiv.org/abs/2410.05229](https://arxiv.org/abs/2410.05229)  
Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar - Apple

TechCrunch - Devin Coldewey: Researchers question AI’s ‘reasoning’ ability as models stumble on math problems with trivial changes: [https://techcrunch.com/2024/10/11/researchers-question-ais-reasoning-ability-as-models-stumble-on-math-problems-with-trivial-changes/](https://techcrunch.com/2024/10/11/researchers-question-ais-reasoning-ability-as-models-stumble-on-math-problems-with-trivial-changes/)

Mehrdad Farajtabar, one of the co-authors, breaks down the paper in this thread on X: [https://x.com/MFarajtabar/status/1844456880971858028](https://x.com/MFarajtabar/status/1844456880971858028)",Nunki08,1g1wbir,https://reddit.com/r/MachineLearning/comments/1g1wbir/r_gsmsymbolic_understanding_the_limitations_of/,https://www.reddit.com/r/MachineLearning/comments/1g1wbir/r_gsmsymbolic_understanding_the_limitations_of/,2024-10-12 09:20:19,53,0.92,53,0,23,0,0,False,False,True,False,False,Research,self,t3_1g1wbir
MachineLearning,[P] Improving AI agents using DSPy,"In this post, I will explore how I improved the performance of one of these agents using DSPy. DSPy if you don’t know is a LLM framework specifically designed to programmatically improve prompts and signatures for your LLM. So you can get a better performance without changing the LLM.",phicreative1997,1g1w6qa,https://reddit.com/r/MachineLearning/comments/1g1w6qa/p_improving_ai_agents_using_dspy/,https://medium.com/firebird-technologies/how-to-improve-ai-agent-s-using-dspy-b9e5e15c9d3c,2024-10-12 09:09:35,0,0.44,0,0,0,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/IxI-7TjzXHjZrLrpbhpNA3akZACHHaBMi87zZrJP85E.jpg,t3_1g1w6qa
MachineLearning,[R] Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning (Research from Deepmind) ,"Abstract: A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask: ""How should we design process rewards?"". Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is &gt;8% more accurate, and 1.5−5× more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with 5−6× gain in sample efficiency, and &gt;6% gain in accuracy, over ORMs.",DickMasterGeneral,1g1uf90,https://reddit.com/r/MachineLearning/comments/1g1uf90/r_rewarding_progress_scaling_automated_process/,https://arxiv.org/abs/2410.08146,2024-10-12 06:52:02,35,0.91,35,0,4,0,0,False,False,False,False,False,Research,default,t3_1g1uf90
MachineLearning,[R] Building an MLOps pipeline with Dagger.io and KitOps ,,codes_astro,1g1u0fj,https://reddit.com/r/MachineLearning/comments/1g1u0fj/r_building_an_mlops_pipeline_with_daggerio_and/,https://jozu.com/blog/building-an-mlops-pipeline-with-dagger-io-and-kitops/,2024-10-12 06:20:54,0,0.5,0,0,0,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/IOygUzpdkLRIDXiLXhfMYvijdHhE3EINKk_sXnfE8gg.jpg,t3_1g1u0fj
MachineLearning,The kernel trick (RKHS) applied to logic: Logical Properties and Quantiﬁers in a Semantic Space Framework,,musescore1983,1g1sfz2,https://reddit.com/r/MachineLearning/comments/1g1sfz2/the_kernel_trick_rkhs_applied_to_logic_logical/,https://www.academia.edu/124637124/Logical_Properties_and_Quantifiers_in_a_Semantic_Space_Framework_working_draft_,2024-10-12 04:33:18,42,0.94,42,0,12,0,0,False,False,False,False,False,,https://b.thumbs.redditmedia.com/ibLSGmAC89ntb-8fLm6ZK2fJNxRlW7A4L8dKOSLEowM.jpg,t3_1g1sfz2
MachineLearning,[D] AAAI 2025 Phase 1 decision Leak?,"Has anyone checked the revisions section of AAAI submission and noticed that the paper has been moved to a folder ""Rejected\_Submission"". It should be visible under the Venueid tag. The twitter post that I learned this from:  
[https://x.com/balabala5201314/status/1843907285367828606](https://x.com/balabala5201314/status/1843907285367828606)",Wise_Witness_6116,1g1plva,https://reddit.com/r/MachineLearning/comments/1g1plva/d_aaai_2025_phase_1_decision_leak/,https://www.reddit.com/r/MachineLearning/comments/1g1plva/d_aaai_2025_phase_1_decision_leak/,2024-10-12 01:43:26,50,0.9,50,0,236,0,0,False,False,True,False,False,Discussion,self,t3_1g1plva
MachineLearning,[D] Why does it seem like Google's TPU isn't a threat to nVidia's GPU?,"Even though Google is using their TPU for a lot of their internal AI efforts, it seems like it hasn't propelled their revenue nearly as much as nVidia's GPUs have. Why is that? Why hasn't having their own AI-designed processor helped them as much as nVidia and why does it seem like all the other AI-focused companies still only want to run their software on nVidia chips...even if they're using Google data centers?",kugelblitz_100,1g1okem,https://reddit.com/r/MachineLearning/comments/1g1okem/d_why_does_it_seem_like_googles_tpu_isnt_a_threat/,https://www.reddit.com/r/MachineLearning/comments/1g1okem/d_why_does_it_seem_like_googles_tpu_isnt_a_threat/,2024-10-12 00:44:55,194,0.93,194,0,138,0,0,False,False,True,False,False,Discussion,self,t3_1g1okem
MachineLearning,"[D] Is there an available model for detection of all different objects in a picture? Not descriptions for each, just coordinates ",Title says it all! I've been looking for one in Hugging Face and Replicate with no success.,meet_me_at_seven,1g1mm9h,https://reddit.com/r/MachineLearning/comments/1g1mm9h/d_is_there_an_available_model_for_detection_of/,https://www.reddit.com/r/MachineLearning/comments/1g1mm9h/d_is_there_an_available_model_for_detection_of/,2024-10-11 23:03:49,0,0.25,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1g1mm9h
MachineLearning,[N] Kaido Orav and Byron Knoll's fx2-cmix Wins 7950€ Hutter Prize Award!,"Kaido Orav and Byron Knoll have improved 1.59% on the [Hutter Prize for Lossless Compression of Human Knowledge](http://prize.hutter1.net/) with their ""[fx2-cmix](https://github.com/kaitz/fx2-cmix)"" entry.Because the Hutter Prize restricts contestants to a single *general* purpose processor and it uses [the most *general* loss function](https://www.youtube.com/watch?v=AKMuA_TVz3A), the required algorithmic advances are *generally* applicable regardless of the industry's ""[Hardware Lottery](https://arxiv.org/abs/2009.06489)"" or loss function compromises.  In this respect it provides a unique and low risk incentive for scientific advancement in machine learning.  
  
Some of fx2-cmix's algorithmic advances over the prior Hutter Prize winning algorithm:  
  
**Mixer and Predictor**: Mixers now skip weight updates when errors are below a certain threshold, which enhances processing speed.  
  
***Single Pass*** **Wikipedia Transform**: This update reduces the time and disk usage needed for processing large datasets like Wikipedia by simplifying the transformation process from a previous multi-step approach to a single pass, thereby significantly speeding up preprocessing stages.  
  
**New Stemming and Context Methods**: Utilizing Natural Language Processing techniques like new word types in stemming processes to create more compact and relevant word streams. This not only improves the quality of the training data but also enhances compression, reducing the storage requirements.  
  
**Efficient Article Ordering**: By embedding entire articles in a large vector and using t-SNE to reduce them to a single dimension, the entire corpus can be rapidly reordered to further speed up training.  
  
Detailed descriptions of the advances along with Jupyter notebooks and additional documents are available in the [fx2-cmix README](https://github.com/kaitz/fx2-cmix/blob/main/README.md).  
",jabowery,1g1l725,https://reddit.com/r/MachineLearning/comments/1g1l725/n_kaido_orav_and_byron_knolls_fx2cmix_wins_7950/,https://www.reddit.com/r/MachineLearning/comments/1g1l725/n_kaido_orav_and_byron_knolls_fx2cmix_wins_7950/,2024-10-11 21:56:14,29,1.0,29,0,1,0,0,False,False,True,False,False,News,self,t3_1g1l725
MachineLearning,[R] Composite Learning Units: Generalized Learning Beyond Parameter Updates to Transform LLMs into Adaptive Reasoners,,jalabulajangs,1g1gtns,https://reddit.com/r/MachineLearning/comments/1g1gtns/r_composite_learning_units_generalized_learning/,https://arxiv.org/abs/2410.08037v1,2024-10-11 18:37:47,31,0.89,31,0,4,0,0,False,False,False,False,False,Research,default,t3_1g1gtns
MachineLearning,[Discussion] ML Models for ODE/PDE or Math Problems,"I am looking for papers/materials related to ML/DL models designed for solving ODE/PDE ( Ordinary and Partial Differential Equations) if there are any. Also in general solving any math related problems like math olympiads. Just wanted to study those as I am coming from a math background and feel like those are really promising areas.

Thanks. ",optimization_ml,1g1e3vt,https://reddit.com/r/MachineLearning/comments/1g1e3vt/discussion_ml_models_for_odepde_or_math_problems/,https://www.reddit.com/r/MachineLearning/comments/1g1e3vt/discussion_ml_models_for_odepde_or_math_problems/,2024-10-11 16:38:30,12,0.88,12,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1g1e3vt
MachineLearning,"[Discussion] Recent Paper shows LLMs are more than just ""Stochaistic Parrots""","# The Criticism: LLMs Lack General Reasoning Skills

One of the main points of criticism of current LLM models is that they **do not display general reasoning** skills but instead work based on **memorisation** and **simple interpolation**. I found [this paper](https://www.arxiv.org/abs/2410.02536) recently that I believe proves that are cable of understanding and thus are more than just ""*stochaistic parrots*"".

# The Evidence: General Intelligence from Complex Systems

The paper shows that models like GPT can actually develop **general intelligence** from their training data. The study demonstrates that when models are pretrained on **complex systems**—in this case, **elementary cellular automata (ECA) rules**, which are far removed from any practical real-world task—these models **perform better** on completely unrelated tasks like **reasoning** and **chess move prediction**.

The models were pretrained on ECA rules that produce either **simple, chaotic**, or **complex patterns**. None of these systems had anything to do with **chess** or **reasoning tasks**. And yet, the models that were trained on **more complex ECA systems** (those that operate at the ""edge of chaos"") performed better when fine-tuned for reasoning and chess.

This shows that the model isn't just **memorizing** or parroting back patterns from the ECA training data. The complexity of the system itself (even if unrelated to chess) seems to give the model a better ability to handle reasoning tasks. **If the model were purely memorizing**, it wouldn’t matter whether it was trained on simple or complex data—the results would be the same, since both datasets are equally unrelated to chess.

# The Key Insight: LLMs Can Extract General Intelligence From Their Data

What this paper suggests is that GPT models (and possibly other LLMs) are capable of **extracting general intelligence** from data. They don’t just memorize—they learn patterns, structure, and complexity that translate into better performance on unrelated tasks. This is important because it directly challenges the idea that these models are ""dumb"" text predictors. If **memorization alone** were the driving force, then training on irrelevant complex systems wouldn’t make a difference. But it does.

# The Takeaway: GPT Models Are Moving Toward General Intelligence

**GPT and models like it aren't just good at regurgitating what they've been trained on**—they might actually be on the path toward **general intelligence**, and they get better at reasoning when trained on data that forces them to navigate complexity.",PianistWinter8293,1g194gk,https://reddit.com/r/MachineLearning/comments/1g194gk/discussion_recent_paper_shows_llms_are_more_than/,https://www.reddit.com/r/MachineLearning/comments/1g194gk/discussion_recent_paper_shows_llms_are_more_than/,2024-10-11 12:56:08,0,0.38,0,0,42,0,0,False,False,True,False,False,Discussion,self,t3_1g194gk
MachineLearning,MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering,,MTGTraner,1g18dq4,https://reddit.com/r/MachineLearning/comments/1g18dq4/mlebench_evaluating_machine_learning_agents_on/,https://openai.com/index/mle-bench/,2024-10-11 12:16:30,8,0.79,8,0,0,0,0,False,False,False,False,False,Research,default,t3_1g18dq4
MachineLearning,[Project] Transfer learning between simulation data trained model and experiment data trained model,"Hello,

I'm working on a surrogate model that predicts parameters for high fidelity models in earthquake engineering for my master thesis.

The problem I'm trying to solve is those high fidelity models rely on parameters that we have no practical ways of measuring and are set based on rules of thumbs and/or engineers intuition. This leads us to have big differences between simulations and experiments.

The experiments we run are expensive both economically and in terms of time in addition to often being destructive. So we would like to be able to tune high fidelity model using preliminary results from these experiments before the destructive phase.

So the workflow I'm planning is the following:

1. Build a foundation model on simulation data, possibly from multiple different high fidelity models
2. Fine tune this foundation model using experimental data from a specific experiment.

I'm currently looking at multiple models architectures but find very few promising ones ...

[Expected workflow](https://preview.redd.it/wtn2dza824ud1.png?width=2007&amp;format=png&amp;auto=webp&amp;s=33b911b89713ac262ebe2fd740891c841e1b8b23)

**My question is:**

Do you have recommendation in terms of techniques and models that offers good finetuning of models based on physical system with very limited dataset ?

Any other recommendation is gladly accepted !

Sorry, if this post seems a bit naive this topic is beyond the scope of the courses I had on ML at uni.

Thanks a lot !",DurandilAxe,1g17ev6,https://reddit.com/r/MachineLearning/comments/1g17ev6/project_transfer_learning_between_simulation_data/,https://www.reddit.com/r/MachineLearning/comments/1g17ev6/project_transfer_learning_between_simulation_data/,2024-10-11 11:20:03,10,0.92,10,0,1,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/7FJIAHAy3Zmz_aIrmZCvgCwHf1dT7f0t4-eF5mju6w4.jpg,t3_1g17ev6
MachineLearning,[r][d] what is the SOTA of BigBench?,"BIgBench made a splash 2 years ago (https://github.com/google/BIG-bench).

But the Leaderboard is pretty non-updated. [https://paperswithcode.com/sota/machine-learning-on-big-bench](https://paperswithcode.com/sota/machine-learning-on-big-bench)

Does anyone know what the SOTA for bigbench is? I've seen this: [https://www.reddit.com/r/singularity/comments/1akz9u8/selfdiscover\_google\_deepmind\_large\_language/](https://www.reddit.com/r/singularity/comments/1akz9u8/selfdiscover_google_deepmind_large_language/)  
for instance. But not more recent papers. Thanks!",sunchipsster,1g16afj,https://reddit.com/r/MachineLearning/comments/1g16afj/rd_what_is_the_sota_of_bigbench/,https://www.reddit.com/r/MachineLearning/comments/1g16afj/rd_what_is_the_sota_of_bigbench/,2024-10-11 10:04:45,6,0.8,6,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1g16afj
MachineLearning,[D] Are Voice Biometrics Possible?,"I'm working with a company that handles a bunch of audio data. All of the data is single-speaker, and 99% is between 1-10 minutes long. We would like to sort this data by speaker, but could never handle the load with human labelling.

I've been reading about Speaker Diarization and libraries like [SpeechBrain](https://github.com/speechbrain/speechbrain), and [PyannoteAudio](https://github.com/pyannote/pyannote-audio), but I'm not sure these fit my use case. I don't care about separating speakers, I care about high-accuracy clusterable embeddings.

I've had a cursory look at the literature and found the following:

* [Deep Speaker: an End-to-End Neural Speaker Embedding System](https://arxiv.org/abs/1705.02304) (2017) \[575 citations\]
* [Deep Speaker Embeddings for Short-Duration Speaker Verification](https://www.isca-archive.org/interspeech_2017/bhattacharya17_interspeech.pdf) (2017) \[166 citations\]
* [VoxCeleb2: Deep Speaker Recognition](https://arxiv.org/abs/1806.05622) (2018) \[2551 citations\]
* [Voxceleb: Large-scale speaker verification in the wild](https://www.sciencedirect.com/science/article/pii/S0885230819302712) (2020, possibly the official print of above) \[722 citations\]
* [Speaker recognition based on deep learning: An overview](https://www.sciencedirect.com/science/article/abs/pii/S0893608021000848) (2021) \[398 citations\]

I'm new to this field, but intuitively, this seems a solvable problem. Am I missing anything obvious? Can someone working in this field shed light on what's SOTA?",FPGA_Superstar,1g169v7,https://reddit.com/r/MachineLearning/comments/1g169v7/d_are_voice_biometrics_possible/,https://www.reddit.com/r/MachineLearning/comments/1g169v7/d_are_voice_biometrics_possible/,2024-10-11 10:03:36,9,0.91,9,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1g169v7
MachineLearning,[Discussion] How can I expand my ML specialization to ML in healthcare?,"Hello! I'm finishing my bachelor's degree and I really want to connect my ML specialization with the medical field. How can I do that? 

The problem is, the last biology/chemistry lessons I had were in high school, and I didn't pay attention to any of it at the time. Which I now regret.

I guess, the starting point would be to finish some school-level biology/chemistry courses. What's next?

Are there some year/two-year long courses that can meaningfully expand my specialization to what I want? Is it possible to pursue Master's degree which combines both ML and biology? What's more important, would I be able to keep up considering my bachelor's didn't cover any biology/chemistry?

Getting a second bachelor's seems like a complete no-go, as it would be too hard financially, psychologically and time-wise.

If you have an advice for me, I would greatly appreciate it.",OneEconomist1010,1g15ofx,https://reddit.com/r/MachineLearning/comments/1g15ofx/discussion_how_can_i_expand_my_ml_specialization/,https://www.reddit.com/r/MachineLearning/comments/1g15ofx/discussion_how_can_i_expand_my_ml_specialization/,2024-10-11 09:18:03,1,0.57,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1g15ofx
MachineLearning,[R] Differential Transformer,"### [Paper](https://arxiv.org/abs/2410.05258)

### Abstract

&gt; Transformer tends to overallocate attention to irrelevant context. 
&gt; In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. 
&gt; Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps.
&gt; The subtraction cancels noise, promoting the emergence of sparse attention patterns. [...]
&gt; [...] it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. [...]",fliiiiiiip,1g13gkd,https://reddit.com/r/MachineLearning/comments/1g13gkd/r_differential_transformer/,https://www.reddit.com/gallery/1g13gkd,2024-10-11 06:26:11,228,0.95,228,0,16,0,0,False,False,False,False,False,Research,https://a.thumbs.redditmedia.com/VP1HzOcvQYjrIYs5-NSBgc6jLGSdNgfpFk8DHwJl6I8.jpg,t3_1g13gkd
MachineLearning,[D] Machine learning for good,"Hello, I've been having a kind of existential crisis while being aty daily corporate job. I think all my knowledge is going to waste, because it's not used to help more people around me and I was wondering about how can we use our skills to make ML more available to small businesses.

Have you ever worked at a small business and apply some ML or at least some degree of data enginering to help them streamline processes?
How can we actually help to improve small businesses using technology?

Do you have some articles or books that talk about something like this, it'd be pretty nice to read them as well as knowing all of your opinions!",[deleted],1g0z57p,https://reddit.com/r/MachineLearning/comments/1g0z57p/d_machine_learning_for_good/,https://www.reddit.com/r/MachineLearning/comments/1g0z57p/d_machine_learning_for_good/,2024-10-11 02:00:37,29,0.77,29,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1g0z57p
MachineLearning,[Project] CNN + Tranformers," Hi guys, I’m learning ML and I want to do a model to from images get a description. So I planned to use a CNN model like resnet50 to obtain the images information, without the last two layers so the result would be (B, C, H, W)
And I want any suggestions on how to modify this tensor to fit it in the transformer along with the image description. So this tensor combined would be the input of the causal transformer like gpt2

I think in modifying the tensor to be (B, H*W, C) the a linear to be (B, H*W, emb) and the apply a MultiheadAttention with the description and the result of the tensor. So I get a combined input to the model and I maintain the space representation.

Any more suggestions? Thanks for all",Timely-Reindeer-5292,1g0vfiu,https://reddit.com/r/MachineLearning/comments/1g0vfiu/project_cnn_tranformers/,https://www.reddit.com/r/MachineLearning/comments/1g0vfiu/project_cnn_tranformers/,2024-10-10 22:47:51,0,0.2,0,0,6,0,0,False,False,True,False,False,Project,self,t3_1g0vfiu
MachineLearning,[D] Is Pointrend the best semantic segmentation model in 2024?,"Hi, I’m working on a video semantic segmentation task, and I found that PointRend significantly outperforms DeepLabv3. However, I’m curious if it’s currently the best pre-trained model available or if there are newer models that might perform even better. Thank you!",distancejay,1g0v3ye,https://reddit.com/r/MachineLearning/comments/1g0v3ye/d_is_pointrend_the_best_semantic_segmentation/,https://www.reddit.com/r/MachineLearning/comments/1g0v3ye/d_is_pointrend_the_best_semantic_segmentation/,2024-10-10 22:32:02,1,0.55,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1g0v3ye
MachineLearning,[Project] Llama 3 8B is not doing well at text understanding: alternatives?,"Hey! I've been trying to use Llama-3-8B-Instruct to recognise and extract quantities, descriptions and prices of various products. Regex is not an option as the documents are not well structured enough. NER is not an option as I have no labeled dataset. Therefore I opted for a LLM, but Llama3 is not doing well. It cannot deal with variation very well. I've tried with few-shot and CoT, but same unsatisfactory results.

Apart from asking the company to pay a few hundreds of buck for GPT4 (which would do this really well), what are my other options? Any other models I can run locally that are more powerful than this version of Llama3? 

Thanks!",No_Possibility_7588,1g0tg1m,https://reddit.com/r/MachineLearning/comments/1g0tg1m/project_llama_3_8b_is_not_doing_well_at_text/,https://www.reddit.com/r/MachineLearning/comments/1g0tg1m/project_llama_3_8b_is_not_doing_well_at_text/,2024-10-10 21:15:52,6,0.69,6,0,19,0,0,False,False,True,False,False,Project,self,t3_1g0tg1m
MachineLearning,[p] I wrote a blog post about the new Whisper Turbo,"Hi,

Many people are interested in the new whisper turbo model so I wrote a blog post about it. 

We cover the architecture, training strategy and how to compares to similar work like distill whisper.

Please have a go at it and let me know if you have any questions or feedback.

https://amgadhasan.substack.com/p/demystifying-openais-new-whisper
",Amgadoz,1g0rp88,https://reddit.com/r/MachineLearning/comments/1g0rp88/p_i_wrote_a_blog_post_about_the_new_whisper_turbo/,https://www.reddit.com/r/MachineLearning/comments/1g0rp88/p_i_wrote_a_blog_post_about_the_new_whisper_turbo/,2024-10-10 20:00:00,11,0.87,11,0,0,0,0,False,False,True,False,False,Project,self,t3_1g0rp88
MachineLearning,"[R] Cellular Automaton-Driven Mirrored Tensor Surface for Structured Perturbation in Neural Networks: A Novel Approach to Dynamic Regularization, Enhanced Plasticity, and Multi-Scale Learning through Continuous State-Based Weight Modulation","# Cellular Automaton-Driven Weight Perturbation: A Novel Approach to Neural Network Optimization

## Abstract

Current neural network training methodologies often result in models that, while converged, may not represent optimal weight configuration for given datasets. Traditional approaches, including dropout and noise injection, attempt to address this by introducing randomness during training. However, these methods lack structure and may not efficiently explore the weight space. This shrek paper proposes a novel training paradigm utilizing cellular automaton-driven weight perturbation to enhance neural network optimization.

Our approach introduces a mirrored tensor surface governed by a [continuous state cellular automaton](https://streamable.com/va555y), which interacts with the network's weight space during training. The full model architecture is duplicated into a mirror whose weights now represent cells of a continuous state automaton. This method aims to provide structured, multi-scale perturbations that are more aligned with the inherent patterns in data and the network's learned representations than uniform noise.

Key intuitions driving this approach include:

1. The potential for structured perturbations to guide more meaningful exploration of the weight space.
2. The ability of cellular automata to generate complex, emergent behaviors from simple rules.
3. The possibility of multi-scale effects that could enhance feature learning across various levels of abstraction.

We hypothesize that this method addresses several limitations of current models:

1. Overcoming local optima: The structured perturbations may help models escape suboptimal convergence points.
2. Enhancing generalization: By promoting more diverse weight configurations, the approach could lead to better generalization.
3. Improving adaptability: The dynamic nature of the perturbations could result in more adaptable models.

This approach can be viewed as an advanced form of dropout, offering more controlled and potentially more beneficial regularization. Unlike dropout, which randomly deactivates neurons, our method introduces structured changes to weights, potentially preserving important learned features while encouraging exploration.

# Intuitions

More intuitions behind this method can be found here https://www.reddit.com/r/LocalLLaMA/comments/1fyx27y/im_pretty_happy_with_how_my_method_worked_out/lqzoqfg/ but in essence:

We theorize that current models make poor use of the available total weight count, and that backpropagation on models initialized from uniform noise lead to **enormous representation redundancy**, which results in...

1. Hallucinations due to redundancies with slight variances.
2. Slower convergence as a result of meta-structures required in the late layers which negotiate redundancies to produce coherent outputs that humans like.
3. A need for larger models as a result of the negotiation structure required to mediate redundancy.

Using more sophisticated training methods, we postulate that models in the millions of parameters could be made to perform on the level of models hundreds of time their size. We encourage the /r/LocalLLaMA community to experiment and play with this concept.

# Future Research Directions

If this preliminary concept is proven, we can then hope to augment it with policy networks which learn to pilot the automaton, receiving the loss history and quality evaluations as input to instill a feedback loop. Using a larger evaluation LLM, the model in training is dynamically probed for functional progress and 'levels of consciousness' such that we have a meta-optimization loop where we train this policy network to pilot perturbations in more and more effective ways. The LR could increase, enabling faster and faster convergence back to a functioning model.

The CSA itself could be swapped to a Neural State Automaton with dynamically learned rules. The insight embedded in diffusion model could also be fine-tuned and repurposed into a temporal pattern generator, and instead use text prompts to craft a whole realm of possible dynamics.

# Implementation Steps

Here's a concrete explanation of how you might implement CADMTS:

1. **Initialize the Neural Network:**
   - Create a standard neural network architecture (e.g., a transformer for language tasks).
   - Initialize the weights normally (e.g., using Xavier or He initialization).

2. **Create the Mirrored Cellular Automaton:**
   - Duplicate the structure of your neural network.
   - Instead of normal weight values, initialize this mirror with cellular automaton states.
   - These states could be continuous values between 0 and 1, representing the ""activity"" of each cell.

3. **Define Cellular Automaton Rules:**
   - Create rules for how each ""cell"" (mirroring a weight in the original network) updates based on its neighbors.
   - For example, a simple rule could be: `new_state = (avg_of_neighbors + current_state) / 2`
   - More complex rules could involve thresholds, non-linear functions, or even small neural networks.
   - The automaton in the video above is available for experimentation here https://claude.site/artifacts/f28fcfb9-8718-4305-bacc-03a2e1912b18 

4. **Training Loop:**
   For each training batch:
   1. *Forward Pass:*
      - Perform a normal forward pass through the neural network.
   
   2. *Backward Pass:*
      - Compute gradients as usual.
   
   3. *Cellular Automaton Update:*
      - Update the state of each cell in the mirrored CA based on your defined rules.
   
   4. *Weight Perturbation:*
      - Use the CA states to perturb the weights of the original network.
      - For example: `perturbed_weight = original_weight + (ca_state - 0.5) * perturbation_strength`
      - Or: `perturbed_weight = original_weight + (ca_state * random_value) * perturbation_strength` where `random_value` is generated with `randn_like` for the tensor being modified. 
   
   5. *Weight Update:*
      - Apply the computed gradients to the perturbed weights.

5. **Hyperparameter Tuning:**
   - Adjust the strength of the CA influence (`perturbation_strength`).
   - Experiment with different CA update rules.
   - Try various schedules for when to apply the CA perturbation (e.g., every N steps)
   - Try various schedules of the `perturbation_strength` (e.g. a cyclical sine wave, the dB of a rotating corpus of jazz music, ...)

6. **Evaluation:**
   - Compare the performance of your CA-perturbed model against a baseline without perturbation.
   - Analyze how the CA states evolve over time and correlate with model performance.

7. **Advanced Implementations:**
   1. *Multi-scale CA:*
      - Implement different CA rules at different layers of the network.
      - For example, faster-changing CAs in lower layers, slower in higher layers.
   
   2. *Adaptive CA Rules:*
      - Implement meta-learning to adapt the CA rules based on model performance.
   
   3. *Visualization:*
      - Create tools to visualize the CA states and how they correlate with weight importance.

8. **Integration with Existing Techniques:**
   - Combine this method with other regularization techniques like dropout or weight decay.
   - Experiment with using the CA states to influence learning rates for each weight.

9. **Continuous Refinement:**
   - Based on empirical results, continuously refine your CA rules and perturbation strategies.
   - Consider implementing a policy network that learns to control the CA based on model performance metrics.

This implementation approach allows for a great deal of experimentation. You could start with simple, uniform CA rules and gradually increase complexity. The key is to create a system where the CA provides structured, meaningful perturbations to the weights, potentially allowing the network to explore weight configurations that might be missed by standard gradient descent.

Remember, the goal is to create perturbations that are more structured and potentially more meaningful than random noise, hopefully leading to better exploration of the weight space and ultimately better model performance.",ryunuck,1g0rhsx,https://reddit.com/r/MachineLearning/comments/1g0rhsx/r_cellular_automatondriven_mirrored_tensor/,https://www.reddit.com/r/MachineLearning/comments/1g0rhsx/r_cellular_automatondriven_mirrored_tensor/,2024-10-10 19:50:47,21,0.67,21,0,28,0,0,False,False,True,False,False,Research,self,t3_1g0rhsx
MachineLearning,[R] Research on applying embedding space optimization techniques like BYOL or JEPA to LLMs,"I've been exploring training techniques that work in embedding space, like BYOL or JEPA, but I haven't been able to find any research on applying similar methods to large language models. Does anyone know of work that optimizes LLMs in embedding space, particularly using joint embedding architectures that don't rely on contrastive approaches for pre-training a text encoder like BERT?",One-Heron6488,1g0ni80,https://reddit.com/r/MachineLearning/comments/1g0ni80/r_research_on_applying_embedding_space/,https://www.reddit.com/r/MachineLearning/comments/1g0ni80/r_research_on_applying_embedding_space/,2024-10-10 16:57:18,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1g0ni80
MachineLearning,[D] What are the pros and cons of using a VAE to provide a latent space for generative modelling? (especially for images or video),"I am of the opinion that the variational autoencoders (VAEs) are a hack to make certain kinds of  generative models (latent diffusion, latent consistency models, latent flow models) work with current hardware limitations.

But I also understand the point of some of my colleagues who say the compressed representation provided by VAEs force the generative model to be efficient, focusing on the factors that matter for correctly modelling the data distribution.

Current crop of state-of-the-art video generative models are almost all using some sort of compressed representation (generally a VAE). So, they work. But are they really necessary?

What is your take on this? Are VAEs a crutch? Or an essential part of generative models?",pm_me_your_pay_slips,1g0jpzq,https://reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/,https://www.reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/,2024-10-10 14:10:57,41,0.89,41,0,34,0,0,False,False,True,False,False,Discussion,self,t3_1g0jpzq
MachineLearning,[R] nGPT: Normalized Transformer with Representation Learning on the Hypersphere,"**Paper:** [https://arxiv.org/pdf/2410.01131](https://arxiv.org/pdf/2410.01131)

**Abstract:**

&gt;We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.

**Highlights:**

&gt;Our key contributions are as follows:   
  
*Optimization of network parameters on the hypersphere* We propose to normalize all vectors forming the embedding dimensions of network matrices to lie on a unit norm hypersphere. This allows us to view matrix-vector multiplications as dot products representing cosine similarities bounded in \[-1,1\]. The normalization renders weight decay unnecessary.   
  
*Normalized Transformer as a variable-metric optimizer on the hypersphere* The normalized Transformer itself performs a multi-step optimization (two steps per layer) on a hypersphere, where each step of the attention and MLP updates is controlled by eigen learning rates—the diagonal elements of a learnable variable-metric matrix. For each token t\_i in the input sequence, the optimization path of the normalized Transformer begins at a point on the hypersphere corresponding to its input embedding vector and moves to a point on the hypersphere that best predicts the embedding vector of the next token t\_i+1 .   
  
*Faster convergence* We demonstrate that the normalized Transformer reduces the number of training steps required to achieve the same accuracy by a factor of 4 to 20.

**Visual Highlights:**

https://preview.redd.it/0jdj23ew6ytd1.png?width=1313&amp;format=png&amp;auto=webp&amp;s=144f4fa881d05bd1bc90faa2a0bb2c74e58c71df

[Not sure about the difference between 20k and 200k budgets; probably the best result from runs with different initial learning rates is plotted](https://preview.redd.it/8tf5tw0x6ytd1.png?width=1187&amp;format=png&amp;auto=webp&amp;s=4f9dfbe1f49bdc8aed6fa953dc9220556d7dc947)

https://preview.redd.it/waof2llr7ytd1.png?width=1337&amp;format=png&amp;auto=webp&amp;s=3f82cee29c5fe753e219edf55ab16460fcf9a11a

https://preview.redd.it/a5vburms7ytd1.png?width=859&amp;format=png&amp;auto=webp&amp;s=a3f34b73a580a5798bd5e10e9a4cc950b93fa691

",StartledWatermelon,1g0lnij,https://reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/,https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/,2024-10-10 15:37:27,125,0.97,125,0,54,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/Blq-RZqJ1X_DH6X1SI5d4sPc0I3-AGHpvOwCDgt8YGo.jpg,t3_1g0lnij
MachineLearning,[R] Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions,"**TL;DR:** another variant of recurrent/SSM language model, optionally augmented with attention, claiming SotA.

**Paper:** [https://arxiv.org/pdf/2410.06577](https://arxiv.org/pdf/2410.06577)

**Abstract:**

&gt;Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a O(T) complexity for per-token generation, where T represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus+. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus+ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus+-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints will be available soon.

**Visual Abstract:**

https://preview.redd.it/7cs3hk7vzwtd1.png?width=1015&amp;format=png&amp;auto=webp&amp;s=2078931d95af1b1ff6cb2ac1ece1edd2f73c478d

**Limitations:**

&gt;...Additionally, Rodimus\* lacks the highly I/O-aware optimization found in models such as Mamba and Mamba2. There’s potential to enhance its performance by designing I/O-aware multi-head scalar decay and integrating it with Rodimus’s DDTS to broaden gating mechanisms without significantly impacting training efficiency. Furthermore, for Rodimus+, the memory usage of SW-SKA can be further reduced while achieving better practical application performance. We aim to address these issues in future work.

**Visual Highlights:**

https://preview.redd.it/8awoivwg1xtd1.png?width=1167&amp;format=png&amp;auto=webp&amp;s=e81cdb467f42f86b6a4b7398a06dceff4b8cb9be

https://preview.redd.it/ef9d9nqh1xtd1.png?width=1305&amp;format=png&amp;auto=webp&amp;s=e2f32a8753510ec1a9672cb8ebcf7104f50ea220

https://preview.redd.it/nkm1uo7l1xtd1.png?width=1281&amp;format=png&amp;auto=webp&amp;s=f587f0ceea2cee7761d88445b76095ea096a7893

[Note that only 130M-class results constitute a fair architecture evaluations \(trained on the same dataset\). The larger models for comparison were taken \\""as is\\"". While I consider Qwen-2 a strong baseline, given the limited resources of the authors' team that they could dedicate to downstream performance enhancment, attributing the high scores solely to the proposed architecture would be unjustified](https://preview.redd.it/ur57zj8m1xtd1.png?width=815&amp;format=png&amp;auto=webp&amp;s=7741139dd6539e4f7cd1c11625a27b95a98ddfc7)

https://preview.redd.it/7w8e889c3xtd1.png?width=1295&amp;format=png&amp;auto=webp&amp;s=336370fd6e4e18eeee21d9e322208770f97c977f

[A neat cheat sheet on modern RNN\/SSM architectures](https://preview.redd.it/h0hedjud3xtd1.png?width=1133&amp;format=png&amp;auto=webp&amp;s=a804218a4cee9848990c48174cc02a6f6a538a05)

[The proposed SKA cuts KV cache roughly by factor of two. Unfortunately, the authors don't mention compression factor for GQA in the experiment. If it's 4, as is common, the results look pretty trivial](https://preview.redd.it/2aprut918xtd1.png?width=531&amp;format=png&amp;auto=webp&amp;s=d75b125f4c6a594783189fe974d0c28ebe3ab4ff)

",StartledWatermelon,1g0hi2m,https://reddit.com/r/MachineLearning/comments/1g0hi2m/r_rodimus_breaking_the_accuracyefficiency/,https://www.reddit.com/r/MachineLearning/comments/1g0hi2m/r_rodimus_breaking_the_accuracyefficiency/,2024-10-10 12:20:57,24,0.96,24,0,1,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/G3Onpsckcl42h7LPHSZ4HdGPYJHwWHkdTaecBkfZqGk.jpg,t3_1g0hi2m
MachineLearning,[D] Is there any online platform to test Falcon models?,"Hello everyone! I am testing LLM capabilities in a custom-made dataset. I opted for AWS to test Anthropic and Meta models, but I can't find anything for Falcon models. I did try a Python script (only Falcon 7B due to hardware requirements) with a specific prompt to run the Falcon models, but it took a long time and I was not satisfied with the output. Has anyone tested the bigger models from Falcon?

Thanks!",Fresh_Plant_2653,1g0h0i9,https://reddit.com/r/MachineLearning/comments/1g0h0i9/d_is_there_any_online_platform_to_test_falcon/,https://www.reddit.com/r/MachineLearning/comments/1g0h0i9/d_is_there_any_online_platform_to_test_falcon/,2024-10-10 11:53:47,0,0.33,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1g0h0i9
MachineLearning,[R] Intelligence at the Edge of Chaos,,hardmaru,1g0elvw,https://reddit.com/r/MachineLearning/comments/1g0elvw/r_intelligence_at_the_edge_of_chaos/,https://www.arxiv.org/abs/2410.02536,2024-10-10 09:08:13,56,0.89,56,0,21,0,0,False,False,False,False,False,Research,default,t3_1g0elvw
MachineLearning,[D] How Multinode AI Training Boosts Efficiency by Up to 5X 🚀,"Demystifying AI Training with MultiNode for Running Large Parameter LLM

Ever wondered how advanced AI models get trained? Learn how using multiple nodes can [slash training times by up to 80%](https://greennode.ai/blog/demystify-ai-training-with-multinode?utm_source=reddit&amp;utm_medium=social_post_multinode&amp;utm_campaign=seeding) and enhance computational power without breaking the bank. We break down the advantages, share critical insights, and showcase real-world applications where MultiNode setups are transforming the AI landscape.

**Why does multinode config matter for H100**  
The key lies in the seamless communication between nodes, which is enabled by technologies like **InfiniBand** and the **NVIDIA Collective Communications Library (NCCL)**. NCCL plays a critical role in optimizing workload distribution, ensuring that data is efficiently spread across multiple nodes without significant latency. This is something that’s difficult to achieve with other architectures, which often struggle with communication bottlenecks. 

**Tech Insight:** 

* **Measurable Benefits**: MultiNode configurations can reduce training times for large models by as much as 30-40%\* compared to single-node setups. Additionally, energy consumption is optimized because of efficient workload balancing across nodes, saving resources, and reducing operational costs. 
* **Real-world Benchmarks**: In one example, a MultiNode H100 setup achieved a training time reduction of 35% for a GPT-4 scale model when compared to a single-node setup with the same total GPU count

https://preview.redd.it/dqmuf95evutd1.png?width=924&amp;format=png&amp;auto=webp&amp;s=e167ed64d5d634cafadf7d9fdd45ae13df465170

**This is just the beginning—**[click to explore the deeper insights!](https://greennode.ai/blog/demystify-ai-training-with-multinode?utm_source=reddit&amp;utm_medium=social_post_multinode&amp;utm_campaign=seeding)",SquirrelEffective,1g0ara3,https://reddit.com/r/MachineLearning/comments/1g0ara3/d_how_multinode_ai_training_boosts_efficiency_by/,https://www.reddit.com/r/MachineLearning/comments/1g0ara3/d_how_multinode_ai_training_boosts_efficiency_by/,2024-10-10 04:22:00,0,0.25,0,0,0,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/jo6CBTK8tQd3AqMSJC8AucqCIfy1bM344tDYxCUKC6E.jpg,t3_1g0ara3
MachineLearning,[R] ML Expert opinion for Paper Review in Cancer research,"\[R\]



Hi! 

We are a group of doctors working on this review. 

There have been recent studies utilizing ML models in the management of Tumor Lysis Syndrome (TLS), and I’m seeking a machine learning expert to provide insights on their use and potential improvements. Key areas include model interpretability, data quality, robustness, integration with clinical workflows, and suggesting advancements for better clinical application.

Your contribution would involve co-writing a section of the manuscript (with proper credit), offering a great opportunity to apply your expertise in a precision medicine-focused project and collaborate in advancing cancer research—feel free to reach out if you're interested!

Thanks!",1SageK1,1fzy0yi,https://reddit.com/r/MachineLearning/comments/1fzy0yi/r_ml_expert_opinion_for_paper_review_in_cancer/,https://www.reddit.com/r/MachineLearning/comments/1fzy0yi/r_ml_expert_opinion_for_paper_review_in_cancer/,2024-10-09 18:10:45,4,0.83,4,0,13,0,0,False,False,True,False,False,Research,self,t3_1fzy0yi
MachineLearning,[N] Jurgen Schmidhuber on 2024 Physics Nobel Prize,"The NobelPrizeinPhysics2024 for Hopfield &amp; Hinton rewards plagiarism and incorrect attribution in computer science. It's mostly about Amari's ""Hopfield network"" and the ""Boltzmann Machine.""

 

1. The Lenz-Ising recurrent architecture with neuron-like elements was published in 1925 . In 1972, Shun-Ichi Amari made it adaptive such that it could learn to associate input patterns with output patterns by changing its connection weights. However, Amari is only briefly cited in the ""Scientific Background to the Nobel Prize in Physics 2024."" Unfortunately, Amari's net was later called the ""Hopfield network."" Hopfield republished it 10 years later, without citing Amari, not even in later papers.

2. The related Boltzmann Machine paper by Ackley, Hinton, and Sejnowski (1985) was about learning internal representations in hidden units of neural networks (NNs) [S20]. It didn't cite the first working algorithm for deep learning of internal representations by Ivakhnenko &amp; Lapa. It didn't cite Amari's separate work (1967-68) on learning internal representations in deep NNs end-to-end through stochastic gradient descent (SGD). Not even the later surveys by the authors nor the ""Scientific Background to the Nobel Prize in Physics 2024"" mention these origins of deep learning. ([BM] also did not cite relevant prior work by Sherrington &amp; Kirkpatrick &amp; Glauber)

3. The Nobel Committee also lauds Hinton et al.'s 2006 method for layer-wise pretraining of deep NNs (2006). However, this work neither cited the original layer-wise training of deep NNs by Ivakhnenko &amp; Lapa, nor the original work on unsupervised pretraining of deep NNs (1991).

4. The ""Popular information"" says: “At the end of the 1960s, some discouraging theoretical results caused many researchers to suspect that these neural networks would never be of any real use."" However, deep learning research was obviously alive and kicking in the 1960s-70s, especially outside of the Anglosphere.

5. Many additional cases of plagiarism and incorrect attribution can be found in the following reference [DLP], which also contains the other references above. One can start with Sec. 3:  J. Schmidhuber (2023). How 3 Turing awardees republished key methods and ideas whose creators they failed to credit. Technical Report IDSIA-23-23, Swiss AI Lab IDSIA, 14 Dec 2023. https://people.idsia.ch/~juergen/ai-priority-disputes.html… See also the following reference [DLH] for a history of the field: [DLH] J. Schmidhuber (2022). Annotated History of Modern AI and Deep Learning. Technical Report IDSIA-22-22, IDSIA, Lugano, Switzerland, 2022. Preprint arXiv:2212.11279. https://people.idsia.ch/~juergen/deep-learning-history.html… (This extends the 2015 award-winning survey https://people.idsia.ch/~juergen/deep-learning-overview.html…)


Twitter post link: https://x.com/schmidhuberai/status/1844022724328394780?s=46&amp;t=Eqe0JRFwCu11ghm5ZqO9xQ",optimization_ml,1fzw5b1,https://reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/,https://www.reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/,2024-10-09 16:52:42,351,0.93,351,0,145,0,0,False,False,True,False,False,News,self,t3_1fzw5b1
MachineLearning,[D] Schmidhuber’s Take on Hopfield &amp; Hinton’s Nobel Prize in Physics: Overlooked Contributions in Neural Networks and Deep Learning,"Source:

[https://twitter.com/SchmidhuberAI/status/1844022724328394780](https://twitter.com/SchmidhuberAI/status/1844022724328394780)  


The [#NobelPrizeinPhysics2024](https://twitter.com/hashtag/NobelPrizeinPhysics2024?src=hashtag_click) for Hopfield &amp; Hinton rewards plagiarism and incorrect attribution in computer science. It's mostly about Amari's ""Hopfield network"" and the ""Boltzmann Machine.""    
  
1. The Lenz-Ising recurrent architecture with neuron-like elements was published in 1925 \[L20\]\[I24\]\[I25\]. In 1972, Shun-Ichi Amari made it adaptive such that it could learn to associate input patterns with output patterns by changing its connection weights \[AMH1\]. However, Amari is only briefly cited in the ""Scientific Background to the Nobel Prize in Physics 2024."" Unfortunately, Amari's net was later called the ""Hopfield network."" Hopfield republished it 10 years later \[AMH2\], without citing Amari, not even in later papers.   
  
2. The related Boltzmann Machine paper by Ackley, Hinton, and Sejnowski (1985) \[BM\] was about learning internal representations in hidden units of neural networks (NNs) \[S20\]. It didn't cite the first working algorithm for deep learning of internal representations by Ivakhnenko &amp; Lapa (Ukraine, 1965)\[DEEP1-2\]\[HIN\]. It didn't cite Amari's separate work (1967-68)\[GD1-2\] on learning internal representations in deep NNs end-to-end through stochastic gradient descent (SGD). Not even the later surveys by the authors \[S20\]\[DL3\]\[DLP\] nor the ""Scientific Background to the Nobel Prize in Physics 2024"" mention these origins of deep learning. (\[BM\] also did not cite relevant prior work by Sherrington &amp; Kirkpatrick \[SK75\] &amp; Glauber \[G63\].)  
  
3. The Nobel Committee also lauds Hinton et al.'s 2006 method for layer-wise pretraining of deep NNs (2006) \[UN4\]. However, this work neither cited the original layer-wise training of deep NNs by Ivakhnenko &amp; Lapa (1965)\[DEEP1-2\] nor the original work on unsupervised pretraining of deep NNs (1991) \[UN0-1\]\[DLP\].   
  
4. The ""Popular information"" says: “At the end of the 1960s, some discouraging theoretical results caused many researchers to suspect that these neural networks would never be of any real use."" However, deep learning research was obviously alive and kicking in the 1960s-70s, especially outside of the Anglosphere \[DEEP1-2\]\[GD1-3\]\[CNN1\]\[DL1-2\]\[DLP\]\[DLH\].   
  
5. Many additional cases of plagiarism and incorrect attribution can be found in the following reference \[DLP\], which also contains the other references above. One can start with Sec. 3:   
  
\[DLP\] J. Schmidhuber (2023). How 3 Turing awardees republished key methods and ideas whose creators they failed to credit. Technical Report IDSIA-23-23, Swiss AI Lab IDSIA, 14 Dec 2023. [https://people.idsia.ch/\~juergen/ai-priority-disputes.html…](https://t.co/Nz0fjc6kyx)   
  
See also the following reference \[DLH\] for a history of the field:  
  
\[DLH\] J. Schmidhuber (2022). Annotated History of Modern AI and Deep Learning. Technical Report IDSIA-22-22, IDSIA, Lugano, Switzerland, 2022. Preprint arXiv:2212.11279. [https://people.idsia.ch/\~juergen/deep-learning-history.html…](https://t.co/Ys0dw5hkF4) (This extends the 2015 award-winning survey [https://people.idsia.ch/\~juergen/deep-learning-overview.html…](https://t.co/7goTtI5Uwv))",Erling-Haaland,1fzt98g,https://reddit.com/r/MachineLearning/comments/1fzt98g/d_schmidhubers_take_on_hopfield_hintons_nobel/,https://www.reddit.com/r/MachineLearning/comments/1fzt98g/d_schmidhubers_take_on_hopfield_hintons_nobel/,2024-10-09 14:51:44,1,0.53,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1fzt98g
MachineLearning,[D] What happened to the guy who was making a classifier for posts here?,"Not entirely sure where else to post this, so please remove if not allowed.

I think a couple of years back there was a guy who wanted to build a bot that did text classification on the posts here to better flag questions as suitable or not.

What happened to that? Does anybody know? I'm curious if Reddit's new API pricing would affect that.",Seankala,1fzstxw,https://reddit.com/r/MachineLearning/comments/1fzstxw/d_what_happened_to_the_guy_who_was_making_a/,https://www.reddit.com/r/MachineLearning/comments/1fzstxw/d_what_happened_to_the_guy_who_was_making_a/,2024-10-09 14:32:53,8,0.72,8,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1fzstxw
MachineLearning,[D] Concept drift in network dataset,"Hello ML friends, 

I'm working on a network project where we are trying to implement concept drift in dataset generated from our test bed. So to introduce the drift, we changed payload of packets in the network. And we observed the performance  of model got degraded. Here we trained the model without using payload as a feature. 

  
I'm here thinking whether change in payload size is causing data drift or concept drift. or simple how can we prove that this is concept drift or this is data drift. Share your thoughts please. Thank you",dumbestindumb,1fzsmue,https://reddit.com/r/MachineLearning/comments/1fzsmue/d_concept_drift_in_network_dataset/,https://www.reddit.com/r/MachineLearning/comments/1fzsmue/d_concept_drift_in_network_dataset/,2024-10-09 14:24:14,2,1.0,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1fzsmue
MachineLearning,[P] Pretrain BERT from scratch with two low resource language,Hello everyone. I am planning to train BERT from scratch using SSP and MLM. Is it doable with only 2.2m words total for both language?,emanuel01251,1fzohtz,https://reddit.com/r/MachineLearning/comments/1fzohtz/p_pretrain_bert_from_scratch_with_two_low/,https://www.reddit.com/r/MachineLearning/comments/1fzohtz/p_pretrain_bert_from_scratch_with_two_low/,2024-10-09 10:47:25,1,0.55,1,0,4,0,0,False,False,True,False,False,Project,self,t3_1fzohtz
MachineLearning,[N] The 2024 Nobel Prize in Chemistry goes to the people Google Deepmind's AlphaFold. One half to David Baker and the other half jointly to Demis Hassabis and John M. Jumper.,Announcement: https://twitter.com/NobelPrize/status/1843951197960777760,aagg6,1fznxyr,https://reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/,https://www.reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/,2024-10-09 10:09:22,418,0.97,418,0,112,0,0,False,False,True,False,False,News,self,t3_1fznxyr
MachineLearning,[D] How far would you go to be the best AI experience?,"There is plenty of room for integration of AI into our life and its happening at rapid rate. I am at a point where I would consider letting AI see exactly what I am doing on my computer and let it get the context of my current problem asap, i.e. without the need to type it in some chatgpt interface.

The analogy here is same as with Cursor for example, it sees the codebase your are in and wpp (words oper prompt) is much less than using just chat interface for most tasks. Also Adobe and more apps have integrated ai into their system in some way.

So - writing a report, working in a spreadsheet, doing math, editing videos. Would you be comfortable integrating AI into your computer?

Hope im not being too vague, interpret the terminology of 'integrating AI into your computer' as you want, whats the limit you would into and why.",iceee-coffee,1fznox0,https://reddit.com/r/MachineLearning/comments/1fznox0/d_how_far_would_you_go_to_be_the_best_ai/,https://www.reddit.com/r/MachineLearning/comments/1fznox0/d_how_far_would_you_go_to_be_the_best_ai/,2024-10-09 09:51:28,0,0.16,0,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1fznox0
MachineLearning,[D] Why is there so little statistical analyses in ML research?,"Why is it so common in ML research to not do any statistical test to verify that the results are actually significant? Most of the times, a single outcome is presented, instead of doing multiple runs and performing something like a t-test or Mann Whitney U Test etc. Drawing conclusions based on a single sample would be impossible in other disciplines, like psychology or medicine, why is this not considered a problem in ML research?

  
Also, can someone recommend a book for exactly this, statistical tests in the context of ml?",BommiBrainBug,1fznaa9,https://reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/,https://www.reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/,2024-10-09 09:21:06,210,0.96,210,0,119,0,0,False,False,True,False,False,Discussion,self,t3_1fznaa9
MachineLearning,[R] Loss function for multiple decoder model ,"Hello!

I'm using a single encoder - multiple decoder model for segmentation (binary segmentation). It's working in on BCE w LogitsL for now because I'm trying to match a baseline and it considers average loss overall.

Is there any better approach than using average loss (bce, dice, etc.) because the class ratio varies alot for all my classes and I feel that few of my classes underfits with my model starts overfitting overall.

Is there any approach for such cases? Any idea?",ade17_in,1fzn4c8,https://reddit.com/r/MachineLearning/comments/1fzn4c8/r_loss_function_for_multiple_decoder_model/,https://www.reddit.com/r/MachineLearning/comments/1fzn4c8/r_loss_function_for_multiple_decoder_model/,2024-10-09 09:07:52,1,0.67,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1fzn4c8
MachineLearning,[D] Has anyone tried combining Adagrad and Adam optimizers?,"Basically the title. I was thinking what if we could harness the powers of both Adam and Adagrad, so we have changing learning rate as well as momentum and RMS.

Here is the more mathematical version:-

https://preview.redd.it/r18fcv4crotd1.png?width=498&amp;format=png&amp;auto=webp&amp;s=0b72fbe7118ac918bcfd047cb9ef78018b0531fd

",lel_73,1fzm4w2,https://reddit.com/r/MachineLearning/comments/1fzm4w2/d_has_anyone_tried_combining_adagrad_and_adam/,https://www.reddit.com/r/MachineLearning/comments/1fzm4w2/d_has_anyone_tried_combining_adagrad_and_adam/,2024-10-09 07:48:12,0,0.38,0,0,6,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/5faat6hJzv6pxYsupIttkCJ_Pg3zweFBIAELOfYB41Q.jpg,t3_1fzm4w2
MachineLearning,[D] Please point me in the right direction for further exploring my line of thinking in AI alignment,"I'm not a researcher or working in AI or anything, but I have a hobby interest in it, and I've been reading a bunch of research papers lately that are related to an idea I have. The problem I'm trying to solve is what objective function to give a reinforcement learning AI so that it will be empathetic towards conscious agents and help them fulfill their goals. I'm basically trying to solve Collaborative Inverse Reinforcement Learning, but where the AI isn't told who it's supposed to help, so it helps anything that acts like an agent. My assumption is that something that acts like an agent is likely to be conscious, and cases where this isn't true aren't big enough to matter. The reason I'm interested in this reward function is that I want the AI to learn friendliness without the bottleneck of labelled examples, and I want it to generalize to helping non-humans such as animals. One primary challenge whose solution would make it so much easier is how to do Inverse Reinforcement Learning without access to action labels. For example, inferring goals just from a video. I'm basically trying to come up with an objective function for benevolence in general.

The thread I'm currently thinking along is empowerment (mutual information between actions and world states). I think you can measure the alignment between 2 agents by subtracting their individual empowerment from their joint empowerment. When they're collaborating, this measure would presumably be positive. When they're working independently it'd be zero and when they're competing it'd be negative. By maximizing this, the AI can account for wasted effort when helping one agent causes a disadvantage to the other. Once this is corrected for, if you then help the agents by maximizing their individual empowerment, the terms cancel and you're just left maximizing their joint empowerment. Maximizing joint empowerment is the same as maximizing individual empowerment plus alignment. The logical conclusion is that to empower everyone, you'd want to maximize the empowerment of a joint agent whose actions are equal to the world states themselves.

Empowerment is only an instrumental goal though. Once you empower everyone jointly, you'd want to measure what they're empowered towards and help with that, even if it reduces empowerment. Maybe that means that you just directly do Collaborative Inverse Reinforcement Learning on this overall agent whose actions are the world states. However, that just seems like accelerationism which could have problems. I also don't know how to factor in the idea that this ""world agent"" might not be rational while it's still unempowered.

If this idea of a ""world agent"" doesn't pan out, I'm back to the problem of how to detect agent-like behavior and adding up the preferences of all these agents. I'd definitely like more options of how to do so, but I haven't been able to think of anything else. Please show me any research that's relevant to this.",NeuroPyrox,1fzkh4y,https://reddit.com/r/MachineLearning/comments/1fzkh4y/d_please_point_me_in_the_right_direction_for/,https://www.reddit.com/r/MachineLearning/comments/1fzkh4y/d_please_point_me_in_the_right_direction_for/,2024-10-09 05:42:06,0,0.33,0,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1fzkh4y
MachineLearning,[P] Hugging Face CLI Autocompletion for Easier Model Downloads,"# Usage

`$ huggingface-cli download google/`

Press TAB, and this will suggest available models:

        google/codegemma-7b                google/flan-t5-xl
        google/gemma-2-2b-jpn-it-pytorch   google/matcha-plotqa-v2
        google/gemma-2-9b                  google/owlvit-base-patch32
        google/gemma-2-9b-it               google/paligemma-3b-pt-224
        google/gemma-2b                    google/pegasus-cnn_dailymail
        google/gemma-2b-it                 google/siglip-so400m-patch14-384
        google/gemma-7b                    google/timesfm-1.0-200m

# Installation

    curl -sSL https://huggingface.co/spaces/pavel321/huggingface-cli-completion/resolve/main/huggingface-cli-completion.sh &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc

# ",RetiredApostle,1fzj4d1,https://reddit.com/r/MachineLearning/comments/1fzj4d1/p_hugging_face_cli_autocompletion_for_easier/,https://www.reddit.com/r/MachineLearning/comments/1fzj4d1/p_hugging_face_cli_autocompletion_for_easier/,2024-10-09 04:13:21,4,1.0,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1fzj4d1
MachineLearning,[D] anyone interested in a discussion group for Jaynes' Probability Theory the Logic of Science?,"Have been reading this book for some time, and struggling with some chapters, wonder if there are any interests",Illustrious-Pay-7516,1fzf7b7,https://reddit.com/r/MachineLearning/comments/1fzf7b7/d_anyone_interested_in_a_discussion_group_for/,https://www.reddit.com/r/MachineLearning/comments/1fzf7b7/d_anyone_interested_in_a_discussion_group_for/,2024-10-09 00:39:46,6,0.88,6,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1fzf7b7
MachineLearning,[R] Diffusion Models are Evolutionary Algorithms,"[Abstract](https://arxiv.org/abs/2410.02543): In a convergence of machine learning and biology, we reveal that diffusion models are evolutionary algorithms. By considering evolution as a denoising process and reversed evolution as diffusion, we mathematically demonstrate that diffusion models inherently perform evolutionary algorithms, naturally encompassing selection, mutation, and reproductive isolation. Building on this equivalence, we propose the Diffusion Evolution method: an evolutionary algorithm utilizing iterative denoising -- as originally introduced in the context of diffusion models -- to heuristically refine solutions in parameter spaces. Unlike traditional approaches, Diffusion Evolution efficiently identifies multiple optimal solutions and outperforms prominent mainstream evolutionary algorithms. Furthermore, leveraging advanced concepts from diffusion models, namely latent space diffusion and accelerated sampling, we introduce Latent Space Diffusion Evolution, which finds solutions for evolutionary tasks in high-dimensional complex parameter space while significantly reducing computational steps. This parallel between diffusion and evolution not only bridges two different fields but also opens new avenues for mutual enhancement, raising questions about open-ended evolution and potentially utilizing non-Gaussian or discrete diffusion models in the context of Diffusion Evolution.",ptuls,1fzbvq3,https://reddit.com/r/MachineLearning/comments/1fzbvq3/r_diffusion_models_are_evolutionary_algorithms/,https://www.reddit.com/r/MachineLearning/comments/1fzbvq3/r_diffusion_models_are_evolutionary_algorithms/,2024-10-08 22:01:07,50,0.8,50,0,21,0,0,False,False,True,False,False,Research,self,t3_1fzbvq3
MachineLearning,[N] Gymnasium v1.0 is now released with a stable core API!,"We are excited to announce Gymnasium v1.0, a maintained fork of OpenAI Gym used to define reinforcement learning environments. Read the [release notes](https://github.com/Farama-Foundation/Gymnasium/releases/tag/v1.0.0) to find out all the changes we’ve made. This is the combined work of our amazing volunteers for the last 3 years. Over that time, we have steadily improved the library - fixing bugs, adding new features and API changes where we believed necessary. With v1.0, this will be Gymnasium’s first stable release with no planned changed to the core API (Env, Space or VectorEnv) meaning that if you were waiting to update your project, now is the time, see our [migration guide](https://gymnasium.farama.org/main/introduction/migration_guide/) for more info.",jkterry1,1fzbd5r,https://reddit.com/r/MachineLearning/comments/1fzbd5r/n_gymnasium_v10_is_now_released_with_a_stable/,https://www.reddit.com/r/MachineLearning/comments/1fzbd5r/n_gymnasium_v10_is_now_released_with_a_stable/,2024-10-08 21:38:11,1,1.0,1,0,0,0,0,False,False,True,False,False,News,self,t3_1fzbd5r
MachineLearning,[D] What is an extraordinary project I can build?,"I am trying to break into the field as an ML engineer. I have currently been software engineer for 5-6 years and have started learning ML and have even started my masters in AI/ML. It feels like when I first started learning coding, I don’t know what projects to build or where to start. I would love to work for xAI but no they expect something extraordinary from you. So what is a good project I can build to showcase my skills and also learn a lot? I have built a few projects in school but can not show case them due to honor code rules.

I was thinking about using Spotiy’s api to extract my user data and build a ML model that can predict my music taste but am not sure if this is too easy. I am open to any suggestions. Thank you",Suspicious_Stable_25,1fz8bjj,https://reddit.com/r/MachineLearning/comments/1fz8bjj/d_what_is_an_extraordinary_project_i_can_build/,https://www.reddit.com/r/MachineLearning/comments/1fz8bjj/d_what_is_an_extraordinary_project_i_can_build/,2024-10-08 19:28:19,0,0.15,0,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1fz8bjj
MachineLearning,[D] Run Code in the Cloud from a Local Notebook,"I want to share a new Python library we built that lets you write run cloud functions from a local Jupyter notebook.

# How does it work?

When you run a notebook cell, the code executes on a powerful server in the cloud instead of your laptop. 

The logs from the remote server get streamed back to your notebook. It feels like the code is still running in your local notebook, but it’s actually running remotely.

https://preview.redd.it/aj7k4uy9jktd1.png?width=790&amp;format=png&amp;auto=webp&amp;s=e24f7d1580ab01472cf4b529335ba3ae266ea4be

# Benefits 

**You can develop on the cloud without using a cloud notebook.** 

If you’ve ever used a cloud notebook, you’ve probably had your cloud notebook crash and lost your work. 

This lets you develop on a local, low-powered system, while streaming the computation to the cloud.

**Local files automatically sync with the cloud runtime** 

You can use files from your local machine in your remote function executions. No need to upload and download weights from Google drive or S3. 

**You can mix-and-match compute across cells**  

Does your training code need the same hardware as your inference code? Probably not. This lets you customize the hardware used in your notebook, function-by-function. 

We’d be happy if you gave this a try! Let us know if you have any feature ideas or suggestions. 

**Example Notebook:** [https://github.com/beam-cloud/examples/blob/main/jupyter\_notebooks/beam-notebook.ipynb](https://github.com/beam-cloud/examples/blob/main/jupyter_notebooks/beam-notebook.ipynb)

**Website**: [https://beam.cloud](https://beam.cloud)

**Docs**: [https://docs.beam.cloud](https://docs.beam.cloud)",velobro,1fz5r5t,https://reddit.com/r/MachineLearning/comments/1fz5r5t/d_run_code_in_the_cloud_from_a_local_notebook/,https://www.reddit.com/r/MachineLearning/comments/1fz5r5t/d_run_code_in_the_cloud_from_a_local_notebook/,2024-10-08 17:41:08,8,0.75,8,0,5,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/4huQfpflEPsfBSjDebtinit6CKya7Mj8DUmq9JBoGXo.jpg,t3_1fz5r5t
MachineLearning,[P] Objective Bayesian inference for comparing binomial proportions,"Suppose we observe values from two independent binomial distributions with unknown probabilities of success p\_1 and p\_2.  I'm working on a project [https://github.com/rnburn/bbai](https://github.com/rnburn/bbai) that allows you to construct an objective posterior distribution for the delta of the two probabilities, p\_1 - p\_2.

Here's a brief demonstration in Python.

    # Let a1 and b1 denote the successes and failures from
    # the first binomial distribution; and a2 and b2, the successes
    # and failures from the second.
    a1, b1, a2, b2 = 5, 3, 2, 7
    
    # Fit a posterior distribution with likelihood function
    #     L_y(theta, x) = (theta + x)^a1 * (1 - theta - x)^b1 * x^a2 (1-x)^b2
    # where x represents the probability of success for the second 
    # binomial binomial distribution and theta represents
    #    (probability of success first distribution) - x
    # 
    # The posterior distribution uses a reference prior, derived by applying 
    # the process described in Proposition 0.2 of [2]. See
    #   https://www.objectivebayesian.com/p/binomial-comparison
    # for the steps of the derivation.
    from bbai.model import DeltaBinomialModel
    model = DeltaBinomialModel()
    model.fit(a1, b1, a2, b2)
    
    # Print the probability that theta &lt; 0.123
    #
    # The implementation uses efficient deterministic numerical algorithms
    # that provide results to a high level of accuracy. See [3] and [4] for
    # details on the numerical methods.
    print(model.cdf(0.123))
         # Prints 0.10907436812863071

# Background

As far as I know, the first person to analyze the problem of comparing binomial proportions in a Bayesian context was Laplace in 1778 (\[[1](http://www.probabilityandfinance.com/pulskamp/Laplace/memoir_probabilities.pdf)\]).

Laplace observed that the ratio of boys-to-girls born in London was notably higher than the ratio of boys-to-girls born in Paris and sought to determine whether the difference was statistically significant.

[Table 1: Birth records for London and Paris. Ratio represents #boys\/#girls \[1, p. 59\]](https://preview.redd.it/wn2jbia67ktd1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=ffba64b8ce4bd1db9098c5ac74353a844c558279)

Using what would now be called Bayesian inference together with a uniform prior, Laplace computed a posterior probability that the birth ratio in London was less than the birth ratio in Paris,

https://preview.redd.it/27rveomd7ktd1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=e6a12b891f26c3932568a3b98e4b6a6db886d49e

where

https://preview.redd.it/tny2n63g7ktd1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=9af0c23d1be65424421fe0a54c8ae3603d162636

Laplace concluded

&gt;there is therefore odds of more than four hundred thousand against one that the births of boys are more facile in London than in Paris. Thus we can regard as a very probable thing that there exists, in the first of these two cities, a cause more than in the second, which facilitates the births of boys, and which depends either on the climate or on the nourishment of the mothers. \[[1](http://www.probabilityandfinance.com/pulskamp/Laplace/memoir_probabilities.pdf), p. 61\]

Laplace explained his use of the uniform prior as follows:

&gt;When we have nothing given a priori on the possibility of an event, it is necessary to assume all the possibilities, from zero to unity, equally probable; thus, observation can alone instruct us on the ratio of the births of boys and of girls, we must, considering the thing only in itself and setting aside the events, to assume the law of possibility of the births of a boy or of a girl constant from zero to unity, and to start from this hypothesis into the different problems that we can propose on this object. \[[1](http://www.probabilityandfinance.com/pulskamp/Laplace/memoir_probabilities.pdf), p. 26\]

But of course, we know now that the uniform prior leads to arbitrary results that depend on the scale used for a parameter, as Fisher notes in \[[5](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Likelihood/Fisher1922.pdf?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions)\].

A much better and less arbitrary of a way to build a prior is to use reference analysis (\[6, part 3\]).

Reference analysis provides processes to produce an objective prior given a likelihood function and a parameter of interest. It can be viewed as a refinement of Jeffreys prior. Jeffreys' method works well for single parameter models where it produces a prior that is asymptotically optimal in terms of frequentist matching coverage (see §0.2.3.2 of \[[2](https://www.uv.es/~bernardo/OBayes.pdf?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions)\] and \[7\]); however, it often fails to produce a good prior for models with multiple parameters. Berger et al note

&gt;We have seen three important examples — the normal model with both parameters unknown, the Neyman-Scott problem and the multinomial model — where the multiparameter form of the Jeffreys-rule prior fails to provide appropriate objective posteriors. We actually know of no multiparameter example in which the Jeffreys-rule prior has been verified to be satisfactory. In higher dimensions, the prior always seems to be either ‘too diffuse’ as in the normal examples, or ‘too concentrated’ as the multinomial example \[6, p. 117\]

Reference analysis, in contrast, produces priors with good frequentist matching coverage in nearly all cases.

# Validation

A simple way to test whether a prior is good for objective Bayesian inference is to run frequentist coverage simulations. Suppose we fix models parameters to true values, generate sample observations, then produce a 95% posterior credible set with the prior. If we repeat the process many times, then a good objective prior should contain the true parameter of interest approximately 95% of the time regardless of the number of observations or the parameters' true values.

Here's how we can code up such simulation in Python for binomial comparison:

    from bbai.model import DeltaBinomialModel
    import math
    
    alpha = 0.95
    low = (1 - alpha) / 2.0
    high = 1.0 - low
    
    def coverage(prior, theta, x, n1, n2):
        model = DeltaBinomialModel(prior=prior)
        p1 = theta + x
        p2 = x
        theta = p1 - p2
        res = 0.0
        for k1 in range(n1+1):
            prob1 = math.comb(n1, k1) * p1 ** k1 * (1 - p1)**(n1 - k1)
            for k2 in range(n2+1):
                prob = prob1 * math.comb(n2, k2) * p2 ** k2 * (1 - p2)**(n2 - k2)
                model.fit(k1, n1-k1, k2, n2-k2)
                cdf = model.cdf(theta)
                if low &lt;= cdf and cdf &lt;= high:
                    res += prob
        return res

Running the simulation across a range of parameters and testing the reference prior, Laplace's uniform prior, and Jeffreys prior, I get these results

[Table 2: Frequentist matching coverage results for the Laplace, Jeffreys, and reference prior. Values closer to 0.95 indicate better performance.](https://preview.redd.it/0fju5l418ktd1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=566b618d038cfcba02d97138b1aaf2543f7e1600)

While for many parameters the priors give similar and decent results, we can see that coverage for the uniform prior is poor when θ is close to the extremes ±1. Coverage for these values of θ improve for Jeffreys prior, and they are the best for the reference priors.

For the full source code of the simulation, see the Jupyter notebook [https://github.com/rnburn/bbai/blob/master/example/21-delta-binomial-coverage.ipynb](https://github.com/rnburn/bbai/blob/master/example/21-delta-binomial-coverage.ipynb)

# References

\[1\]: Laplace, P. (1778). [Mémoire sur les probabilités](http://www.probabilityandfinance.com/pulskamp/Laplace/memoir_probabilities.pdf). Translated by Richard J. Pulskamp.

\[2\]: Berger, J., J. Bernardo, and D. Sun (2022). [Objective bayesian inference and its relationship to frequentism.](https://www.uv.es/~bernardo/OBayes.pdf?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions)

\[3\]: How to approximate functions with adaptive sparse grids in Chebyshev nodes. [https://www.objectivebayesian.com/p/approximation](https://www.objectivebayesian.com/p/approximation)

\[4\]: Trefethen Lloyd N\*.\* [Approximation Theory and Approximation Practice, Extended Edition](https://www.amazon.com/Approximation-Theory-Practice-Applied-Mathematics/dp/1611972396?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions). USA: Society for Industrial and Applied Mathematics, 2019.

\[5\]: Fisher, R. (1922). [On the mathematical foundations of theoretical statistics.](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Likelihood/Fisher1922.pdf?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions) *Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 222*, 309–368.

\[6\]: Berger, J., J. Bernardo, and D. Sun (2024). [*Objective Bayesian Inference*](https://www.worldscientific.com/worldscibooks/10.1142/13640?utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions#t=aboutBook). World Scientific.

\[7\]: Welch, B. L. and H. W. Peers (1963). [On formulae for confidence points based on integrals of weighted likelihoods.](https://academic.oup.com/jrsssb/article-abstract/25/2/318/7035245?redirectedFrom=PDF&amp;utm_source=www.objectivebayesian.com&amp;utm_medium=referral&amp;utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions) *Journal of the Royal Statistical Society Series B-methodological 25*, 318–329.",rnburn,1fz49wf,https://reddit.com/r/MachineLearning/comments/1fz49wf/p_objective_bayesian_inference_for_comparing/,https://www.reddit.com/r/MachineLearning/comments/1fz49wf/p_objective_bayesian_inference_for_comparing/,2024-10-08 16:40:09,7,1.0,7,0,1,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/zPLn1cAuj8HC7l6GlmPcXJvfeglDqRjlBiviZWWIRVI.jpg,t3_1fz49wf
MachineLearning,[R] Differential Transformer (Microsoft Research),"Abstract: Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",Decent_Action2959,1fz0pya,https://reddit.com/r/MachineLearning/comments/1fz0pya/r_differential_transformer_microsoft_research/,https://arxiv.org/abs/2410.05258,2024-10-08 14:10:25,198,0.99,198,0,41,0,0,False,False,False,False,False,Research,default,t3_1fz0pya
MachineLearning,[R] Addition is All You Need for Energy-Efficient Language Models,"**TL;DR:** approximate floating-point multiplication via addition; get excellent accuracy/hardware efficiency tradeoff.

**Paper:** [https://arxiv.org/pdf/2410.00907](https://arxiv.org/pdf/2410.00907)

**Abstract:**

&gt;Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8\_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8\_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8\_e4m3 as accumulation precision in both fine-tuning and inference.

**The Gist:**

https://preview.redd.it/s2pqz5f6ajtd1.png?width=1119&amp;format=png&amp;auto=webp&amp;s=00729823841c3f50fcc54a1e56657941972fd371

**The Gist, visually:**

https://preview.redd.it/jqt81xtmajtd1.png?width=1113&amp;format=png&amp;auto=webp&amp;s=499225094c7eed7570e29b460e2ae5c859f5ae25

**Highlights:**

https://preview.redd.it/zkenk9ijdjtd1.png?width=1111&amp;format=png&amp;auto=webp&amp;s=ac1260b4d5739fa7b32ae306cc7fd0d9de4b48ff

https://preview.redd.it/qca456akdjtd1.png?width=975&amp;format=png&amp;auto=webp&amp;s=9e04ec40cc6f90fe7430291ce12aabaa3067e168

https://preview.redd.it/qi5z575ldjtd1.png?width=1123&amp;format=png&amp;auto=webp&amp;s=451b4992195270f8f33345ed8d8fd5486627bc45

https://preview.redd.it/3cnq9u7mdjtd1.png?width=629&amp;format=png&amp;auto=webp&amp;s=d1b9a15b60a6b2791468e5de04ae78e51910b4f2

**~~Complaints~~** **Discussion:**

The section dedicated to all-layers L-mul LLM (as opposed to using L-mul within attention block) is quite sketchy, just 2 paragraphs and a tiny table. The only experiment is fine-tuning a Gemma-2-2b-it on GSM8k and evaluating on the same benchmark.

I mean, guys, if you have the audacity to name you paper ""Addition is all you need"", &gt;!virtually guaranteeing the scorn of the whole r/MachineLearning community for using such a deadbeat meme template,!&lt; you might as well show a substantiated proof that L-mul is indeed a comrehensive replacement for vanilla fp multiplications in transformers.

The issue is serious: the efficient implenentation of the method requires highly specialized accelerator hardware. If L-mul fails to replace vanilla fp multiplications in a general way, the need for specialized accelerators becomes way less clear. And without specialized hardware implementations you cannot capture the full benefit of the proposed simplification.",StartledWatermelon,1fz0jza,https://reddit.com/r/MachineLearning/comments/1fz0jza/r_addition_is_all_you_need_for_energyefficient/,https://www.reddit.com/r/MachineLearning/comments/1fz0jza/r_addition_is_all_you_need_for_energyefficient/,2024-10-08 14:03:15,45,0.87,45,0,4,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/z6lqNuEgT2vkYrNSA18RxQ5s4PYL5sMWYFh5k5_uqLM.jpg,t3_1fz0jza
MachineLearning,[R] Publicly available state-of-the-art methods for learning Markov network structure from continuous data?,"As the title suggests, I’m looking for recommendations on the best available models for learning the structure of Markov networks specifically for continuous data. I would appreciate it if you could share any links to the code or implementations as well.",solingermuc,1fz08mg,https://reddit.com/r/MachineLearning/comments/1fz08mg/r_publicly_available_stateoftheart_methods_for/,https://www.reddit.com/r/MachineLearning/comments/1fz08mg/r_publicly_available_stateoftheart_methods_for/,2024-10-08 13:48:54,1,0.67,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1fz08mg
MachineLearning,[Project] Figuring out whether Deep Learning would be an overkill for this NER problem (extracting key information from cost estimate documents),"I need to work on a named entity recognition project. I have a CSV file containing text from 270 documents with estimates of costs. My task is to extract the following information, not only from these 270, but also from future documents.

a) The person to whom the document is addressed  
b) The company of that person  
c) The document ID code  
d) The general service the document is about  
e) The product quantity  
f) The description of the product  
f) The product price

For the first four points, the documents generally follow a consistent structure, with clear patterns. For instance, the person the document is addressed to always appears after the same letters. I managed to extract them using regex, although I had to use lots of rules to handle variations (something that I don't like, as a little potential change in the future could make everything collapse).

The problem is that when we talk about the last three, there is variation. Sometimes there's no quantity, a very long textual description and the final price. Sometimes there's a clear structure: quantity, description, price. I am pretty sure that in a few days I could come up with some rules that would allow me to extract everything I need from those 270. But a slight change in a future document could easily compromise everything. On the other hand, a LLM would easily perform well at a task like this. What do you think? Would that be an overkill?

I was specifically thinking of doing manual annotation, using this dataset to fine-tune BERT on NER, and then go with that.",No_Possibility_7588,1fyzp1r,https://reddit.com/r/MachineLearning/comments/1fyzp1r/project_figuring_out_whether_deep_learning_would/,https://www.reddit.com/r/MachineLearning/comments/1fyzp1r/project_figuring_out_whether_deep_learning_would/,2024-10-08 13:23:28,4,0.7,4,0,11,0,0,False,False,True,False,False,Project,self,t3_1fyzp1r
MachineLearning,[R] SWE-bench Multimodal: Do AI Agents Generalize to Visual Software Domains?,"Hi!

We just put [SWE-bench Multimodal](https://www.swebench.com/multimodal) out!

It has 617 brand new task instances- in each of them, an agent is evaluated on its ability to fix a real bug from one of 17 JavaScript GitHub repositories. All of the bugs that we use in this dataset have an image in the issue text. And all of the repositories are user-facing libraries like mapping, plotting, and web UI libraries.

[Here are a few example bugs from SWE-bench Multimodal, with their associated images](https://preview.redd.it/4i2ybld59jtd1.png?width=2423&amp;format=png&amp;auto=webp&amp;s=75c6bb47998861c69a36121ae7d9bbdfa36a0e58)

This dataset was \*super\* challenging for existing open source agents, including Agentless, AutoCodeRover, and Moatless. We made a new version of SWE-agent that's able to correctly solve 12% of the issues. Our new Multimodal SWE-agent has the ability to take screenshots of the web page that it's editing, in order to iteratively fix visual bugs.

https://preview.redd.it/0n5kqqzg9jtd1.png?width=1256&amp;format=png&amp;auto=webp&amp;s=60a54b411436987d1036909ab5d6b0a4e7513a29

Paper: [https://arxiv.org/pdf/2410.03859](https://arxiv.org/pdf/2410.03859)

We're super excited about this paper and think that there's lots of room for further research in this direction.

I'll be around here if you have any questions, and you can also find me today at COLM.",ofirpress,1fyzo4x,https://reddit.com/r/MachineLearning/comments/1fyzo4x/r_swebench_multimodal_do_ai_agents_generalize_to/,https://www.reddit.com/r/MachineLearning/comments/1fyzo4x/r_swebench_multimodal_do_ai_agents_generalize_to/,2024-10-08 13:22:13,22,0.89,22,0,4,0,0,False,False,True,False,False,Research,self,t3_1fyzo4x
MachineLearning,[N] 2024 Nobel Prize for Physics goes to ML and DNN researchers J. Hopfield and G. Hinton,"Announcement: https://x.com/NobelPrize/status/1843589140455272810

Our boys John Hopfield and Geoffrey Hinton were rewarded for their foundational contributions to machine learning and deep learning with the Nobel prize for physics 2024!

I hear furious Schmidhuber noises in the distance!

On a more serious note, despite the very surprising choice, I am generally happy - as a physicist myself with strong interest in ML, I love this physics-ML cinematic universe crossover.

The restriction to Hopfield and Hinton will probably spark discussions about the relative importance of {Hopfield, Hinton, LeCun, Schmidhuber, Bengio, Linnainmaa, ...} for the success of modern ML/DL/AI.
A discussion especially Schmidhuber very actively engages in.

The response from the core physics community however is rather mixed, as shown in the [/r/physics thread](https://www.reddit.com/r/Physics/comments/1fyw12p/the_2024_nobel_prize_in_physics_is_awarded_to/). There, the missing link/connection to physics research is noted and the concurrent ""loss"" of the '24 prize for physics researchers.",PrittEnergizer,1fywi9h,https://reddit.com/r/MachineLearning/comments/1fywi9h/n_2024_nobel_prize_for_physics_goes_to_ml_and_dnn/,https://www.reddit.com/r/MachineLearning/comments/1fywi9h/n_2024_nobel_prize_for_physics_goes_to_ml_and_dnn/,2024-10-08 10:26:30,1152,0.97,1152,0,307,0,0,False,False,True,False,False,News,self,t3_1fywi9h
MachineLearning,[R] Mechanistic Behavior Editing of Language Models,"Large Language Models trained on web-scale text acquire language generation abilities that can solve a wide range of tasks, particularly when task knowledge is refined into the generative prior using in-context examples. However, spurious features learned from noisy data hinder their generalizability. Supervised finetuning can introduce task specificity, but introduce data inefficiency. Prior studies indicate that (i) noisy neural circuitries coexist with generalizable ones within LLMs, and (ii) finetuning typically enhances (or suppresses) existing abilities without introducing newer ones. Building upon these, we propose TaRot, a novel method for task adaptation. TaRot intervenes in the neural circuitries using learnable rotation matrices that are optimized using Bayesian Optimization, on labelled samples in the order of standard few-shot prompting examples. Experiments on multiple classification and generation tasks using LLMs of varying sizes reveal the efficacy of TaRot, improving upon both zero- as well as few-shot performance, with average improvements (across models and tasks) of 23.81% and 11.15%, respectively.

Paper: [https://arxiv.org/abs/2410.04277](https://arxiv.org/abs/2410.04277)

Code: [https://github.com/joykirat18/TaRot](https://github.com/joykirat18/TaRot)",Gaussian_Kernel,1fyuake,https://reddit.com/r/MachineLearning/comments/1fyuake/r_mechanistic_behavior_editing_of_language_models/,https://www.reddit.com/r/MachineLearning/comments/1fyuake/r_mechanistic_behavior_editing_of_language_models/,2024-10-08 07:36:57,13,0.93,13,0,0,0,0,False,False,True,False,False,Research,self,t3_1fyuake
MachineLearning,[D] finetuning base vs instruct model for agentic behavior,"one is better for agentic finetuning on say 10k-20k samples for a particular task, bases or instruct. i can imagine each having its pros and cons, but in general what is preferred/chose?

task is agentic behavior, function calling, based basically.",Raise_Fickle,1fytyq6,https://reddit.com/r/MachineLearning/comments/1fytyq6/d_finetuning_base_vs_instruct_model_for_agentic/,https://www.reddit.com/r/MachineLearning/comments/1fytyq6/d_finetuning_base_vs_instruct_model_for_agentic/,2024-10-08 07:11:39,1,0.57,1,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1fytyq6
MachineLearning,[P] GPT-2 Circuits - Mapping the Inner Workings of Simple LLMs,"I built an app that extracts interpretable ""circuits"" from models using the GPT-2 architecture. While some tutorials present hypothetical examples of how the layers within an LLM produce predictions, this app provides concrete examples of information flowing through the system. You can see, for example, the formation of features that search for simple grammatical patterns and trace their construction back to the use of more primitive features. Please take a look if you're working on interpretability! I'd love your feedback and hope to connect with folks who can help. Project link: [https://peterlai.github.io/gpt-mri/](https://peterlai.github.io/gpt-mri/)",ptarlye,1fymczh,https://reddit.com/r/MachineLearning/comments/1fymczh/p_gpt2_circuits_mapping_the_inner_workings_of/,https://www.reddit.com/r/MachineLearning/comments/1fymczh/p_gpt2_circuits_mapping_the_inner_workings_of/,2024-10-07 23:51:05,77,0.95,77,0,8,0,0,False,False,True,False,False,Project,self,t3_1fymczh
MachineLearning,Imbalanced Multi Label Image Classification Problem - Advice Needed! [P],"Hello! I hope this is the right sub-reddit!

I am currently working on a multi label image classification problem using ResNet152 as the backbone architecture for my CNN model, developed using Keras and TensorFlow. When training the model, I am getting a high binary accuracy score but low precision, recall and F1 scores, and the results are even poorer on the validation set: this points to the model overfitting on the training data. My dataset is quite imbalanced across the labels. I thought I could tackle this issue using a custom loss function, and in this case I have tried to use a weighted binary cross entropy loss function, feeding in the weights of each label. I have also explored over and under sampling the data, but this is proving to be a challenge as I cannot find anything that natively works with multi label datasets. I did try to modify SMOTE, but this did not do the trick. Below is an example of my data. What is the best way to deal with the poor model performance/data imbalance? Thank you!!!



|image\_path|label\_1|label\_2|label\_3|label\_4|label\_5|
|:-|:-|:-|:-|:-|:-|
|.../image1.jpg|1|0|0|1|0|
|.../image2.jpg|0|0|1|0|0|
|.../image3.jpg|0|0|1|1|0|
|.../image4.jpg|0|1|1|0|0|
|.../image5.jpg|0|0|1|0|0|

",Odd-Eagle-6716,1fylca7,https://reddit.com/r/MachineLearning/comments/1fylca7/imbalanced_multi_label_image_classification/,https://www.reddit.com/r/MachineLearning/comments/1fylca7/imbalanced_multi_label_image_classification/,2024-10-07 23:03:17,0,0.29,0,0,10,0,0,False,False,True,False,False,Project,self,t3_1fylca7
MachineLearning,[D] Learning ML coming from a competitive programming background ,"Hi guys, I hope you're all doing well

As a CS student who've been in the competitive programming path for a while, I'd like to ask how your competitive programming background helped you when you got into your first steps of learning ML and beyond to NN and other advanced topics?

Especially with the ones who reached ACM regional/ global finals or similar high rankings in IEEEXtreme, Meta Hackercup, etc",Omar0xPy,1fyjzu5,https://reddit.com/r/MachineLearning/comments/1fyjzu5/d_learning_ml_coming_from_a_competitive/,https://www.reddit.com/r/MachineLearning/comments/1fyjzu5/d_learning_ml_coming_from_a_competitive/,2024-10-07 22:02:16,0,0.15,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1fyjzu5
MachineLearning,[D][R] What is the equivalent of Cepstral Alanysis for Images?,"What is the analog of Cepstral Alanysis for Images?

I am attacking an image classification problem and trying to use an ""innovative approach"" with Reservoir Computing (RC), which is supposed to give a huge save in computing time and data size requirements, at least for dynamical signals and forecasting, but not necessarily for Images or classification. I've seen an example of voice-signals classification using RC, in which the signals are presented in the cepstral domain (new to me, as well), and this seems to facilitate the job for the network. I tried applying the definition of cepstrum to my images, but it doesn't look like it helps at all (I don't see any improvement in the features that might make the job easier for the machine, maybe I should give it a try). My question is: is there a known or standard method to improve/facilitate the extraction or recognition of features from images, as cepstrum does for voice? The images are galaxies observation from a cosmic evolution model. Trying to detect ""mergers"". ",HugoLDSC,1fyjqlz,https://reddit.com/r/MachineLearning/comments/1fyjqlz/dr_what_is_the_equivalent_of_cepstral_alanysis/,https://www.reddit.com/r/MachineLearning/comments/1fyjqlz/dr_what_is_the_equivalent_of_cepstral_alanysis/,2024-10-07 21:51:29,0,0.5,0,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1fyjqlz
MachineLearning,[Discussion] How does sample packing affect model learning in LLM training?,"Hi everyone,

I've been experimenting with sample packing during large language model (LLM) training to improve memory and compute efficiency by reducing padding. I applied it to a dataset with a max sequence length of 16, just to see how it works.

In most of the packed samples (I used the SFTTrainer and trl package from HuggingFace), I’m seeing sequences like this:

'&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nThe line of longitude that divides North and'

As you can see, this sample starts at the end of one response and immediately transitions into the start of a new one. I understand that the attention mask prevents cross-contamination between the packed samples, so the model shouldn't confuse different sequences.

But here’s my concern:

During training, we want to teach the model to predict the next token based on the full context of previous tokens. It feels like starting samples mid-context (especially right after special tokens like &lt;|im_start|&gt;) could degrade learning. Doesn’t this limit the model’s ability to fully learn how to handle full sequences, special tokens, or even the overall flow of a conversation?

Is there something I’m missing about how the model processes these transitions?
I thought that packing samples was mathematically the same as training with separated samples and padding, with the only difference being the number of tokens processed in each batch.

Thanks for your insights!",JeanMichelRanu,1fyjbnz,https://reddit.com/r/MachineLearning/comments/1fyjbnz/discussion_how_does_sample_packing_affect_model/,https://www.reddit.com/r/MachineLearning/comments/1fyjbnz/discussion_how_does_sample_packing_affect_model/,2024-10-07 21:34:14,6,0.87,6,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1fyjbnz
MachineLearning,[D] A suitable system for Nvidia A100 40G,"# 

I’ve been in the machine learning space since 2019, and my current 4090 setup is no longer sufficient for my growing needs. Here's what I focus on:

* Fine-tuning large language models (LLMs)
* Training Hidden Markov Models (HMMs)
* Training Long Short-Term Memory networks (LSTMs)

Handling diverse data types including text, images, videos, and audio

I don't have much knowledge in rack servers or workstations, I would be looking to add more cards in the near future too(H100 or A100), and I have a budget of around $3000-4000 for the rack server or workstation(apart from the cards obviously)

PS. Evaluated almost every card out there, A100 was the winner in the majority of aspects so it became a clear choice. The runner up was L40S tho. Open to suggestions on the card preference too.",Dapper_Ad79,1fyj5sw,https://reddit.com/r/MachineLearning/comments/1fyj5sw/d_a_suitable_system_for_nvidia_a100_40g/,https://www.reddit.com/r/MachineLearning/comments/1fyj5sw/d_a_suitable_system_for_nvidia_a100_40g/,2024-10-07 21:27:26,15,0.72,15,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1fyj5sw
MachineLearning,[D] Embeddings as data structures 2.0? Learning optimal task specific data representations (slides),"I gave a talk recently on embeddings as data structures 2.0 and thought this could be of interest here. Slides -&gt;  [https://docs.google.com/presentation/d/1GAiYOYTfzx-fyaHRNXYHCkA-y2wx1hnQwNiue0vj1tE/edit?usp=sharing](https://docs.google.com/presentation/d/1GAiYOYTfzx-fyaHRNXYHCkA-y2wx1hnQwNiue0vj1tE/edit?usp=sharing)

In 2017, [Andrej coined](https://karpathy.medium.com/software-2-0-a64152b37c35) the term “Software 2.0” - software that is learned from data instead of being manually crafted through programming rules. This paradigm shift has enabled far more capable software to be developed than was previously possible. 

There are a lot of parallels with embeddings as Data structures 2.0 and representing data using embeddings represents a similar shift.

""Data structures 2.0"" are learned representations of data - embeddings. Instead of manually crafting rules for storing data, you can learn optimal task specific ways of representing your data through embeddings.

*“Data structures 2.0 is written in human unfriendly language, such as the floating point values of an embedding. No human is involved in writing this code ... and coding directly in the floating point values is kind of tedious but possible (I tried).""*

Let me know what you think!",Jesse_marqo,1fybtdh,https://reddit.com/r/MachineLearning/comments/1fybtdh/d_embeddings_as_data_structures_20_learning/,https://www.reddit.com/r/MachineLearning/comments/1fybtdh/d_embeddings_as_data_structures_20_learning/,2024-10-07 16:25:18,14,0.77,14,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1fybtdh
MachineLearning,[P] Model2Vec: Distill a Small Fast Model from any Sentence Transformer,"Hey 👋!

I wanted to share a project we've been working on for the past couple of months called [Model2Vec](https://github.com/MinishLab/model2vec) that we recently open-sourced. It's a technique to distill Sentence Transformer models and create very small static embedding models (30mb on disk) that are up to 500x faster than the original model, making them very easy to use on CPU. Distillation takes about 30 seconds on a CPU.

These embeddings outperform similar methods such as GloVE and BPEmb by a large margin on MTEB while being much faster to create, and no dataset is needed. It's designed as an eco-friendly alternative to (Large) Language Models and particularly useful for situations where you are time-constrained (e.g. search engines), or don't have access to fancy hardware.

The idea is pretty straightforward, but works surprisingly well:

1: Take the token output embeddings of any Sentence Transformer.

2: Reduce the dimensionality using PCA. This reduces the model size, but also normalizes the output space.

3: Apply zipf weighting to the embeddings based on the word/token frequencies. This essentially downweights frequent words, meaning you don't need to remove stopwords for example.

We've created a couple of easy to use methods that can be used after installing the package with `pip install model2vec`:

**Inference:**

    from model2vec import StaticModel
    
    # Load a model from the HuggingFace hub (in this case the M2V_base_output model)
    model_name = ""minishlab/M2V_base_output""
    model = StaticModel.from_pretrained(model_name)
    
    # Make embeddings
    embeddings = model.encode([""It's dangerous to go alone!"", ""It's a secret to everybody.""])

**Distillation:**

    from model2vec.distill import distill
    
    # Choose a Sentence Transformer model
    model_name = ""BAAI/bge-base-en-v1.5""
    
    # Distill the model
    m2v_model = distill(model_name=model_name, pca_dims=256)
    
    # Save the model
    m2v_model.save_pretrained(""m2v_model"")

I'm curious to hear your thoughts on this, and happy to answer any questions!

Links:

* [Repo link](https://github.com/MinishLab/model2vec)
* [Results link](https://github.com/MinishLab/model2vec?tab=readme-ov-file#results)",Pringled101,1fyb9jj,https://reddit.com/r/MachineLearning/comments/1fyb9jj/p_model2vec_distill_a_small_fast_model_from_any/,https://www.reddit.com/r/MachineLearning/comments/1fyb9jj/p_model2vec_distill_a_small_fast_model_from_any/,2024-10-07 16:02:31,77,0.93,77,0,21,0,0,False,False,True,False,False,Project,self,t3_1fyb9jj
MachineLearning,[P] A Visual Guide to Mixture of Experts (MoE) in LLMs,"Hi all! I’m excited to introduce a highly illustrative guide to Mixture of Experts (MoE) in LLMs!

From exploring the role of experts, their routing mechanism, the sparse MoE layer, and load balancing tricks (such as KeepTopK, auxiliary loss, and expert capacity), to MoE in vision models and computational requirements. 

[https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

I loved creating the visuals and had to stop myself after creating more than 55 custom visuals!

The visual nature of this guide allows for a focus on intuition, hopefully making all these techniques easily accessible to a wide audience, whether you are new to Mixture of Experts or more experienced.",MaartenGr,1fya2ks,https://reddit.com/r/MachineLearning/comments/1fya2ks/p_a_visual_guide_to_mixture_of_experts_moe_in_llms/,https://www.reddit.com/r/MachineLearning/comments/1fya2ks/p_a_visual_guide_to_mixture_of_experts_moe_in_llms/,2024-10-07 15:13:45,71,0.96,71,0,7,0,0,False,False,True,False,False,Project,self,t3_1fya2ks
MachineLearning,[D]Did keras stop working on Google collab?,"Model.fit is refusing to do anything other than prind epoch 1/150, Two computers, different types of models and different accounts, no errors and interrupt doesn't work, tried everything i could think of during the last two days, anyone has an idea what's going on?",ihaveagoodusername2,1fy9c9s,https://reddit.com/r/MachineLearning/comments/1fy9c9s/ddid_keras_stop_working_on_google_collab/,https://www.reddit.com/r/MachineLearning/comments/1fy9c9s/ddid_keras_stop_working_on_google_collab/,2024-10-07 14:43:36,0,0.29,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1fy9c9s
MachineLearning,[D] Can we directly process ML models on google drive files without downloading?,I am making a project for my boss and am stuck at this problem. I wish there to be someway so model can process without downloading all stuff atleast...Is it possible someway...?,wannasleepforlong,1fy6e5y,https://reddit.com/r/MachineLearning/comments/1fy6e5y/d_can_we_directly_process_ml_models_on_google/,https://www.reddit.com/r/MachineLearning/comments/1fy6e5y/d_can_we_directly_process_ml_models_on_google/,2024-10-07 12:28:18,0,0.1,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1fy6e5y
MachineLearning,[D] Flexible compute  deployment based on task complexity,"Hello ML people. I’m a cognitive science student working at the intersection of neuroscience and machine learning. Probably one of the coolest things about the brain is just how freaking efficient it is for what it can accomplish— running on lightbulb. To my understanding, this likely comes from not using all parameters if it is unnecessary for the task. So far, the only ML approach that I have found akin to this has been Mixture of Experts. 
Aside from that, in Deep Learning, optimising the inference process seems to be somewhat neglected — usually just using all parameters regardless of top down context or input statistics. 

I’m almost sure I am wrong, so I was hoping you guys could point me to good papers delving into this? Or perhaps the formal name of the problem? As an example, you might take an LLM. If the prediction of the next word in a sentence is simple (e.g. herbivores eat [plants]) I might not need all parameters to get a perfectly good prediction (Claude and LLama would likely do equally fine, but Claude cost more to solve this one), as opposed to one that is more technical and requires more attention and context into the processing (e.g., solving a mathematical proof).

",Karioth1,1fy54kn,https://reddit.com/r/MachineLearning/comments/1fy54kn/d_flexible_compute_deployment_based_on_task/,https://www.reddit.com/r/MachineLearning/comments/1fy54kn/d_flexible_compute_deployment_based_on_task/,2024-10-07 11:16:17,0,0.5,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1fy54kn
MachineLearning,[R][P] AI Agents LlamaIndex,"AI Agents LlamaIndex Crash Course

It covers:

- Function Calling  
- Function Calling Agents + Agent Runner  
- Agentic RAG  
- REAcT Agent: Build your own Search Assistant Agent

[https://youtu.be/bHn4dLJYIqE](https://youtu.be/bHn4dLJYIqE)",External_Ad_11,1fy2y5d,https://reddit.com/r/MachineLearning/comments/1fy2y5d/rp_ai_agents_llamaindex/,https://www.reddit.com/r/MachineLearning/comments/1fy2y5d/rp_ai_agents_llamaindex/,2024-10-07 08:40:13,0,0.31,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1fy2y5d
MachineLearning, [D] Looking for Advice: LLMs for Handwriting OCR vs Google Vision? ,"Hello all!

I’m working on a project where I need to extract text from images of handwritten. So far, I’ve been using Google Vision API, which has worked well for some text including handwriting, but I’m wondering if there’s a more direct solution for handling handwriting specifically.

Would it make sense to use an LLM that can directly process and read handwriting, or is sticking with traditional OCR methods (like Google Vision) still the way to go? I’m aware LLMs like GPT-4o/Gemini have these capabilities, but I’m not sure how well they would handle image-based input or handwriting.

Has anyone experimented with LLMs for OCR? What would you recommend, and are there specific models that excel at this task?

The idea is to also use an LLM to summarise the handwritten text, so at some point in the pipeline I will require an LLM anyway.

Thanks.",Practical_Estate4971,1fy1iqx,https://reddit.com/r/MachineLearning/comments/1fy1iqx/d_looking_for_advice_llms_for_handwriting_ocr_vs/,https://www.reddit.com/r/MachineLearning/comments/1fy1iqx/d_looking_for_advice_llms_for_handwriting_ocr_vs/,2024-10-07 06:49:54,0,0.46,0,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1fy1iqx
MachineLearning,[R] Is Mamba and SSMs on Language Modelling Task a Great Research Trajectory?,"I just came by Mamba and SSMs as my Professor said that I should try to explore it. I am a master's student for context and I just started my research journey, I originally wanted to do research on transformers LM like the rest of the students in my department do. Someone said that this traps me into doing something that someone hasn't done before and will make my study/research harder than it is supposed to be (and maybe end up yielding mediocre results). Do you guys have any opinion regarding this? Thank you.

",worthlesspineapple,1fxzor7,https://reddit.com/r/MachineLearning/comments/1fxzor7/r_is_mamba_and_ssms_on_language_modelling_task_a/,https://www.reddit.com/r/MachineLearning/comments/1fxzor7/r_is_mamba_and_ssms_on_language_modelling_task_a/,2024-10-07 04:41:08,18,0.73,18,0,25,0,0,False,False,True,False,False,Research,self,t3_1fxzor7
MachineLearning,"[P] working on a customer churn prediction project, what is the churn window the model outputs","If i’m using a dataset that have all active customers and all churned customers for example the last 15 years, how do I decide that I want my model to predict for the next 90 days? I’m confident there’s some sort of “time framing” I should do in my data before training but I’m not sure how to approach such problem",[deleted],1fxyzkz,https://reddit.com/r/MachineLearning/comments/1fxyzkz/p_working_on_a_customer_churn_prediction_project/,https://www.reddit.com/r/MachineLearning/comments/1fxyzkz/p_working_on_a_customer_churn_prediction_project/,2024-10-07 03:57:40,0,0.33,0,0,4,0,0,False,False,True,False,False,Project,self,t3_1fxyzkz
MachineLearning,[D] What are some interesting papers about tool-use and LLM agents? ,"Currently, I’m looking into voyager (https://arxiv.org/abs/2305.16291) but would love some more suggestions. TIA. ",a1_jakesauce_,1fxvsp7,https://reddit.com/r/MachineLearning/comments/1fxvsp7/d_what_are_some_interesting_papers_about_tooluse/,https://www.reddit.com/r/MachineLearning/comments/1fxvsp7/d_what_are_some_interesting_papers_about_tooluse/,2024-10-07 01:04:03,6,0.75,6,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1fxvsp7
MachineLearning,"[D] Sensitivity Analysis of the ML Paper Got Better Results, What Now?","I wrote an ML paper using a novel approach on a specific dataset, which yielded some positive results. I trained several models, evaluated them, and conducted extensive interpretation and discussion based on the findings. One of the reviewers requested a sensitivity analysis on a few preprocessing parameters/algorithms. Interestingly, one of the changes resulted in slightly better outcomes than my original approach.

My question is: what are the expectations in this case? Do I need to rewrite the entire paper, or should I simply report this observation in the sensitivity analysis? While it’s nice that the changes improved the results, it’s pretty frustrating to think about rewriting much of the interpretation (e.g., feature importance, graphs, discussion, etc.) based on the new run. What are your thoughts and experiences?",anagreement,1fxu3zw,https://reddit.com/r/MachineLearning/comments/1fxu3zw/d_sensitivity_analysis_of_the_ml_paper_got_better/,https://www.reddit.com/r/MachineLearning/comments/1fxu3zw/d_sensitivity_analysis_of_the_ml_paper_got_better/,2024-10-06 23:37:02,47,0.92,47,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1fxu3zw
MachineLearning,"[P] Ever wanted to fine tune Xtts on your m1 16gb ram Mac? Well idk made a repo for it idk,","https://github.com/DrewThomasson/finetuneXtts_apple_silicone

You need 16 gb ram to run tho and the docker version requires even more ram to run :/

Final_output_files from the compress model button are compatable with https://github.com/DrewThomasson/ebook2audiobookXTTS",Impossible_Belt_7757,1fxt77o,https://reddit.com/r/MachineLearning/comments/1fxt77o/p_ever_wanted_to_fine_tune_xtts_on_your_m1_16gb/,https://github.com/DrewThomasson/finetuneXtts_apple_silicone,2024-10-06 22:51:36,1,0.54,1,0,2,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/EAEfnCVqyJb6Sfn5k-rkW9q9mOEMgZljlL6JUpb-a04.jpg,t3_1fxt77o
MachineLearning,Context aware word replacement [P] [R],"Hello!

I'm into CV research so not very proficient in NLP, so reaching out for inputs.

I'm working on replacing 'word' in a 'sentence' keeping context in picture so that it would be easier for us to search suitable image for that word in our dataset. For example:

sentence - 'Students should counter cyber bullying so that attackers don't harm them'

word - 'attackers'

Why it is expected - cyber criminal, online bully, etc so that I can then search for relevant images.

What BeRT and other models replace it with - terrorists, computers, hostile attackers, etc.

I want to run something locally and can't figure out ajy solution. Any idea or inputs I should try? Any resources or code notebooks?",ade17_in,1fxsuhg,https://reddit.com/r/MachineLearning/comments/1fxsuhg/context_aware_word_replacement_p_r/,https://www.reddit.com/r/MachineLearning/comments/1fxsuhg/context_aware_word_replacement_p_r/,2024-10-06 22:34:34,10,0.78,10,0,7,0,0,False,False,True,False,False,Research,self,t3_1fxsuhg
