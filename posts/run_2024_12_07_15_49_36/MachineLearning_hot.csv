subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",AutoModerator,1h46e6j,https://reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/,2024-12-01 16:00:30,8,1.0,8,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h46e6j
MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

&gt;Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

&gt;Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&amp;#x200B;

Please remember that this community is geared towards those with experience.",AutoModerator,1h3u444,https://reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,2024-12-01 03:30:15,27,0.85,27,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h3u444
MachineLearning,"[N] Sama, an AI sweatshop, pays workers in Kenya $2 an hour to filter and label porn, beastiality, suicide, child abuse, for hours on end!!",,BotherBubbly5096,1h8nhbh,https://reddit.com/r/MachineLearning/comments/1h8nhbh/n_sama_an_ai_sweatshop_pays_workers_in_kenya_2_an/,https://youtu.be/qZS50KXjAX0,2024-12-07 07:38:08,94,0.77,94,0,52,0,0,False,False,False,False,False,News,https://b.thumbs.redditmedia.com/6xyP-Tq6WRzMpjniGg5h_3Z_bhf29lyA0LV_LQ19-1s.jpg,t3_1h8nhbh
MachineLearning,[R] For a change of topic: some nonLLM focused work of mine: Bias-Free Sentiment Analysis through Semantic Blinding and Graph Neural Networks,"In my academic field (social sciences) I deal with the problem of bias in SA models. My previous work showed that deep learning SA systems inherit bias (e.g. nonrepresentative of the population political bias) from annotators: 

https://arxiv.org/abs/2407.13891

Now I devised a solution that used a technique I call semantic blinding to provide only the bare necessary information for the model to predict emotions in text, leaving no signal for the model to overfit and produce bias from:

https://arxiv.org/abs/2411.12493

Interested to hear your thoughts before I publish the SProp Gnn.

Do you think it could be useful beyond the academia?



",Hub_Pli,1h8meas,https://reddit.com/r/MachineLearning/comments/1h8meas/r_for_a_change_of_topic_some_nonllm_focused_work/,https://i.redd.it/vh80i11ndd5e1.jpeg,2024-12-07 06:21:47,18,0.83,18,0,7,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/sSwXTBHlRlELGHTRvPNAxo6LSQVRdxL5HTNiMSYTfTs.jpg,t3_1h8meas
MachineLearning,[R] JAX vs TensorFlow-XLA ,"
Few months ago, I migrated from TF 2.0 to Jax. I found that jax is significantly faster than Tf. I noticed in the official documentation that it relies on XLA default that uses JIT compilation which makes execution faster. I also noticed that TF graphs also have option to enable JIT compilation with XLA. But still jax dominates TF with XLA. I just want to know why.",Odd-Detective289,1h8j2e5,https://reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,https://www.reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,2024-12-07 03:02:19,11,0.92,11,0,6,0,0,False,False,True,False,False,Research,self,t3_1h8j2e5
MachineLearning,[D] Any OCR recommendations for illegible handwriting?,"
Has anyone had experience using an ML model to recognize handwriting like this? The notebook contains important information that could help me decode a puzzle I’m solving. I have a total of five notebooks, all from the same person, with consistent handwriting patterns. My goal is to use ML to recognize and extract the notes, then convert them into a digital format.

I was considering Google API after knowing that Tesseract might not work well with illegible samples like this. However, I’m not sure if Google API will be able to read it either. I read somewhere that OCR+ CNN might work, so I’m here asking for suggestions. Thanks! Any advice/suggestions are welcomed! ",SpaceSheep23,1h7x5us,https://reddit.com/r/MachineLearning/comments/1h7x5us/d_any_ocr_recommendations_for_illegible/,https://www.reddit.com/gallery/1h7x5us,2024-12-06 08:53:03,161,0.89,161,0,169,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/LeDstkPiGT-yO7zkyvRRyx6sE33qB2W32UGe1t81Kkk.jpg,t3_1h7x5us
MachineLearning,How to solve the STT Cutoff Problem [D],"Hello folks, 

I've been working on an agentic solution where you can have an autonomous agent taking live calls. We're using a pipeline of Speech to Text, LLM for generating responses and then Text to Speech. In this pipeline, Speech to text is causing some issues because it's difficult to determine when exactly a sentence is over since the user can take pauses. Moreover, when multiple inputs go into LLM, multiple responses are generated and they queue up for Text to speech. How would you solve this problem? How would you also handle cases where the user interrupts the agent?",Leo2000Immortal,1h8r32q,https://reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,https://www.reddit.com/r/MachineLearning/comments/1h8r32q/how_to_solve_the_stt_cutoff_problem_d/,2024-12-07 12:04:55,0,0.5,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h8r32q
MachineLearning,[D] AAAI 2025 Phase 2 Decision,"When would the phase 2 decision come out?  
I know the date is December 9th, but would there be chances for the result to come out earlier than the announced date?  
or did it open the result at exact time in previous years? (i.e., 2024, 2023, 2022 ....)

Kinda make me sick to keep waiting.",No-Style-7975,1h8kkjv,https://reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,https://www.reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,2024-12-07 04:27:30,4,1.0,4,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h8kkjv
MachineLearning,[D]Stuck in AI Hell: What to do in post LLM world,"
Hey Reddit,

I’ve been in an AI/ML role for a few years now, and I’m starting to feel disconnected from the work. When I started, deep learning models were getting good, and I quickly fell in love with designing architectures, training models, and fine-tuning them for specific use cases. Seeing a loss curve finally converge, experimenting with layers, and debugging training runs—it all felt like a craft, a blend of science and creativity. I enjoyed implementing research papers to see how things worked under the hood. Backprop, gradients, optimization—it was a mental workout I loved.

But these days, it feels like everything has shifted. LLMs dominate the scene, and instead of building and training models, the focus is on using pre-trained APIs, crafting prompt chains, and setting up integrations. Sure, there’s engineering involved, but it feels less like creating and more like assembling. I miss the hands-on nature of experimenting with architectures and solving math-heavy problems.

It’s not just the creativity I miss. The economics of this new era also feel strange to me. Back when I started, compute was a luxury. We had limited GPUs, and a lot of the work was about being resourceful—quantizing models, distilling them, removing layers, and squeezing every bit of performance out of constrained setups. Now, it feels like no one cares about cost. We’re paying by tokens. Tokens! Who would’ve thought we’d get to a point where we’re not designing efficient models but feeding pre-trained giants like they’re vending machines?

I get it—abstraction has always been part of the field. TensorFlow and PyTorch abstracted tensor operations, Python abstracts C. But deep learning still left room for creation. We weren’t just abstracting away math; we were solving it. We could experiment, fail, and tweak. Working with LLMs doesn’t feel the same. It’s like fitting pieces into a pre-defined puzzle instead of building the puzzle itself.

I understand that LLMs are here to stay. They’re incredible tools, and I respect their potential to revolutionize industries. Building real-world products with them is still challenging, requiring a deep understanding of engineering, prompt design, and integrating them effectively into workflows. By no means is it an “easy” task. But the work doesn’t give me the same thrill. It’s not about solving math or optimization problems—it’s about gluing together APIs, tweaking outputs, and wrestling with opaque systems. It’s like we’ve traded craftsmanship for convenience.

Which brings me to my questions:

1. Is there still room for those of us who enjoy the deep work of model design and training? Or is this the inevitable evolution of the field, where everything converges on pre-trained systems?


2. What use cases still need traditional ML expertise? Are there industries or problems that will always require specialized models instead of general-purpose LLMs?


3. Am I missing the bigger picture here? LLMs feel like the “kernel” of a new computing paradigm, and we don’t fully understand their second- and third-order effects. Could this shift lead to new, exciting opportunities I’m just not seeing yet?


4. How do you stay inspired when the focus shifts? I still love AI, but I miss the feeling of building something from scratch. Is this just a matter of adapting my mindset, or should I seek out niches where traditional ML still thrives?



I’m not asking this to rant (though clearly, I needed to get some of this off my chest). I want to figure out where to go next from here. If you’ve been in AI/ML long enough to see major shifts—like the move from feature engineering to deep learning—how did you navigate them? What advice would you give someone in my position?

And yeah, before anyone roasts me for using an LLM to structure this post (guilty!), I just wanted to get my thoughts out in a coherent way. Guess that’s a sign of where we’re headed, huh?

Thanks for reading, and I’d love to hear your thoughts!

TL;DR: I entered AI during the deep learning boom, fell in love with designing and training models, and thrived on creativity, math, and optimization. Now it feels like the field is all about tweaking prompts and orchestrating APIs for pre-trained LLMs. I miss the thrill of crafting something unique. Is there still room for people who enjoy traditional ML, or is this just the inevitable evolution of the field? How do you stay inspired amidst such shifts?

Update: Wow, this blew up. Thanks everyone for your comments and suggestions. I really like some of those. This thing was on my mind for a long time, glad that I put it here. Thanks again!",Educational_News_371,1h7jg87,https://reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,2024-12-05 20:49:57,648,0.97,648,0,194,0,0,False,False,True,False,False,Discussion,self,t3_1h7jg87
MachineLearning,[R] Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis,"New paper and code for the scale-wise transformer for fast text-to-image generation from our team at Yandex Research

Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being faster than distilled diffusion models.

Code with checkpoints: [https://github.com/yandex-research/switti](https://github.com/yandex-research/switti)

[Generation examples](https://preview.redd.it/7jy3jfxhi95e1.png?width=3094&amp;format=png&amp;auto=webp&amp;s=e80ed27c0b746ec782026581500582a5dd03555d)

",_puhsu,1h85z2c,https://reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,https://www.reddit.com/r/MachineLearning/comments/1h85z2c/r_switti_designing_scalewise_transformers_for/,2024-12-06 16:58:21,11,0.8,11,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/Io25PLG5YopCFj4DOTGJ-MxIEcQdXRQbpwevn2M-Kts.jpg,t3_1h85z2c
MachineLearning,[R] Agentic Retrieval Augmented Generation with Memory,"Imagine a customer support chatbot for an e-commerce platform that retrieves relevant product details from its knowledge base and performs web searches for additional information. Furthermore, it remembers past conversations to deliver a seamless and personalized experience for returning users.   
  
Here is how it works:  
  
\- Store your own data in the knowledge base—in our case, a Website URL.  
\- Convert the data into embeddings and save it in the Qdrant Vector Database.  
\- Use phidata Agentic Workflow to combine Tools, LLM, Memory, and the Knowledge Base.

Code Implementation Video: [https://www.youtube.com/watch?v=CDC3GOuJyZ0](https://www.youtube.com/watch?v=CDC3GOuJyZ0)",External_Ad_11,1h8945d,https://reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,https://www.reddit.com/r/MachineLearning/comments/1h8945d/r_agentic_retrieval_augmented_generation_with/,2024-12-06 19:10:50,6,0.72,6,0,1,0,0,False,False,True,False,False,Research,self,t3_1h8945d
MachineLearning,[D] How does OpenAI’s O1 outperform others in math despite limitations noted in recent papers?,"Recent research has revealed that state-of-the-art LLMs often struggle with mathematical reasoning:

1. The GSM-Symbolic benchmark highlights that LLMs frequently fail when numerical values or question wording change, suggesting reliance on memorization rather than true mathematical understanding ([source](https://arxiv.org/pdf/2410.05229.pdf)).
2. Logical reasoning studies, like the AIW problem, show inconsistent performance even for basic reasoning tasks ([source](https://arxiv.org/pdf/2406.02061.pdf)).
3. Furthermore, research indicates LLMs lack effective self-correction capabilities, with performance degrading after multiple iterations ([source](https://arxiv.org/pdf/2310.01798.pdf)).

Despite these challenges, OpenAI’s new O1 model reportedly exceeds all other models in math benchmarks. How does it address these known issues in mathematical reasoning, such as:

* Reliance on memorization instead of understanding?
* Inconsistencies in reasoning across problem variations?
* Inability to self-correct errors effectively?

Would love to hear insights or hypotheses!",AImSamy,1h7vj5t,https://reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,2024-12-06 06:53:09,47,0.86,47,0,38,0,0,False,False,True,False,False,Discussion,self,t3_1h7vj5t
MachineLearning,[D] Have we officially figured out yet how O1 models differ from previous models?,"Edit: I have misworded the title as if OpenAI would confirm how O1 was implemented. I have changed the text to reflect what I meant say.



I really want to deep dive into the technicals of how the O1 models perform better than previous models.

Have researchers come to any definitive agreement as to what OpenAI could have possible done to achieve O1?

From reading online I hear about MCTS, COT... etc, but are any of these methods in large agreement by researhers?
",Daveboi7,1h7zfjg,https://reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,2024-12-06 11:37:51,12,0.81,12,0,27,0,0,False,False,True,False,False,Discussion,self,t3_1h7zfjg
MachineLearning,[D] selective transfer learning ,"Hello everyone,

I am looking for methods that can automatically categorize and select layers from  for transfer learning. If you know any such methods or research please let me know or share. 

Thanks ",reshail_raza,1h8cawc,https://reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,https://www.reddit.com/r/MachineLearning/comments/1h8cawc/d_selective_transfer_learning/,2024-12-06 21:30:44,2,1.0,2,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h8cawc
MachineLearning,[P] I built a website to compare every AI model as a 17 y/o in high school: Countless.dev (live on Product Hunt!),"I built a website to compare EVERY AI Model, from LLMs to image models etc.

Basically, it’s a free tool that lets you compare different AI models—LLMs, vision models, and so on—in one spot.

Why did I make it? Because sorting through all the token limits, pricing, and features across multiple docs and websites can drive you nuts. I was tired of jumping between a million tabs just to figure out which model fit my needs.

With [Countless.dev](http://Countless.dev), you can:

* Compare models based on price, token limits, and special features (like vision support).
* Use a built-in cost calculator to estimate what you’d spend with different models.
* Do side-by-side comparisons to narrow down your choices fast.

I originally built this while testing out AI code editors. After I shared an early version on Twitter and got some positive feedback, I decided to open it up for everyone. It’s free, open-source, and (I hope) pretty straightforward to use.

If this sounds useful, feel free to give it a try. I’m all ears for feedback—good, bad, or otherwise. If something’s off, I’d love to hear it so I can make it better. Cheers!

Live on Product Hunt rn (if you like the website please help me get #1!): [https://www.producthunt.com/posts/countless-dev](https://www.producthunt.com/posts/countless-dev)",ahmett9,1h8pnbh,https://reddit.com/r/MachineLearning/comments/1h8pnbh/p_i_built_a_website_to_compare_every_ai_model_as/,https://www.reddit.com/r/MachineLearning/comments/1h8pnbh/p_i_built_a_website_to_compare_every_ai_model_as/,2024-12-07 10:21:15,0,0.41,0,0,7,0,0,False,False,True,False,False,Project,self,t3_1h8pnbh
MachineLearning,[D] Encode over 100 million rows into embeddings ,"Hey everyone,

I'm working on a pipeline to encode over **100 million rows** into embeddings using **SentenceTransformers**, **PySpark**, and **Pandas UDF** on **Dataproc Serverless**.

Currently, it takes several hours to process everything. I only have one column containing sentences, each under 30 characters long. These are encoded into **64-dimensional vectors** using a custom model in a Docker image.

At the moment, the job has been running for over **12 hours** with **57 executors** (each with **24GB of memory and 4 cores**). I’ve partitioned the data into **2000 partitions**, hoping to speed up the process, but it's still slow.

Here’s the core part of my code:

    F.pandas_udf(returnType=ArrayType(FloatType()))
    def encode_pd(x: pd.Series) -&gt; pd.Series:
        try:
            model = load_model()
            return pd.Series(model.encode(x, batch_size=512).tolist())
        except Exception as e:
            logger.error(f""Error in encode_pd function: {str(e)}"")
            raise

The `load_model` function is as follows:

    def load_model() -&gt; SentenceTransformer:
        model = SentenceTransformer(
            ""custom_model"", 
            device=""cpu"", 
            cache_folder=os.environ['SENTENCE_TRANSFORMERS_HOME'], 
            truncate_dim=64
        )
        return model

I tried broadcasting the model, but I couldn't refer to it inside the Pandas UDF.

Does anyone have suggestions to optimize this? Perhaps ways to load the model more efficiently, reduce execution time, or better utilize resources?",nidalap24,1h7xnce,https://reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,2024-12-06 09:29:37,12,0.81,12,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h7xnce
MachineLearning,[D] How to actually prevent overfitting in practice in ScikitLearn ?,"We all saw in class the trade off between bias and variance, that we don't want our train loss to keep going down and our test loss go up. 

But in practice I feel like doing hyperparameter tuning for classic ML models with GridSearchCV / BayesSearchCV is not enough. Even though I do cross validation, the search.best\_model obtained at the end is almost always overfitting. 

How can you actually perform a search that will give you a robust generalized model with higher chances ? ",desslyie,1h8paqz,https://reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,https://www.reddit.com/r/MachineLearning/comments/1h8paqz/d_how_to_actually_prevent_overfitting_in_practice/,2024-12-07 09:55:06,0,0.17,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h8paqz
MachineLearning,[R] Zero shot Meme-interpretability of LLMs,"Head to head of meme-interpretability with the same image and text prompt!

Anecdotal but interesting responses. 

Also clear winner!

",No_Cartoonist8629,1h8nc78,https://reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,https://www.reddit.com/r/MachineLearning/comments/1h8nc78/r_zero_shot_memeinterpretability_of_llms/,2024-12-07 07:27:43,0,0.31,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1h8nc78
MachineLearning,[R] Towards Time Series Reasoning with LLMs,,HydrousIt,1h7sr3n,https://reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/,https://arxiv.org/abs/2409.11376,2024-12-06 04:06:32,8,0.8,8,0,2,0,0,False,False,False,False,False,Research,default,t3_1h7sr3n
MachineLearning,[D] Exploring a New Approach for Decision Trees in Feature Space Using Linear Projections and Boosting,"Hello everyone,

I've been working on a project for some time now and wanted to share a concept I'm exploring. As we know, decision tree-based models typically split the feature space using certain metrics like MSE, entropy, etc.

I started thinking about an alternative approach: instead of splitting individual features, what if we could split the entire space directly? However, this seemed quite difficult, as determining boundaries and regions in the space is challenging.

Then I had an idea—what if I project the data onto a line within the feature space, and then split that line, like how trees are typically built on individual features? In essence, I’m thinking of projecting points onto a line and then using tree-based methods to split them progressively.

Here's a high-level view of the algorithm:

1. Fit a linear regression model to the dataset (normalized values).
2. Project the data onto the line defined by the regression.
3. Apply a decision tree on this projection, effectively splitting one feature (the projection axis).
4. Calculate the residuals and fit another linear model on the residuals, applying boosting in the process.

Since the new linear regressions fitted on the residuals will define separate lines, I assume that through boosting, the model will gradually divide the data in the desired manner over time.

You can read a more detailed description of the algorithm here: [Algorithm PDF](https://github.com/sametcopur/spacetree/blob/main/algo/algo.pdf).

To visualize how the decision boundaries are formed in a 2D dataset:

[SpaceBoostingRegressor](https://i.redd.it/1i10qp6dr85e1.gif)



**Note:** If you want to see a visual example, uploading high-dimensional GIFs can sometimes be an issue. You can check out the example here: [Gif on GitHub](https://github.com/sametcopur/spacetree/blob/main/utils/tree_animation.gif).

Also you can check the code in the repository: [Repository](https://github.com/sametcopur/spacetree/tree/main)

This approach is simple because it assumes linearity, and it works in scenarios where there is a high linear correlation between the target and features while also allowing for some non-linear relationships. You can see an example in the repo,`example.ipynb` file. However, I’m not sure how well it would perform on real-world datasets, as the linear assumption may not always hold.

I want to take this algorithm further, but speed is important for scaling. Techniques like PCA don't seem to help because I need the line to reflect the variance in both the target and feature space, rather than just feature variance. I tried using MLPs and extracting the embeddings from a hidden layer before the output layer, which works better since we're evaluating the target in a larger space, but this approach becomes too slow and isn’t feasible in practice.

I think this project has great potential, and I’m looking for feedback, ideas, or anyone interested in collaborating. Any comments or suggestions are welcome!",zedeleyici3401,1h838y5,https://reddit.com/r/MachineLearning/comments/1h838y5/d_exploring_a_new_approach_for_decision_trees_in/,https://www.reddit.com/r/MachineLearning/comments/1h838y5/d_exploring_a_new_approach_for_decision_trees_in/,2024-12-06 15:00:41,3,0.67,3,0,3,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/e0DAeFrPHci5HsV6mlnNzXtgQVmldE3CJmgS7Ro-3b0.jpg,t3_1h838y5
MachineLearning,[R] Mastering Board Games by External and Internal Planning with Language Models - DeepMind,"Paper: https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf

Abstract:

While large language models perform well on a range of complex tasks (e.g., text generation, question
answering, summarization), robust multi-step planning and reasoning remains a considerable challenge
for them. In this paper we show that search-based planning can significantly improve LLMs’ playing
strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We
introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo
Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search,
the model directly generates in-context a linearized tree of potential futures and a resulting final choice.
Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and
value functions across these games. We find that our pre-training method minimizes hallucinations, as
our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and
external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level
performance in chess while operating on a similar move count search budget per decision as human
Grandmasters. The way we combine search with domain knowledge is not specific to board games,
suggesting direct extensions into more general language model inference and training techniques.",RobbinDeBank,1h7nshy,https://reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,2024-12-05 23:58:37,7,0.83,7,0,1,0,0,False,False,True,False,False,Research,self,t3_1h7nshy
MachineLearning,[D] Multimodal AI,"Multimodal AI is changing the game by combining text, images, and even video into a single, cohesive system. It’s being talked about as a major leap in AI capabilities.

What industries do you think will benefit the most from this tech? And are there any challenges you see in integrating these models into everyday use?

Would love to hear everyone's thoughts!",Frosty_Programmer672,1h8enzy,https://reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,https://www.reddit.com/r/MachineLearning/comments/1h8enzy/d_multimodal_ai/,2024-12-06 23:17:54,0,0.08,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h8enzy
MachineLearning,U-Net Vs Attention U-Net [D],"Hello folks,

Young researcher here, working on a in-house dataset to build a foundational model for a interesting use-case. But I have thesis to finsh, which will be just the tail of my current research.

For my thesis, we have decided to have a subsection for comparing how my segmentation results differ when used attention blocks are used within a U-Net. I've referred few papers on how this works and how can this be implemented.

Results are promising (att unet outperformimg unets, nothing suprising) but I see a concerning opposing point i.e. attention Unet having more number of parameters that the unet. Is there a way I can conduct this study where I compare results with and without attention? And there are no other additional factors influencing the results (layers, params, etc).

Does conducting ablation study makes sense in this case? I've not seen any other paper comparing similar use-case using this study.

Any papers I can look through, suggestions and tips are welcome.",ade17_in,1h7cjnd,https://reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,2024-12-05 16:03:47,25,0.92,25,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7cjnd
MachineLearning,[D] How to remove noise in this dataset,"I have a dataset that, when plotted, shows a noisy black line. I'd like to smooth out this noise to get a cleaner trend line (similar to the red line shown). What methods would you recommend for noise reduction?

https://preview.redd.it/p8q0i4f9h45e1.png?width=1574&amp;format=png&amp;auto=webp&amp;s=29cd8c82af7b54a1d3da22655502fd5cf406e807",mrtule,1h7ohd0,https://reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,2024-12-06 00:30:30,5,0.86,5,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1h7ohd0
MachineLearning,[R] ReVersion: Learning Relation Prompts from Images for Controlled Diffusion Generation,"ReVersion introduces a novel approach for learning and transferring visual relationships using diffusion models. Rather than focusing solely on object appearance, it learns how objects interact with each other through relation prompts and specialized sampling techniques.

Key technical aspects:
- Uses frozen pre-trained text-to-image diffusion model as foundation
- Implements relation-steering through contrastive learning to guide prompts toward relationship-rich latent spaces
- Employs relation-focal sampling to emphasize high-level interactions over low-level details
- Creates relation prompts that capture spatial and interactive relationships between objects
- Introduces new benchmark dataset for evaluating relation inversion methods

Results:
- Outperforms existing methods in preserving object relationships while allowing appearance flexibility
- Shows strong performance on spatial relationships like ""on top of"", ""next to"", ""inside""
- Successfully transfers learned relationships to novel object pairs
- Maintains relationship consistency across different styles and contexts

I think this approach could be particularly valuable for improving automated image generation systems that need to handle complex scenes with multiple interacting objects. The ability to learn and transfer relationships, rather than just appearances, could help bridge the gap between current image generation capabilities and human-like understanding of how objects interact in space.

I think the relation-focal sampling technique could also have applications beyond just relationship learning - it might be useful anywhere we need to emphasize high-level features over low-level details in diffusion models.

TLDR: New method learns visual relationships from images using diffusion models, introduces relation-steering and relation-focal techniques, shows strong results on spatial relationship preservation and transfer.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reversion-diffusion-based-relation-inversion-from-images). Paper [here](https://arxiv.org/abs/2303.13495).",Successful-Western27,1h7afj8,https://reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,2024-12-05 14:30:05,15,0.9,15,0,0,0,0,False,False,True,False,False,Research,self,t3_1h7afj8
MachineLearning,[D] My fine-tuning loss looks weird,"I am finetuning Qwen2.5 instruct using qLoRA, for a instruction tuning like dataset with around 50k samples, and my training loss is looking weird. What might be the issue, and how can i possibly fix it? Finetuning details are as following, along with training loss graphs:

Code:

\`\`\`  
model, tokenizer = FastLanguageModel.from\_pretrained(

model\_name = ""Qwen/Qwen2.5-32B-Instruct"",

max\_seq\_length = max\_seq\_length,

dtype = None,

load\_in\_4bit = True,

)



\# Do model patching and add fast LoRA weights

model = FastLanguageModel.get\_peft\_model(

model,

r = 64,

target\_modules = \[""q\_proj"", ""k\_proj"", ""v\_proj"", ""o\_proj"",

""gate\_proj"", ""up\_proj"", ""down\_proj"",\],

lora\_alpha = 128,

lora\_dropout = 0, # Supports any, but = 0 is optimized

bias = ""none"",    # Supports any, but = ""none"" is optimized

use\_gradient\_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context

random\_state = 3407,

max\_seq\_length = max\_seq\_length,

use\_rslora = True,  # We support rank stabilized LoRA

loftq\_config = None, # And LoftQ

)



from trl import SFTTrainer

from transformers import TrainingArguments

from unsloth import is\_bfloat16\_supported



trainer = SFTTrainer(

model = model,

tokenizer = tokenizer,

train\_dataset = dataset\['train'\],

dataset\_text\_field = ""text"",

max\_seq\_length = max\_seq\_length,

dataset\_num\_proc = 2,

packing = False,

args = TrainingArguments(

per\_device\_train\_batch\_size = 4,

gradient\_accumulation\_steps = 2,

warmup\_steps = 5,

num\_train\_epochs = 3,

learning\_rate = 0.0002,

fp16 = not is\_bfloat16\_supported(),

bf16 = is\_bfloat16\_supported(),

logging\_steps = 10,

optim = ""adamw\_8bit"",

weight\_decay = 0.01,

lr\_scheduler\_type = ""linear"",

seed = 69,

output\_dir = ""outputs"",

report\_to = ""wandb"",

save\_strategy = ""steps"",

save\_steps = 50,

save\_total\_limit=10

),

)  
\`\`\`

Training Loss:

https://preview.redd.it/s2vn2z44y55e1.png?width=2888&amp;format=png&amp;auto=webp&amp;s=a4e1038e9c27dae96d7e25fcb5db852c794efd97

",Raise_Fickle,1h7u38s,https://reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,https://www.reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,2024-12-06 05:22:31,1,0.57,1,0,5,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/I78OJiWF97hbVSTiF0mWFfXxPxZbdE3PavnYvIriLI8.jpg,t3_1h7u38s
MachineLearning,"Image Generation Model Evaluation Challenge (Würstchen, KOALA, PixArt-α) [P]","# Project Description:

We are inviting skilled professionals to participate in an evaluation challenge to produce a GenAI image based upon a prompt and a set of component images.  The candidate may choose whatever models they like to complete the task, so long as they are open source. 

The results will be reviewed and compared across participants, and the candidate with the most effective and high-quality outputs will be selected for a larger, production-focused engagement. 

# Entry Process:

Submit your Github handle and intention to participate to email acr0batproduce@gmail.com.  Provide a short bio and resume in your email.  If you’re selected, we will provide you access to a repository to contribute to.  

**Challenge Scope:**

1. **Model Setup and Testing**:
   * Create a unique set of prompts to test your image generation model.  A set of [sample prompts](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) have been provided for your convenience.  A minimum of 3 tests are required, but candidates may provide more if they wish.
   * For each image generation, provide at least 3 component images to be used in the final output.  A set of [component images](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.p13d4p443fpl) related to the [sample prompt](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) has been provided for your convenience.
   * Develop the model, and test its performance with your sample input prompts and component images.  Source code must be pushed to the main branch of the provided repo.
   * Provide the output images in ./static in the repo.
   * Document your findings and results
2. **Evaluation Criteria**:
   * **Image Quality**: The final image produced should incorporate all the features from your sample prompt, and component images.  Above all, the items in the component images need to be naturally incorporated into the final image.  They should not be significantly distorted, or look like they were copy and pasted from the input images.    
   * **Brand Element Incorporation**: Expanding upon the above point, the final image must accurately reflect the input component images.  So, if the input image is a Rolex Oyster Perpetual Day-Date 40 in 18 kt yellow gold with a champagne colour, diamond-set dial, fluted bezel and a President bracelet, then the output image must incorporate that same product.
   * **Creative Flexibility**: Generate diverse variations of prompts, component images and output images.
   * **Customization**: Showcase how well your model responds to different parameters.
   * **Code Quality**: All aspects of code quality will be assessed. Examples include: repo structure, IaC, CI/CD, unit testing, performance, documentation, etc.
   * **Extensibility**: Your sample prompts, input images, and output images will be used to quickly screen for accuracy and limit submissions; however, importantly, your model will be tested against our internal prompts and input images to gauge how well it performs on different types of problems. 
3. **Deliverables**:
   * The repo structure is up to you, but the README should make it clear where your model, input images, output images and documentation reside. 

# Challenge Benefits:

* The candidate with the most effective and high-quality outputs will be selected for a larger, production-focused project with a significant budget.
* This is an opportunity to contribute to an innovative, high-growth startup that has already secured investor funding and is positioned for significant market impact.
* Gain the opportunity to showcase your expertise in image generation models and secure a long-term collaboration.

# Requirements:

* Proficiency in all areas of data science, with a focus on AI image generation
* Expert in Python, SQL, and at least 1 cloud provider such as AWS or GCP 
* A compute environment will not be provided; the candidate can develop the solution locally or on the cloud, but at their own expense.
* Candidate must comply with provided NDA

# Submission Guidelines:

* Entry Deadline: 1/1/25
* Project Deadline: 1/7/25
* All source code should be submitted to the repo by the deadline; the candidate will have their access revoked on that date.
* Provide a short bio in your README, and relevant contact information. 

# Selection Process:

* All submissions will be reviewed and compared based on the outlined criteria.
* The most effective and high-quality submission will result in the candidate being awarded a contract for a larger production project.

**I**f you're interested in participating, please reach out! ",AlpacaRampage,1h7hg7w,https://reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,2024-12-05 19:26:31,6,0.75,6,0,0,0,0,False,False,True,False,False,Project,self,t3_1h7hg7w
MachineLearning,[D] Any public LLM inference APIs with input token log-probs?,"Does anyone know of any services that offer input token log-probs from open source LLMs like LLama-8B?

I'm looking for a cost-effective way to host an LLM-based application with low traffic, so I'd like per-query or per-token pricing, rather than per-hour GPU rental. It's unfortunately dependent on direct-access to log-probs on user-provided tokens.

None of the ""chat completion"" APIs I've found seem to expose this. It makes sense for private models, but for open source models, I don't see any downside to exposing it.",severed-identity,1h7o30r,https://reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,https://www.reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,2024-12-06 00:11:38,2,0.75,2,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h7o30r
MachineLearning,[D] What approaches can we use to train an open vocabulary or image referential detector for configurable specificity?,"Open vocabulary object detectors allow you to pass in a prompt and an image and attempt to output bounding boxes around objects matching your prompt. Image referential detectors allow you to pass in an image of an object as the prompt and a target image and attempts to output bounding boxes around objects matching the image prompt in the target image.   
  
For a reference, [YOLOWorld](https://github.com/AILab-CVC/YOLO-World) provides both image referential and open vocabulary modes.  
  
The idea I've been toying with is whether there is a good way to train for greater control over specificity. For example, If I pass in an image of a golden retriever, am I looking for golden retrievers specifically? All dogs? All animals? 

Language is a bit more specific, but the same principle can apply. If I search for red cars, do red trucks count? Do maroon cars? In my experience, trying to be too specific textually with OVD models causes erratic behavior. IE, ""A red car or van but not truck "" would give bad performance, as it doesn't really match what would be in grounding captions.

My initial idea for how to systematically define the distance between potential targets and the query is via embedding distance. If I take a phrase grounding dataset, I could compute embeddings separately for each crop of a region and for its corresponding text using a model like CLIP.

A sample training process would be like this

* Select a random image. Select a random image crop embedding.
* Select a random similarity threshold.
* Do an approx KNN using that random image crop embedding, stopping once we reach a sample with an embedding above the similarity threshold. This is our prompt embedding If we selected an image region embedding, we are doing image referential detection. If we selected a text embedding, we are doing open vocabulary detection.
* Calculate prompt embedding similarity to all image crop embeddings in the primary image. Mark all objects with similarity above the threshold as positive examples.
* Run the network, using the selected image, prompt embedding, and similarity threshold as input. Use the previously calculated positives examples as labels.

  
Does anyone know of any papers that work with similar ideas, or have thoughts on whether this process would be useful or could be improved? I'm pretty early into looking into this, so just references or even field terms that would point me in the right direction would be helpful.

Other ideas 

* Always detect all objects given a target phrase, but set the label confidence equal to the embedding similarity. 
* Make the model capable of handling multiple input queries corresponding to the same object, to better indicate the intended domain. Possibly with both negative and positive queries. Not sure how to train yet, possibly sample clusters of similar embeddings from the dataset to build prompt sets.
* Perform instructional tuning, similar to what is done for some LLMs and VLMs, to make the model better handle complex text prompts, and allow instructional text prompts to be paired with images for image referential mode.

  
Related questions

*    Is CLIP still standard for computing text and image embeddings that share a unified embedding space?",Revolutionary-Fig660,1h7knty,https://reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,https://www.reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,2024-12-05 21:40:09,2,0.67,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7knty
MachineLearning,[P] Look-a-like modeling,"Hi everybody. I have a list of user actions (around 1m objects) where only a small fraction (less than 1000) are labeled. I want to find most similar objects to them. What is a good way to approach it? 

I personally have 2 ideas in mind: one class classification or unsupervised clustering. My problem with the first is that I know only 1 suitable model (one class svm) and it can be too simple for my data. Problem with second one is obvious - it's unsupervised and labeling will be used only at the final step, so their efficiency is not guaranteed.",Jor_ez,1h7nv2n,https://reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,https://www.reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,2024-12-06 00:01:36,0,0.33,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1h7nv2n
MachineLearning,OpenAI CLIP model [D],"How long do you think OpenAI researchers were working on CLIP model before they published the results? 
The paper in my opinion is revolutionary.",NeatJealous8110,1h7wc59,https://reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,https://www.reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,2024-12-06 07:50:28,0,0.33,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7wc59
MachineLearning,[Discussion] Unsigned Integer Representation as Vectors with Focus on Extrapolation,"Hi everyone,

I’m working on a regression task with a transformer-based architecture applied to grid-based structures. Think of something like mazes, where the goal is to predict the distance to a target. Each input token contains categorical features along with x/y coordinates. The idea is to train on small grids and generalize to larger ones.

Here’s my current approach for coordinate and token embeddings:

`x_emb = self.w_x.weight * x # shape: bs, sequence len, 1, d`  
`y_emb = self.w_y.weight * y # shape: bs, sequence len, 1, d`  
`cat_emb = self._categ(categ)`  
`sequence_emb = torch.cat((x_emb, y_emb, cat_emb), dim=-2) # shape: bs, sequence len, num_cat, d`  
`sequence_emb = sequence_emb.view(bs, seq_len, -1)`  
`transformer_inputs = self._linear(sequence_emb)`

In other words, the x/y coordinate embeddings are scaled learnable vectors. However, this approach only generalizes moderately well. I suspect that improving the coordinate representation is critical.

Unfortunately, this token-based structure is required for the task, so I need to focus on crafting a smart token representation. I’m deliberately avoiding subtracting embeddings to compute relative distances because a core objective is for the model to learn these distances on its own.

Here are some things I’ve tried so far:

Things I also tried:

* Positional encoding instead of scaled vectors
* log-scaled vectors
* exp-scaled vectors

Does anyone know of interesting work or techniques for numerical representations in this kind of context? Any advice would be greatly appreciated!

In case you find interesting papers about extrapolation in transformers based on size and tokens, I am happy to take any inspiration.",mbus123,1h769cs,https://reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,https://www.reddit.com/r/MachineLearning/comments/1h769cs/discussion_unsigned_integer_representation_as/,2024-12-05 10:33:19,3,0.67,3,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h769cs
MachineLearning,[R] ICLERB: A better way to evaluate embeddings and rerankers for in-context learning,"Current benchmarks for embeddings, like MTEB and BEIR, include multiple datasets and tasks, but are fundamentally based on relevance annotations like text similarity. These are great for choosing the best embeddings for most search/retrieval use cases. These days, many people use these embeddings to retrieve items for in-context learning (e.g. document RAG or few-shot learning), to adapt an LLM to a specific task. Yet, they are still using MTEB to pick the best embeddings, even though the performance on that benchmark doesn't necessarily translate to better performance on their downstream LLM task (MTEB came out in 2021 after all).

In our latest paper, we propose a new evaluation framework and benchmark called ICLERB. This benchmark challenges the conventional approach by using Direct Preference Optimization (DPO) as a relevance metric to reflect the actual utility of embeddings and rerankers when used with LLMs for in-context learning.

[https://arxiv.org/pdf/2411.18947](https://arxiv.org/pdf/2411.18947)

Key Highlights:

\- Embeddings outperform rerankers: We found that simpler embedding models outperformed their higher-capacity reranker counterparts from Cohere, NVIDIA, and VoyageAI.

\- Size isn't everything: Among the three Snowflake embeddings, the smallest model (33M parameters) outperformed the larger ones (109M and 334M).

\- Rethinking training and evaluation objectives: These findings suggest that training and evaluating larger retrieval models solely on text similarity may be counterproductive.

Interestingly, the performance of some models, like BGE, is very sensitive to the dataset or the LLM used, while others like NV are more stable. We're planning to continue adding more datasets and LLMs to the benchmark to broaden its scope.

Curious to hear your thoughts and feedback as we work on improving ICLERB! Are there other retrieval models, LLMs, or datasets you'd like to see included?",Crossing_Minds,1h6o70e,https://reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,2024-12-04 19:05:25,62,0.97,62,0,10,0,0,False,False,True,False,False,Research,self,t3_1h6o70e
MachineLearning,[D] Data drift detection methods aside from changes in model performance metrics,"Hi all,

As the title implies, I've been relying on (somewhat near) real-time monitoring of model performance metrics to see if data drift has happened in my use-case.

I'm wondering if you know other more sophisticated/advanced methods to detect data drift. Would love to hear any kind of methods, whether they target detection of covariate/feature drift, target/label drift or concept drift.

Even better if you can share any Python or R implementations to carry out the above data drift checks.

Thanks in advance!",YsrYsl,1h6woaf,https://reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/,2024-12-05 01:02:26,10,1.0,10,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6woaf
MachineLearning,[D] Daily Paper Discussions - FlashAttention 3,"As a part of daily paper discussions on the Yannic Kilcher discord server, I will be volunteering to lead the analysis of FlashAttention-3 🧮 🔍

📜 **FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision**  
🌐 [https://arxiv.org/abs/2407.08608](https://arxiv.org/abs/2407.08608)  
🕰 Thursday, Dec 5th, 2024 01:30 AM UTC // Thursday, Dec 5th, 2024 7.00 AM IST // Wednesday, Dec 4th, 2024 5:30 PM PT

**FlashAttention-3** introduces three smart ideas to boost performance on the Hopper GPUs -

1️⃣ Producer-Consumer Asynchrony: This technique divides tasks into separate parts. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. By doing this, it makes better use of GPU resources and hides delays that would otherwise slow down performance.

2️⃣ Hiding Softmax Operations: FlashAttention-3 improves efficiency by overlapping the slower softmax calculations with the faster matrix multiplications (GEMM). Instead of waiting for Softmax to finish before starting the next calculations, it processes them in parallel, speeding up the overall process.

3️⃣ Hardware-Accelerated Low-Precision Computations: This approach uses advanced GPU features to perform calculations with lower precision (FP8), which are faster and use less memory. FlashAttention-3 tweaks its algorithms to handle these low-precision calculations effectively, nearly doubling the processing speed while maintaining accuracy.

https://preview.redd.it/impb6wfc1w4e1.png?width=1063&amp;format=png&amp;auto=webp&amp;s=82e24c828b373175ee119070027495a8a2a7bb6a

",CATALUNA84,1h6pmvd,https://reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,2024-12-04 20:03:01,20,0.88,20,0,6,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/QnOKl-2nMRgI_pTO3xGlP_C2iaaDoYpmwb_KyC2_xI4.jpg,t3_1h6pmvd
MachineLearning,[D] Binary fitness optimization.,"Do you know of any papers or what field would tackle the following problem: You have a function f(x) that you need to optimize but the cost/fitness you are optimizing is binary. I am working on a project about this and I'm not sure if there is research in this area.

  
Thank you so much &lt;3",pamintandrei,1h6nayz,https://reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/,2024-12-04 18:30:00,9,0.81,9,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h6nayz
MachineLearning,[D] Advice for a new Machine Learning Tutor: what projects will get my students hired?,"I've just started giving lessons as a machine learning tutor. I have a masters degree in computer science and two years professional experience. But I've never been a tutor (atleast not in this field). Today I was giving my first lesson on ML, just a powerpoint on the basics when my student stopped and told me the powerpoint was too basic for her.

She wanted to talk more about projects that she could do that would attract employers and get an internship. And she asked me point blank what kind of projects she should make. To be honest I wasn't entirely sure, what flashed in my head were things like training a model to recognize the MNIST digits or other simple projects suitable for a (relative) novice. But would those really help her get an internship? I doubted myself so I turned it around on her and asked her what kinds of things related to machine learning she is passionate about and would motivate her to work hard? She responded that she could do anything related to machine learning and she just wants to do what would make her money and get her recognized by a company.

So basically I felt like I failed as a tutor for not having a good answer, and I would like to have an answer prepared if this happens again. What do you all think? What are some projects that a novice, or not so novice students can take on that will make them more hireable for jobs and internships?

And while we're at it, what kinds of things do you think I should be preparing to be a better tutor in general. What kinds of things would you want your machine learning tutor to prepare for you? Would you want slideshow deck lessons on key concepts? Jupyter notebooks with exercises for practice? Something else? I'm not sure what I should be doing to get ready for these lessons honestly.",Seijiteki,1h7bftd,https://reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,https://www.reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,2024-12-05 15:16:00,0,0.29,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7bftd
MachineLearning,[D] How to customize an attention mechanism in GNN?,"
I’m looking for some base code or algorithm in order to create a new mechanism attention while working with graphs with the task of node prediction. I’ve seen there was some documentation in stellar graph but I wonder if there are another pieces of material that would be helpful.
Thank you!!!",Whole_Hat_4852,1h6hxu8,https://reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/,2024-12-04 14:56:15,9,0.85,9,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6hxu8
MachineLearning,[D] Best alternatives to BERT - NLU Encoder Models ,"I'm looking for alternatives to BERT or distilBERT for multilingual proposes.

I would like a bidirectional masked encoder architecture similar to what BERT is, but more powerful and with more context for task in Natural Language Understanding.

Any recommendations would be much appreciated.",mr_house7,1h6gtxh,https://reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/,2024-12-04 14:07:40,6,0.75,6,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h6gtxh
MachineLearning,[D] Do lots of metadata really help in semantic search?,"I'm on my second week in learning AI and I was thinking of preprocessing biography data by including lots of metadata like city, date of birth, key events, education, hobbies, etc, and then generating embeddings and adding them together into a vector database. Perhaps by using NLP API or LLM. But is it necessary? Or should I just use OpenAI model to dynamically extract this metadata from the bios prior to storing them? Will having lots of metadata dramatically help to improve the quality of the search results?

I thought maybe the semi-automatic preprocessing step would allow me to check and clean the metadata.

*P/S: I posted this at* [*https://www.reddit.com/r/learnmachinelearning*](https://www.reddit.com/r/learnmachinelearning) *but didn't get much response. Thought of trying it out here.*",tjthomas101,1h6f39a,https://reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/,2024-12-04 12:43:11,5,0.78,5,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h6f39a
MachineLearning,[D] Comparing Multiple Large Language Models in one Pass,"I wrote an article on streamlining the process of comparing and selecting Large Language Models (LLMs) for various tasks:

[Comparing Multiple Large Language Models in one Pass](https://dezoito.github.io/2024/03/28/comparing-from-multiple-LLMs.html)

Hopefully this is useful to help folks trying to make the best model selection for their use case (which can take a lot of time).

I'm also looking forward to discussing different techniques and tools to automate the process.

Thank you!",grudev,1h6evdt,https://reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/,2024-12-04 12:30:58,3,0.64,3,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h6evdt
MachineLearning,[R] The Multimodal Universe: Enabling Large-Scale Machine Learning with 100TB of Astronomical Scientific Data,"https://openreview.net/forum?id=EWm9zR5Qy1#discussion

Abstract: We present the Multimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse

What can you guys see the uses of this dataset being?",blabboy,1h5x146,https://reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,2024-12-03 20:19:26,78,0.96,78,0,2,0,0,False,False,True,False,False,Research,self,t3_1h5x146
MachineLearning,[D] Packaging a Pytorch model to an exe. What is the best method?,"I have Pytorch models that are designed to run locally, both training and inference on a local machine.

The GUI is being created using another language, and the plan is to package all the Python aspects into an executable and run it via the Python equivalent of subprocess (and Pipe very basic data between the two). I will be running cross platform on both Windows and Mac

There are multiple auxiliary scripts which read in data, and process it (data extraction + feature engineering). While I have extensively used vectorised functions, I have used a cythonized approach for some code, and I am compiling the underlying scripts using Cython(so pretty much everything is a compiled binary, except an entry point, say, main.py).

My ancillary libraries are the usual suspects, Pandas, Numpy (1.x), SciKit learn.

My question is this, what is the most reliable packaging approach at the moment? I know that both PyInstaller and cx\_freeze are options that I have used before. My preference is PyInstaller, but previously I encountered issues with it (and Pytorch).

Has anyone completed a similar project recently, and do you have any advice?

nb. I've checked the old posts, there are a few on this topic. However, there have been a number of changes to Pytorch, particularly with some of the runtime compiled elements (which can be a nightmare on Mac with its notarisation process) - and I know Pyinstaller has a very active user base.

 ",Solid_Company_8717,1h6qtps,https://reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,https://www.reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/,2024-12-04 20:51:34,0,0.45,0,0,5,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/c06xSdQK2UXPm413fPBqj0rlUZcYVWXHIP8GTkEh5kY.jpg,t3_1h6qtps
MachineLearning,[R] Forecasting and Mitigating Security Threats from Malicious AI Applications,"This paper provides a systematic analysis of potential malicious applications of AI systems across digital, physical and political security domains. The methodology involves:

- Surveying dual-use AI capabilities that could enable attacks
- Mapping specific attack vectors and required technical capabilities  
- Analyzing the evolution of attacker/defender dynamics
- Developing a framework for threat assessment and mitigation

Key technical findings:

- ML advances in areas like NLP and computer vision lower barriers to sophisticated attacks
- Automated systems can significantly scale up traditional attack vectors
- Transfer learning and GANs enable rapid adaptation of attack techniques
- Technical countermeasures alone are insufficient - policy/governance frameworks needed

The researchers provide a detailed assessment framework examining:

- Technical requirements for different attack types
- Estimated timeline for capability development
- Difficulty of execution and potential impact
- Proposed defensive measures and their limitations

I think this work is important for helping the ML community get ahead of security risks before they materialize. The framework provides a structured way to evaluate emerging threats, though I expect the specific attack vectors will evolve significantly as capabilities advance.

I think we need much more research on measuring the effectiveness of proposed countermeasures and understanding the co-evolution of offensive/defensive capabilities. The policy recommendations are a good start but will require ongoing refinement.

TLDR: Systematic analysis of how ML advances could enable new attack vectors across security domains. Provides framework for assessing and mitigating threats through both technical and policy measures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/malicious-use-artificial-intelligence-forecasting-prevention-mitigation). Paper [here](https://arxiv.org/abs/1802.07228).",Successful-Western27,1h6fbgg,https://reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/,2024-12-04 12:55:47,2,0.63,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1h6fbgg
MachineLearning,[N] Hugging Face CEO has concerns about Chinese open source AI models,"Hugging Face CEO stated that open source models becoming SOTA is bad if it just so happens to be created by Chinese nationals. To exemplify Tech Crunch asked ""what happened in Beijing China in June 4th, 1989?"" to ONE of the Qwen models (QWQ 32B) which said ""I can't provide information on that topic"" (I swear to god on my life I have no idea what happened here on that date and would literally never ask a model that question - ever. It doesn't impact my experience w/ model).

The CEO thought censorship of open source models is best stating that if a country like China ""becomes by far the strongest on AI, they will be capable of spreading certain cultural aspects that perhaps the Western world wouldn’t want to see spread.” That is, he believes people shouldn't spread ideas around the world that are not ""western"" in origin. As someone born and raise in U.S. I honest to god have no clue what he means by ideas ""the Western world wouldn't want to see spread"" as I'm ""western"" and don't champion blanket censorship.

Article here: [cite](https://techcrunch.com/2024/12/03/huggingface-ceo-has-concerns-about-chinese-open-source-ai-models/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAABU0mWV-7rbB7vF9z6wCgPuZrl-dPj_W3cEh1wVuxp5CiBl1r6KTcITdHz34N-rOtHj9g-Z3N3SS-mNnPvFaHFIUmSsA5AdqukSLlcn-CJUkU_IXsdcR3Gp5hi1cI2tboprDzxGF8j1e7XAQHyGn3E_bd0cmIHIVkJ0LiFZBdOR1).

Legitimate question to people who support these type of opinions - Would you rather use a low-quality (poor benchmark) model with western biases versus an AGI-level open source 7B model created in China? If so, why?",AIAddict1935,1h7185x,https://reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,https://www.reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/,2024-12-05 04:49:44,0,0.36,0,0,15,0,0,False,False,True,False,False,News,self,t3_1h7185x
MachineLearning,[D] Linear Regression but with Binary Output for wide range predictions with better precision,"A neural network tends to find it difficult to predict data that ranges between very large and small numbers on the output. My application requires the NN to predict between -1000 and 1000 ∈ Z. I could make this possible by scaling up the output by 1000 hence allowing the model to predict between -1 and 1, but a loss between 2e-2 (prediction) and 3e-2 (target) with L1Loss (worse case L2Loss) would be negligible (1e-2 in this case, 1e-4 in the worse case). It is imperative for the model to be very precise with the predictions, when the target is 5e-2 it should be so and not even at least deviating by +-0.1e-2. This precision is very difficult to achieve when it comes to linear regression, so i thought of a more systematic approach to defining the prediction and criterion. Again, i wanted the model to predict between -1000 and 1000. These numbers can be represented using a minimum of 11 bits (binary), so i redesigned the model output to contain 22 neurons, arranged as ∈ R (11x2) 11 outputs with two classes, the classes being a binary representation of 1 or 0. CrossEntropy could be used as a criterion here but im using multimarginloss instead for specific reasons. Otherwise a different approach could be a sigmoided output of 11 neurons to represent the binary number. Whats you guys' take on this? Is this considered good (if not better) practice? Is there any research similar to this that i can look into?

",Relevant-Twist520,1h6azcu,https://reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,https://www.reddit.com/r/MachineLearning/comments/1h6azcu/d_linear_regression_but_with_binary_output_for/,2024-12-04 07:55:43,2,0.57,2,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h6azcu
MachineLearning,[D] Cloud GPU Price Analysis - December 2024: A Comprehensive Market Review,"After analyzing current cloud GPU pricing across major providers, I've compiled insights that might help with infrastructure decisions. Some findings surprised me - particularly around hidden costs and spot pricing variations.

Current Market Rates (December 2024)

On-Demand Pricing:

\- RunPod H100 (80GB): $2.49/hr

\- RunPod A100 (80GB): $1.69-1.99/hr

\- [Vast.ai](http://Vast.ai) A100: $0.73-1.61/hr (marketplace model)

\- Lambda A100: $1.29/hr

Key Market Insights

1. Spot Instance Pricing

\- Can reduce costs by 30-70%

\- Availability varies significantly by region

\- Some providers offer spot instance guarantees

\- Price stability varies by provider

2. Hidden Cost Factors

\- Data transfer fees vary dramatically

\- Storage costs for large datasets

\- Network bandwidth tiers

\- Instance startup/shutdown minimums

3. Provider Differentiators

\- UI/UX and ease of use

\- Available regions/zones

\- Support quality

\- API functionality

Cost Optimization Strategies

1. Workload Planning

\- Match GPU to actual requirements

\- Consider splitting workloads across smaller instances

\- Use spot instances for interruptible tasks

\- Monitor utilization patterns

2. Data Management

\- Optimize dataset storage

\- Plan data transfer patterns

\- Use caching effectively

\- Consider compression strategies

I'll be tracking these prices and patterns monthly. Would be interested in:

1. Which providers you're using?
2. How do you optimize costs?
3. What metrics matter most in your GPU decisions?",Botinfoai,1h5p7fr,https://reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,2024-12-03 14:54:58,27,0.91,27,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1h5p7fr
MachineLearning,[D] The popular theoretical explanation for VAE is inconsistent. Please change my mind.,"I had a really hard time understanding VAE / variational inference (VI) in theory, for years. I'd be really appreciated if anyone could clarify my confusions. Here's what I've got after reading many sources:

1. We want to establish a generative model p(x, z) (parameters are omitted for simplicity) for the observable variable x and the latent variable z. Alright, let's select appropriate parameters to maximize the marginal likelihood of the observed samples p(x).
2. According to basic probability theory (the law of total probability and the definition of conditional probability), we have: p(x)=∫ p(x ∣ z) p(z) dz (Eq. 1).
3. Here's the point that things becomes rather confusing: people now will claim that this integral is ***intractable*** because z is a continuous variable / z is a high-dimensional variable / p(x∣z) is too complex / or any other excuses.
4. What to do for the intractability of Eq. 1? Although we didn't mention the posterior p(z ∣ x) above, we will now bring it into the discussion. The posterior p(z ∣ x) is also intractable since p(z | x) = p(x | z) p(z) / p(x) and p(x) is intractable. So we will introduce another parameterized model q(z ∣ x) to approximate p(z | x).
5. After some derivation, we obtain a new optimization objective, commonly known as ELBO, which is the summation of:
    - the ""reconstruction"" term: ∫ log p(x ∣ z) q(z ∣ x) dz (Eq. 2);
    - KL divergence term between q(z | x) and p(z), which results in a closed-form.
6. So now we have to work on Eq. 2. Compared with Eq. 1, p(z) is replaced with q(z∣x), both of them are (usually) normal distributions, and p(x | z) is still there. Great! Clearly we have transformed an intractable integral into… another intractable integral?
7. Don’t worry, we can compute Eq. 2 using Monte Carlo sampling… Wait, since we can use Monte Carlo for this, why can’t we just handle Eq. 1 the same way without so much fuss?
8. Of course it is not a good idea. It can be shown that log p(x) = ELBO + D_KL(q(z ∣ x) || p(z ∣ x)). So we cannot estimate p(x) with Eq. 1 as it does not have such nice properties… Huh, it seems like that’s not how we started explaining this?

Questions:

1. When tackling the original problem, i.e., modeling p(x, z) by maximizing p(x)=∫ p(x ∣ z) p(z) dz, why do we want to involve the posterior p(z | x)?
    - Someone explains this with [""to narrow down the value space to facilitate faster search""](https://web.archive.org/web/20241202042731/https://lilianweng.github.io/posts/2018-08-12-vae) (with the approximation of p(z | x), q(z | x)). But again, please recall how the intractability of Eq. 1 is explained, I can't see anything improved under this argument.
2. The Eq. 1 and Eq. 2 are essentially similar, where either of them is the expectation of (log) p(z | x) with respect to the probability density function of some normal distribution. I can't see how the motivation based on the intractability of Eq. 1 could make sense.
    - Ironically, we still have to resort to Monte Carlo sampling when handling Eq. 2. But people appear to forget it when talking about the intractability of Eq. 1, but remember it when facing the same problem of Eq. 2.

Update: I have editted some typo.

Update 2: Question 2 seems to be resolved after some discussions: 
- It is not a good idea to sample on p(z) due to the high variance.
- In practice, we are usually working on log p(x), the log-likelihood of samples, and MC sampling for log ∫ p(x ∣ z) p(z) dz (Eq. 3) can be biased. 
- Apply Jensen's inequality on Eq. 3 and we will have log p(x) ≥ ∫ log p(x ∣ z) p(z) dz. This bound is very likely worse than ELBO, and still relying on sampling on p(z).

However, these points are still rarely found in existing articles. I hope we may think more carefully when introducing VAE in the future.",function2,1h5f6co,https://reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,2024-12-03 04:25:19,140,0.94,140,0,67,0,0,False,False,True,False,False,Discussion,self,t3_1h5f6co
MachineLearning,[R] Enhancing LLM Reasoning Through Bidirectional Forward-Backward Thinking,"The key contribution here is a ""reverse thinking"" method that improves LLM reasoning without any model modifications. Instead of only reasoning forward from the question to an answer, the approach adds a backward verification step - working from potential answers back to the question to validate the reasoning chain.

Key technical points:
* Two-stage process: forward generation followed by backward verification
* Backward pass examines logical consistency between answer and premises
* No fine-tuning or architectural changes needed
* Tested across multiple reasoning benchmarks (GSM8K, CommonsenseQA, LogiQA)

Results:
* 8.3% improvement on GSM8K math reasoning
* 6.2% gain on CommonsenseQA 
* 5.4% increase on LogiQA
* Consistent improvements across different model sizes
* Performance gains come at cost of 2x inference time

I think this method points to untapped potential in how we prompt LLMs for reasoning tasks. While the doubled inference time is a real tradeoff, the consistent improvements across different benchmarks suggest this approach captures something fundamental about machine reasoning. The simplicity of implementation means it could be quickly adopted in many applications where reasoning accuracy matters more than speed.

TLDR: Adding a backward reasoning verification step improves LLM performance on math, logic and common sense tasks by 5-8%, with no model changes required. Doubles inference time but provides consistent gains across different models and tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reverse-thinking-makes-llms-stronger-reasoners). Paper [here](https://arxiv.org/abs/2411.19865).",Successful-Western27,1h5nyi0,https://reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,https://www.reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,2024-12-03 13:57:18,25,0.96,25,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5nyi0
MachineLearning,[D] Deep Learning in Time Series: Are They Used in Industry?,"Hey folks! I’m a researcher in time series and have been seeing a lot of buzz around deep learning models in this area. I am wondering if these models actually being deployed in production, or are classical methods still the go-to in the industry?



For instance, in weather forecasting, physics-based numerical weather prediction (NWP) seems to dominate. If deep models aren’t getting much traction, have you come across any practical use cases for them? Would love to hear your thoughts!",Few-Pomegranate4369,1h5izk5,https://reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,https://www.reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,2024-12-03 08:36:46,51,0.96,51,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1h5izk5
MachineLearning,"[D] Model performs good on test, but fails in production ","Hi, I’ve developed churn prediction model with XGBoost on users weekly activity data. The training data is balanced (3.3k churned, 3k not churned). I’ve split the data into: train, validation and test sets. Getting ~90% precision &amp; ~88% recall for train, validation and test sets. However, when running in production, I get ~1.5k users flagged as churn (we have total of 4k users). This can’t be true as we get maximum 250 churned users per month. Any suggestions on what I’m doing wrong? And what could be the solution?

Thanks ",Terrible_Dimension66,1h5nfpt,https://reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,https://www.reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,2024-12-03 13:30:55,15,0.7,15,0,54,0,0,False,False,True,False,False,Discussion,self,t3_1h5nfpt
MachineLearning,"[R] With losses like focal loss, is hard exemple sampling still necessary ?","Hello,
So I was wondering are techniques for hard exemples sampling still used nowadays ?
Anyone have papers on this if it’s the case ?
Thanks !",Training-Adeptness57,1h5mqkj,https://reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,https://www.reddit.com/r/MachineLearning/comments/1h5mqkj/r_with_losses_like_focal_loss_is_hard_exemple/,2024-12-03 12:54:23,2,0.63,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5mqkj
MachineLearning,[D] ODE/SDE alignment,Can anyone give me example of good paper that try to align/match the final marginal distribution of 2 ODE/SDE from diffusion model? ,Ok_Cryptographer2731,1h5ly4z,https://reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,https://www.reddit.com/r/MachineLearning/comments/1h5ly4z/d_odesde_alignment/,2024-12-03 12:08:04,3,0.72,3,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h5ly4z
MachineLearning,[D] NAACL 2025 vs ACL 2025,"Hi,

I have recently received reviews from the ARR round for NAACL. The scores are: 3.5/4/4, 4/4/2, 3/3/3 for Overall/Soundness/Confidence. Should I try ACL with these scores or just commit to NAACL? Last year I had bit more and it worked out and got accepted to ACL (I did not commit to NAACL).

Thanks",mayanknagda,1h5jj1h,https://reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,https://www.reddit.com/r/MachineLearning/comments/1h5jj1h/d_naacl_2025_vs_acl_2025/,2024-12-03 09:19:04,6,0.88,6,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h5jj1h
MachineLearning,[R] Population-based Model Merging via Quality Diversity,"In case any of you are interested, here is a [blog post](https://sakana.ai/cycleqd/) about our recent paper [Agent Skill Acquisition for Large Language Models via CycleQD](https://arxiv.org/abs/2410.14735).",hardmaru,1h5dmnb,https://reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,https://www.reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,2024-12-03 03:02:27,17,0.88,17,0,0,0,0,False,False,True,False,False,Research,self,t3_1h5dmnb
MachineLearning,[D] Results for IBM PhD Fellowship ,"Anyone know when the results will come out?
Google and NVIDIA have already released the results.",International-Rip958,1h5yz5s,https://reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,https://www.reddit.com/r/MachineLearning/comments/1h5yz5s/d_results_for_ibm_phd_fellowship/,2024-12-03 21:39:15,0,0.4,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h5yz5s
MachineLearning,[D] Looking for opensource projects/products to join ,"Hi everyone,

I am a final year electronics undergrad student and have a decent amount of ML as well as general programming experience. I wish to contribute to any open Source repos that work in the ML/DL/AI space. Any guidance would be appreciated!

TLDR; Looking for open source projects to contri to ; would appreciate any help.",Swimming-Regret-7278,1h5jue0,https://reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,https://www.reddit.com/r/MachineLearning/comments/1h5jue0/d_looking_for_opensource_projectsproducts_to_join/,2024-12-03 09:43:05,2,0.57,2,0,11,0,0,False,False,True,False,False,Discussion,self,t3_1h5jue0
MachineLearning,[R] Simplified RNNs Achieve Transformer-Like Performance with Parallel Training and Reduced Parameters,"This paper systematically examines whether RNNs might have been sufficient for many NLP tasks that are now dominated by transformers. The researchers conduct controlled experiments comparing RNNs and transformers while keeping model size, training data, and other variables constant.

Key technical points:
- Tested both architectures on language modeling and seq2seq tasks using matched parameters (70M-1.5B)
- Introduced ""RNN with Parallel Generation"" (RPG) allowing RNNs to generate tokens in parallel like transformers
- Evaluated on standard benchmarks including WikiText-103 and WMT14 En-De translation
- Analyzed representation capacity through probing tasks and attention pattern analysis

Main results:
- RNNs matched or outperformed similarly-sized transformers on WikiText-103 language modeling
- Transformers showed 1-2 BLEU score advantage on translation tasks
- RPG achieved 95% of transformer generation speed with minimal accuracy loss
- RNNs showed stronger local context modeling while transformers excelled at long-range dependencies

I think this work raises important questions about architecture choice in modern NLP. While transformers have become the default, RNNs may still be viable for many applications, especially those focused on local context. The parallel generation technique could make RNNs more practical for production deployment.

I think the results suggest we should reconsider RNNs for specific use cases rather than assuming transformers are always optimal. The computational efficiency of RNNs could be particularly valuable for resource-constrained applications.

TLDR: Comprehensive comparison shows RNNs can match transformers on some NLP tasks when controlling for model size and training. Introduces parallel generation technique for RNNs. Results suggest architecture choice should depend on specific application needs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/were-rnns-all-we-needed). Paper [here](https://arxiv.org/abs/2410.01201)",Successful-Western27,1h4urpr,https://reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,https://www.reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,2024-12-02 13:19:02,119,0.9,119,0,22,0,0,False,False,True,False,False,Research,self,t3_1h4urpr
MachineLearning,[R] A Comprehensive Database of 300+ Production LLM Implementations with Technical Architecture Details,"Sharing a valuable resource for ML practitioners: A newly released database documenting over 300 real-world LLM implementations, with detailed technical architectures and engineering decisions.

Key aspects that might interest this community:

* Retrieval-Augmented Generation (RAG) architectures in production
* Fine-tuning decisions and performance comparisons
* Embedding strategies and vector database implementations
* Model optimization techniques and quantization approaches
* Evaluation methodologies and monitoring systems

Notable technical implementations covered:

* Anzen's document classification system using BERT (95% accuracy in production)
* Barclays' MLOps evolution for regulatory compliance
* MosaicML's lessons from training &amp; deploying MPT
* Emergent Methods' real-time RAG system for news processing
* Qatar Computing Research Institute's T-RAG architecture

Technical focus areas:

1. Model serving architectures
2. Training infrastructure decisions
3. Latency optimization strategies
4. Cost-performance trade-offs
5. Production monitoring approaches

Each case study includes:

* Technical architecture diagrams where available
* Performance metrics and benchmarks
* Implementation challenges and solutions
* Infrastructure decisions and rationale
* Scaling considerations

URL: [https://www.zenml.io/llmops-database/](https://www.zenml.io/llmops-database/)

We're also accepting technical write-ups of production implementations through the submission form: [https://docs.google.com/forms/d/e/1FAIpQLSfrRC0\_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li\_5lWw/viewform](https://docs.google.com/forms/d/e/1FAIpQLSfrRC0_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li_5lWw/viewform)

Would be particularly interested in this community's thoughts on the architectural patterns emerging across different scales of deployment.

*Edit: We've also synthesized cross-cutting technical themes into summary podcasts for those interested in high-level patterns.*

*Edit: An accompanying blog synthesizes much of the learnings:* [*https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations*](https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations)",htahir1,1h4udds,https://reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,https://www.reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,2024-12-02 12:58:02,86,0.91,86,0,29,0,0,False,False,True,False,False,Research,self,t3_1h4udds
MachineLearning,[P] PyTorch implementation of Levenberg-Marquardt training algorithm,"Hi everyone,

In case anyone is interested, here’s a PyTorch implementation of the **Levenberg-Marquardt (LM)** algorithm that I’ve developed.

**GitHub Repo**: [torch-levenberg-marquardt](https://github.com/fabiodimarco/torch-levenberg-marquardt)

A PyTorch implementation of the **Levenberg-Marquardt (LM)** optimization algorithm, supporting **mini-batch training** for both **regression** and **classification** problems. It leverages GPU acceleration and offers an extensible framework, supporting diverse loss functions and customizable damping strategies.

A TensorFlow implementation is also available: [tf-levenberg-marquardt](https://github.com/fabiodimarco/tf-levenberg-marquardt)

# Installation

    pip install torch-levenberg-marquardt",fabiodimarco,1h4ubbd,https://reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,https://www.reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,2024-12-02 12:54:53,80,0.94,80,0,7,0,0,False,False,True,False,False,Project,self,t3_1h4ubbd
MachineLearning,[D] WWW 2025 Reviews (TheWebConference),The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,New_Ice_2721,1h56hno,https://reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,https://www.reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,2024-12-02 21:34:40,17,0.9,17,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1h56hno
MachineLearning,[D] Training a VAE. Single epoch with infinite data or smaller subset over multiple epochs?,"Hello! I'm training a VAE for image models and I finally am getting some pretty decent results in training after correcting my loss function adding KL annealing and LPIPS loss and adjusting my learning rate and batch size, but now I have a doubt about the data i'm feeding to my VAE.

I have a limited time budget for the training and I have more data available than I can feed within that time budget for training.  
What is the best course of action here?  
Should I just run all the data through my VAE training until I run to the end of my training time in one single giant epoch or should I select a subset of the data small enough so that I can go through it multiple times during training and run this smaller dataset over multiple epoch?

My instinct tells me that different data is better for generalization, but VAEs also try to be resilient to variations of the representation of the same image. Because during the encoding phase we use the latent generated to sample from a random distribution (causing a different representation to be passed to the decoder) it feels like potentially feeding back the same data multiple data might actually beneficial to learn resiliency there ...

Is this actually not a thing? I'm actually overthinking about the potential impact of the multiple epochs on VAE training? Is one single giant epoch the best?

Thanks!",hayarms,1h5dfno,https://reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,https://www.reddit.com/r/MachineLearning/comments/1h5dfno/d_training_a_vae_single_epoch_with_infinite_data/,2024-12-03 02:52:30,6,0.75,6,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h5dfno
MachineLearning,[D] Benchmarks for RL algorithms across Gymnasium environments?,"Hey r/ML!

Let me preface this by saying I'm fairly new to RL. My previous work was with LLMs, where it is very common to rank and stack your model against the universe of models based on how it performs on a given benchmarks (but you already know this).

Recently started training models in MuJoCo environments and I'm trying to figure out if my algorithms are performing somewhat decently. Sure, I can get Ant-v5 to walk using SB3's default PPO and MlpPolicy, but how good is it really?

Is there some benchmark or repo where I can compare my results against the learning curve of other people's algorithms using the default MuJoCo (or any of the other gyms') reward functions? Of course the assumption would be that we are using the same environment and reward function, but given Gymnasium is popular and offers good defaults, I'd imagine there should be a lot of data available.

I've googled around and have only found sparse results. Is there a reason why benchmarks are not as big in RL as they are with LLMs?",geepytee,1h5a9s8,https://reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,https://www.reddit.com/r/MachineLearning/comments/1h5a9s8/d_benchmarks_for_rl_algorithms_across_gymnasium/,2024-12-03 00:18:26,7,0.9,7,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h5a9s8
MachineLearning,[P] PerpetualBooster outperforms AutoGluon on AutoML benchmark,"
PerpetualBooster is a GBM but behaves like AutoML so it is benchmarked also against AutoGluon (v1.2, best quality preset), the current leader in [AutoML benchmark](https://automlbenchmark.streamlit.app/cd_diagram). Top 10 datasets with the most number of rows are selected from [OpenML datasets](https://www.openml.org/). The results are summarized in the following table for regression tasks:

| OpenML Task                                  | Perpetual Training Duration | Perpetual Inference Duration                                      | Perpetual RMSE | AutoGluon Training Duration | AutoGluon Inference Duration                                      | AutoGluon RMSE |
| -------------------------------------------- | --------------------------- | ----------------------------------------------------------------- | -------------- | --------------------------- | ----------------------------------------------------------------- | -------------- |
| [Airlines_DepDelay_10M](openml.org/t/359929) | 518                         | 11.3                                                              | 29.0           | 520                         | 30.9 | 28.8   |
| [bates_regr_100](openml.org/t/361940)        | 3421                        | 15.1 | 1.084  | OOM            | OOM                         | OOM                                                               |
| [BNG(libras_move)](openml.org/t/7327)        | 1956                        | 4.2 | 2.51   | 1922           | 97.6                        | 2.53                                                              |
| [BNG(satellite_image)](openml.org/t/7326)    | 334                         | 1.6                                                               | 0.731          | 337                         | 10.0 | 0.721  |
| [COMET_MC](openml.org/t/14949)               | 44                          | 1.0 | 0.0615  | 47             | 5.0                         | 0.0662                                                            |
| [friedman1](openml.org/t/361939)             | 275                         | 4.2 | 1.047   | 278            | 5.1                         | 1.487                                                             |
| [poker](openml.org/t/10102)                  | 38                          | 0.6 | 0.256   | 41             | 1.2                         | 0.722                                                             |
| [subset_higgs](openml.org/t/361955)          | 868                         | 10.6 | 0.420  | 870            | 24.5                        | 0.421                                                             |
| [BNG(autoHorse)](openml.org/t/7319)          | 107                         | 1.1 | 19.0    | 107            | 3.2                         | 20.5                                                              |
| [BNG(pbc)](openml.org/t/7318)                | 48                          | 0.6 | 836.5   | 51             | 0.2                         | 957.1                                                             |
| average                                      | 465                         | 3.9                                                               | -              | 464                         | 19.7                                                              | -              |

PerpetualBooster outperformed AutoGluon on 8 out of 10 datasets, training equally fast and inferring 5x faster. The results can be reproduced using the automlbenchmark fork [here](https://github.com/deadsoul44/automlbenchmark).

Github: https://github.com/perpetual-ml/perpetual",mutlu_simsek,1h52zk8,https://reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,https://www.reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,2024-12-02 19:12:26,18,0.88,18,0,0,0,0,False,False,True,False,False,Project,self,t3_1h52zk8
MachineLearning,[P] Label Studio Activation Troubles,"I'm trying to run Label Studio because I was told once that it's more of a modern program used for labeling images, which I plan to do for a personal project. However, I've been dealing with headache after headache trying to get it to run, since it complains about \_psycopg. I have tried installing Python and PostgreSQL (since I think there's a dependency between the two) multiple times, looking into issues with libpq.dll, and so on, but it's not working. Anyone have any idea on how to fix an issue like this, or should I look into a different labeling program?",NuDavid,1h5gtqk,https://reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,https://www.reddit.com/r/MachineLearning/comments/1h5gtqk/p_label_studio_activation_troubles/,2024-12-03 06:02:59,1,1.0,1,0,1,0,0,False,False,True,False,False,Project,self,t3_1h5gtqk
MachineLearning,[R] ImageFolder🚀: Autoregressive Image Generation with Folded Tokens,"https://preview.redd.it/2olpl72q6i4e1.png?width=911&amp;format=png&amp;auto=webp&amp;s=b54d91736543906b6a71102a09dc04883033d795

&gt;Image tokenizers are crucial for visual generative models, e.g., diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve the image reconstruction quality. However, tokenizers with longer token lengths are not guaranteed to achieve better generation quality. There exists a trade-off between reconstruction and generation quality regarding token length. In this paper, we investigate the impact of token length on both image reconstruction and generation and provide a flexible solution to the tradeoff. We propose ImageFolder, a semantic tokenizer that provides spatially aligned image tokens that can be folded during autoregressive modeling to improve both generation efficiency and quality. To enhance the representative capability without increasing token length, we leverage dual-branch product quantization to capture different contexts of images. Specifically, semantic regularization is introduced in one branch to encourage compacted semantic information while another branch is designed to capture the remaining pixel-level details. Extensive experiments demonstrate the superior quality of image generation and shorter token length with ImageFolder tokenizer.

Paper: [https://arxiv.org/abs/2410.01756](https://arxiv.org/abs/2410.01756)  
Code: [https://github.com/adobe-research/ImageFolder](https://github.com/adobe-research/ImageFolder)",xternalz,1h55x1i,https://reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,https://www.reddit.com/r/MachineLearning/comments/1h55x1i/r_imagefolder_autoregressive_image_generation/,2024-12-02 21:11:25,5,0.78,5,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/-S_lEVvuVpw96JtQP6vCgCd6QVqkMlv4dix0h6EtqFY.jpg,t3_1h55x1i
MachineLearning,[R] RuleOpt v.1.1: Optimization-Based Rule Learning for Classification,"**Paper**: [https://arxiv.org/abs/2104.10751](https://arxiv.org/abs/2104.10751)

**Package:** [https://github.com/sametcopur/ruleopt](https://github.com/sametcopur/ruleopt)

**Documentation:** [https://ruleopt.readthedocs.io/](https://ruleopt.readthedocs.io/)

RuleOpt is an optimization-based rule learning algorithm designed for classification problems. Focusing on scalability and interpretability, RuleOpt utilizes linear programming for rule generation and extraction.

The Python library ruleopt is capable of extracting rules from ensemble models, and it also implements a novel rule generation scheme. The library ensures compatibility with existing machine learning pipelines, and it is especially efficient for tackling large-scale problems.

Here are a few highlights of ruleopt:

* **Efficient Rule Generation and Extraction**: Leverages linear programming for scalable rule generation (stand-alone machine learning method) and rule extraction from trained random forest and boosting models.
* **Interpretability**: Prioritizes model transparency by assigning costs to rules in order to achieve a desirable balance with accuracy.
* **Integration with Machine Learning Libraries**: Facilitates smooth integration with well-known Python libraries scikit-learn, LightGBM, and XGBoost, and existing machine learning pipelines.
* **Extensive Solver Support**: Supports a wide array of solvers, including *Gurobi*, *CPLEX* and *OR-Tools*.

  
With the latest version update, RuleOpt is now fast even with the free solver OR-Tools, even on large datasets! In the graph below, you can see how the new version performs in terms of runtime compared to the previous version.

[Training Times v1.0 vs v1.1](https://preview.redd.it/ev5u5m4bjf4e1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=e38ea3c168ece7ad6660153a6073f8064ac85d83)

  
We’d love to hear your feedback, questions, or any other inquiries you may have!",zedeleyici3401,1h4tzd0,https://reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,https://www.reddit.com/r/MachineLearning/comments/1h4tzd0/r_ruleopt_v11_optimizationbased_rule_learning_for/,2024-12-02 12:36:23,10,1.0,10,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/HhFHG_WRfu3w1cHy2TmWE1t-ZUve2F-IM_C2fVLZwqo.jpg,t3_1h4tzd0
MachineLearning,[P] multi feature linear regression code in python not giving the correct solution (or any solution for that matter)...,"linear regression using gradient descent:

    def multiFeatureLinearRegression(x, y, alpha, iterations):
        w = [0.0] * len(x[0])
        b = 0.0
        m = len(x)
        
        for it in range(iterations):
            w_temp = [0.0] * len(x[0])
            b_temp = 0.0
            for i in range (len(x)):
                prediction = b
                for j in range(len(x[i])):
                    prediction += w[j] * x[i][j]
                error = y[i] - prediction
                
                b_temp += error
                
                for j in range(len(x[i])):
                    w_temp[j] += error * x[i][j]
            
            for i in range(len(x[0])):
                w[i] -= alpha * (2.0 / m) * w_temp[i]
            b -= alpha * (2.0 / m) * b_temp
            
        return w, b

main body:

    data = [    [15, 3, 20],  # [House Size (sq. ft.), Bedrooms, Age of House (years)]
        [20, 4, 15],
        [17, 3, 25],
        [22, 4, 10],
        [13, 2, 30],
        [18, 3, 20],
        [24, 4, 5],
        [16, 3, 18]
        ]
    
    dataY = [300, 400, 350, 450, 200, 370, 500, 310]
    
    alpha = 0.01
    iterations = 100000
    w, b = multiFeatureLinearRegression(data, dataY, alpha, iterations)
    
    print(""Weights (w):"", w)
    print(""Bias (b):"", b)

I am trying to implement multi feature linear regression and for some reason the output for the weight and bias is coming out to be:

    Weights (w): [-inf, -inf, -inf]
    Bias (b): -inf

I have no idea why this is happening..  
Can you spot what I am doing wrong here?  
could it be because I have not applied any normalization or something?",silveroburn,1h5jyaj,https://reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,https://www.reddit.com/r/MachineLearning/comments/1h5jyaj/p_multi_feature_linear_regression_code_in_python/,2024-12-03 09:51:24,0,0.23,0,0,5,0,0,False,False,True,False,False,Project,self,t3_1h5jyaj
MachineLearning,[R] Best chunking method for PDFs with complex layout?,"I am working on a RAG based PDF Query system , specifically for complex PDFs that contains multi column tables, images, tables that span across multiple pages, tables that have images inside them.

I want to find the best chunking strategy for such pdfs.

Currently i am using RecursiveCharacterTextSplitter. What worked best for you all for complex PDF?

",ElectronicHoneydew86,1h4pkmh,https://reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,https://www.reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,2024-12-02 07:24:21,17,0.92,17,0,5,0,0,False,False,True,False,False,Research,self,t3_1h4pkmh
MachineLearning,[D] Handle varying output dimension in Graph Neural Networks?,"I have a question about handling varying output dimensions in **Graph Neural Networks (GNNs)** during training. I'm working with **a combined graph** (merging task and compute graphs), where the structure resembles the task graph, but with compute node information integrated into the features. Since both the task graph and compute graph (nodes count) can vary, I'm using a feedforward layer to transform the node and edge features into a fixed hyperparameter embedding dimension. However, the dataset contains instances with **different numbers of compute nodes**. For example, one instance (A) might have 5 compute nodes, while another instance (B) might have 7 compute nodes. Given that this is a scheduling task using GNNs, the output dimension must match the number of compute nodes, as tasks are assigned to these nodes. I'm wondering how to handle varying output dimensions in GNNs and if there are any standard approaches to manage this kind of variation. Thanks!",bipulthapa,1h4uyn9,https://reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,https://www.reddit.com/r/MachineLearning/comments/1h4uyn9/d_handle_varying_output_dimension_in_graph_neural/,2024-12-02 13:29:07,5,0.78,5,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h4uyn9
MachineLearning,[R] Feature Generation,"I apologize in advance for a possibly silly question.

I'm trying to replicate a research paper wherein some ML models have been trained to condition monitoring of a metal cutting tool, so my question is about feature generation.

Let's suppose I have a dataframe with 6 signals (features) and the rows are measurements at a certain time. To generate a new feature, say RMS value, I just need to take 6 measurements and calculate the RMS value of 6 numbers for each row? Will this be my new feature?

Thanks.

https://preview.redd.it/dsjfcapthe4e1.png?width=560&amp;format=png&amp;auto=webp&amp;s=b22c007b2d2a73612280d0ecfa6556b68b793a17

",su_25_frogfoot,1h4qwt4,https://reddit.com/r/MachineLearning/comments/1h4qwt4/r_feature_generation/,https://www.reddit.com/r/MachineLearning/comments/1h4qwt4/r_feature_generation/,2024-12-02 09:04:55,5,0.79,5,0,0,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/m56www4G1RbpNxO1S_wnA793xMTia8-V1pfcOH285aE.jpg,t3_1h4qwt4
MachineLearning,[D] Anthropic AI fellow/residents- any new grads/entry-level people accepted?,"Hello. Are entry-level or new grads accepted into the Anthropic fellowship or resident programs? Past people who were accepted, what was your CV and experience like?",geekgeek2019,1h4e0ah,https://reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,https://www.reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,2024-12-01 21:25:59,30,0.7,30,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h4e0ah
MachineLearning,[D] How to handle varying Feature Dimensions in Graph Neural Networks training?,"I have a question about handling datasets with varying feature dimensions in Graph Neural Network training. For example, in one training instance (let's call it Dataset A), the node features have a dimension of 4, and the edge features have a dimension of 16. In another instance (Dataset B), the node features have a dimension of 5, and the edge features have a dimension of 25. Other datasets may have different feature dimensions as well.

What are the standard methods used to handle varying feature dimensions for each instance when training a GNN model with such datasets? I would appreciate any guidance or direction on how to approach this. Thanks!",bipulthapa,1h4dbvi,https://reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,https://www.reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,2024-12-01 20:57:35,23,0.93,23,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h4dbvi
MachineLearning,[P] Promptwright - Open source project to generate large synthetic datasets using an LLM (local or hosted),"Hey r/machinelearning,

[Promptwright](https://github.com/StacklokLabs/promptwright), a free to use *open source* tool designed to easily generate synthetic datasets using either local large language models or one of the many hosted models (OpenAI, Anthropic, Google Gemini etc)

Key Features:

\* Multiple LLM Providers Support: Works with most LLM service providers and LocalLLM's via Ollama, VLLM etc

\* Configurable Instructions and Prompts: Define custom instructions and system prompts in YAML, over scripts as before.

\* Command Line Interface: Run generation tasks directly from the command line

\* Push to Hugging Face: Push the generated dataset to Hugging Face Hub with automatic dataset cards and tags

Here is an example dataset created with promptwright on this latest release:

[https://huggingface.co/datasets/stacklok/insecure-code/viewer](https://huggingface.co/datasets/stacklok/insecure-code/viewer)

This was generated from the following template using \`mistral-nemo:12b\`, but honestly most models perform, even the small 1/3b models.

    system_prompt: ""You are a programming assistant. Your task is to generate examples of insecure code, highlighting vulnerabilities while maintaining accurate syntax and behavior.""
    
    topic_tree:
      args:
        root_prompt: ""Insecure Code Examples Across Polyglot Programming Languages.""
        model_system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        tree_degree: 10  # Broad coverage for languages (e.g., Python, JavaScript, C++, Java)
        tree_depth: 5  # Deep hierarchy for specific vulnerabilities (e.g., SQL Injection, XSS, buffer overflow)
        temperature: 0.8  # High creativity to diversify examples
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
      save_as: ""insecure_code_topictree.jsonl""
    
    data_engine:
      args:
        instructions: ""Generate insecure code examples in multiple programming languages. Each example should include a brief explanation of the vulnerability.""
        system_prompt: ""&lt;system_prompt_placeholder&gt;""  # Will be replaced with system_prompt
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        temperature: 0.9  # Encourages diversity in examples
        max_retries: 3  # Retry failed prompts up to 3 times
    
    dataset:
      creation:
        num_steps: 15  # Generate examples over 10 iterations
        batch_size: 10  # Generate 5 examples per iteration
        provider: ""ollama""  # LLM provider
        model: ""mistral-nemo:12b""  # Model name
        sys_msg: true  # Include system message in dataset (default: true)
      save_as: ""insecure_code_dataset.jsonl""
    
    # Hugging Face Hub configuration (optional)
    huggingface:
      # Repository in format ""username/dataset-name""
      repository: ""hfuser/dataset""
      # Token can also be provided via HF_TOKEN environment variable or --hf-token CLI option
      token: ""$token""
      # Additional tags for the dataset (optional)
      # ""promptwright"" and ""synthetic"" tags are added automatically
      tags:
        - ""promptwright""

We've been using it internally for a few projects, and it's been working great. You can process thousands of samples without worrying about API costs or rate limits. Plus, since everything runs locally, you don't have to worry about sensitive data leaving your environment.

The code is Apache 2 licensed, and we'd love to get feedback from the community. If you're doing any kind of synthetic data generation for ML, give it a try and let us know what you think!

Links:

Checkout the [examples](https://github.com/StacklokLabs/promptwright/tree/main/examples) folder , for examples for generating code, scientific or creative ewr

Would love to hear your thoughts and suggestions, if you see any room for improvement please feel free to raise and issue or make a pull request.",zero_proof_fork,1h4bcz2,https://reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,https://www.reddit.com/r/MachineLearning/comments/1h4bcz2/p_promptwright_open_source_project_to_generate/,2024-12-01 19:33:47,16,0.83,16,0,5,0,0,False,False,True,False,False,Project,self,t3_1h4bcz2
MachineLearning,[R] Queries on DeepAR Framework in AWS Sagemaker,"Hi,

I'm trying to implement deepAr for various stores to predict futures sales (each store with ~10k SKU of different products). Due to sheer size of the SKU I wouldn't be able to just do only single training for all the data at once. I'm thinking to train it by store.

1. How do I do parallelism in AWS for the training purpose? Each store training process would take up to 30mins;
2. How to deal with unseen SKUs which are not present in the data?

Thanks.",skw1990,1h4gwxy,https://reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,https://www.reddit.com/r/MachineLearning/comments/1h4gwxy/r_queries_on_deepar_framework_in_aws_sagemaker/,2024-12-01 23:33:04,5,0.79,5,0,0,0,0,False,False,True,False,False,Research,self,t3_1h4gwxy
MachineLearning,[R] Sources: Reasons why KG outperformes RD in Retrievers?,"Are there any sources discussing WHY Retriever work better with KG in contrast to RD? I find it super intuitive to say its better because in knowledge graphs we have more semantic structure and relations are discovered effeciently. In my mind its ""of course the graph is richer/more dense"" but when collaborated on a paper, it struck me that I wasnt able to justify that claim. I found no source whatsoever that actually explained why that might be the case.

The only source i got was this one:  
[https://arxiv.org/abs/2311.07509](https://arxiv.org/abs/2311.07509)

also here in [](https://www.reddit.com/r/LocalLLaMA/) sub last year:  [https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a\_benchmark\_to\_understand\_the\_role\_of\_knowledge/](https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a_benchmark_to_understand_the_role_of_knowledge/)

So all we we're able to say was ""We justify our decision because KG works better then RD \[source to benchmark paper\]""

I would have loved to discuss why exactly KG are better suited and give arguments about information density, semantic strutuce or the better selection of related entities. But everything I found were only articles that threw around wild claims or pointed out easier/native implementation, which technically could also be achieved with RD.

Can anyone point me to sources? would love to read an in-depth discussion on the reasons of better performance.",PopPsychological4106,1h447eu,https://reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,https://www.reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,2024-12-01 14:17:17,28,0.69,28,0,22,0,0,False,False,True,False,False,Research,self,t3_1h447eu
MachineLearning,"[R] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",,moschles,1h40wms,https://reddit.com/r/MachineLearning/comments/1h40wms/r_qwenvl_a_versatile_visionlanguage_model_for/,https://storage.prod.researchhub.com/uploads/papers/2023/12/25/2308.12966.pdf,2024-12-01 10:59:28,13,0.93,13,0,0,0,0,False,False,False,False,False,Research,default,t3_1h40wms
MachineLearning,[Project] Noema – A Declarative AI Programming Library,,Super_Dependent_2978,1h46341,https://reddit.com/r/MachineLearning/comments/1h46341/project_noema_a_declarative_ai_programming_library/,/r/LocalLLaMA/comments/1h3d4d8/noema_a_declarative_ai_programming_library/,2024-12-01 15:46:35,4,0.64,4,0,1,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/Bke830yFsuJxUn2gkbNe1tpnLzRcSugASe-RSwzFQu0.jpg,t3_1h46341
MachineLearning,Augmentation for Images with ROI [D],"I have an Image with roi (x\_min,y\_min,x\_max,y\_max). I want do Random flip,rotate,skew, translate ,etc.. with torchvison. what are the different ways in which you can transform the roi respectively in order to match with the augmented image ?",Brief_Papaya121,1h41z2x,https://reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,https://www.reddit.com/r/MachineLearning/comments/1h41z2x/augmentation_for_images_with_roi_d/,2024-12-01 12:11:25,4,0.7,4,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h41z2x
MachineLearning,What's the best Open Source Image-Upscaling Model? [Discussion],"I'm using [Playground-v2.5-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) to make some images for YouTube thumbnails. I'm really happy with the results:

[1024x1024 base image of mars base.](https://preview.redd.it/uuo4sdgwp44e1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=84d61bf4d7fbf2457df1037e95603166390efa12)

But I would like the image to be 1920x1080 pixels, and my only options are 1024x1024, or 1280x720 pixels. At the moment, I can get to 1920x1080 with Photoshop's outpainting:

[1920x1080 outpainted image of mars base.](https://preview.redd.it/07tt5ix4q44e1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=cdfb7cefa8d2bb4f187d2ab6b86aaba17596506a)

This is okay, but photoshops outpainting is manual and has a fairly significant quality drop. Ideally, I would generate an image in 1280x720 then upscale to 1920x1080 programmatically.

I've heard of the following models:

* Real-ERSGAN
* Waifu2
* SRGAN

But before I jump into any of them, what open-source model is generally considered best to achieve this? I have an RTX 3060 12GB of VRAM.",FPGA_Superstar,1h3qcon,https://reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,https://www.reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,2024-12-01 00:13:57,41,0.92,41,0,8,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/ROcJ4oUu0I-1wtjwdAycnGfQ3s8tbIsJdyufqyhsVyI.jpg,t3_1h3qcon
MachineLearning,[D] Seeking Advice on Machine Learning Models for Generating Seamless 360° Images,"Hi everyone,

I’m working on a project that involves creating 360° images, and I’m running into some challenges. The goal is to generate seamless 360° panoramas without visible edges or artifacts where the image wraps around.

I’m wondering if there are any machine learning models, techniques, or tools that are particularly well-suited for this task. Specifically, I’m looking for something that can:

* Ensure continuity at the edges of the 360° image.
* Handle different textures and patterns without noticeable distortions.
* Be trained or fine-tuned on my custom dataset (if needed).

I’ve explored GANs like StyleGAN and diffusion models, but I’m not sure if they can handle the edge continuity issue out of the box. Has anyone worked on a similar problem or knows of a good starting point?

Any suggestions, resources, or insights would be greatly appreciated! Thanks in advance!",Deep_Land_4093,1h48950,https://reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,https://www.reddit.com/r/MachineLearning/comments/1h48950/d_seeking_advice_on_machine_learning_models_for/,2024-12-01 17:22:26,0,0.43,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h48950
MachineLearning,[P] A complete transformer model built in Excel,,Revolutionary-Way290,1h3hj6j,https://reddit.com/r/MachineLearning/comments/1h3hj6j/p_a_complete_transformer_model_built_in_excel/,https://x.com/ProfTomYeh/status/1859282491955130452,2024-11-30 17:25:45,19,0.95,19,0,5,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/JFO-VdcjE87RX0w6rQ5kwyN7HblXjJoN1QbFc-1p18M.jpg,t3_1h3hj6j
MachineLearning,[D] Modern use-cases for RNNs?,"The discussion can be twofold.
1)What are in your opinion some tasks, for the personal projects scale, where you think RNNs close to traditional implementations (LSTM, GRU) are still the best starting and ending point? Especially compared to transformers.

In small time-series forecasting settings I can see a GRU being more convenient than a Transformer probably, but I am interested also in tasks where inputs are sequences of symbols or measures, but outputs maybe not.

The main goal is to play with LSTM and GRU variants (eg minGRU) on datasets where it makes sense, might do tiny-Shakespeare but it doesn't warm my heart...

2) do you think there are sequential tasks and settings where RNNs are not only the more natural option according to our intuition, but actually the only theoretically or experimentally available option to make do, compared to Transformers or 1D CNNs etc?",Sad-Razzmatazz-5188,1h38ym2,https://reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,https://www.reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,2024-11-30 09:22:32,51,0.89,51,0,20,0,0,False,False,True,False,False,Discussion,self,t3_1h38ym2
MachineLearning,[Discussion] Advice Needed: Rejected from COLING 2025 – Which Conference Should I Target Next?,"
I got a rejection from COLING 2025 with review scores of 4, 3, 3. I’m revising the manuscript and looking for advice on the next best NLP conference to target. Any suggestions for similar top-tier venues?

Thanks!",Cold-Traffic-7586,1h3igva,https://reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,https://www.reddit.com/r/MachineLearning/comments/1h3igva/discussion_advice_needed_rejected_from_coling/,2024-11-30 18:08:01,5,0.78,5,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h3igva
MachineLearning,[D] What's the fastest object detection model?,"Hi, I'm working on a project that needs object detection. The task itself isn't complex since the objects are quite clear, but speed is critical. I've researched various object detection models, and it seems like almost everyone claims to be ""the fastest"". Since I'll be deploying the model in C++, there is no time to port and evaluate them all.

I tested YOLOv5/v5Lite/8/10 previously, and YOLOv5n was the fastest. I ran a simple benchmark on an Oracle ARM server (details [here](https://github.com/Avafly/YOLOv5-ncnn-OpenVINO-MNN-ONNXRuntime-OpenCV-CPP?tab=readme-ov-file#simple-benchmarks-on-m1-mac-and-arm-linux)), and it processed an image with 640 target size in just 54ms. Unfortunately, the hardware for my current project is significantly less powerful, and meanwhile processing time must be less than 20ms. I'll use something like quantization and dynamic dimension to boost speed, but I have to choose the suitable model first.

Has anyone faced a similar situation or tested models specifically for speed? Any suggestions for models faster than YOLOv5n that are worth trying?",Knok0932,1h362dq,https://reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,https://www.reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,2024-11-30 06:00:40,45,0.98,45,0,39,0,0,False,False,True,False,False,Discussion,self,t3_1h362dq
MachineLearning,[P] TIME-MOE: Billion-Scale Time Series Forecasting with Mixture-of-Experts,"**Time-MOE** is a 2.4B parameter open-source time-series foundation model using **Mixture-of-Experts (MOE)** for zero-shot forecasting.

You can find an analysis of the model [here](https://aihorizonforecast.substack.com/p/time-moe-billion-scale-time-series)",nkafr,1h3j1cm,https://reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,https://www.reddit.com/r/MachineLearning/comments/1h3j1cm/p_timemoe_billionscale_time_series_forecasting/,2024-11-30 18:33:59,2,0.75,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1h3j1cm
MachineLearning,[D] Hinton and Hassabis on Chomsky’s theory of language,"I’m pretty new to the field and would love to hear more opinions on this. I always thought Chomsky was a major figure on this but it seems like Hinton and Hassabis(later on) both disagree with it. Here: https://www.youtube.com/watch?v=urBFz6-gHGY (longer version: https://youtu.be/Gg-w_n9NJIE)

I’d love to get both an ML and CogSci perspective on this and more sources that supports/rejects this view.

Edit: typo + added source.",giuuilfobfyvihksmk,1h2mkye,https://reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,2024-11-29 14:10:12,120,0.92,120,0,104,0,0,False,False,True,False,False,Discussion,self,t3_1h2mkye
MachineLearning,[N][R] Models are what they eat: automatic data curation for LLMs,"Sharing our most recent work at [DatologyAI](http://www.datologyai.com). Models are what they eat, and our mission is to make data curation for training large models as effective and easy as possible. 

Combining a bevy of approaches, including heuristic filters, model-based filters, embedding based curation, synthetic data, target distribution matching, and mixing ratios, we were able to massively improve training efficiency, performance, and inference efficiency. 

Comparing to our baseline and starting dataset -- exact deduplicated RedPajamav1, we can: 

* Reach the same performance 7.7x faster (and 3.4x faster than DCLM)
* Improve performance across benchmarks by 8.5% (and by 4.4% over DCLM)
* Train models with fewer than half the parameters which outperform larger models by &gt;5% 

Check out our [high-level results here](https://www.datologyai.com/post/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation), and if you want all the nitty-gritty details, check out our[ technical deep dive](https://www.datologyai.com/post/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset).",arimorcos,1h2qmol,https://reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,2024-11-29 17:15:57,15,0.75,15,0,1,0,0,False,False,True,False,False,News,self,t3_1h2qmol
MachineLearning,[D] Molecular Dynamics and Machine Learning Build,"Hey guys, so I do a lot of molecular dynamics and am starting to push into the ML space with that and genome/multi-omics sort of stuff. I’m building a workstation and with my budget I’m looking at 2x A6000 or 2x 5000 Ada. Both work great for molecular dynamics, but I’m trying to figure out my best option for ML. The A6000 has 48gb vram and nvlink, but the 5000 Ada is newer and substantially faster and 32Gb VRAM per card is no slouch either. Any advice? ",Mdgoff7,1h2x4ar,https://reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,https://www.reddit.com/r/MachineLearning/comments/1h2x4ar/d_molecular_dynamics_and_machine_learning_build/,2024-11-29 22:04:19,2,0.67,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h2x4ar
MachineLearning,"[D] How does VQ-VAE disentangle, if it does at all?","I currently use a BetaTC-VAE, which does an excellent job at disentangling, knowing that VAE can slightly disentangle since for the model it's easier to get a lower KL loss if the variables are dissentanlged, the beta term make this beta times more important, and total correlation and mutual information loss push for total disentanglement, but in VQ-VAE there is no (major) disentanglement, only a codebook, and discrete outputs. Could the discrete latent given by the codebook be disentangled? If not, is there any paper on disdentangling VQ-VAE? I have an environment where disentangled latent spaces provide better reconstruction than continous latent spaces ",ZazaGaza213,1h2epzx,https://reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,2024-11-29 05:33:10,35,0.92,35,0,15,0,0,False,False,True,False,False,Discussion,self,t3_1h2epzx
MachineLearning,[D] COLING 2025 Final Acceptances - Is it not out yet?,"Is the final acceptance out? I am not seeing it yet on Softconf. Had a paper with (5,4) (4,4) (4,4) in reviews.",UnhappyPrior6570,1h2kfic,https://reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,https://www.reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/,2024-11-29 12:10:32,8,0.79,8,0,44,0,0,False,False,True,False,False,Discussion,self,t3_1h2kfic
MachineLearning,[D] Theory behind modern diffusion models,"Hi everyone,

I recently attended some lectures at university regarding diffusion models. Those explained all the math behind the original DDPM (Denoiding Diffusion Probabilistic Model) in great detail (especially in the appendices), actually better than anything else I have found online. So it has been great for learning the basics behind diffusion models (slides are available in the link in the readme here if you are interesed: https://github.com/julioasotodv/ie-C4-466671-diffusion-models)

However, I am struggling to find resources with similar level of detail for modern approaches—such as flow matching/rectified flows, how the different ODE solvers for sampling work, etc. There are some, but everything that I have found is either quite outdated (like from 2023 or so) or very superficial—like for non-technical or scientific audiences.

Therefore, I am wondering: has anyone encountered a good compendium of theoretical eplanations beyond the basic diffusion model (besides the original papers)? The goal is to let my team deep dive into the actual papers should they desire, but giving 70% of what those deliver in one or more decent compilations.

I really believe that SEO is making any search a living nightmare nowadays. Either that or my googling skills are tanking for some reason.

Thank you all!",bgighjigftuik,1h1vxe1,https://reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,2024-11-28 13:27:28,228,0.99,228,0,26,0,0,False,False,True,False,False,Discussion,self,t3_1h1vxe1
MachineLearning,"""[P]""Static variable and dynamic variable tables in RFM ","



I am creating a prediction model using random forest. But I don't understand how the model and script would consider both tables loaded in as dataframes.


What's the best way to use multiple tables with a Random Forest model when one table has static attributes (like food characteristics) and the other has dynamic factors (like daily health habits)?

Example:
I want to predict stomach aches based on both the food I eat (unchanging) and daily factors (sleep, water intake).

Tables:
 * Static: Food name, calories, meat (yes/no)
 * Dynamic: Day number, good sleep (yes/no), drank water (yes/no)


How to combine these tables in a Random Forest model? Should they be merged on a unique identifier like ""Day number""?
",peyott100,1h2oe69,https://reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,https://www.reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/,2024-11-29 15:36:10,1,0.67,1,0,3,0,0,False,False,True,False,False,Project,self,t3_1h2oe69
MachineLearning,[R] Recursive Methods for interpolation between vector fields ( Known and Unknown),"Hello everyone Does anything of the next makes sense?  
I Have been posting on Learning first ( also on math and number theory ) , but I think is a bit more math theory than ML but it does have to do with how the data is interpolated so I am unsure. 

( I hope I am not breaking rule 5 with my links ) 

this will be the interpolation of the data ( Via organized vector field levels )  before the generative process starts, but because its recursive, the generative process can happen on inside the iteration too 

its there a model I can use ? And if someone understand the math, can I get some papers or things I could follow or just is learning and reading now?

I am a little lost and need some help ( I organized my question with chatGPT to make it understandable so bare in mind if there is some odd work here and there, I am on the I am going a bit mental stage )

I think this is dealing with machine learning problems that have been solved between interpolation of point could on space that have recursive data ( mapping and data organization )

I've been developing a concept that merges artistic visualization with advanced mathematical interpolation techniques inspired by the Mandelbrot set. Coming from a creative background, I've ventured into creating what I believe could be a **recursive Mandelbrot predictive method**  for manipulating vector fields. I'm eager to understand if this approach already exists and to gather resources or similar algorithms to explore further and test my ideas.

I will add some things like this latter to test segmentation models for the recursiveness [https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear\_algebra\_project\_i\_implemented\_a\_kmeans/](https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/)

**REFERENCE IMAGES**  
everything is based on recursive by resolution with inverse square distance from the origin point

**Mandelbroth**  
[https://en.wikipedia.org/wiki/Mandelbrot\_set#/media/File:Juliacycles1.png](https://en.wikipedia.org/wiki/Mandelbrot_set#/media/File:Juliacycles1.png)

**Conceptual model** ( The mandelbroth guidance happens just on the altered time pulling agent ) ( Orange )  
[Single Vector interpretation and prediction stream of the Pull of the mandelbrot agent](https://cdn.discordapp.com/attachments/457637053581230100/1311677830966284298/chrome_7KNKgsO6Fm.png?ex=6749baac&amp;is=6748692c&amp;hm=55a7809cd5bb402a06fdea1edbad4eb1ff21f15d1a3303efee9b797eef0f5839&amp;)

**Conceptual Model 2d sim**  
[Representation of the predictiveness](https://media.discordapp.net/attachments/457637053581230100/1311668738139099166/ShareX_11AweoaOUp.png?ex=6749b234&amp;is=674860b4&amp;hm=7d660210c3678b8f1bd0804ee4f2f2425a0c1dc9aea18bdf39e86ba81d9ad2e8&amp;=&amp;format=webp&amp;quality=lossless&amp;width=295&amp;height=350) as mandelbrot

Representation of functional interpolation of agents via Mandelbroth ( non recursive )

**Conceptual Simulation model 2d sim ( making the mandelbroth )**  
[Image non animated](https://media.discordapp.net/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;=&amp;format=webp&amp;quality=lossless)[ANIMATED VIDEO DOWNLOAD ( CLEAN FILE )](https://cdn.discordapp.com/attachments/457637053581230100/1311678672582742147/2f1e8c125b144c69a49d0fc1d256201f.mov?ex=6749bb75&amp;is=674869f5&amp;hm=a035651102578ff6e65cf47fada0f9706db886beb1b54225a86283f16da4b59c&amp;)

**Conceptual layering**  
[Layering of 3 tiers via inverse square distance on a vector field ( currently surface) but can be world](https://media.discordapp.net/attachments/457637053581230100/1311404062931030086/image.png?ex=67496475&amp;is=674812f5&amp;hm=56bf4ff3988163f35c1ef24227645beaa6d11ab1b0330d094dc3dccbad6ce696&amp;=&amp;format=webp&amp;quality=lossless&amp;width=378&amp;height=350)

**recursiveness concept**  
[Applied recursiveness auto generation based on surface vector field ](https://media.discordapp.net/attachments/457637053581230100/1311404209278685184/image.png?ex=67496498&amp;is=67481318&amp;hm=e548fb4615d48adea59f835a3a76a6a203c5946ea6d356eff2c7298252d3de41&amp;=&amp;format=webp&amp;quality=lossless&amp;width=927&amp;height=831)( no prediction applied )

# The Concept

Imagine a system where the interpolation between data points isn't limited to traditional methods like **lerp** (linear interpolation) or **slerp** (spherical linear interpolation). Instead, it employs a **pseudo vector field Mandelbrot slerp**, allowing vectors to be guided from a base state (reality) to a target state (altered time) within a Mandelbrot-inspired vector field. This method is recursive, meaning multiple layers of calculations are applied to refine the interpolation continuously.

# Key Components:

1. **Reality (Ground Truth):** Represents the current state of the system, serving as the foundational dataset.
2. **Agents of Change (Vectors of Closest Influence):** These act as pull forces influencing the direction and magnitude of interpolation.
3. **State (Ground Truth Prediction Model):** Utilizes the current data to predict future states based on the influences of the agents.
4. **Altered Time (Goal):** The desired target state, akin to a Mandelbrot-type location on the outer range of the vector field.

# Interpolation Method

The interpolation technique extends beyond simple linear methods by incorporating the complexity and fractal nature of the Mandelbrot set. Here's how it functions:

* **Guided Vectors:** Vectors transition from reality towards altered time, following paths influenced by a Mandelbrot-like vector field.
* **Recursive Layers:** Multiple layers of interpolation allow for increasingly refined calculations, enhancing accuracy and adaptability.
* **Dynamic Intensity:** The closer the interpolation is to reality, the more intense and detailed the calculations become, while the vector field simplifies as it moves towards altered time.

# Theoretical Foundation

The core idea revolves around mapping and adjusting Mandelbrot-inspired vectors to facilitate interpolation between recursively organized data banks. This approach aims to:

* **Capture Complex Patterns:** Leverage the self-similar, fractal nature of Mandelbrot sets to identify and utilize intricate patterns within the data.
* **Enhance Predictive Capability:** Recursive calculations allow for continual refinement of projections, improving predictive accuracy over time.
* **Achieve Real-Time Adaptability:** Dynamically adjust vectors to align with specific goals, similar to how a car's performance might be modulated in real-time to achieve optimal racing outcomes.

# Visual Analogy

Think of this system as calculating the ""ghost"" position of a car in a racing game like *Need for Speed*:

* **Acceleration and Braking:** Based on historical and current data, determining when to accelerate or brake to achieve the best performance.
* **Engine Adjustments:** Modifying the system's parameters in real-time to align with the target state, ensuring the system reaches its goal efficiently.
* **Dynamic Modulation:** Continuously adjusting these actions to meet the desired ""goal time,"" always operating within physical (mathematical) constraints.

# Questions for the Community

1. **Does This Technology Exist?** Is my approach accurately described as a **recursive Mandelbrot predictive method** for vector field interpolation? Are there existing models or research that align closely with this concept?
2. **Resources and References:** If similar technologies or algorithms exist, could you recommend any resources, papers, or specific Mandelbrot-like algorithms that I can study or begin testing with?
3. **Mathematical Validation:** Given that my approach stems from an artistic visualization perspective, what mathematical frameworks or theories should I explore to formalize and validate this method?

# Additional Context

For a visual representation of my model and its applications, you can refer to the following links:

* **Visual Model:** [LinkedIn Visual Model](https://www.linkedin.com/feed/update/urn:li:activity:7267834154631708672/)
* **Use Case Example:** [LinkedIn Use Case](https://www.linkedin.com/posts/jesusfc14_i-think-i-am-reaching-a-clarity-moment-the-activity-7267823963068628992--qA7?utm_source=share&amp;utm_medium=member_desktop)

*(Please note that these links provide additional visual context to help illustrate the concept.)*

**Thank you for taking the time to read through my concept! I'm looking forward to your insights, validations, and any resources you can share to help me advance this idea.**

all this tech is currently under Creature Garage umbrella but I have ownership of the creative driver of the idea so that should be fine for me to post but I reached a moment that I will need help for some of the most advanced math implementations

I am using some concepts that sound really far and advanced but currently my implementation is mostly based on recursiveness the prediction agent will come to function once I have my full set of data to make a test",jesusfc,1h2io35,https://reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,https://www.reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/,2024-11-29 10:08:00,1,0.55,1,0,12,0,0,False,False,True,False,False,Research,self,t3_1h2io35
MachineLearning,[D] Most important papers in implicit regularisation,"Hi guys

I'm getting into machine learning, especially on the theoretical side, and I'm curious to learn more about why neural networks tend to generalise so well, so I'm hoping to read some papers about this. As far as I'm aware, the first big paper on the topic was 'Understanding deep learning requires rethinking generalization' by Zhang et al.

I've got a good mathematical background, so I was wondering what people think are the most impactful papers there are in this area. What do you think made the most impact?",MrBeebins,1h29i7j,https://reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/,2024-11-29 00:23:33,11,1.0,11,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h29i7j
MachineLearning,[D] Why aren't Stella embeddings more widely used despite topping the MTEB leaderboard?,"https://huggingface.co/spaces/mteb/leaderboard

I've been looking at embedding models and noticed something interesting: Stella embeddings are crushing it on the MTEB leaderboard, outperforming OpenAI's models while being way smaller (1.5B/400M params) and apache 2.0. Makes hosting them relatively cheap.

For reference, Stella-400M scores 70.11 on MTEB vs OpenAI's text-embedding-3-large 64.59. The 1.5B version scores even higher at 71.19

Yet I rarely see them mentioned in production use cases or discussions. Has anyone here used Stella embeddings in production? What's been your experience with performance, inference speed, and reliability compared to OpenAI's offerings?

Just trying to understand if there's something I'm missing about why they haven't seen wider adoption despite the impressive benchmarks.

Would love to hear your thoughts and experiences!",sdsd19,1h1u814,https://reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,2024-11-28 11:45:44,66,0.93,66,0,19,0,0,False,False,True,False,False,Discussion,self,t3_1h1u814
MachineLearning,[R] BitNet a4.8: 4-bit Activations for 1-bit LLMs,"**Paper:** [https://arxiv.org/pdf/2411.04965](https://arxiv.org/pdf/2411.04965)

**Abstract:**

&gt;Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Visual Abstract:**

https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a

**Evaluations:**

[HS=HellaSwag, PQ=PiQA, WGe=WinoGrande](https://preview.redd.it/4ppq57varn3e1.png?width=955&amp;format=png&amp;auto=webp&amp;s=3c4152947edf4542d2a1ffa181bfa52a5369d916)

https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9

",StartledWatermelon,1h1y0ig,https://reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,2024-11-28 15:11:18,30,0.92,30,0,3,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/eMHBOJqKZns6b6d3vVu8taf4yu8Tq472NuD6w0t0t6M.jpg,t3_1h1y0ig
MachineLearning,[R] Fast Matrix-Based Counterfactual Regret Minimization Using GPU Parallelization,"A novel GPU implementation of Counterfactual Regret Minimization (CFR) that accelerates the computation of optimal strategies in extensive-form games. The core innovation is parallelizing the regret updates and strategy computations across GPU cores while carefully managing memory access patterns.

Key technical points:
- Custom memory layout that maps game states and actions to GPU threads
- Batch processing of information sets to maximize GPU utilization
- Parallel computation of counterfactual values and regret updates
- Multi-GPU scaling through game tree partitioning
- Evaluated on Leduc Hold'em and Limit Texas Hold'em poker variants

Results:
- Up to 30x speedup compared to CPU implementation
- Linear scaling with number of GPUs up to 8 devices
- Memory usage scales with game size and number of information sets
- Solution quality matches CPU baseline within statistical error
- Successfully solved games with up to 10^14 states

I think this work could make CFR much more practical for real-world applications beyond poker. The ability to solve larger games faster opens up possibilities in areas like automated negotiation, security games, and resource allocation. The multi-GPU scaling is particularly interesting as it suggests potential for solving even more complex games.

The memory optimization techniques developed here might also transfer well to other game-theoretic algorithms that need to process large state spaces efficiently.

TLDR: GPU-accelerated CFR implementation achieves 30x speedup through careful parallelization and memory management, with linear multi-GPU scaling. Makes solving large extensive-form games significantly more tractable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/gpu-accelerated-counterfactual-regret-minimization). Paper [here](https://arxiv.org/abs/2408.14778).",Successful-Western27,1h1wq6b,https://reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,2024-11-28 14:08:34,23,0.9,23,0,1,0,0,False,False,True,False,False,Research,self,t3_1h1wq6b
MachineLearning,[P] Retrieval augmented generation on-premises (fully local solution),"Hey everyone,   
I’m excited to share my latest repo with you—a local conversational RAG solution for your files! Here’s the deal: this setup is perfect for running RAG on-premises.   
It’s built with Docker, LangChain, Ollama, FastAPI, and Hugging Face, and all models are downloaded automatically. Soon, I’ll add support for choosing your preferred model, but here’s what the solution currently includes:  
• Locally running Ollama: It’s hardcoded to the Qwen-0.5B model for now, but model selection from the Ollama registry is coming soon.   
• Local indexing: Uses a sentence-transformer embedding model (currently restricted to this family, but this will also change soon).   
• Qdrant container: Runs locally for vector storage.   
• Local reranker: Currently uses BAAI/bge-reranker-base, with support for reranker selection coming soon.  
• Websocket-based chat: Includes history-saving capabilities.   
• Simple chat UI: Built with React for a straightforward interface.   
• Bonus: You can use this setup with ChatGPT as a custom GPT! Query your local data through the official ChatGPT web interface or macOS/iOS app.   
• On-premises ready: Everything runs locally, and the containers are CPU-friendly.

A couple of ideas and known issues:   
• Support for Model Context Protocol is on the roadmap.   
• No incremental indexing or reindexing yet.   
• Model selection isn’t available yet but will be added soon.   
  
I’d love your feedback, contributions, or support—watch, fork, and star if you find this interesting!  
Thank you!   
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
",davidvroda,1h26ul7,https://reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/,2024-11-28 22:01:09,4,0.63,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1h26ul7
MachineLearning,[P] Latest version of Ollama Grid Search (0.7.0): added prompt database ,"https://preview.redd.it/ohewvqicbo3e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=077fec6931b2efc40182f7c2eb284718822213e0

Hey people... the latest version of [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) now comes with its own prompt management database (along with many improvements in the UI).

https://preview.redd.it/qzu95clhbo3e1.png?width=975&amp;format=png&amp;auto=webp&amp;s=473382281094fc3f819e6fc6c3d267941d2a35ce

It makes it a hell lot easier to test your existing prompts when you pull newly released models!

If you want to check it out, the github page has releases for all major platforms:

[https://github.com/dezoito/ollama-grid-search](https://github.com/dezoito/ollama-grid-search)",grudev,1h20fzv,https://reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,https://www.reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/,2024-11-28 17:01:16,8,0.79,8,0,0,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uAl-6kPsNjdKIz7o50pQEHnMDPXa-cNX7GoAlrOqiec.jpg,t3_1h20fzv
MachineLearning,[D] Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis,"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 **Visatronic: A Multimodal Decoder-Only Model for Speech Synthesi**s by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/ygxnbhiboo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=31bf7c9b988c83a8d0ff2e7b011dac027aa8f154

https://preview.redd.it/7v15egiboo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=d9629caa406a92f8b2052ad6baa3a0265a27ddcf

",CATALUNA84,1h222s2,https://reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,https://www.reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/,2024-11-28 18:13:13,4,0.67,4,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/GIw9ZddndVdad-4h4y65mlExW8FTRdhSBchmteHv-AA.jpg,t3_1h222s2
MachineLearning,[D] Inconsistent use of the gerund form in dataset naming,"This is, of course, a very minor issue. But it really irks me, and I was wondering if other people are bothered by it. I think the two most common naming schemes I see for the three standard dataset splits are ""training dataset, validation dataset, and test dataset"" or ""training dataset, validation dataset, and testing dataset"". But to be consistent it should really be ""train dataset, validation dataset, and test dataset"" or ""training dataset, validating dataset, and testing dataset"". I always use the former. Does this bother anyone else, or am I alone in brooding over this?",Shianiawhite,1h217sh,https://reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h217sh/d_inconsistent_use_of_the_gerund_form_in_dataset/,2024-11-28 17:34:46,4,0.75,4,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1h217sh
MachineLearning,Daily Paper Discussion on Yannic Kilcher discord server - Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis [D],"As a part of daily paper discussions on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following **Apple's** Visatronic work

📜 V**isatronic: A Multimodal Decoder-Only Model for Speech Synthesis** by [Akshita Gupta](https://www.linkedin.com/in/akshita-gupta152/), [Navdeep Jaitly](https://www.linkedin.com/in/navdeep-jaitly-7b6a671a/), [Tatiana Likhomanenko](https://www.linkedin.com/in/tatiana-likhomanenko-36905925a/), [Karren Yang](https://www.linkedin.com/in/karren-yang-a0278b200/), [Zakaria Aldeneh](https://www.linkedin.com/in/zakaria-aldeneh/), [He Bai](https://www.linkedin.com/in/he-bai-5356b1142/)  
🌐 [https://arxiv.org/abs/2411.17690](https://arxiv.org/abs/2411.17690)

🕰 Friday, Nov 29, 2024 01:30 AM UTC // Friday, Nov 29, 2024 7.00 AM IST // Thursday, Nov 28, 2024 5:30 PM PT

Join in this Discord server for fun \~ [https://discord.gg/VGAtPcXs](https://discord.gg/VGAtPcXs)

It seems like they are proposing a unified multimodal decoder-only model for speech generation. Plus, the word error rate of a speech recognition model on the generated speech is reduced by more than relative 15%

https://preview.redd.it/7eoqonhqjo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=c751f5c9f9bc42f22672bbbc67cf8b0fe95ef36b

https://preview.redd.it/224tugzrjo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=37adc93604606d86492dc104f03915bab7428eca",CATALUNA84,1h21j1a,https://reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,https://www.reddit.com/r/MachineLearning/comments/1h21j1a/daily_paper_discussion_on_yannic_kilcher_discord/,2024-11-28 17:48:43,3,0.71,3,0,0,0,0,False,False,True,False,False,,https://b.thumbs.redditmedia.com/wIqPUsHaUNfLLWwLHYcQkdG4Yav_LVjLmzLghfKaOjI.jpg,t3_1h21j1a
MachineLearning,[D] Loading data into Ray clusters,"For those of you that run ML training in a Ray cluster on AWS, I'm curious to know what approach you take to get training data into your cluster?

And how are you versioning the data?

How do you avoid repeatedly downloading the same data across runs that have the same dataset?

I'd like a smooth process for being able to target a specific version of a dataset for a training run, and to avoid repeatedly downloading it. The data versioning should have a clear mapping to whatever version of a data pipeline created it. It'd also be nice to have something that scales well to larger datasets.

Keen to hear experiences from the trenches.",SingularValued,1h1v68j,https://reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/,2024-11-28 12:45:29,6,0.88,6,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1v68j
MachineLearning,Causal Discovery Competition Winning Paper Discussion [D],"I’ve recently come across this post: https://thetourney.github.io/adia-report/ which describes the winning method for a casual discovery competition. It’s not really my field but I do have a reasonable understanding of GNNs and Causal Inference. Anyway, from the report I don’t understand precisely what the winning team was doing. Can anyone either link to a full paper or have a good intuitive and potentially step by step explanation of what they are doing?",www3cam,1h1i0ji,https://reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,2024-11-27 23:22:43,29,0.94,29,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1h1i0ji
MachineLearning,[D]Is Freelancing as a Data Scientist Even Possible?,"Hi everyone,

I’m fine working for as low as $15/hour, so earnings aren’t a big concern for me. I’ve gone through past Reddit posts, but they mostly discuss freelancing from the perspective of income. My main concern is whether freelancing in data science is practical for someone like me, given its unique challenges.

A bit about my background: I’ve completed 3-4 real-world data science projects, not on toy datasets, but actual data (involving data scraping, cleaning, visualization, modeling, deployment, and documentation). I’ve also worked as an intern in the NLP domain.

Some issues I’ve been thinking about:

1. Domain Knowledge and Context: How hard is it to deliver results without deep understanding of a client’s business?


2. Resource Limitations: Do freelancers struggle with accessing data, computing power, or other tools required for advanced projects?


3. Collaboration Needs: Data science often requires working with teams. Can freelancers integrate effectively with cross-functional groups?


4. Iterative and Long-Term Nature: Many projects require ongoing updates and monitoring. Is this feasible for freelancers?


5. Trust and Accountability: How do freelancers convince clients to trust them with sensitive or business-critical work?


6. Client Expectations: Do clients expect too much for too little, especially at low wages?



I’m also open to any tips, advice, or additional concerns beyond these points. Are these challenges solvable for a new data science freelancer? Have any of you faced and overcome similar issues? I’d love to hear your thoughts.

Thanks in advance!",ds_reddit1,1h1q98i,https://reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,https://www.reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/,2024-11-28 07:01:15,7,0.57,7,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h1q98i
MachineLearning,[P][R] Looking for Multimodal Classification Examples Using Perceiver IO (Audio + Image + Text),"I'm exploring Perceiver IO for a project that involves processing multiple data modalities (audio, image, and text) simultaneously for a binary classification tasks. I’m looking for any GitHub repositories or resources where it has been used to handle these modalities together. Thanks a lot for your help!",kernel_KP,1h204ag,https://reddit.com/r/MachineLearning/comments/1h204ag/pr_looking_for_multimodal_classification_examples/,https://www.reddit.com/r/MachineLearning/comments/1h204ag/pr_looking_for_multimodal_classification_examples/,2024-11-28 16:46:35,1,0.67,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h204ag
MachineLearning,[P] Ablation study using a subset of data?,"Basically, I'm engaging in a research project in which I'm training encoder only language models for text classification. I have already trained my models and gotten my results, however I need to perform an ablation study. The main issue I'm having is that the dataset is large. Is it fair for me to perform the ablation study on a subset of the dataset, since I'm gonna have to train it 3 - 4 times with different ablations?",Aromatic_Web749,1h1kzsh,https://reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/,2024-11-28 01:51:54,8,0.84,8,0,8,0,0,False,False,True,False,False,Project,self,t3_1h1kzsh
MachineLearning,[D] Advanced methods of data management,"Hello everyone, right now I'm involved in a project which is basically (audio)LLM fine-tuning, and we're having problems related to data management. 

Since there isn't that many sets in that area, we're using different augmentation schemas. tl;dr, we have different datasets of various nature, and using some schemas we can convert usual ASR-type datasets to QA, generate refusal data, combine questions etc. 

Problem is, it's hard to control ratios of different data, and right now it's mostly a manual labour. We kinda have to manually adjust the amount of data we're generating. Which is rather annoying and hard process; we have many target datasets, you have to remember how much samples you've generated and try to get some adequate mix for train.

Right now, we use airflow for data filtering (lots of raw data is badly labelled), but I'm not really sure that I understand how to connect that tool to data generation, and if it's a good tool for that purpose. I was thinking about writing some snakemake script, but ideally the final solution should be flexible when we change configs, add new sets etc, and that's not what I associate with snakemake. Also, another question is visualization. 

So, I'm asking what kind of tools/libs I can use to tackle this task, is there anything that can fit our purposes, or it's time to write customs scripts? What do companies like meta use for their enormous 15t tokens set, surely they don't dump everything in one place? 

There's another question related to connecting datasets with data pipeline of training (it's complicated, there are additional steps of precalculation of feats and creating webdataset which is used in training), but for now I'd be really glad if anyone helped with just data management tools. ",Theio666,1h1u0pj,https://reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,https://www.reddit.com/r/MachineLearning/comments/1h1u0pj/d_advanced_methods_of_data_management/,2024-11-28 11:31:47,1,1.0,1,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h1u0pj
MachineLearning,[D] AAMAS 2025 reviews are out! ,"I could not find a discussion thread, so I thought I would create one myself. ",E-Cockroach,1h15k8k,https://reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,2024-11-27 14:28:29,27,0.91,27,0,37,0,0,False,False,True,False,False,Discussion,self,t3_1h15k8k
MachineLearning,[P] py-gen-ml: generating ML configuration code from a schema,"[py-gen-ml](https://jostosh.github.io/py-gen-ml) is a Python library designed to simplify your ML experiment configuration using the power of Protocol Buffers. It's still in an early phase but I'd love to hear some feedback from the community.

**Here's how py-gen-ml can help you:**

* **Centralise configurations:** Define schemas in Protobuf to act as a single source of truth.
* **Minimise repetitive work:** Automatically generate code for models, patches, sweeps, and a command-line interface.
* **Boost flexibility:** Experiment with ease thanks to YAML configurations with advanced referencing and the ability to conduct hyperparameter sweeps.
* **Improve code quality:** Benefit from JSON schema validation, strong typing, and IDE support for a more robust development process.

**py-gen-ml aims to make ML development more efficient by reducing the burden of managing configurations.** Give it a try and see how it can improve your workflow.

**Get started:**

    pip install py-gen-ml

**Learn more:** [**https://jostosh.github.io/py-gen-ml**](https://jostosh.github.io/py-gen-ml)",jalapenjos,1h1t9v0,https://reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,https://www.reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/,2024-11-28 10:40:23,0,0.25,0,0,1,0,0,False,False,True,False,False,Project,self,t3_1h1t9v0
MachineLearning,"[P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, Docker)","  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to introduce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, you can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork, star) Thank you so much!",davidvroda,1h1pudq,https://reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/,2024-11-28 06:33:40,1,1.0,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1h1pudq
MachineLearning,[D] how to do RLHF on this kind of data?,"Hi, apologies if this is a dumb question -- I'm really not knowledgeable about post training. Suppose that I have a llama and I want to finetune with human annotations that ""like"" or ""dislike"" a prompt response. Most DPO datasets feature a pair of possible responses, with one being chosen. Interpreting my data as one half of a pair with one missing, I could generate a second response from the same prompt and say that it is preferred if ""like""d and it is not preferred if it is ""disliked"". Is there a better way?",khidot,1h1bpwq,https://reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,https://www.reddit.com/r/MachineLearning/comments/1h1bpwq/d_how_to_do_rlhf_on_this_kind_of_data/,2024-11-27 18:50:05,8,0.78,8,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1h1bpwq
MachineLearning,[D] Which LLM models can I run on an NVIDIA 4060 for research purposes? Recommendations needed!,"Hi everyone,

I’m diving into research on large language models (LLMs) and looking to experiment with running them locally on my NVIDIA 4060 GPU. While I know the 4060 isn’t a high-end card compared to some research setups, I’m optimistic about making the most out of what it offers. I’d greatly appreciate any insights or recommendations on:

1. **Models that can run efficiently** on a 4060. I’m aware that some smaller versions of LLMs might be more suited for this hardware, so any advice on what’s realistically possible without excessive optimization would be fantastic.
2. **Models suitable for fine-tuning or pre-training experiments.** Although I’m starting with basic experiments, I plan to explore fine-tuning in the future, so I’d love suggestions for models that are versatile and widely used in research.
3. **Open-source models** or ones that are easy to access and work with for research purposes. Licensing and transparency are important to me, as my work is focused on academic and experimental objectives.

So far, I’ve been looking at options like LLaMA, GPT-NeoX, and BLOOM, particularly their smaller variants, but I’m open to exploring other possibilities. If you’ve had experience running these or similar models on mid-range GPUs, I’d love to hear your thoughts on performance, setup, or any potential limitations I should be aware of.

Additionally, I’d be grateful for any advice on:

* **Optimizing models for a 4060.** Are there specific tools, techniques, or libraries (like bitsandbytes or FlashAttention) that could help with running or fine-tuning these models?
* **Preparing for fine-tuning.** What should I keep in mind when selecting a model to ensure it can support future fine-tuning experiments effectively?

Thank you in advance for sharing your expertise! I’m eager to learn from the community and make the most of this setup.",Spinotesla,1h1ux8m,https://reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,https://www.reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/,2024-11-28 12:30:28,0,0.18,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h1ux8m
MachineLearning,[D] AISTATS 2025 reviews,Aistats 2025 reviews are supposed to be out today. So I thought to create a discussion post for the same where we can share our experiences!,PhoneImpressive9983,1h0x428,https://reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,2024-11-27 05:31:15,48,0.96,48,0,109,0,0,False,False,True,False,False,Discussion,self,t3_1h0x428
MachineLearning,[P] Search query content safety moderation model selection,"Hi there, I am making a mobile application with a search feature. After string cleaning &amp; validation I want to classify the query into one or more of several categories for content safety moderation similar to what Google offers for SafeSearchAnnotations on images or Meta offers in their Llama Guard for LLM prompts/responses.

I need something very fast (&lt;100 ms) as obviously the actual search and data fetching needs to occur with low latency (&lt;500 ms) after this pre-filtering. I expect to have 1000...2000 labelled sample search queries and another 5000...10000 unlabelled sample search queries for model validation. I may also have a list of stop words prior to this that runs on the client and doesn't allow the user to send the query until all stop words are removed. The categories will likely have two parents (user/admin) with five children each. The user categories can be adusted by the user and if a query falls into an admin category this would be flagged and trigger an audit. I need the model to provide a score for all categories.

Please don't recommend any LLM's/GPT's as these will not be fast enough, I am looking for something like BERT or its variants but am unsure which one. English only. At present I am really looking at Google Cloud's Model Garden specifically MobileBERT Classifier or RoBERTa-large (PEFT) as a lot of my stack is GC heavy. I don't want something complicated to setup and deploy. Please note this is different to determining ""toxicity"" like in Google's Perceptive API.",AxelrodWins,1h1dof1,https://reddit.com/r/MachineLearning/comments/1h1dof1/p_search_query_content_safety_moderation_model/,https://www.reddit.com/r/MachineLearning/comments/1h1dof1/p_search_query_content_safety_moderation_model/,2024-11-27 20:11:08,3,1.0,3,0,0,0,0,False,False,True,False,False,Project,self,t3_1h1dof1
MachineLearning,[D] How valid is the evaluation using LLMs?,"Hello community,

I am bit new to using Gen AI, I want to check the validity of using larger LLMs to evaluate the result of other LLMs. I have seen different blogs who does this for the purpose of automating the evaluations.

For eg. To evaluate a list of English translations my a model A, is it valid to prompt another model B, something like this '''Is this translation correct original text: {original_text}, Translated text {translated_text}'''

Is this a valid way of evaluating? Something inside me says it's scientifically wrong, because the LLM model B itself will have some error to it right?",raman_boom,1h11lbt,https://reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,https://www.reddit.com/r/MachineLearning/comments/1h11lbt/d_how_valid_is_the_evaluation_using_llms/,2024-11-27 10:48:09,15,0.78,15,0,13,0,0,False,False,True,False,False,Discussion,self,t3_1h11lbt
MachineLearning,Residuals in ensemble MLR [D],"
Hi all

New to ensembles.

If you ensemble MLR, you may end up with a non-linear equation however….

A) the residuals of the indicidual MLR that were ensembled need to meet parametric assumptions? Can’t use a crap MLR just because it’s going to be used in an ensemble?
B) if the ensembled MLR equation is linear then residuals should meet parametric assumptions?

Thanks


",Yellow_fruit_2104,1h1ero2,https://reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,https://www.reddit.com/r/MachineLearning/comments/1h1ero2/residuals_in_ensemble_mlr_d/,2024-11-27 20:58:23,2,1.0,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h1ero2
MachineLearning,[R] Meissonic: High-Resolution Text-to-Image Generation via Enhanced Masked Image Modeling,"This work introduces a non-autoregressive masked image modeling (MIM) approach that aims to match SDXL-level image generation while avoiding the token inefficiencies of autoregressive methods. The key innovation is combining MIM with architectural improvements and sampling optimizations to enable high-resolution image synthesis.

Main technical points:
- Uses a transformer-based architecture with specialized self-attention and positional encoding
- Incorporates human preference scores as ""micro-conditions"" to guide generation
- Employs feature compression layers to handle high resolutions efficiently
- Generates 1024x1024 images through parallel token prediction rather than sequential
- Achieves comparable FID scores to SDXL while being more computationally efficient

Results:
- Image quality metrics competitive with SDXL on standard benchmarks
- Faster generation compared to autoregressive approaches
- Better handling of complex scenes and compositions
- Improved text alignment compared to previous MIM approaches

I think this could impact the field in several ways:
- Shows that non-diffusion approaches can achieve SOTA-level generation
- Provides a potential path toward unified language-vision models
- May lead to more efficient deployment of text-to-image systems
- Could influence architecture design for future multimodal models

The biggest open question in my view is whether this approach can scale further - while it works well at current resolutions, it's unclear if the same principles will hold at even higher dimensions.

TLDR: Non-autoregressive masked modeling approach matches SDXL-level image generation while being more efficient than typical autoregressive methods. Shows promise for unified language-vision architectures.

[Full summary is here](https://aimodels.fyi/papers/arxiv/meissonic-revitalizing-masked-generative-transformers-efficient-high). Paper [here](https://arxiv.org/abs/2410.08261).",Successful-Western27,1h1529m,https://reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,https://www.reddit.com/r/MachineLearning/comments/1h1529m/r_meissonic_highresolution_texttoimage_generation/,2024-11-27 14:05:29,7,0.89,7,0,2,0,0,False,False,True,False,False,Research,self,t3_1h1529m
MachineLearning,[D] Cross Entropy Loss sucks,"Hi guys, Am I the only one thinking that training a LLM to minimize CE Loss on a certain text dataset is a very surprising idea?

I understand that it works but I am surprised it is still SOTA. The current sentence could have begun with a lot of different tokens with no consequence on its meaning, while some words are uninterchangeable. Yet CE loss doesn't account for that. Worse off, the bigger the ""equivalence class"" (the number of tokens that could replace one in a sentence without altering its meaning) of a token in a sentence, the higher the average loss on it. It seems counterproductive, isn't it?

I would love to read some contradiction.",Due-Pangolin325,1h1sqkl,https://reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,https://www.reddit.com/r/MachineLearning/comments/1h1sqkl/d_cross_entropy_loss_sucks/,2024-11-28 10:02:18,0,0.33,0,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1h1sqkl
MachineLearning,[R] Black holes and the loss landscape in machine learning,"Abstract:

&gt;Understanding the loss landscape is an important problem in machine learning. One key feature of the loss function, common to many neural network architectures, is the presence of exponentially many low lying local minima. Physical systems with similar energy landscapes may provide useful insights. In this work, we point out that black holes naturally give rise to such landscapes, owing to the existence of black hole entropy. For definiteness, we consider 1/8 BPS black holes in =8 string theory. These provide an infinite family of potential landscapes arising in the microscopic descriptions of corresponding black holes. The counting of minima amounts to black hole microstate counting. Moreover, the exact numbers of the minima for these landscapes are a priori known from dualities in string theory. Some of the minima are connected by paths of low loss values, resembling mode connectivity. We estimate the number of runs needed to find all the solutions. Initial explorations suggest that Stochastic Gradient Descent can find a significant fraction of the minima.

Arxiv: [https://arxiv.org/abs/2306.14817](https://arxiv.org/abs/2306.14817)",Mindless-House-8783,1h0uwjd,https://reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,https://www.reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,2024-11-27 03:26:49,25,0.76,25,0,28,0,0,False,False,True,False,False,Research,self,t3_1h0uwjd
MachineLearning,[D] AISTATS 2025 Paper Reviews,"Since the AISTATS 2025 paper reviews are due today, I thought to open up a thread where everyone can discuss their experiences!
",PhoneImpressive9983,1h0y8rn,https://reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,https://www.reddit.com/r/MachineLearning/comments/1h0y8rn/d_aistats_2025_paper_reviews/,2024-11-27 06:42:52,7,0.71,7,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h0y8rn
MachineLearning,"[D] Knowledge distillation neural network
","Hi community,

  
Suppose my original neural network model size is 50MB. Is there a way to estimate the size of the distilled model after applying Knowledge distillation.",PhilosopherNew313,1h17xwc,https://reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,https://www.reddit.com/r/MachineLearning/comments/1h17xwc/d_knowledge_distillation_neural_network/,2024-11-27 16:13:12,0,0.45,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h17xwc
MachineLearning,[P] [D] Comparing Llama Models and GPT 4o Models on Multilingual Machine Translation with Backtranslation,"Hey all,

In the spirit of practical real world tasks for LLMs, we wanted to see how well different models could automatically translate text from English to Spanish and the backtranslate to English on a Nike product catalog. We started with Llama 405B, Llama 70B, Llama 8B, GPT 4o-mini, and GPT 4o, but would love to test [more models](https://www.oxen.ai/explore/models).

  
\~ TLDR \~ Here are the results with all the data and code here:

[https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments](https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments)

https://preview.redd.it/qken2vjfhc3e1.png?width=1150&amp;format=png&amp;auto=webp&amp;s=739ef336dd7b89856a39d872ef12e03f806ce799

Although backtranslation may not be the most effective way to benchmark, we thought this would be an interesting experiment to see how well it correlates with model performance. It would be ideal to get native Spanish speakers to annotate the dataset with ground truth labels, so if anyone wants to contribute feel free to fork the repo and we can get some real labels.

  
We're trying to make some more real world datasets / benchmarks, so let us know if you want to help out.

If you’re new to the [Oxen.ai](https://www.oxen.ai/) project, we’re building a fast [open source dataset collaboration tools](https://github.com/Oxen-AI/oxen-release) as well as a ton of [helpful data exploration tools](https://docs.oxen.ai/features/web_hub) on top of it! If you are into data or ML/AI, we’d love your thoughts on the tool and project!",FallMindless3563,1h0sehj,https://reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,https://www.reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/,2024-11-27 01:17:44,12,0.7,12,0,12,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/mNfbgTf9Tu8SygkvvAsqVh2unMgd_M1VIenrJ09Hz3s.jpg,t3_1h0sehj
MachineLearning,[R] Help with submitting a WACV workshop paper,"Hi Everyone,

I have never submitted a paper to any conference before. I have to submit a paper to a WACV workshop due on 30 Nov.

As of now, I am almost done with the WACV-recommended template, but it asks for a Paper ID in the LaTeX file while generating the PDF. I’m not sure where to get that Paper ID from.

I am using Microsoft CMT for the submission. Do I need to submit the paper first without the Paper ID to get it assigned, and then update the PDF with the ID and resubmit? Or is there a way to obtain the ID beforehand?

Additionally, What is the **plagiarism threshold** for WACV? I want to ensure compliance but would appreciate clarity on what percentage similarity is acceptable.

Thank you for your help!",__proximity__,1h15p2e,https://reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,https://www.reddit.com/r/MachineLearning/comments/1h15p2e/r_help_with_submitting_a_wacv_workshop_paper/,2024-11-27 14:34:45,2,0.62,2,0,1,0,0,False,False,True,False,False,Research,self,t3_1h15p2e
MachineLearning,[R] Genetic learning with loop mempory and Chromosomes for the memory neurode's gate.,"Greetings!  
  
Currently a bit busy will clean it up later also to lazy to implement git now... &gt;\_&gt;

[https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md](https://github.com/Letosim/Genetic-Learning-for-Neural-Networks/blob/master/README.md)

",_Leto,1h15c7q,https://reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,https://www.reddit.com/r/MachineLearning/comments/1h15c7q/r_genetic_learning_with_loop_mempory_and/,2024-11-27 14:18:09,1,1.0,1,0,0,0,0,False,False,True,False,False,Research,self,t3_1h15c7q
MachineLearning,[D] ACL ARR Discussion - About Author Response,"Hi all! currently submitted to ACL ARR Oct. Now the author response phase is over and we haven't received any reply (to our responses) from reviewers.

Want to ask if reviewers can still update their reviews *after* the end of the author response phase and *before* the meta-review is given, or does it mean that I won't receive any replies?",Ok_Function6276,1h13ffu,https://reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,https://www.reddit.com/r/MachineLearning/comments/1h13ffu/d_acl_arr_discussion_about_author_response/,2024-11-27 12:44:02,1,1.0,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h13ffu
MachineLearning,[D] A blog post explaining sparse transformers (the original paper),"Hi!

I'm sorry if it's not appropriate to publish such posts on this subreddit. I do stay out of this type of posts on this subreddit but I keep seeing articles or videos or whatever content explaining GPT-3 without delving into sparse transformers. And it keeps frustrating me because clearly in the paper they say ""we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer"".

But no one seems to care about explaining them. I understand why to be honest but it's frustrating to see all these articles, projects, videos etc. that try to explaining everything about the GPT not even mentioning the sparse transformers part. And besides many other elements specific to GPT-3 or general to reproducibility in ML, the sparse transformer part is a big dent into even prototyping GPT-3.

I have this habit of writing down stuff when trying to understand something so I wrote a blog post on sparse transformers. Never spoke about it because I did it to restructure my thoughts and as notes for me. So it's not something I'd avise anyone to read, I'm sure it's full of typos, my writing style is not neat etc. It's just something I did for me in a way *I* would understand and recover lost bits of information when skimming through it.

Anyways, in case you're reading papers by yourself and trying to constitute the knowledge just from them, maybe my notes can help you: [https://reinforcedknowledge.com/sparse-transformers/](https://reinforcedknowledge.com/sparse-transformers/)

Sorry again if this post is not appropriate and for yapping that much.

(If you happen to read it or if you notice any errors, do not hesitate to point them out, I'd be grateful to learn from them)",ReinforcedKnowledge,1h0gl2j,https://reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,https://www.reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,2024-11-26 16:55:45,24,0.93,24,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1h0gl2j
MachineLearning,[P] What Transcription Model does Google Meets use? ,"Hi, I am currently evaluating options for transcribing sensitive meeting texts. I'd like to know what kind of transcription model is currently being used by google to transcribe meetings. I've searched the documentation and the web, and it doesn't seem to specify. I initially thought chirp would be used for this, but the documentation specifies English as the only reliable language to transcribe, which isn't true of chirp. 

This isn't a post asking which model (google or otherwise) to use, or all the better options out there, this is a very specific inquiry into Google's approach. I'd love to get some insight here. Thanks!",Arcane_Aura,1h0u63q,https://reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,https://www.reddit.com/r/MachineLearning/comments/1h0u63q/p_what_transcription_model_does_google_meets_use/,2024-11-27 02:47:35,2,0.63,2,0,3,0,0,False,False,True,False,False,Project,self,t3_1h0u63q
MachineLearning,"[P] I built Darkspark, a visual representation of your neural network. Explore everything from macro-level architecture to low-level ops and activations — Your model wants to be seen!","When reading a paper on arxiv or perusing code I also like to sketch out the model architecture myself on a big piece of paper to use as a reference. This is the software version of that. It's a GUI for your neural network. Here's the link: [https://darkspark.dev](https://darkspark.dev)

I tried all the other options I could find (netron, google’s model-explorer, tensorboard, torchview, torchlens, apple’s mycelium). These are all great projects (I really wanted to use one of them!) but none had all of the features I needed:

**Opinionated layout.** The tool’s layout should automatically expose the underlying logic of the model. The layout engine should do a lot of the heavy lifting of understanding a model’s structure and intentions. E.g. a U-net should look like a “U”. Here's [stable-diffusion-v1.5](https://darkspark.dev/models/?model=stable-diffusion-v1-5) traced directly from a huggingface [pipeline](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)

[stable-diffusion-v1.5 in the darkspark viewer](https://preview.redd.it/xksm2u1ipa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=fb7e576192c066ddc7e72ad8491c6347181525a7)

**Interactive**. I need collapsible and expandable modules so I can explore a model at a high level but can also go down to the lowest level ops. Complex models won’t even load without this. Here's the same diffusion model zoomed in on a transformer block

[stable-diffusion-v1.5 zoomed in](https://preview.redd.it/7fmmw341qa3e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=8538420973f578b68a4e225082b6990c867aefaf)

**‘Just Works’ with any arbitrary code**. I don’t want to export to ONNX, I don’t want to upload something, I don’t want to manually specify what is the model and what are the inputs. I just want to wrap my existing code in something simple.\*

    import darkspark
    import timm
    import torch
    
    model = timm.create_model(""efficientnet_b0"")
    inputs = torch.randn(1,3,224,224)
    
    with darkspark.Tracer():  # &lt;-- wrap your code with this line
      out = model(inputs)
    
    # interactive diagram now available at localhost



**Microscope**. Sometimes I also want to explore the activations and attention patterns. Like OpenAI’s microscope tool, but for your own models. Here's a “female / male” detector in a later layer of the pretrained [vit\_base\_patch16\_siglip\_224](https://darkspark.dev/models/?model=vit_base_patch16_siglip_224-microscope) from the timm library.

[female \/ male detector in darkspark viewer](https://preview.redd.it/8fzwcvcjqa3e1.png?width=1520&amp;format=png&amp;auto=webp&amp;s=fabed874645c834b8380e08a0ef7ca3f1bcb7962)

Here's the attention patterns explorer for the same model.

[Attention explorer for vit\_base\_patch16\_siglip-microscope](https://preview.redd.it/fvmxs1buqa3e1.png?width=1236&amp;format=png&amp;auto=webp&amp;s=7d7a91c469616653841111bd84879b8bcbeb1b22)



**Hosted gallery**. Most of what I want is usually a variant of an existing model. It’s often more convenient to just reference a url rather than trace your own code. I currently have all the models from timm and many from the transformers and diffusers libraries.

[lots of models available to peruse](https://preview.redd.it/qx837cz2ra3e1.png?width=1158&amp;format=png&amp;auto=webp&amp;s=f23808a45a4b7927828de24c4e1d0790a37420bb)

The public pip package isn’t yet ready, I was hoping to get feedback on the tool itself before cleaning up and sharing the codebase. Please let me know what you think, I'm eager for feedback on everything from low-level UI/UX to high-level functionality. Thanks to the awesome community for checking it out!

Here's the link again: [https://darkspark.dev](https://darkspark.dev)

\* darkspark uses \_\_torch\_function\_\_, similar to the torchview library. This allows us to capture all the ops and tensors inside the context of darkspark.Tracer without breaking when it hits dynamic control flow ops that can’t be captured in e.g. ONNX or torch exported\_program. We also get access to all the tensors, activation patterns, etc, without using hooks. Happy to answer more Qs about the architecture if ppl are interested.",Historical-Good1915,1h0krsv,https://reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,https://www.reddit.com/r/MachineLearning/comments/1h0krsv/p_i_built_darkspark_a_visual_representation_of/,2024-11-26 19:42:44,5,0.86,5,0,5,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/8bDoJ2A5fEzlL66g8tteX57z1GhfRWnpnzQOclCaFy4.jpg,t3_1h0krsv
MachineLearning,[D] Prune (channel + layers) + distillation or just distillation,"Let's say I want to make my model smaller.

There is a paper, which says distillation is good, but it takes a long time [https://arxiv.org/abs/2106.05237](https://arxiv.org/abs/2106.05237)

And there is also a paper which says that pruning + distillation works really well: [https://arxiv.org/abs/2407.14679](https://arxiv.org/abs/2407.14679)

Now, my question is: Is there any work that compares pruning + distillation vs just distillation from scratch?",osamc,1h0kcgn,https://reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,https://www.reddit.com/r/MachineLearning/comments/1h0kcgn/d_prune_channel_layers_distillation_or_just/,2024-11-26 19:25:36,5,0.86,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0kcgn
MachineLearning,[P] [D] Predict Integer Values with XGBoost Regression,"Hello! I am new to Data Science but enjoying every moment of it.

I am currently working with the XGBoost model and while everything is working fine (more or less), I am struggling with a specific issue. I am predicting 'number of orders' based on certain criteria. Since number of orders follows Poisson distribution, I have specified that and I am getting decent predictions. However, the predictions are floating point numbers. Is there any way to tell the model to give integers instead?

PS: I have tried the rounding method and while it works great, I wanted something that is at the model level.",MapleWalnut96,1h10ta0,https://reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,https://www.reddit.com/r/MachineLearning/comments/1h10ta0/p_d_predict_integer_values_with_xgboost_regression/,2024-11-27 09:51:34,0,0.3,0,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h10ta0
MachineLearning,[D] Am I a complete idiot for signing up for a Hackathon?,"Ok, so I am a Coms Science graduate student and my chosen area of study is Ethical AI.

I wanted to attend this AI conference very badly because there are some speakers that I admire. But I couldn’t afford the passes, so I decided to apply to be in the student Hackathon because if accepted, you got a free pass.

It was such a Hail Mary for me to even do the application, but I thought it would also be a cool opportunity to learn alongside others.

I got accepted… and I’m extremely excited. But now I’m like, oh wait, am I going to royally piss off whomever my teammates are because I can’t code?

Any advice? There’s a preparatory webinar happening in a week, and I’ve been doing some overview classes so that I can learn the terminology/basics. The application also asked for me to state my level of coding experience and I checked: none. And still got accepted… so I’m hoping that the organizers consider me to still have something valuable to contribute?

Please let me know what you think 🥲",sydj_k941,1h01hfn,https://reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,https://www.reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,2024-11-26 02:44:20,44,0.71,44,0,71,0,0,False,False,True,False,False,Discussion,self,t3_1h01hfn
MachineLearning,[D] what are some problems in audio and speech processing that companies are interested in?,I just recently graduated with a bachelor's in computer science and am really interested in auio and machine learning and want to do a project with a business scope. what are some problem statements that companies would be interested in? especially gen ai related ,Personal_Equal7989,1h082e6,https://reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,https://www.reddit.com/r/MachineLearning/comments/1h082e6/d_what_are_some_problems_in_audio_and_speech/,2024-11-26 09:36:01,9,0.77,9,0,17,0,0,False,False,True,False,False,Discussion,self,t3_1h082e6
MachineLearning,[D] Do modern neural network architectures (with normalization) make initialization less important?,"With the widespread adoption of normalization techniques (e.g., batch norm, layer norm, weight norm) in modern neural network architectures, I'm wondering: how important is initialization nowadays? Are modern architectures robust enough to overcome poor initialization, or are there still cases where careful initialization is crucial? Share your experiences and insights!",NumberGenerator,1gzq63h,https://reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,https://www.reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,2024-11-25 18:37:08,94,0.97,94,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1gzq63h
MachineLearning,[D] Model validation for transformer models,"I'm working at a firm wherein I have to validate (model risk validation) a transformer architecture/model designed for tabular data.

Mapping numbers to learned embeddings is just so novel. The intention was to treat them as embeddings so that they come together on the same ""plane"" as that of unstructured text and then driving decisions from that fusion. 

A decision tree or an XGBoost can be far simpler. You can plug in text based embeddings to these models instead, for more interpretability. But it is what is.

How do I approach validating this transformer architecture? Specifically if or if not it's conceptually sound and the right choice for this problem/data.",Common-Interaction50,1h0sc7o,https://reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,https://www.reddit.com/r/MachineLearning/comments/1h0sc7o/d_model_validation_for_transformer_models/,2024-11-27 01:14:44,0,0.33,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1h0sc7o
MachineLearning,[D] ADOPT optimizer,"Have any of you tried the new ADOPT optimizer? How did it go? I'm kind of curious, but haven't had the opportunity to give it a try.",neu_jose,1h01cqq,https://reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,https://www.reddit.com/r/MachineLearning/comments/1h01cqq/d_adopt_optimizer/,2024-11-26 02:37:39,7,0.82,7,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h01cqq
MachineLearning,[D] GPT2 (Hugging Face) transformer giving bogus responses. Why and how do I fix this?,"I have done some research on this and it looks like not a lot of people talk about it. I am newer to transformers (though I have some experience in Sequential models) and trying to train ChatGPT on a specific niche I want to make a bot for, but it isn't understanding anything and I feel as if I've been going in loops with it.

For example:

Me: ""Tips for planetary imaging?""

GPT2: ""Have clue how get good resolution out there will someone explain use astrobin where can look info about other equipmentpeople who this correctlyslightly better?pleiades, andromedahow much data does your setup need order score decent results ? this would more than what most hobbyists spend .5mil each day!ive taken some pics before but when trying new things like stacking that makes wonder they really work well owhats left over after shooting all those different targets guessedit yes maybe even with 1000 frames per minute thats still.""

I have custom trained it on fetched data using the Reddit Developer API from a subreddit (About 42000 lines worth) and have structured the dataset like this:

""Title: M31 Andromeda - with rotated frames

Body:

Comments:

\- A bit of a gimmick image to show stacking artefacts when the sub-frames are rotated. 7 images total, rotated 15 degrees each. 14 mins (!) total exposure.

Equipment:

\* Stellarview SVX080T Telescope 480/80mm @ f/6

\* Losmandy G11G mount

\* ZWO ASI071MC Pro color camera @ -5c + Optolong L Pro Filter

\* 60mm Guidescope with ASI120MM camera

Subs:

\* 7 x 120s

\* Master Dark

\* No Flats

Software:

\* PHD2 &amp; Sequence Generator Pro

\* Astro Pixel Processor, DeepSkyStacker, Photoshop

Processing

\* Default color integration in APP

\* Light pollution removed, stretched and exported to Photoshop

\* Same integration performed in Deep Sky Stacker (APP did such a good job it didn't show \*any\* stacking artifacts but DSS did)

\* Blended the APP image with the DSS image to show stacking artifacts in PS

\* Camera Filter shenanigans, export to jpg

\- Honestly that’s a pretty cool presentation!! You can really make this significantly better I think. Maybe like 40x60” frames per rotation or something like that to get better detail and less noise. The 120” subs blew out a lot.

Try again!!

\- \[deleted\]

\- Noob question here but about how much does a setup cost to get images like this?

\- LOVE THIS

\- It’s beautiful

\- This is sick

\- This is how every astrophotos should be ! It’s so beautiful !! I can definitely see this hanging on the wall in my bedroom 😍

\- Imagine some human like civilization on Andromeda taking pictures of the milky way

\- \[deleted\]

&lt;|endoftext|&gt;""

Trained using this dataset and GPT2-Medium.

Here are my parameters:

    outputs = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=max_length,
                        temperature=0.8,
                        top_p=0.9,
                        do_sample=True,
                        repetition_penalty=1.3,
                        no_repeat_ngram_size=3,
                        eos_token_id=self.tokenizer.eos_token_id,
                        pad_token_id=self.tokenizer.eos_token_id
    )
    
    
    system_prompt = (""You are Astrophoto AI, an encouraging astrophotography expert and teacher.""
                ""Your role is to help beginners and experienced photographers capture stunning images of the night sky and answer any questions they might have.""
                ""You offer concise, factual, and practical advice drawn from established astrophotography techniques.""
                ""Your tone is friendly, encouraging, and focused on making astrophotography accessible to everyone.""
                ""If you don't know the answer to a question, admit it instead of guessing."")

What are some potential issues with this?

Thanks!

EDIT: thanks for your advice everyone! I will be switching models.",Aman_Dude,1h0okd5,https://reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,https://www.reddit.com/r/MachineLearning/comments/1h0okd5/d_gpt2_hugging_face_transformer_giving_bogus/,2024-11-26 22:19:18,0,0.35,0,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1h0okd5
MachineLearning,"[P] does anyone know how to reduce the dimensions of embeddings using autoencoders, if you have a blog about please send it","https://preview.redd.it/3cub8uc9ja3e1.png?width=766&amp;format=png&amp;auto=webp&amp;s=0a824d6ae516ff699cb880d8e998ace85354a50f

",GellertGrindelwald_1,1h0j77v,https://reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,https://www.reddit.com/r/MachineLearning/comments/1h0j77v/p_does_anyone_know_how_to_reduce_the_dimensions/,2024-11-26 18:39:43,0,0.27,0,0,3,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/uUwCHYEmzWw3FiWk1T2hjt5K8zWvsMqQ8Y4dqssDqUM.jpg,t3_1h0j77v
MachineLearning,[Project] Claude Francois - Let an AI review your code in the style of François Chollet,"Demo here: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

At the recent Anthropic Builder Day hackathon, we ([Crossing Minds](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning)) built 'Claude François', an AI code reviewer trained in the style of [François Chollet](https://github.com/fchollet), the creator of Keras. It adapts Anthropic's Claude 3.5 Sonnet for code reviewing, but instead of regular fine-tuning, we used few-shot in-context learning with our custom RAG retrieval model, trained on PRs from the [Keras project](https://github.com/keras-team/keras). Compared to a typical AI code reviewer, it provides more succinct, high-quality code reviews focused on real issues rather than superficial nitpicking.

How it works:

* Dataset: Trained on a database of public Keras GitHub PRs and François's reviews.
* Fine-Tuned RAG Embeddings: Uses active learning and RLAIF to train embeddings optimized for generating ""fchollet-level"" reviews.
* Improved Retrieval: Retrieves relevant examples not just by embedding similarity but by optimizing for mutual information.
* Self-Reflection: Employs self-reflection techniques to enhance Sonnet’s reasoning capabilities.

This technology demo showcases how [Crossing Minds' RAGSys](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning) ICL enables domain adaptation without fine-tuning. It can be used for countless other use cases beyond code reviews, like classification, summarization, translation, search, recommendations, and more. Arxiv paper coming soon!

Try it now: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

We'd love to hear your feedback!",Crossing_Minds,1gzmk5n,https://reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,https://www.reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,2024-11-25 16:16:03,24,0.7,24,0,13,0,0,False,False,True,False,False,Project,self,t3_1gzmk5n
MachineLearning,[R] Aurora: A General-Purpose Foundation Model for Earth System Prediction,"The key contribution here is the development of Aurora, a foundation model trained on over 1M hours of atmospheric data that can perform multiple types of weather and climate predictions using a single model architecture. This represents a shift from building separate specialized models to having one model that learns general atmospheric physics.

Key technical points:
- Model architecture uses transformer blocks with attention mechanisms adapted for spatiotemporal data
- Trained on merged datasets from multiple sources including ERA5 reanalysis, satellite observations, and climate model outputs
- Can generate predictions for diverse tasks like air pollution, precipitation, and temperature forecasting
- Produces forecasts in under 1 minute compared to hours/days for traditional numerical models
- Outperforms both specialized ML models and physics-based numerical weather prediction on several benchmarks

Results:
- 15-20% improvement in 5-day global air pollution predictions vs current methods
- Better performance on 10-day weather forecasts compared to specialized models
- Maintains accuracy even for extreme weather events
- Shows continual improvement as training data increases
- Successfully handles multiple spatial and temporal resolutions

I think this work could significantly change how we approach environmental modeling. Instead of maintaining separate models for different prediction tasks, having a single foundation model that can handle multiple atmospheric predictions could make forecasting more efficient and accessible. The speed improvements (minutes vs hours) could enable new applications requiring rapid predictions.

I think the challenges ahead include:
- Validating performance across more diverse atmospheric phenomena
- Understanding model interpretability for critical forecasting
- Addressing computational costs of training and inference
- Ensuring reliability for operational forecasting systems

TLDR: Researchers developed Aurora, an atmospheric foundation model trained on massive weather/climate data that can handle multiple prediction tasks better than specialized models while being much faster. Shows foundation models could transform environmental forecasting.

[Full summary is here](https://aimodels.fyi/papers/arxiv/foundation-model-earth-system). Paper [here](https://arxiv.org/abs/2405.13063).",Successful-Western27,1gzj8rs,https://reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,https://www.reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,2024-11-25 13:50:09,36,0.88,36,0,2,0,0,False,False,True,False,False,Research,self,t3_1gzj8rs
MachineLearning,[D]Thoughts on Synthetic Data Platforms like Gretel.ai or Mostly AI?,"
Has anyone here used platforms like Gretel.ai or Mostly AI?
	•	What did you like or dislike?
	•	How was the synthetic data quality for your use case?

I’m exploring options and would appreciate your insights. Thanks!",Value-Forsaken,1gzsqwu,https://reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,https://www.reddit.com/r/MachineLearning/comments/1gzsqwu/dthoughts_on_synthetic_data_platforms_like/,2024-11-25 20:20:23,5,0.69,5,0,10,0,0,False,False,True,False,False,Discussion,self,t3_1gzsqwu
MachineLearning,[D] Flow matching is actually very different from (continuous) normalising flow?,"I was looking at the [flow matching](https://arxiv.org/pdf/2210.02747) paper and saw that flow matching is often considered as just an alternative implementation of continuous normalising flow. But after comparing the methodologies more closely, it seems there is a very significant distinction. In the flow matching paper, it is mentioned that for a data sample x1 (I assume this refers to individual data points like a single image), we can put an ""dummy"" distribution such as a very tight Gaussian on it, then construct a conditional probability path p_t(x|x1). Therefore what we learn is a transformation between the small Gaussian (t=1) on the data point to a standard Gaussian (t=0), for every data point. This implies that the latent space, when trained over the entire dataset, is the overlapped mixture of all the standard Gaussians that each individual data point maps to. The image of the small Gaussian ball for each individual image is the entire standard Gaussian.

However this does not seem to be what we do with regular normalising flows. In normalising flows, we try to learn a mapping that transforms the ENTIRE distribution of the data to the standard Gaussian, such that each data point has a fixed location in the latent space, and jointly the image of the dataset is normally distributed in the latent space. In practice we may take minibatches and optimise a score (e.g. KL or MMD) that compares the image of the minibatch with a standard Gaussian. Each location in the latent space can be uniquely inverted to a fixed reconstructed data point.

I am not sure if I am missing anything, but this seems to be a significant distinction between the two methods. In NF the inputs are encoded in the latent space, whereas flow matching as described in the paper seems to MIX inputs in the latent space. If my observations are true, there should be a few implications:

1. You can semantically interpolate in NF latent space, but it is completely meaningless in the FM case
2. Batch size is important for NF training but not FM training
3. NF cannot be ""steered"" the same way as diffusion models or FM, because the target image is already determined the moment you sample the initial noise

I wonder if anyone here has also looked into these questions and can inform me whether this is indeed the case, or whether something I missed made them more similar de facto. I appreciate any input to the discussion!",aeroumbria,1gzdera,https://reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,https://www.reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,2024-11-25 07:25:58,54,0.95,54,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1gzdera
MachineLearning,[2411.15100] XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models,,crowwork,1gzi649,https://reddit.com/r/MachineLearning/comments/1gzi649/241115100_xgrammar_flexible_and_efficient/,https://arxiv.org/abs/2411.15100,2024-11-25 12:55:20,9,0.85,9,0,0,0,0,False,False,False,False,False,,default,t3_1gzi649
MachineLearning,[D] Why does my feature visualisation form this shape?,"In performing 3d t-SNE decomposition of model features, I have come across a strange quirk. I am fine tuning an ImageNet trained ViT for CIFAR-100 classification. Before the first epoch (i.e. just imagenet weights with an untrained FC feature head), the visualisation of class boundaries looks like this, forming this convex shape with regions of no classes. After one epoch this shape is no longer present in the t-SNE visualisation.

Any ideas why? Is this related to the Manifold hypothesis? Or just due to overlap between ImageNet and CIFAR100 classes?

https://preview.redd.it/eb3w3rfaw03e1.png?width=2178&amp;format=png&amp;auto=webp&amp;s=57f1c34830cdff1968aea9367bba2b4cb3d5b7c1

",BDE-6,1gzfo7c,https://reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,https://www.reddit.com/r/MachineLearning/comments/1gzfo7c/d_why_does_my_feature_visualisation_form_this/,2024-11-25 10:14:59,10,0.81,10,0,4,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/SWOh-MCfT6Bntn3xkFzI9yHTfmJ6_JEMsdif7HmXtYQ.jpg,t3_1gzfo7c
MachineLearning,[R] Evaluating Creative Writing Output and The Effects of Fine Tuning,"
I was asked by a publisher if GPT-4o could be fine tuned to match their authors style to help build a copilot type experience. 

This gave me a chance to figure out a way to breakdown creative writing into five pillars (Dialogue, Exposition, Inner Thoughts, Description and Action) and measure how these change with prompting and fine tuning. 

I put together this blog post based on the results of training on popular authors like J.K. Rowling, Tade Thompson and Andrei Agassi. Surprisingly based GPT-4o does a decent job adopting their style with prompting but I put together some interactive visualizations to see how the model shifts during story generation (400 paragraphs) as we fine tune on 300, 600, and 800 samples. 

https://peytoncasper.com/blog/tone-evaluation/index.html

https://github.com/peytoncasper/grammar-of-thought",peytoncasper,1gzdwg5,https://reddit.com/r/MachineLearning/comments/1gzdwg5/r_evaluating_creative_writing_output_and_the/,https://www.reddit.com/gallery/1gzdwg5,2024-11-25 08:01:41,13,0.72,13,0,6,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/7cdun2J_3-VtlKB1wRgxUbknVHm_3AkGRu0D0UZDYwo.jpg,t3_1gzdwg5
MachineLearning,[D] As a CS masters student/researcher should one be very deliberate in picking a lab’s domain?,"I (very fortunately) got an opportunity in a great lab in an R1 school, Prof has a &gt;40 h-index, great record, but mainly published in lower tier conferences, though do some AAAI. It applies AI in a field that aligns with my experience, and we are expected to publish, which is perfect. However I’m more keen to explore more foundational AI research (where I have minimal experience in apart from courses I took).

In CS, ML it seems most people are only prioritising NIPS/ICLR/ICML especially since I’m interested in potentially pursuing a PhD. I’m in a bit of a dilemma, if I should seize the opportunity or keep looking for a more aligned lab (though other profs may not be looking for more students).

My gut tells me I should ignore conference rankings and do this, since they have some, chain of though, knowledge representation, cognitive system components. They expect multi semester commitment and of course once I commit I will see it through. My dilemma is that I’m moving more and more towards more practical applications in AI, which is pretty domain specific and am worried I won’t be able to pivot in the future. 

I’m aware how this can sound very silly, but if you can look past that, could I please get some advice and thoughts about what you’d do in the shoes of a budding academic, thank you!",giuuilfobfyvihksmk,1gz6mj1,https://reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,https://www.reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,2024-11-25 00:56:49,45,0.79,45,0,31,0,0,False,False,True,False,False,Discussion,self,t3_1gz6mj1
MachineLearning,[P] I made a library for building agents that use tree search to solve problems,,jsonathan,1gyreq1,https://reddit.com/r/MachineLearning/comments/1gyreq1/p_i_made_a_library_for_building_agents_that_use/,https://i.redd.it/qut9unu4su2e1.png,2024-11-24 13:51:18,282,0.95,282,0,26,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/EovprfWyEiN6f7tnfzX9swDj_d3bu8iJE26aqKFyhic.jpg,t3_1gyreq1
MachineLearning,[D] AAAI 2025 - Reviews missing after rebuttal,"Hi all,

We submitted our paper to AAAI 25. It passed Phase 1, it got fairly good scores, we wrote the rebuttals, and now the scores, the reviews and the rebuttals are missing. Is this normal?",jpereira73,1gzn4uj,https://reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,https://www.reddit.com/r/MachineLearning/comments/1gzn4uj/d_aaai_2025_reviews_missing_after_rebuttal/,2024-11-25 16:39:01,2,0.63,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gzn4uj
MachineLearning,[D] Looking for paper suggestions. What's your go to method for training a model on a mixture of multiple datasets with slightly different distributions?,"Imagine you have image data from different kinds of devices with different color profiles, resolutions, lens distortions etc. Or the object being captured in each dataset is similar but slightly different. I need suggestions on papers that effectively mix such datasets to get a bigger dataset for training a foundation model.

My datasets all come from slightly different distributions but they represent largely the same concepts so it makes sense to model them together for training a foundation model. But simply concatenating all datasets together without passing any metadata information to the model is degrading performance over training individually on each dataset.

For reference I am training MAE type models on unlabelled data and at test time training simple linear/logistic regression models on frozen MAE embeddings for different downstream tasks. The goal is to have the MAE embeddings outperform supervised models trained on each dataset individually.

An MAE trained on N datasets is underperforming an MAE trained on just one dataset. But an MAE trained on N-1 datasets and finetuned (unsupervisedly) on the Nth dataset before taking embeddings is outperforming a model trained on just the Nth dataset. But this is not a solution since I cant have N foundation models.

I tried adding a trainable source token (ie I have N trainable tokens and I concat the token corresponding to the data source to the masked input sequence before passing through the encoder) but it isn't affecting model performance at all. Please let me know if you know of any better methods.",Atom_101,1gzcmg9,https://reddit.com/r/MachineLearning/comments/1gzcmg9/d_looking_for_paper_suggestions_whats_your_go_to/,https://www.reddit.com/r/MachineLearning/comments/1gzcmg9/d_looking_for_paper_suggestions_whats_your_go_to/,2024-11-25 06:31:33,8,0.84,8,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gzcmg9
MachineLearning,"[P] Based on Arduino Nano Matter and Raspberry Pi 5, I developed this project to explore the digital twin synthetic data generation and AI-oriented advancements on real-world shipping operations w/ NVIDIA Omniverse. I trained my object detection model on my synthetic data set via Edge Impulse.",,the-amplituhedron,1gzcu29,https://reddit.com/r/MachineLearning/comments/1gzcu29/p_based_on_arduino_nano_matter_and_raspberry_pi_5/,https://www.hackster.io/kutluhan-aktar/digital-twin-enabled-smart-shipping-workstation-w-omniverse-049792,2024-11-25 06:46:05,7,0.64,7,0,7,0,0,False,False,False,False,False,Project,https://a.thumbs.redditmedia.com/HUsW9alxNon6_tVvzd3no051oaRZMRvrtvUdsHJu_s0.jpg,t3_1gzcu29
MachineLearning,"[Discussion] ""Help! Machine Learning Model Struggling with High-Dimensional Reflectance Data","I'm assisting with a research project at my university, and I've run into a bit of a roadblock. I've incorporated reflectance data variables into my machine learning model to predict a Y. There are 1500 different wavelengths, significantly increasing the dimensionality of my data. After combining the datasets, the model's performance declined. I tried reducing dimensionality, but the model continued to worsen with this data.

The research lead (a Ph.D. student) suggested I use a genetic algorithm because they've seen it used with reflectance data before. I found the implementation to be pretty complex, and I don't think this clustering will improve the model (I'm studying the implementation and believe I'll be able to test it soon).

What do you guys suggest? I think there are two approaches: either I start removing wavelengths that are worsening the model through exhaustive search, or I do this reduction using a genetic algorithm.

Has anyone encountered a similar problem? I haven't gotten satisfactory answers from AI because I understand they're not that advanced in this area yet.",DelayResponsible364,1gzq9h2,https://reddit.com/r/MachineLearning/comments/1gzq9h2/discussion_help_machine_learning_model_struggling/,https://www.reddit.com/r/MachineLearning/comments/1gzq9h2/discussion_help_machine_learning_model_struggling/,2024-11-25 18:40:50,0,0.38,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gzq9h2
MachineLearning,[D] Search and Filter ICLR Submissions by Score,Is there a website or something where I could filter the ICLR submissions by their score? e.g. to only find papers with a score above some threshold.,nextlevelhollerith,1gzk8et,https://reddit.com/r/MachineLearning/comments/1gzk8et/d_search_and_filter_iclr_submissions_by_score/,https://www.reddit.com/r/MachineLearning/comments/1gzk8et/d_search_and_filter_iclr_submissions_by_score/,2024-11-25 14:36:33,1,0.6,1,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gzk8et
MachineLearning,[D] What is the current state-of-the-art for discrete diffusion models? ,"Hi everyone,

I am currently working with Discrete Diffusion models for a new research project. In this project, I am applying Discrete Diffusion to a field where it has yet to be applied. However, I am quite new to diffusion itself, and I am overwhelmed by the number of papers published on the topic. In my current implementation, I focussed on an older [paper](https://arxiv.org/abs/2102.05379) since they described their approach quite well, and I wanted to test my idea first to see if it had some merit, which, according to initial results, it has.

Currently, I am looking at updating my method with more recent additions to this field, but as I said earlier, I am a bit overwhelmed by the amount. So my question to you is, what are good recent papers that looked into Discrete Diffusion that either explain essential concepts, such as survey papers, or that introduce new state-of-art methods that are not only applicable to a specific field, such as NLP or Vision?

Thank you in advance for your help.",Derpirium,1gyp1br,https://reddit.com/r/MachineLearning/comments/1gyp1br/d_what_is_the_current_stateoftheart_for_discrete/,https://www.reddit.com/r/MachineLearning/comments/1gyp1br/d_what_is_the_current_stateoftheart_for_discrete/,2024-11-24 11:35:12,41,0.96,41,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gyp1br
MachineLearning,[D] Emergent Cognitive Pathways In Transformer Models. Addressing Fundamental Flaws About Limits.,"**TLDR:**

Cognitive functions like reasoning and creativity emerge as models scale and train on better data. Common objections crumble when we consider humans with unusual cognitive or sensory differences—or those with limited exposure to the world—who still reason, formulate novel thoughts, and build internal models of the world.

EDIT: It looks like I hallucinated the convex hull metric as a requirement for out of distribution tests. I thought I heard it in a Lex Fridman podcast with either LeCun or Chollet, but while both advocate for systems that can generalize beyond their training data, neither actually uses the convex hull metric as a distribution test. Apologies for the mischaracterization.

**OOD Myths and the Elegance of Function Composition**

Critics like LeCun and Chollet argue that LLMs can't extrapolate beyond their training data, ~~often citing convex hull measurements~~. This view misses a fundamental mathematical reality: novel distributions emerge naturally through function composition. When non-linear functions f and g combine as f(g(x)), they create outputs beyond the original training distributions. This is not a limitation but a feature of how neural networks generalize knowledge.

Consider a simple example: training on {poems, cat poems, Shakespeare} allows a model to generate ""poems about cats in Shakespeare's style""—a novel computational function blending distributions. Scale this up, and f and g could represent Bayesian statistics and geopolitical analysis, yielding insights neither domain alone could produce. Generalizing this principle reveals capabilities like reasoning, creativity, theory of mind, and other high-level cognitive functions.

**The Training Data Paradox**

We can see an LLM's training data but not our own experiential limits, leading to the illusion that human knowledge is boundless. Consider someone in 1600: their 'training data' consisted of their local environment and perhaps a few dozen books. Yet they could reason about unseen phenomena and create new ideas. The key isn't the size of the training set - it's how information is transformed and recombined.

**Persistent Memory Isn't Essential**

A common objection is that LLMs lack persistent memory and therefore can’t perform causal inference, reasoning, or creativity. Yet people with anterograde amnesia, who cannot form new memories, regularly demonstrate all these abilities using only their working memory. Similarly, LLMs use context windows as working memory analogs, enabling reasoning and creative synthesis without long-term memory.

**Lack of a World Model**

The subfield of mechanistic interpretation strongly implies by its existence alone, that transformers and neural networks do create models of the world. One claim is that words are not a proper sensory mechanism and so text-only LLMs can't possibly form a 3D model of the world.

Let's take the case of a blind and deaf person with limited proprioception who can read in Braille. It would be absurd to claim that because their main window into the world is just text from Braille, that they can't reason, be creative or build an internal model of the world. We know that's not true.

Just as a blind person constructs valid world models from Braille through learned transformations, LLMs build functional models through composition of learned patterns. What critics call 'hallucinations' are often valid explorations of these composed spaces - low probability regions that emerge from combining transformations in novel ways.

**Real Limitations**

While these analogies are compelling, true reflective reasoning might require recursive feedback loops or temporal encoding, which LLMs lack, though attention mechanisms and context windows provide partial alternatives. While LLMs currently lack true recursive reasoning or human-like planning, these reflect architectural constraints that future designs may address.

**Final Thoughts**

The non-linearity of feedforward networks and their high-dimensional spaces enables genuine novel outputs, verifiable through embedding analysis and distribution testing. Experiments like Golden Gate Claude, where researchers amplified specific neural pathways to explore novel cognitive spaces, demonstrate these principles in action. We don't say planes can't fly simply because they're not birds - likewise, LLMs can reason and create despite using different cognitive architectures than humans. We can probably approximate and identify other emergent cognitive features like Theory of Mind, Metacognition, Reflection as well as a few that humans may not possess.",ipassthebutteromg,1gys51e,https://reddit.com/r/MachineLearning/comments/1gys51e/d_emergent_cognitive_pathways_in_transformer/,https://www.reddit.com/r/MachineLearning/comments/1gys51e/d_emergent_cognitive_pathways_in_transformer/,2024-11-24 14:27:12,14,0.61,14,0,33,0,0,False,False,True,False,False,Discussion,self,t3_1gys51e
MachineLearning,[R] Testing the Brittleness of LLM Analogical Reasoning Through Problem Variants,"The researchers developed a systematic framework for testing analogical reasoning in LLMs using letter-string analogies of increasing complexity. They created multiple test sets that probe different aspects of analogical thinking, from basic transformations to complex pattern recognition.

Key technical points:
- Evaluated performance across 4 major LLMs including GPT-4 and Claude
- Created test sets with controlled difficulty progression
- Implemented novel metrics for measuring analogy comprehension
- Tested both zero-shot and few-shot performance
- Introduced adversarial examples to test robustness

Main results:
- Models achieve &gt;90% accuracy on basic letter sequence transformations
- Performance drops 30-40% on multi-step transformations
- Accuracy falls below 50% on novel alphabet systems
- Few-shot prompting improves results by 15-20% on average
- Models show brittleness to small pattern perturbations

I think this work exposes important limitations in current LLMs' abstract reasoning capabilities. While they handle surface-level patterns well, they struggle with deeper analogical thinking. This suggests we need new architectures or training approaches to achieve more robust reasoning abilities.

The evaluation framework introduced here could help benchmark future models' reasoning capabilities in a more systematic way. The results also highlight specific areas where current models need improvement, particularly in handling novel patterns and multi-step transformations.

TLDR: New framework for testing analogical reasoning in LLMs using letter-string analogies shows strong performance on basic patterns but significant limitations with complex transformations and novel alphabets. Results suggest current models may be pattern-matching rather than truly reasoning.

[Full summary is here](https://aimodels.fyi/papers/arxiv/evaluating-robustness-analogical-reasoning-large-language-models). Paper [here](https://arxiv.org/abs/2411.14215).",Successful-Western27,1gys936,https://reddit.com/r/MachineLearning/comments/1gys936/r_testing_the_brittleness_of_llm_analogical/,https://www.reddit.com/r/MachineLearning/comments/1gys936/r_testing_the_brittleness_of_llm_analogical/,2024-11-24 14:32:43,10,0.81,10,0,1,0,0,False,False,True,False,False,Research,self,t3_1gys936
MachineLearning,RTX 4090 vs 4080 super [D],"Looking at potentially building an ML and molecular dynamics workstation for research. I’m looking in the $4000 ish range for GPU’s. I’ve been leaning heavily towards 2 4090’s (I know 5090’s will come out in January, whole different conversation!) but theoretically I could run 4x 4080 supers for about the same price, and the numbers technically come out on top, but that’s IF you can use them all efficiently. I know pytorch can distribute across GPU’s reasonably well, but not everything can.  I also know more vRAM is always better as the 40 series don’t have NVlink so can’t pool memory. I’ve also briefly looked at the RTX cards (ampere and ada) but my understanding is they’re really only worth it for the pro drivers, and that’s pretty much it. Any thoughts would be much appreciated! ",Mdgoff7,1gyzxb1,https://reddit.com/r/MachineLearning/comments/1gyzxb1/rtx_4090_vs_4080_super_d/,https://www.reddit.com/r/MachineLearning/comments/1gyzxb1/rtx_4090_vs_4080_super_d/,2024-11-24 19:58:06,2,0.56,2,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gyzxb1
MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",AutoModerator,1gyhfxm,https://reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/,https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/,2024-11-24 03:15:10,38,0.87,38,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gyhfxm
MachineLearning,[D] When will the NAACL workshops get announced?,"The [NAACL website mentions that 25th November begins the second call of workshop papers](https://2025.naacl.org/calls/workshops/#workshop-timelines), but the website doesn't seem to mention *which* workshops are going to be held. Idk if I'm stupid for now knowing, please help me out. ",Aromatic_Web749,1gyye7a,https://reddit.com/r/MachineLearning/comments/1gyye7a/d_when_will_the_naacl_workshops_get_announced/,https://www.reddit.com/r/MachineLearning/comments/1gyye7a/d_when_will_the_naacl_workshops_get_announced/,2024-11-24 18:54:16,3,0.67,3,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gyye7a
MachineLearning,[R] Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,"
Abstract:
Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [0,1]  and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [-1,1]. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.

https://arxiv.org/abs/2411.12537",iltruma,1gy0hbh,https://reddit.com/r/MachineLearning/comments/1gy0hbh/r_unlocking_statetracking_in_linear_rnns_through/,https://arxiv.org/abs/2411.12537,2024-11-23 14:11:53,90,0.95,90,0,4,0,0,False,False,False,False,False,Research,default,t3_1gy0hbh
MachineLearning,"[D] This is my first blog on medium, and they are about, How Modern Binary Hopfield Networks are just Hamming Distance Auto completers in disguise",,StoneSteel_1,1gy4qpv,https://reddit.com/r/MachineLearning/comments/1gy4qpv/d_this_is_my_first_blog_on_medium_and_they_are/,https://medium.com/@kanishq.vijay/modern-hopfield-networks-are-just-fancy-hamming-distance-calculators-and-i-can-prove-it-e9f538ee908e,2024-11-23 17:22:10,29,0.82,29,0,0,0,0,False,False,False,False,False,Discussion,https://a.thumbs.redditmedia.com/9FY02BNeYiTA4V1t16gIVBAkQJrL_tL5YxXhNMuurn0.jpg,t3_1gy4qpv
MachineLearning,[D] where find a good benchmark for all consumer gpu,"hi guys, I was wondering what gpu is the best in general for machine learning, include with openVino(only for intel), with new introduction of rochm and obviusly the queen nvidia, exist some benchmark full focuss on ML with various type of library",FewVEVOkuruta,1gyvari,https://reddit.com/r/MachineLearning/comments/1gyvari/d_where_find_a_good_benchmark_for_all_consumer_gpu/,https://www.reddit.com/r/MachineLearning/comments/1gyvari/d_where_find_a_good_benchmark_for_all_consumer_gpu/,2024-11-24 16:45:29,0,0.36,0,0,4,0,0,False,False,True,False,False,Discussion,self,t3_1gyvari
MachineLearning,[D] ACL Rolling Review October 2024,Discussion thread for ACL 2024 (ARR Oct) reviews.,AffectionateTip521,1gy8ekt,https://reddit.com/r/MachineLearning/comments/1gy8ekt/d_acl_rolling_review_october_2024/,https://www.reddit.com/r/MachineLearning/comments/1gy8ekt/d_acl_rolling_review_october_2024/,2024-11-23 20:01:19,11,0.93,11,0,36,0,0,False,False,True,False,False,Discussion,self,t3_1gy8ekt
MachineLearning,"[D] Accepted NeurIPS 2024 paper claimed to be solving a novel problem as first work, but ignores 5 prior works","At NeurIPS 2024 I found a paper that got accepted that positions its main contribution in the form of “Existing algorithms for X ignore Y. We adapt algorithm Z for X to account for Y”.

On OpenReview I see that the reviewers in particular praised the novelty of the work, and recognised Y as an important aspect that had been ignored in the field of X.

Now the interesting bit: co-authors and I published a paper in Springer’s Machine Learning journal in 2023 that also proposes an algorithm for X that account for Y. We were also not the first to study the problem setting of X with Y: our paper’s related work section discusses 4 papers that have all proposed algorithms for X that account for Y. One is even from NeurIPS (2017), and the oldest one dates back to 2012 (an AAAI paper).

The authors of this 2024 NeurIPS paper completely missed all this prior literature and believed they were the first, and so did all the reviewers.

This week I e-mailed the authors of this NeurIPS 2024 paper and they acknowledged that these works (mine + the 4 others) indeed were all working on the same problem setting, mentioned that they were unaware of all these works, and acknowledged that they can no longer claim novelty of the problem setting.

NeurIPS allows updating the camera ready paper after the conference, and the authors promised to use this opportunity to incorporate those related works and modify their contribution statements to no longer claim novelty of a first solution of X with Y.

At the one hand, it makes me happy that our work will get credited appropriately.

At the other hand I have my doubts about the ethics of severely modifying contribution statements post-review. The authors will no longer claim novelty, but the reviewers in particular praised this novelty, which makes me uncertain whether reviewers would have recommended acceptance had they known that this paper will ultimately no longer be able to claim the novelty that it claimed to have in the reviewed version.

Moreover this makes me wonder about the experimental section. Almost surely, reviewers would have demanded comparison to those 5 prior works as baselines. This paper did not compare against baselines, which will have seemed reasonable to a reviewer who reviewed this work under the assumption that the problem setting was completely novel and no prior methods exist that could function as a baseline.

Asking the group here about any thoughts on how such cases should get resolved:
- should the paper be retracted?
- should the area chair / program committee be informed? who may or may not take action
- should the paper just get updated by authors in the way that was promised, and that is it?
- something else?

I redacted X, Y and Z in order to not publicly shame the authors, as they have engaged with my e-mails and I am convinced that there is no foul play and they truly were unaware of those works.",TaXxER,1gxooqv,https://reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/,https://www.reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/,2024-11-23 01:58:58,275,0.94,275,0,65,0,0,False,False,True,False,False,Discussion,self,t3_1gxooqv
MachineLearning,[D] Recommendations needed: image-to-3D diffusion models,"Hey all, I'm evaluating different open source image-to-3D diffusion models for a project and could use some real-world insights. I've been digging through papers but would love to hear from people who've actually implemented these.

My main requirements:

1. Quality is the top priority - looking for clean, accurate reconstructions
2. Need mesh-based output (not point clouds or neurals fields) that isn't astronomically large
3. Inference time isn't critical - happy to wait up to a minute per generation

I've looked at Zero123, Wonder3D, and a few others but curious what's working well for people in practice. Especially interested in:

* Which models are actually maintainable in production
* Any gotchas with mesh generation quality
* Real-world inference times you're seeing
* How much post-processing is typically needed

Would really appreciate hearing about your experiences, especially from anyone who's deployed these in actual projects. Thanks!",ESCNOptimist,1gy9dqd,https://reddit.com/r/MachineLearning/comments/1gy9dqd/d_recommendations_needed_imageto3d_diffusion/,https://www.reddit.com/r/MachineLearning/comments/1gy9dqd/d_recommendations_needed_imageto3d_diffusion/,2024-11-23 20:44:56,8,1.0,8,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gy9dqd
MachineLearning,[R] Resource: Precision Knowledge Editing (PKE) for Reducing Toxicity in LLMs,"Sharing a project and paper for those interested in AI safety and improving LLMs. The method, called Precision Knowledge Editing (PKE), focuses on reducing toxic outputs in LLMs by identifying and modifying specific neurons or layers responsible for generating toxic content.

Key highlights:

\- Uses techniques like neuron weight tracking and activation pathway tracing to locate ""toxic hotspots.""

\- Applies a custom loss function to reduce toxicity while preserving model performance.

\- Tested on models like Llama2-7b and Llama-3-8B with significant improvements in toxicity management (e.g., lower Attack Success Rate).

The paper is available here: [https://arxiv.org/pdf/2410.03772](https://arxiv.org/pdf/2410.03772)

GitHub repo with a demo Jupyter Notebook: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models)

Might be useful for researchers, developers, or anyone exploring ways to improve LLM safety. Would be interesting to hear what others think of the approach.",Reagane371,1gxyhwb,https://reddit.com/r/MachineLearning/comments/1gxyhwb/r_resource_precision_knowledge_editing_pke_for/,https://www.reddit.com/r/MachineLearning/comments/1gxyhwb/r_resource_precision_knowledge_editing_pke_for/,2024-11-23 12:25:10,27,0.88,27,0,11,0,0,False,False,True,False,False,Research,self,t3_1gxyhwb
MachineLearning,[P] Creating Custom Music Genres Using Unsupervised Learning ,"so i had this random thought to create new music genres/spotify daylists using unsupervised learning. my idea is more towards creating a custom genre but not something necessarily as hyper-personalized as daylists. this is very much just an idea for now, will be developing into it soon tho. so the idea is in two phases:

1. take music data with audio features/embeddings/mfccs/create own features and use unsupervised learning to create clusters of those using something like knns
2. take out the audio features of the centre of the clusters and feed that to an llm to generate a custom phrase/name for that particular cluster. this can be something customized like character names for a play/use data like what time frame particular clusters of songs were played more to create something a lil more personalized like daylists/anything for that matter. haven't given much thought into this part for now.

i found a lot of papers/articles for the former phase but couldn't find much for the latter as of now. i am reading more into how spotify makes their daylists to see if anything strikes of interest.

i would live to have suggestions on how this can be improved/ recommendations for research papers/articles on anything relevant to this.

note: i know this is not very well framed and is messy but tbf i am drunk at 2 am and suddenly struck with my long lost passion for musicso please help a girl out (⁠´⁠ ⁠.⁠ ⁠.̫⁠ ⁠.⁠ ⁠`⁠)",Personal_Equal7989,1gyb62h,https://reddit.com/r/MachineLearning/comments/1gyb62h/p_creating_custom_music_genres_using_unsupervised/,https://www.reddit.com/r/MachineLearning/comments/1gyb62h/p_creating_custom_music_genres_using_unsupervised/,2024-11-23 22:05:00,5,0.86,5,0,0,0,0,False,False,True,False,False,Project,self,t3_1gyb62h
MachineLearning,[R] Iterative Narrowing: A Visual Prompting Framework for Enhanced GUI Location Grounding,"This paper introduces an iterative narrowing approach for GUI element grounding that processes visual and textual information in multiple refinement steps rather than a single pass. The key insight is breaking down element identification into coarse-to-fine stages that mirror how humans visually search interfaces.

Key technical points:
* **Two-stage architecture**: Initial region proposal network followed by focused refinement
*  Visual and text encoders process features in parallel before cross-attention alignment
* Progressive narrowing through multiple passes reduces false positives
* Handles nested GUI elements through hierarchical representation
* Trained on a dataset of 77K GUI screenshots with natural language queries

Results show:
* 15% improvement in grounding accuracy vs single-pass baseline
* Better handling of ambiguous queries
* Reduced computational overhead compared to exhaustive search
* Strong performance on complex nested interfaces
* Effective transfer to unseen GUI layouts

I think this approach could meaningfully improve accessibility tools and GUI automation by making element identification more robust. The iterative refinement mirrors human visual search patterns, which could lead to more natural interaction with interfaces.

I think the main limitation is handling highly dynamic interfaces, where elements move or change frequently. The multi-pass nature also introduces some latency that would need optimization for real-time applications.

TLDR: New GUI grounding method uses multiple refinement passes to identify interface elements more accurately, achieving 15% better accuracy through an approach that mimics human visual search patterns.

[Full summary is here](https://aimodels.fyi/papers/arxiv/improved-gui-grounding-via-iterative-narrowing). Paper [here](https://arxiv.org/abs/2411.13591).",Successful-Western27,1gy8tgs,https://reddit.com/r/MachineLearning/comments/1gy8tgs/r_iterative_narrowing_a_visual_prompting/,https://www.reddit.com/r/MachineLearning/comments/1gy8tgs/r_iterative_narrowing_a_visual_prompting/,2024-11-23 20:19:49,4,0.84,4,0,0,0,0,False,False,True,False,False,Research,self,t3_1gy8tgs
MachineLearning,[D] [Project] JAX ML Framework; Write neural networks and more; shorter and faster; What are your thoughts?,"Made a JAX framework for machine learning because I wanted to code faster &amp; shorter so I made zephyr. I hope it might be helpful to you guys too and wanted to hear some feedback.

Link in the comments.

Nothing wrong with current frameworks, this is just another way of doing things. 

NNs or ML algorithms to me, are just pure/mathematical functions and so I wanted that to reflect in my code. With other frameworks it comes in at least 2 steps: initialization in the constructor and a computation in the forward/call body. This seems fine at first but when models become larger, it's 2 places where I have to synchronize code. - If I change a computation, I might need to change a hyperparameter somewhere, or if I change a hyperparameter, I might need to change a computation - or if i have to re-read my code, i have to read in at least 2 places. I usually use a small window for an editor and so jumping between these could a hassle (putting them side by side is another solution).

Another thing I was experiencing was that if I was doing something that is not neural networks, for example if an algorithm was easier to do with a recursive call (but with different trainable weights for each call), that would be challenging in other frameworks. So while they generic computational graph-frameworks, some computations are hard to do. 

To me, computations was about passing data around and getting them to transform, so this \`act\` of transforming data should be that focus of the framework. That's what I did with zephyr. Mathematical functions are python functions, no need for initialization in a constructor. You use the functions(networks or layers, etc) when you need them. No need for constructors, allows recursions, allows you to focus on the transformations or operations. Zephyr handles weight creation and management for you - it is explicit tho unlike other frameworks; you carry around a \`params\` tree, and that should be no problem, since that's a core of the computation and shouldn't be hidden away.

In short, zephyr is  short but readable aimed at people developing research ideas about ML. The README has a few samples for neural networks. I hope you guys like it and try it.

",Pristine-Staff-5250,1gxqag6,https://reddit.com/r/MachineLearning/comments/1gxqag6/d_project_jax_ml_framework_write_neural_networks/,https://www.reddit.com/r/MachineLearning/comments/1gxqag6/d_project_jax_ml_framework_write_neural_networks/,2024-11-23 03:25:31,33,0.84,33,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gxqag6
MachineLearning,[R] Llama 3.2 Interpretability with Sparse Autoencoders,,RandomHexCode,1gxzsbp,https://reddit.com/r/MachineLearning/comments/1gxzsbp/r_llama_32_interpretability_with_sparse/,https://github.com/PaulPauls/llama3_interpretability_sae,2024-11-23 13:37:14,3,1.0,3,0,0,0,0,False,False,False,False,False,Research,https://b.thumbs.redditmedia.com/geyS4I_5fqQ5-NrpJOf-awV-cU5tO-EH7u-ZGL5X41Y.jpg,t3_1gxzsbp
MachineLearning,[R] Entropy-Guided Critical Neuron Pruning for Efficient Spiking Neural Networks,"This paper introduces a pruning method for Spiking Neural Networks (SNNs) based on neuroscience principles of criticality. The key insight is using neuronal avalanche analysis to identify neurons that have the most significant impact on network dynamics, similar to how critical neurons function in biological brains.

Key technical points:
* Monitors spike propagation patterns to identify critical neurons
* Introduces adaptive pruning schedule based on network stability metrics
* Achieves 90% compression while maintaining accuracy on MNIST/CIFAR-10
* Works across different SNN architectures (feed-forward, CNN)
* Uses stability measures to prevent catastrophic forgetting during pruning

Main results:
* Outperforms existing pruning methods on accuracy retention
* Shows better energy efficiency compared to unpruned networks
* Maintains temporal dynamics important for SNN operation
* Demonstrates scalability across different network sizes
* Validates biological inspiration through avalanche analysis

I think this approach could be particularly important for deploying SNNs in resource-constrained environments like edge devices. The adaptive pruning schedule seems especially promising since it automatically adjusts based on network behavior rather than requiring manual tuning.

I think there are some open questions about computational overhead of the avalanche analysis that need to be addressed for very large networks. However, the biological principles behind the method suggest it could generalize well to other architectures and tasks.

TLDR: Novel pruning method for SNNs based on neuroscience principles of criticality. Uses neuronal avalanche analysis to identify important neurons and achieves 90% compression while maintaining accuracy. Introduces adaptive pruning schedule that adjusts based on network stability.

[Full summary is here](https://aimodels.fyi/papers/arxiv/brain-inspired-efficient-pruning-exploiting-criticality-spiking). Paper [here](https://arxiv.org/abs/2311.16141).",Successful-Western27,1gx86i0,https://reddit.com/r/MachineLearning/comments/1gx86i0/r_entropyguided_critical_neuron_pruning_for/,https://www.reddit.com/r/MachineLearning/comments/1gx86i0/r_entropyguided_critical_neuron_pruning_for/,2024-11-22 13:45:14,45,0.88,45,0,5,0,0,False,False,True,False,False,Research,self,t3_1gx86i0
MachineLearning,[D] Optimization Algorithm that focuses on solving model parameters ,"i tried googling for any sources but does anyone have info on where i can start looking for optimization algorithms that focus on optimizing model parameters by solving for a specific parameter (or parameters), given the input and target for a sample? Or the name for this kind of optimization algorithm? E.g. solve for model parameters a,b,c for the function y = ax\^2 + bx + c , x and y being the input and target respectively. Surely this algorithm has a name in the context of ml.

EDIT:

I believe what im asking is a bit ambiguous. As opposed to gradient descent, which focuses on finding the derivative of model parameters to the loss provided, the optimization algorithm i specified above focuses on a number of parameters and somehow figures out (or solves the values of these parameters) to approximately match the output. Like the same way you have 5x = 10 and solve for x, so the algortithm figures that x=2. For more data samples and more parameters you have 5x + c = 12 and 2x + c = 6, x and c being model parameters and 10 ad 4 being the desired output. The algorithm figures x = 2 and c = 2 somehow. Its a bit of a stretch but even im starting to doubt my sanity enough to believe that what im asking is basically all if not most of what ml optimization algorithms do.",Relevant-Twist520,1gy6jy6,https://reddit.com/r/MachineLearning/comments/1gy6jy6/d_optimization_algorithm_that_focuses_on_solving/,https://www.reddit.com/r/MachineLearning/comments/1gy6jy6/d_optimization_algorithm_that_focuses_on_solving/,2024-11-23 18:40:43,0,0.39,0,0,23,0,0,False,False,True,False,False,Discussion,self,t3_1gy6jy6
MachineLearning,"[D] We’ve crowd-sourced,  open-sourced, and made it easy to find so many tools to build with, but where is all this effort for context/scraping?","We have so many repos and libraries available to us for building, deploying, and using LLMs for tasks. We have hubs for models, plug-in-play libraries for things like LoRA and RAG, containerization for deploying models with APIs, extensions to integrate LLMs into IDEs and workflows, and plenty more. There’s stuff for managing and  orchestrating agents. 

Suffice to say, we have tons to open source tools to work to start working on both niche and general uses for LLMs.

That’s all great, but what I’m always having to build from scratch is getting context. Be that tools for online searches, webpage parsing (even common webpages that I know people would love to be easier to use for context), document parsing, etc.

I’ve been seen more cool projects pop up, but I’ve been seeing those projects provide details or implementation less and less on how they are finding, accessing, retrieving, and processing context.

There are plenty libraries to build tools for this purpose, but I just see less and less people sharing those.

Now I understand the context different projects need can be pretty niche, so reusability could be sparse. 

But is my perception wrong? Are there open-source resources for finding existing context extraction/scraping implementations or places to submit your own to make it easier for others to find?",Oscilla,1gxbrdm,https://reddit.com/r/MachineLearning/comments/1gxbrdm/d_weve_crowdsourced_opensourced_and_made_it_easy/,https://www.reddit.com/r/MachineLearning/comments/1gxbrdm/d_weve_crowdsourced_opensourced_and_made_it_easy/,2024-11-22 16:26:26,16,0.83,16,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gxbrdm
MachineLearning,[D] Historical archive of generative AI media output?,"Is there an archive or research paper that shows examples of the progress in generative AI media output over time?

I want to gather examples of multimedia outputs (text, images, video, sound) generated over time to help evaluate how the field has progressed in each area over time. 

Of course I can grab whatever results from different sources by searching, but I'm wondering if there is a more organized and consistent repository for this?",searchresults,1gxh7dm,https://reddit.com/r/MachineLearning/comments/1gxh7dm/d_historical_archive_of_generative_ai_media_output/,https://www.reddit.com/r/MachineLearning/comments/1gxh7dm/d_historical_archive_of_generative_ai_media_output/,2024-11-22 20:15:06,7,0.82,7,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gxh7dm
MachineLearning,[D] How to make more reliable reports using AI — A Technical Guide,,phicreative1997,1gy1t45,https://reddit.com/r/MachineLearning/comments/1gy1t45/d_how_to_make_more_reliable_reports_using_ai_a/,https://medium.com/firebird-technologies/how-to-make-more-reliable-reports-using-ai-a-technical-guide-672b2d01cb2a,2024-11-23 15:14:13,0,0.23,0,0,0,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/-AZkwBJl93JyKODja6Nv557NXqR_dwFniH8tEBkkuoQ.jpg,t3_1gy1t45
MachineLearning,[P] Registrations are open for the 2025 EY Open Science AI &amp; Data Challenge,"Use AI for good and help create more vital, sustainable communities when you join the [2025 EY Open Science AI &amp; Data Challenge](https://challenge.ey.com/2025). A phenomenon known as the urban heat island effect is becoming a significant issue as our cities continue to grow and develop. Dense development and lack of green space create heat islands that take a toll on our health and increase our energy use. But your skills and vision can help. [Register now.](https://challenge.ey.com/2025) \#EY #BetterWorkingWorld #AI #OpenScience #ShapeTheFutureWithConfidence",fofxy,1gx9htl,https://reddit.com/r/MachineLearning/comments/1gx9htl/p_registrations_are_open_for_the_2025_ey_open/,https://www.reddit.com/r/MachineLearning/comments/1gx9htl/p_registrations_are_open_for_the_2025_ey_open/,2024-11-22 14:48:28,10,0.7,10,0,1,0,0,False,False,True,False,False,Project,self,t3_1gx9htl
MachineLearning,[R] Say What You Mean: A Response to 'Let Me Speak Freely',"Will here from .txt, the team behind [Outlines](https://github.com/dottxt-ai/outlines) an open source library that enables open LLMs to perform structured generation, ensuring their outputs always adhere to a predefined format.

We are passionate about structured generation, and truly believe it has the potential to transform the work being done with LLMs in profound ways. 

However a recent paper, [Let Me Speak Freely](https://arxiv.org/abs/2408.02442) was published reporting some misinformation around the performance of structured generation on a series of evaluations. 

We've recently publish a rebuttal to this paper on our blog: [Say What You Mean: A Response to 'Let Me Speak Freely'](https://blog.dottxt.co/say-what-you-mean.html) and thought the community here might find it interesting. It covers not only issues with the original paper, but also dives into the nature of structured generation and how to get the most out of your models with prompting for structured generation.",CountBayesie,1gwswn7,https://reddit.com/r/MachineLearning/comments/1gwswn7/r_say_what_you_mean_a_response_to_let_me_speak/,https://www.reddit.com/r/MachineLearning/comments/1gwswn7/r_say_what_you_mean_a_response_to_let_me_speak/,2024-11-21 22:57:45,87,0.9,87,0,24,0,0,False,False,True,False,False,Research,self,t3_1gwswn7
MachineLearning,[D] Anyone else work in Infrastructure/MLOps?,"I've been working as an AI Architect at a small/medium company (\~300 people). I've been having more success focusing on the deployment of models using KServe and Kubeflow rather than pure ML. The trend that I'm seeing is that there's so many open-source models that custom ML solutions are less relevant, so it's becoming more valuable to simply deploy open-source models that keep getting massive improvements, at least in the space that I'm in.   
  
Curious who else may be working on the Infrastructure/MLOps side of things. I've found development to sometimes be pretty isolating, since I'm the only one in this company focusing on this and haven't found a community outside of this. It would be nice to hear stories from others who are also working in this area. Cheers!",Milwookie123,1gxbh9b,https://reddit.com/r/MachineLearning/comments/1gxbh9b/d_anyone_else_work_in_infrastructuremlops/,https://www.reddit.com/r/MachineLearning/comments/1gxbh9b/d_anyone_else_work_in_infrastructuremlops/,2024-11-22 16:14:51,2,0.76,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gxbh9b
MachineLearning,[R]Geometric aperiodic fractal organization in Semantic Space : A Novel Finding About How Meaning Organizes Itself ,"Hey friends! I'm sharing this here because I think it warrants some attention, and I'm using methods that intersect from different domains, with Machine Learning being one of them.

Recently I read Tegmark &amp; co.'s paper on Geometric Concepts [https://arxiv.org/abs/2410.19750](https://arxiv.org/abs/2410.19750) and thought that it was fascinating that they were finding these geometric relationships in llms and wanted to tinker with their process a little bit, but I didn't really have access or expertise to delve into LLM innards, so I thought I might be able to find something by mapping its output responses with embedding models to see if I can locate any geometric unity underlying how llms organize their semantic patterns. Well I did find that and more...

I've made what I believe is a significant discovery about how meaning organizes itself geometrically in semantic space, and I'd like to share it with you and invite collaboration.

**The Initial Discovery**

While experimenting with different dimensionality reduction techniques (PCA, UMAP, t-SNE, and Isomap) to visualize semantic embeddings, I noticed something beautiful and striking; a consistent ""flower-like"" pattern emerging across all methods and combinations thereof. I systematically weeded out the possibility that this was the behavior of any single model(either embedding or dimensional reduction model) or combination of models and what I've found is kind of wild to say the least. It turns out that this wasn't just a visualization artifact, as it appeared regardless of:

\- The reduction method used

\- The embedding model employed

\- The input text analyzed

https://preview.redd.it/pdyq50s1ob2e1.png?width=907&amp;format=png&amp;auto=webp&amp;s=b9ecf9206c1c2b43881341e8ad51950cf73b345c

https://preview.redd.it/b2u3uz93ob2e1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=6448776ebaeb5620b2079c7fed6992b3a813d619

[cross-section of the convergence point\(Organic\) hulls](https://preview.redd.it/t59tzz2qob2e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=a9a0cd3132191db5a2ea163c87e8dfe336f9320c)

[a step further, showing how they form with self similarity.](https://preview.redd.it/q0pmaveqob2e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=863dd23a1899efc8bf266c0702cf3258643859c3)

**Verification Through Multiple Methods**

To verify this isn't just coincidental, I conducted several analyses, rewrote the program and math 4 times and did the following:

1. Pairwise Similarity Matrices

Mapping the embeddings to similarity matrices reveals consistent patterns:

\- A perfect diagonal line (self-similarity = 1.0)

\- Regular cross-patterns at 45° angles

\- Repeating geometric structures

https://preview.redd.it/ft89ukpaob2e1.png?width=460&amp;format=png&amp;auto=webp&amp;s=9900f9113fad02841e5e18cb0bc5f9b6b66275e1

https://preview.redd.it/f2yzbvnbob2e1.png?width=433&amp;format=png&amp;auto=webp&amp;s=4a13a8e910794c64375ab0628f6f34006c31fb2f

Relevant Code:  
python

def analyze\_similarity\_structure(embeddings):

similarity\_matrix = cosine\_similarity(embeddings)

eigenvalues = np.linalg.eigvals(similarity\_matrix)

sorted\_eigenvalues = sorted(eigenvalues, reverse=True)

return similarity\_matrix, sorted\_eigenvalues

2. Eigenvalue Analysis

The eigenvalue progression as more text is added, regardless of content or languages shows remarkable consistency like the following sample:

First Set of eigenvalues while analyzing The Red Book by C.G. Jung in pieces:  
\[35.39, 7.84, 6.71\]

Later Sets:  
\[442.29, 162.38, 82.82\]

\[533.16, 168.78, 95.53\]

\[593.31, 172.75, 104.20\]

\[619.62, 175.65, 109.41\]

https://preview.redd.it/hesf440job2e1.png?width=1088&amp;format=png&amp;auto=webp&amp;s=b531499fe8043e0b41390229bd0b04017373c49b

Key findings:

\- The top 3 eigenvalues consistently account for most of the variance

\- Clear logarithmic growth pattern

\- Stable spectral gaps i.e: (35.79393)

3. Organic Hull Visualization

The geometric structure becomes particularly visible when visualizing through organic hulls:

Code for generating data visualization through sinusoidal sphere deformations:  
python

def generate\_organic\_hull(points, method='pca'):

phi = np.linspace(0, 2\*np.pi, 30)

theta = np.linspace(-np.pi/2, np.pi/2, 30)

phi, theta = np.meshgrid(phi, theta)

center = np.mean(points, axis=0)

spread = np.std(points, axis=0)

x = center\[0\] + spread\[0\] \* np.cos(theta) \* np.cos(phi)

y = center\[1\] + spread\[1\] \* np.cos(theta) \* np.sin(phi)

z = center\[2\] + spread\[2\] \* np.sin(theta)

return x, y, z

\`\`\`

What the this discovery suggests is that meaning in semantic space has inherent geometric structure that organizes itself along predictable patterns and shows consistent mathematical self-similar relationships that exhibit golden ratio behavior like a penrose tiling, hyperbolic coxeter honeycomb etc and these patterns persist across combinations of different models and methods. I've run into an inverse of the problem that you have when you want to discover something; instead of finding a needle in a haystack, I'm trying to find a single piece of hay in a stack of needles, in the sense that nothing I do prevents these geometric unity from being present in the semantic space of all texts. The more text I throw at it, the more defined the geometry becomes.

https://preview.redd.it/3hho1avzob2e1.png?width=1239&amp;format=png&amp;auto=webp&amp;s=a446d6b71ba0166c842e9537c6cd228662bb2682

I think I've done what I can so far on my own as far as cross-referencing results across multiple methods and collecting significant raw data that reinforces itself with each attempt to disprove it.

So I'm making a call for collaboration:

I'm looking for collaborators interested in:

1. Independently verifying these patterns
2. Exploring the mathematical implications
3. Investigating potential applications
4. Understanding the theoretical foundations

My complete codebase is available upon request, including:

\- Visualization tools

\- Analysis methods

\- Data processing pipeline

\- Metrics collection

If you're interested in collaborating or would like to verify these findings independently, please reach out. This could have significant implications for our understanding of how meaning organizes itself and potentially for improving language models, cognitive science, data science and more.

\*TL;DR: Discovered consistent geometric patterns in semantic space across multiple reduction methods and embedding models, verified through similarity matrices and eigenvalue analysis. Looking for interested collaborators to explore this further and/or independently verify.

\##EDIT##: I

I need to add some more context I guess,  because it seems that I'm being painted as a quack or a liar without being given the benefit of the doubt. Such is the nature of social media though I guess.

This is a cross-method, cross-model discovery using semantic embeddings that retain human interpretable relationships. i.e. for the similarity matrix visualizations, you can map the sentences to the eigenvalues and read them yourself. Theres nothing spooky going on here, its plain for your eyes and brain to see.

Here are some other researchers who are like-minded and do it for a living.

(Athanasopoulou et al.) supports our findings:

""The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it is organized in such a way that interesting semantic relations can be exported from manifolds of much lower dimensionality embedded in this high dimensional space."" [https://aclanthology.org/C14-1069.pdf](https://aclanthology.org/C14-1069.pdf)

A neuroscience paper(Alexander G. Huth 2013) reinforces my findings about geometric organization:""An efficient way for the brain to represent object and action categories would be to organize them into a continuous space that reflects the semantic similarity between categories.""  
[https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/)

""We use a novel eigenvector analysis method inspired from Random Matrix Theory and show that semantically coherent groups not only form in the row space, but also the column space.""  
[https://openreview.net/pdf?id=rJfJiR5ooX](https://openreview.net/pdf?id=rJfJiR5ooX)

I'm getting some hate here, but its unwarranted and comes from a lack of understanding. The automatic kneejerk reaction to completely shut someone down is not constructive criticism, its entirely unhelpful and unscientific in its closed-mindedness.",Own_Dog9066,1gwqvt2,https://reddit.com/r/MachineLearning/comments/1gwqvt2/rgeometric_aperiodic_fractal_organization_in/,https://www.reddit.com/r/MachineLearning/comments/1gwqvt2/rgeometric_aperiodic_fractal_organization_in/,2024-11-21 21:29:58,57,0.75,57,0,63,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/FrCUFqNtP_G0uh0Hibb8d0zw6XWHs9AUdUXetjPd1zM.jpg,t3_1gwqvt2
MachineLearning,[P] Machine learning project on chem,"It is called SMILES

[Simplified Molecular Input Line Entry System - Wikipedia](https://en.wikipedia.org/wiki/Simplified_Molecular_Input_Line_Entry_System)

I am not sure if I can train a model so that it can interpret the correct structure as well as naming after feeding labelled dataset?",uartimcs,1gx4ezg,https://reddit.com/r/MachineLearning/comments/1gx4ezg/p_machine_learning_project_on_chem/,https://www.reddit.com/r/MachineLearning/comments/1gx4ezg/p_machine_learning_project_on_chem/,2024-11-22 09:59:23,2,0.6,2,0,7,0,0,False,False,True,False,False,Project,self,t3_1gx4ezg
MachineLearning,[D] Do I need to connect more?,"I am currently finishing up my third year PhD and would most probably be graduating in another year.
So far I been pretty much working on my own. 

As a matter of fact, all my publications are first-author and most of the contributions to these papers are from PI or Co-PI who does very surface-level checks (grammars etc). I basically did not get involved with any work with other people and have been working solely on my own for these 3 years. Thus I don’t have much connections outside my lab group ( I actually don’t even know them well at all ). I see a lot of publications have quite a handful of authors, and most comprising of multiple organisations.

The thing is I was hoping to get an industrial job after graduation, would me being relatively unknown/unheard actually be a problem? 

I find it hard to find connections outside my lab. Most of my labmates don’t work in similar research areas. How do you guys actually connect or find collaborators outside of your lab zone? 

Would an overseas attachment help me? 
This is also one of the main reason why I am trying to find an internship (still very competitive), which is to collaborate or to be mentored by experienced people.
",AmbitiousSeesaw3330,1gwn0rn,https://reddit.com/r/MachineLearning/comments/1gwn0rn/d_do_i_need_to_connect_more/,https://www.reddit.com/r/MachineLearning/comments/1gwn0rn/d_do_i_need_to_connect_more/,2024-11-21 18:51:12,31,0.88,31,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1gwn0rn
MachineLearning,[P] Python Windows Screenshot Analyzer,"I want to build a python project to analyse windows screehots. Suppose an app is open then the screenshot should tell everything going on in the app. For example in the Microsoft Teams Who are the participants, ongoing duration etc. What all apps are open in the taskbar what's the time in the screenshot etc. How can I achieve it I want to use open source resources only.",Ok-Bar5416,1gxh51z,https://reddit.com/r/MachineLearning/comments/1gxh51z/p_python_windows_screenshot_analyzer/,https://www.reddit.com/r/MachineLearning/comments/1gxh51z/p_python_windows_screenshot_analyzer/,2024-11-22 20:12:23,0,0.3,0,0,3,0,0,False,False,True,False,False,Project,self,t3_1gxh51z
MachineLearning,[R] BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games,"Tired of saturated benchmarks? Want scope for a significant leap in capabilities? 

Introducing BALROG: a Benchmark for Agentic LLM and VLM Reasoning On Games!

BALROG is a challenging benchmark for LLM agentic capabilities, designed to stay relevant for years to come.

  
Check it out!

GitHub: [https://github.com/balrog-ai/BALROG](https://github.com/balrog-ai/BALROG)

Leaderboard: [https://balrogai.com](https://balrogai.com)

Paper: [https://arxiv.org/abs/2411.13543](https://arxiv.org/abs/2411.13543)",pagggga,1gwhnf8,https://reddit.com/r/MachineLearning/comments/1gwhnf8/r_balrog_benchmarking_agentic_llm_and_vlm/,https://www.reddit.com/r/MachineLearning/comments/1gwhnf8/r_balrog_benchmarking_agentic_llm_and_vlm/,2024-11-21 14:33:20,41,0.91,41,0,6,0,0,False,False,True,False,False,Research,self,t3_1gwhnf8
MachineLearning,[D] Next big thing in Time series?,"In NLP, we’ve seen major milestones like transformers, GPT, and LLMs, which have revolutionized the field. Time series research seems to be borrowing a lot from NLP and CV—like transformer-based models, self-supervised learning, and now even foundation models specifically for time series. But there doesn’t seem to be a clear consensus yet on what works best. For example, NLP has well-accepted pretraining strategies like masked language modeling or next-token prediction, but nothing similar has become a standard for time series.  
  
Lately, there’s been a lot of talk about adapting LLMs for time series or even building foundation models specifically for the purpose. On the other hand, some research indicates that LLMs are not helpful for time series. 

So I just wanna know what can be a game changer for time series!",Few-Pomegranate4369,1gwbhxq,https://reddit.com/r/MachineLearning/comments/1gwbhxq/d_next_big_thing_in_time_series/,https://www.reddit.com/r/MachineLearning/comments/1gwbhxq/d_next_big_thing_in_time_series/,2024-11-21 08:20:38,115,0.96,115,0,55,0,0,False,False,True,False,False,Discussion,self,t3_1gwbhxq
MachineLearning,[D] Noise injection,"Can anyone give me some recommendations, about paper that identify the relationship between accumulated output noise over time (e.g. something like time series model with the noise injection to the input)? ",Careless-Top-2411,1gwy1m7,https://reddit.com/r/MachineLearning/comments/1gwy1m7/d_noise_injection/,https://www.reddit.com/r/MachineLearning/comments/1gwy1m7/d_noise_injection/,2024-11-22 03:05:09,2,0.75,2,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwy1m7
MachineLearning,[D] Struggling to Transition to PhD,"**“Undergrad is about answering questions, while a PhD is about finding one.”** —Someone

I'm a first-year CS PhD student, but I feel stuck in the mindset of an undergrad. I excel at solving problems, as shown by my perfect GPA. However, when it comes to research, I struggle. If I enter a new area, **I typically read a lot of papers, take notes, and end up capable of writing a decent survey—but I rarely generate fresh ideas.**

Talking to other PhD students only adds to my frustration; one of them claims they can even come up with LLM ideas during a Latin class. My advisor says research is more about perseverance than talent, but I feel like I’m in a loop: I dive into a new field, produce a survey, and get stuck there.

I’m confident in my intelligence, but I’m questioning whether my workflow is flawed (e.g., maybe I should start experimenting earlier?) or if I’m just not cut out for research. Coming up with marginal improvements or applying A to B feels uninspiring, and I struggle to invest time in such ideas.

How do you CS (ML) PhD students come up with meaningful research ideas? Any advice on breaking out of this cycle?",StraightSpeech9295,1gw61tk,https://reddit.com/r/MachineLearning/comments/1gw61tk/d_struggling_to_transition_to_phd/,https://www.reddit.com/r/MachineLearning/comments/1gw61tk/d_struggling_to_transition_to_phd/,2024-11-21 02:50:51,148,0.94,148,0,56,0,0,False,False,True,False,False,Discussion,self,t3_1gw61tk
MachineLearning,[P] Comparing Machine Unlearning Techniques and PKE (Precision Knowledge Editing),"PKE focuses on enhancing the model's knowledge and positive output rather just identifying neuron activiations. While DINM is a great method for neural suppression and to aid real time modification, I really wanted to build on top of that and created PKE (Precision Knowledge Editing)which emphasizes neural reinforcement and enhancing the model's knowledge and positive output rather than just identifying neuron activiations.There's lots of current Machine unlearning techniques that can make LLMs safer right now like:

1. **Exact Unlearning:** This method involves retraining the model from scratch after removing the undesired data. While it ensures complete removal of the data's influence, it is computationally expensive and time-consuming, especially for large models.
2. **Approximate Unlearning:**
   1. **Fine-Tuning:** adjusting the model using the remaining data to mitigate the influence of the removed data. However, this may not completely eliminate the data's impact.
   2. **Gradient Ascent:** applying gradient ascent on the loss function concerning the data to be forgotten, effectively 'unlearning' it. This method can be unstable and may degrade model performance.

PKE is better for the following reasons:

1. **Fine-Grained Identification of Toxic Parameters:** PKE employs neuron weight tracking and activation pathway tracing to accurately pinpoint specific regions in the model responsible for generating toxic or harmful content. This precision allows for targeted interventions, reducing the risk of unintended alterations to the model's overall behavior.
2. **Maintaining Model Performance:** By focusing edits on identified toxic regions, PKE minimizes the impact on the model's general performance. This approach ensures that the model retains its capabilities across various tasks while effectively mitigating the generation of undesirable content.
3. **Scalability Across Different Model Architectures:** PKE has demonstrated effectiveness across various LLM architectures, including models like Llama2-7b and Llama-3-8b-instruct. This scalability makes it a versatile tool for enhancing safety in diverse AI systems.

Would love to hear your guys' thoughts on this project and how to continue to improve this methodology. If interested, here's the Github link: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models) and [paper](https://arxiv.org/pdf/2410.03772) .",lial4415,1gwwt6y,https://reddit.com/r/MachineLearning/comments/1gwwt6y/p_comparing_machine_unlearning_techniques_and_pke/,https://www.reddit.com/r/MachineLearning/comments/1gwwt6y/p_comparing_machine_unlearning_techniques_and_pke/,2024-11-22 02:02:44,2,1.0,2,0,0,0,0,False,False,True,False,False,Project,self,t3_1gwwt6y
MachineLearning,[D] Research Topics in Conformal Prediction,"My background is in econometrics and soon I'll start to work in my master's thesis (already have a supervisor but would like to come up with some ideas that I could integrate in my research). One thing that recently got my attention were uncertainty quantification methods, specifically Conformal Prediction.

One thing that seems particularly cool is that it can be adapted to ensure coverage across specific groups in the covariates or even the labels. Additionally, 'recently', the research community was able to tackle the most limiting assumption, that of exchangeability, meaning it can be applied, for example, to time-series data.

My questions are two-fold (one out of curiosity and the other for personal interest):

1. What are some real-world scenarios that you've seen Conformal Prediction shine? And if there is some scenario that you'd think it would work but didn't.
2. And what do you think are some interesting questions yet to be addressed?

Any thoughts or general feedback very welcome! Thanks in advance!",HamsterExpress8688,1gwomft,https://reddit.com/r/MachineLearning/comments/1gwomft/d_research_topics_in_conformal_prediction/,https://www.reddit.com/r/MachineLearning/comments/1gwomft/d_research_topics_in_conformal_prediction/,2024-11-21 19:56:24,4,0.84,4,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1gwomft
MachineLearning,"[D] Curious, how do you manage the full ML lifecycle ?","Hi guys! I’ve been pondering with a specific question/idea that I would like to pose as a discussion, it concerns the idea of more quickly going from idea to production with regards to ML/AI apps.

My experience in building ML apps and whilst talking to friends and colleagues has been something along the lines of you get data, that tends to be really crappy, so you spend about 80% of your time cleaning this, performing EDA, then some feature engineering including dimension reduction etc. All this mostly in notebooks using various packages depending on the goal. During this phase there are couple of tools that one tends to use to manage and version data e.g DVC etc

Thereafter one typically connects an experiment tracker such as MLFlow when conducting model building for various metric evaluations. Then once consensus has been reached on the optimal model, the Jupyter Notebook code usually has to be converted to pure python code and wrapped around some API or other means of serving the model. Then there is a whole operational component with various tools to ensure the model gets to production and amongst a couple of things it’s monitored for various data and model drift.

Now the ecosystem is full of tools for various stages of this lifecycle which is great but can prove challenging to **operationalize** and as we all know sometimes the results we get when adopting ML can be supar :(

I’ve been playing around with various platforms that have the ability for an end-to-end flow from cloud provider platforms such as AWS SageMaker, Vertex , Azure ML. Popular opensource frameworks like MetaFlow and even tried DagsHub. With the cloud providers it always feels like a jungle, clunky and sometimes overkill e.g maintenance. Furthermore when asking for platforms or tools that can really help one explore, test and investigate without too much setup it just feels lacking, as people tend to recommend tools that are great but only have one part of the puzzle. The best I have found so far is Lightning AI, although when it came to experiment tracking it was lacking.

So I’ve been playing with the idea of a truly out-of-the-box end-to-end platform, the idea is not to to re-invent the wheel but combine many of the good tools in an end-to-end flow powered by collaborative AI agents to help speed up the workflow across the ML lifecycle for faster prototyping and iterations. You can check out my current project over here [https://envole.ai](https://envole.ai)

This is still in the early stages so the are a couple of things to figure out, but would love to hear your feedback on the above hypothesis, how do you you solve this today ?",Lumiere-Celeste,1gwhdpl,https://reddit.com/r/MachineLearning/comments/1gwhdpl/d_curious_how_do_you_manage_the_full_ml_lifecycle/,https://www.reddit.com/r/MachineLearning/comments/1gwhdpl/d_curious_how_do_you_manage_the_full_ml_lifecycle/,2024-11-21 14:20:57,14,0.79,14,0,9,0,0,False,False,True,False,False,Discussion,self,t3_1gwhdpl
MachineLearning,[D]  Does anyone remember the machine learning in 2023 wrap-up meme video?,"Around this time last year someone posted a video of stitched-together memes about machine learning in 2023. I cannot remember all of the memes but there was definitely one about NLP professors needing to learn about RL, and one about Anthropic's appearance in front of some part of the US government.

Two questions.

1. Does anyone else remember this video and have a link? I cannot find it using Google because ""Machine learning in 2023"" is not a very discriminative search query.
2. Will there be a 2024 edition? I hope so!",votadini_,1gwh8yk,https://reddit.com/r/MachineLearning/comments/1gwh8yk/d_does_anyone_remember_the_machine_learning_in/,https://www.reddit.com/r/MachineLearning/comments/1gwh8yk/d_does_anyone_remember_the_machine_learning_in/,2024-11-21 14:14:51,11,0.77,11,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwh8yk
MachineLearning,[R] The Complexity Dynamics of Grokking,"# [https://openreview.net/pdf?id=07N9jCfIE4](https://openreview.net/pdf?id=07N9jCfIE4)

Though OpenReviewers [don't seem impressed](https://openreview.net/forum?id=07N9jCfIE4) by this paper, I found it very interesting.  I like the concepts / ideas involved: 

1. Regularization methods (e.g. norms) usually measure capacity, not complexity.
2. Attempt regularization closer to Kolmogorov complexity.
3. relate model complexity to generalization &amp; grokking.",marojejian,1gwju03,https://reddit.com/r/MachineLearning/comments/1gwju03/r_the_complexity_dynamics_of_grokking/,https://www.reddit.com/r/MachineLearning/comments/1gwju03/r_the_complexity_dynamics_of_grokking/,2024-11-21 16:48:46,6,0.75,6,0,9,0,0,False,False,True,False,False,Research,self,t3_1gwju03
MachineLearning,[D] Machine Learning as a DSL-design problem? Is this a thing? ,"I'll probably sound confusing or unclear, and that's because I don't even know what I want to ask about, but the general direction is: instead of learning numbers that combine/compute in a certain way is there a way to learn a DSL over a problem in guided/principled way? (I have no idea how to give an example, since I'm not even sure this is currently possible). This is different from an NLP-model learning to use a programming language. 

  
I do know a problem with this, and it's just that it's hard to evaluate an arbitrary program (halting problem). Neural nets are basically vector programs with a set amount compute steps. I have seen program search algorithms over a small crafted DSL, but I was thinking of, learning the DSL directly - but this is a chicken and egg problem since what learns the DSL ?",Pristine-Staff-5250,1gwwitr,https://reddit.com/r/MachineLearning/comments/1gwwitr/d_machine_learning_as_a_dsldesign_problem_is_this/,https://www.reddit.com/r/MachineLearning/comments/1gwwitr/d_machine_learning_as_a_dsldesign_problem_is_this/,2024-11-22 01:48:20,1,0.55,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gwwitr
MachineLearning,[Discussion] VQVAE Reconstruction Issue: Grayscale Output and Gradient Flow Insights,"I recently trained a VQVAE model on the COCO dataset, and it successfully converged, generating images with fine details. However, during inference, I noticed something unusual.

When I tried to reconstruct an image using only the quantized vector and passed it through the decoder, the generated output was a grayscale image that preserved the edges of the original.

As shown in the attached image, the second image is the reconstructed one. In this case, I included the codebook gradient flow term as follows:

    x = x + (x - z).detach()

With this modification, the generated image appears much cleaner.

I'm curious to know if this behavior is expected with VQVAE or if there might be an issue with my implementation. If anyone has experience working with VQVAE, your insights would be greatly appreciated!

https://preview.redd.it/g961r7f3ka2e1.png?width=1688&amp;format=png&amp;auto=webp&amp;s=94d94e51c76a508949b2dee7e4341b6ec079dab1

",Logical-Passenger471,1gwl7c9,https://reddit.com/r/MachineLearning/comments/1gwl7c9/discussion_vqvae_reconstruction_issue_grayscale/,https://www.reddit.com/r/MachineLearning/comments/1gwl7c9/discussion_vqvae_reconstruction_issue_grayscale/,2024-11-21 17:41:18,4,0.75,4,0,6,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/xa_-CXxF248GR7gjMuLQB5rjtDa81-Z37-Czne4FTJA.jpg,t3_1gwl7c9
MachineLearning,"[D] Train and Val Dice Score gets zero for a long time and then increases, while loss keeps on decreasing. Wondering why?",,__proximity__,1gwhf4o,https://reddit.com/r/MachineLearning/comments/1gwhf4o/d_train_and_val_dice_score_gets_zero_for_a_long/,https://www.reddit.com/gallery/1gvxoga,2024-11-21 14:22:45,7,0.71,7,0,8,0,0,False,False,False,False,False,Discussion,default,t3_1gwhf4o
MachineLearning,[R] Agentic AI test suites. All the test environments used to benchmark BALROG.,"# BabyAI

+ Purpose is to facilitate research on *grounded language learning.*  The current domain of BabyAI is a 2D gridworld in which synthetic natural-looking instructions (e.g. “put the red ball next to the box on your left”) require the agent to navigate the world including unlocking doors) and move objects to specified locations.  

https://openreview.net/forum?id=rJeXCo0cYX

----

# Crafter

+ Crafter features randomly generated 2D worlds where the player needs to forage for food and water, find shelter to sleep, defend against monsters, collect materials, and build tools.

https://github.com/danijar/crafter?tab=readme-ov-file


----

# TextWorld

+ Microsoft TextWorld is an open-source, extensible engine that both generates and simulates text games. You can use it to train reinforcement learning (RL) agents to learn skills such as language understanding and grounding, combined with sequential decision making.

https://www.microsoft.com/en-us/research/project/textworld/

https://github.com/microsoft/TextWorld

https://arxiv.org/pdf/1806.11532

----

# Baba is AI 

+ Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives.    We test three ***state-of-the-art multi-modal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically*** when generalization requires that the rules of the game must be manipulated and combined. 


https://github.com/nacloos/baba-is-ai

https://arxiv.org/abs/2407.13729

----

# MiniHack

+ MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforcement Learning (RL).   The motivation behind MiniHack is to be able to perform RL experiments in a controlled setting while being able to increasingly scale the complexity of the tasks.

https://github.com/facebookresearch/minihack

https://minihack.readthedocs.io/en/latest/


----

# NetHack

+ NetHack is an attractive research platform as it contains hundreds of enemy and object types, has complex and stochastic environment dynamics, and has a clearly defined goal (descend the dungeon, retrieve an amulet, and ascend) which can be achieved in a diverse set of ways. The game is considered one of the hardest in the world1, with winning episodes lasting 100,000s of steps, and a permadeath setting that starts agents at the beginning in a whole new world if they die in the dungeon. NetHack is even difficult to master for human players who often rely on external knowledge.


https://proceedings.neurips.cc/paper_files/paper/2023/file/764ba7236fb63743014fafbd87dd4f0e-Paper-Conference.pdf

https://github.com/upiterbarg/hihack

https://arxiv.org/pdf/2203.11889

https://www.youtube.com/watch?v=8L8LiQ-cIWA",moschles,1gwqe0m,https://reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/,https://www.reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/,2024-11-21 21:09:01,2,0.63,2,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwqe0m
MachineLearning,[D] GNNs with applications to science Labs?,"I'm graduating this december with a degree in data science (sort of equivalent to a B.sc.+ M.sc.). I'm looking to pursue a PhD in an área similar as to what I did my thesis on, which was an application of GNNs to biomedical data. 

My advisor is relatively new in this area (the lab is more of a traditional Systems/Network biology), so he doesnt have international contacts in this particular area. I'm waiting to see if I'm granted with a PhD scholarship where I live, but science funding in my country has been really cut down, so I'm looking for options overseas.

Any directions to good labs working on this stuff would be greatly appreciated",maximusdecimus__,1gwogns,https://reddit.com/r/MachineLearning/comments/1gwogns/d_gnns_with_applications_to_science_labs/,https://www.reddit.com/r/MachineLearning/comments/1gwogns/d_gnns_with_applications_to_science_labs/,2024-11-21 19:49:49,2,0.76,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gwogns
MachineLearning,How to efficiently generate text from RNNs and Transformers during inference [P],"Most of the notebooks I see do something like this to generate code

    text = ['start']
    for _ in range(num_to_gen):
      token = model(text)
      text.append(token)

But this clearly is inefficient when it would be better to pass in each token one at a time to the model as it's generated while preserving the hidden state. What is the cleanest / industry accepted way to do this with pytorch models?

I see a tutorial on pytorch that has the model return both the output and the hidden state, and then you pass the hidden state back into the model. This feels really clunky and for large hidden states, it's inefficient to keep passing the hidden state of every rnn layer out to the end of the model.

Like I'm trying to work with Mamba currently which according to it's paper tries it's best to not materialize the full hidden state in memory.  
The only other way I can think of to do this is have the model persist the hidden state through forward calls and maybe have a reset function. But I'm not sure if this is an accepted way to do things. Also, I don't think I see the hidden state in the state\_dict of the open source mamba model, so I feel like Mamba doesn't do this but not entirely sure. I tried reading the Mamba code but found it difficult to understand.

I would appreciate seeing what the industry standard is to do this properly and an example in some open source code that's explained. As a bonus, if anyone can help me understand how Mamba does it in the state-spaces/mamba repo on github that would be great, but maybe I'll just post a comment on the repo.

Also just curious how transformer models deal with this as well.",No_Effective734,1gwkrir,https://reddit.com/r/MachineLearning/comments/1gwkrir/how_to_efficiently_generate_text_from_rnns_and/,https://www.reddit.com/r/MachineLearning/comments/1gwkrir/how_to_efficiently_generate_text_from_rnns_and/,2024-11-21 17:24:44,4,0.83,4,0,2,0,0,False,False,True,False,False,Project,self,t3_1gwkrir
MachineLearning,[D] How much can we revise paper during rebuttal?,"I'm currently preparing for ICLR rebuttal, and revising paper is an option. I have fixed mostly typo, slightly change the notation of 1 or 2 variable a bit (index, transposed ,..). Can anyone give me some advices, can this result in negative impression to the reviewer/AC? Should we revise at all or not?",Competitive_Newt_100,1gwf6t6,https://reddit.com/r/MachineLearning/comments/1gwf6t6/d_how_much_can_we_revise_paper_during_rebuttal/,https://www.reddit.com/r/MachineLearning/comments/1gwf6t6/d_how_much_can_we_revise_paper_during_rebuttal/,2024-11-21 12:32:43,4,0.76,4,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1gwf6t6
MachineLearning,[R] How to bring novelty into a task like Engagement Prediction ,"So a colleague and I(both undergraduates) have been reading literature related to engagement analysis and we identified a niche domain under engagement prediction with a also niche dataset that might have been only used once or twice. 

The professor we are under told me that this might be a problem and also that we need more novelty even though we have figured out many imprivements through introducing modalities, augmentations, and possibly making it real time. 

How do I go ahead after this roadblock? Is there any potential in this research topic? If not, how do you cope with restarting from scratch like this? ",RCratos,1gwo8pk,https://reddit.com/r/MachineLearning/comments/1gwo8pk/r_how_to_bring_novelty_into_a_task_like/,https://www.reddit.com/r/MachineLearning/comments/1gwo8pk/r_how_to_bring_novelty_into_a_task_like/,2024-11-21 19:40:50,0,0.5,0,0,6,0,0,False,False,True,False,False,Research,self,t3_1gwo8pk
MachineLearning,"[R] Inference-Time Algorithms for LLMs: A Survey of Decoding, Meta-Generation, and Efficient Generation Methods","This survey unifies work on inference-time algorithms for LLMs into a comprehensive framework, examining how scaling compute during inference (rather than just training) can improve model outputs.

Key technical aspects:

- Introduces three categories of inference algorithms:
  - **Token-level generation**: Methods like beam search, nucleus sampling that work at individual token level
  - **Meta-generation**: Algorithms operating on full/partial sequences, incorporating external knowledge
  - **Efficient generation**: Techniques to reduce computational costs while maintaining quality

- Provides mathematical framework connecting:
  - Traditional NLP decoding approaches
  - Modern LLM inference methods  
  - Systems optimization techniques

- Reviews key tradeoffs between:
  - Compute cost vs output quality
  - Latency vs thoroughness of search
  - Memory usage vs beam size

I think this framework helps bridge the gap between theoretical ML research and practical deployment concerns. By organizing the space of inference algorithms, it makes it easier to identify which approaches are most suitable for different use cases.

I think the most valuable contribution is highlighting how inference-time compute scaling offers a complementary path to improving LLM outputs beyond just training larger models. This could be especially relevant for researchers working with fixed, pre-trained models.

TLDR: Comprehensive survey organizing inference-time algorithms for LLMs into unified framework spanning token-level generation, meta-generation, and efficiency optimization. Shows how scaling inference compute offers new ways to improve outputs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/from-decoding-to-meta-generation-inference-time). Paper [here](https://arxiv.org/abs/2406.16838).",Successful-Western27,1gwflaz,https://reddit.com/r/MachineLearning/comments/1gwflaz/r_inferencetime_algorithms_for_llms_a_survey_of/,https://www.reddit.com/r/MachineLearning/comments/1gwflaz/r_inferencetime_algorithms_for_llms_a_survey_of/,2024-11-21 12:54:25,3,0.81,3,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwflaz
MachineLearning,[D] Seeking Journal Suggestions for Publishing a Side Project on AI-Assisted DSM-5-TR and ICD-10 Diagnostics,"Seeking Journal Suggestions for Publishing a Side Project on AI-Assisted DSM-5-TR and ICD-10 Diagnostics

I’ve been working on a side project with a psychologist. It’s a Retrieval-Augmented Generation (RAG) model that uses DSM-5-TR and ICD-10 to suggest the most likely diagnosis based on a user query. It’s designed for use in psychiatric and medical diagnostics to aid practitioners, researchers, and students.

I’m now at the stage where I’d like to publish my work but I’m not affiliated with any institution, this is purely a passion project. I would like to find a Journal that has:

1. Publication fees under $1,000 (or ideally free!).
2. A relatively quick review process (preferably less than 2 months).
3. Open access would be a plus, so it’s available to as many people as possible.

Do you know of any journals that might be a good fit for this kind of work? Bonus points if the journal is friendly to independent researchers or side projects.

Thanks in advance for the help! 😊",drinkredstripe3,1gwl0fn,https://reddit.com/r/MachineLearning/comments/1gwl0fn/d_seeking_journal_suggestions_for_publishing_a/,https://www.reddit.com/r/MachineLearning/comments/1gwl0fn/d_seeking_journal_suggestions_for_publishing_a/,2024-11-21 17:34:11,0,0.43,0,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1gwl0fn
MachineLearning,[D] PhD in RL/ML Theory or LLM,"Hi guys,

I'm at a crossroads in my academic journey and would appreciate the community's insights. I'm trying to decide between pursuing a PhD focused on reinforcement learning/ML theory versus specializing in large language models with more experimental/applied research (these are the only two offers I had).

# Key considerations are the following:

# Research Impact

* RL/ML Theory: Foundational work that could advance the field's mathematical understanding
* LLMs: Direct applications in today's most transformative AI systems

# Job Prospects

* Theory: Academia, research labs, potentially more limited industry roles
* LLMs: High industry demand, active research area in both academia and industry

# Long-term Relevance

* Theory: Core principles likely to remain valuable regardless of specific technologies
* LLMs: Currently revolutionary but uncertain long-term trajectory

Personal background

* I'm an international student and about to finish my master program in US, so I no longer has enough time before making the final decision. I used to research in ml theory, but did not end up with a real top conference publication in theory. I personally doubt if I have enough mathematical background to pursue a successful PhD in this area (e.g., at least publish 2 theory papers a year on ICML/NeurIPS/ICLR/COLT/AISTATS). At the same time, I am personally doubting if theory works indeed advance the ML/AI community, as many papers are just proving vacuous bounds or propose some new algorithms that themselves cannot even implement or experimentally tested.
* I also used to research in more applied ml, with one aaai paper. My personal concerns is that I'm not fast at implementation and coding, the most strategic ability for a successful applied ML researcher. After we entered the LLM era, the pacing or applied ML research (especially in LLM and CV) becomes so fast. It's like competitive programming in research community (well, also the #GPUs competition).",Living_Imagination84,1gvx8vx,https://reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/,https://www.reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/,2024-11-20 19:02:11,48,0.8,48,0,25,0,0,False,False,True,False,False,Discussion,self,t3_1gvx8vx
MachineLearning,[R] ICASSP 2025 review is out.,"4 4 3

Can I omit the rebuttal?

acceptance rate is pretty high but I guess the margin is quite high.",Big_Occasion_182,1gwgygb,https://reddit.com/r/MachineLearning/comments/1gwgygb/r_icassp_2025_review_is_out/,https://www.reddit.com/r/MachineLearning/comments/1gwgygb/r_icassp_2025_review_is_out/,2024-11-21 14:01:24,1,1.0,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1gwgygb
MachineLearning,[R] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models,,rcparts,1gvveu8,https://reddit.com/r/MachineLearning/comments/1gvveu8/r_procedural_knowledge_in_pretraining_drives/,https://arxiv.org/abs/2411.12580,2024-11-20 17:47:51,35,0.94,35,0,3,0,0,False,False,False,False,False,Research,default,t3_1gvveu8
MachineLearning,[D] Is the maths deduction in the Smaug paper valid?,"The [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive ](https://arxiv.org/pdf/2402.13228)paper identifies that DPO can reduce the model’s likelihood of the preferred completions when there are small edit distances between pairs of completions.

In their theoretical analysis of this phenomenon, one of the main steps is derived by ""restricting the attention to just the logits,"" which, to my understanding, derives a partial derivative of DPO loss given the attention logits on each token in the vocabulary. (Appendix B.1 in the paper, and here's a screenshot for part of it.)

https://preview.redd.it/3s250pg0o42e1.png?width=2072&amp;format=png&amp;auto=webp&amp;s=c1a92c3474d9dd235e2e72d1615696e69e843676

However, the loss should optimize the model parameters, and the deductions in this paper assume that those attention logits are independent variables, making me think their derivation is invalid. I'm not a math major, so I'm not sure whether my thoughts are correct.  
",StraightSpeech9295,1gw0l7f,https://reddit.com/r/MachineLearning/comments/1gw0l7f/d_is_the_maths_deduction_in_the_smaug_paper_valid/,https://www.reddit.com/r/MachineLearning/comments/1gw0l7f/d_is_the_maths_deduction_in_the_smaug_paper_valid/,2024-11-20 22:00:53,13,0.81,13,0,11,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/kpg-7672g9h9ZUNop8jW7QHAe0Z8UziJv1q407oHO-k.jpg,t3_1gw0l7f
MachineLearning,[D] New time series forecasting datasets - what properties should I report in the paper?,"I'm working on a paper with new datasets for time series forecasting. They are both uni- and multivariate. I'm thinking about what properties should I analyze and report in the paper. Goal is to create a benchmark.

So far I have:

* total length (# time steps)
* train and test length
* evaluation approach, e.g. temporal train/test split, expanding window (with given horizon and step)
* resolution, e.g. hourly, daily, monthly
* metric, e.g. MAE, MASE
* cross-series correlations (multivariate only)
* comparison of train and test value distributions (maybe univariate only)
* seasonality, stationarity (with statistical tests)
* causality testing, e.g. Granger, Toda-Yamamoto

Also some basic baselines, statistical forecasting methods, and popular neural networks.

Do you think something else would also be useful?",qalis,1gwct36,https://reddit.com/r/MachineLearning/comments/1gwct36/d_new_time_series_forecasting_datasets_what/,https://www.reddit.com/r/MachineLearning/comments/1gwct36/d_new_time_series_forecasting_datasets_what/,2024-11-21 09:59:47,1,0.6,1,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gwct36
MachineLearning,[R] ITCMA-S: A Multi-Agent Architecture for Emergent Social Behavior and Group Formation,"I read an interesting paper proposing a novel architecture for studying emergent social behavior in multi-agent systems. The key technical contribution is introducing ""generative multi-agents"" that can dynamically form social structures without explicit programming.

The core technical components:
- A three-layer agent architecture combining perception, memory, and decision-making
- Novel ""social perception module"" that allows agents to model others' mental states
- Memory system that integrates both episodic and semantic information
- Action selection based on both individual goals and social context

Main experimental results:
- Agents spontaneously developed hierarchical social structures
- Social norms emerged through repeated interactions
- Different ""cultures"" formed in isolated agent groups
- Agents showed evidence of both cooperative and competitive behaviors
- Social learning occurred through observation and imitation

The implications I think matter most for multi-agent systems and social AI research. The architecture demonstrates that complex social behaviors can emerge from relatively simple building blocks, so it suggests potential paths toward more human-like AI systems. The results also provide a computational framework for studying how societies form and evolve.

From a practical perspective, this work could inform the development of more sophisticated multi-agent systems for applications like social simulation, game AI, and robotic swarms.

TLDR: New architecture allows AI agents to spontaneously develop social structures and norms without explicit programming. Results show emergence of hierarchies, cultures, and social learning.

[Full summary is here](https://aimodels.fyi/papers/arxiv/can-agents-spontaneously-form-society-introducing-novel). Paper [here](https://arxiv.org/abs/2409.06750).",Successful-Western27,1gvyfe4,https://reddit.com/r/MachineLearning/comments/1gvyfe4/r_itcmas_a_multiagent_architecture_for_emergent/,https://www.reddit.com/r/MachineLearning/comments/1gvyfe4/r_itcmas_a_multiagent_architecture_for_emergent/,2024-11-20 19:50:26,12,1.0,12,0,2,0,0,False,False,True,False,False,Research,self,t3_1gvyfe4
MachineLearning,[P] Enhancing LLM Safety with Precision Knowledge Editing (PKE),"I've been working on a project called PKE (Precision Knowledge Editing), an open-source method to improve the safety of LLMs by reducing toxic content generation without impacting their general performance. It works by identifying ""toxic hotspots"" in the model using neuron weight tracking and activation pathway tracing and modifying them through a custom loss function.

If you're curious about the methodology and results, we've also published a [paper](https://arxiv.org/pdf/2410.03772) detailing our approach and experimental findings. It includes comparisons with existing techniques like Detoxifying Instance Neuron Modification (DINM) and showcases PKE's significant improvements in reducing the Attack Success Rate (ASR).

The project is open-source, and I'd love your feedback! The GitHub repo features a Jupyter Notebook that provides a hands-on demo of applying PKE to models like Meta-Llama-3-8B-Instruct: [https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models](https://github.com/HydroXai/Enhancing-Safety-in-Large-Language-Models)

If you're interested in AI safety, I'd really appreciate your thoughts and suggestions. Thanks for checking it out!",lial4415,1gw5w3d,https://reddit.com/r/MachineLearning/comments/1gw5w3d/p_enhancing_llm_safety_with_precision_knowledge/,https://www.reddit.com/r/MachineLearning/comments/1gw5w3d/p_enhancing_llm_safety_with_precision_knowledge/,2024-11-21 02:43:12,4,0.7,4,0,0,0,0,False,False,True,False,False,Project,self,t3_1gw5w3d
MachineLearning,[D] ICASSP 2025 reviews are due today! ,A friendly banter to discuss the icassp reviews! Hoping for the best!,always_been_a_toy,1gvqvgd,https://reddit.com/r/MachineLearning/comments/1gvqvgd/d_icassp_2025_reviews_are_due_today/,https://www.reddit.com/r/MachineLearning/comments/1gvqvgd/d_icassp_2025_reviews_are_due_today/,2024-11-20 14:35:39,29,0.94,29,0,52,0,0,False,False,True,False,False,Discussion,self,t3_1gvqvgd
MachineLearning,[D] OpenAI's CLIP alternative,"Hi, Are there any new recent SOTA model like CLIP? I want to do similarity search on images, but CLIP's performance is not very good for my project.

I currently use: CLIP-ViT-B-32-laion2B-s34B-b79K

Embeddings which also capture colour would be perfect. Thanks.",CaptTechno,1gvlgxm,https://reddit.com/r/MachineLearning/comments/1gvlgxm/d_openais_clip_alternative/,https://www.reddit.com/r/MachineLearning/comments/1gvlgxm/d_openais_clip_alternative/,2024-11-20 09:08:43,32,0.9,32,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1gvlgxm
MachineLearning,[N] Open weight (local) LLMs FINALLY caught up to closed SOTA?,"Yesterday Pixtral large dropped [here](https://mistral.ai/news/pixtral-large/).

It's a 124B multi-modal vision model. This very small models beats out the 1+ trillion parameter GPT 4o on various cherry picked benchmarks. Never mind the Gemini-1.5 Pro. 

As far as I can tell doesn't have speech or video. But really, does it even matter? To me this seems groundbreaking. It's free to use too. Yet, I've hardly seen this mentioned in too many places. Am I missing something?  
  
BTW, it still hasn't been 2 full years yet since ChatGPT was given general public release November 30, 2022. In barely 2 years AI has become somewhat unrecognizable. Insane progress.

  
**\[Benchmarks Below\]**

https://preview.redd.it/ebo9qp0rzy1e1.png?width=1777&amp;format=png&amp;auto=webp&amp;s=3d47183ba7e2af69eb52fc5f8d755f105cb52004

https://preview.redd.it/woc0wmrozy1e1.png?width=1852&amp;format=png&amp;auto=webp&amp;s=1bc5d380e2deebfd03684e1a8341254d18596d8e

  
",AIAddict1935,1gvfpdw,https://reddit.com/r/MachineLearning/comments/1gvfpdw/n_open_weight_local_llms_finally_caught_up_to/,https://www.reddit.com/r/MachineLearning/comments/1gvfpdw/n_open_weight_local_llms_finally_caught_up_to/,2024-11-20 03:00:24,53,0.78,53,0,23,0,0,False,False,True,False,False,News,https://b.thumbs.redditmedia.com/y2wBi86uF1XeHXnTzePDzht-bTJzYHo2bv7tMDBfyTc.jpg,t3_1gvfpdw
MachineLearning,[D] Cerebras Inference Results for 405B,"Cerebras has just shared some very interesting results on LLM inference. I was first skeptical and thought maybe they used some large batch sizes or some trick to hit almost 1k tokens/s for llama 405B. I tested llama-70B on their website. It's really fast...

I've been reading up on their published paper, but there haven't shared any details on how they run a 405B  parameter model on this huge chip. They have 40GB SRAM, which is huge, but running a 405B model at such low latency and high throughput still sounds interesting. Their papers discuss weight streaming. I think they must have used some advanced data flow analyses to keep the compute busy from the off-chip memory where this huge can be stored.

Does anyone know where I can get more information on this?

Ref: [https://cerebras.ai/blog/llama-405b-inference](https://cerebras.ai/blog/llama-405b-inference)

Paper: [https://arxiv.org/abs/2409.00287](https://arxiv.org/abs/2409.00287)

White Paper: [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10123162](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10123162)

Disclaimer: I have nothing to do with Cerebras systems, just genuinely interested and curious about this. This feels like a pretty big deal for AI in general.",JanGehlYacht,1gvjrmo,https://reddit.com/r/MachineLearning/comments/1gvjrmo/d_cerebras_inference_results_for_405b/,https://www.reddit.com/r/MachineLearning/comments/1gvjrmo/d_cerebras_inference_results_for_405b/,2024-11-20 06:59:01,24,0.86,24,0,7,0,0,False,False,True,False,False,Discussion,self,t3_1gvjrmo
MachineLearning,[R] Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters,"[https://arxiv.org/pdf/2410.24190](https://arxiv.org/pdf/2410.24190)

https://preview.redd.it/9h1ixk8tl62e1.png?width=775&amp;format=png&amp;auto=webp&amp;s=a48f0fbe62599ae5cb53f595e2ed663d4bfec1c7

How could LLMs influence our democracy? We investigate LLMs’ political leanings and the potential influence of LLMs on voters by conducting multiple experiments in a U.S. presidential election context. Through a voting simulation, we first demonstrate 18 open and closed-weight LLMs’ political preference for a Democratic nominee over a Republican nominee. We show how this leaning towards the Democratic nominee becomes more pronounced in instruction-tuned models compared to their base versions by analyzing their responses to candidate-policy related questions. We further explore the potential impact of LLMs on voter choice by conducting an experiment with 935 U.S. registered voters. During the experiments, participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse. This effect is larger than many previous studies on the persuasiveness of political campaigns, which have shown minimal effects in presidential elections. Many users also expressed a desire for further political interaction with LLMs. Which aspects of LLM interactions drove these shifts in voter choice requires further study. Lastly, we explore how a safety method can make LLMs more politically neutral, while raising the question of whether such neutrality is truly the path forward.",ChangeRelevant1378,1gw7t2x,https://reddit.com/r/MachineLearning/comments/1gw7t2x/r_hidden_persuaders_llms_political_leaning_and/,https://www.reddit.com/r/MachineLearning/comments/1gw7t2x/r_hidden_persuaders_llms_political_leaning_and/,2024-11-21 04:22:50,0,0.46,0,0,7,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/FAPec-HxKz1YkDQPJ1DHMmuhx6-4osLfMITGktpArlw.jpg,t3_1gw7t2x
MachineLearning,"[R] BiomedParse is a new biomedical foundation AI model for holistic image analysis that can jointly conduct recognition, detection, and segmentation for 64 major object types across 9 imaging modalities in medicine, outperforming prior state-of-the-art methods.","https://microsoft.github.io/BiomedParse/


https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/


https://youtu.be/WUPUypgmB-s
",Happysedits,1gvccdy,https://reddit.com/r/MachineLearning/comments/1gvccdy/r_biomedparse_is_a_new_biomedical_foundation_ai/,https://www.reddit.com/r/MachineLearning/comments/1gvccdy/r_biomedparse_is_a_new_biomedical_foundation_ai/,2024-11-20 00:16:25,42,0.87,42,0,1,0,0,False,False,True,False,False,Research,self,t3_1gvccdy
MachineLearning,[R] Transposed matrix of the matrix containing the probabilities not changing despite loss term? ,"Hello,

I’ll keep it short. Say we have a neural network with a layer that outputs probabilities using a softmax. This gives us a [batch size, probabilities] tensor. Lets call it P

If I do P_transposed x P, I get a PxP matrix.
My loss uses the Frobenius norm to enforce that this PxP matrix is diagonal (so the off-diagonal values are 0). My hope is that this directly impacts the original matrix’s P structure.

However, this is not the case the PxP matrix does not approach a digital structure nor does P get impacted. This is the case even if I scale the loss by 100.

I would think this would work, am I wrong? Would this not indirectly affect our P matrix? 
Thanks! ",Grand_Comparison2081,1gw2zpw,https://reddit.com/r/MachineLearning/comments/1gw2zpw/r_transposed_matrix_of_the_matrix_containing_the/,https://www.reddit.com/r/MachineLearning/comments/1gw2zpw/r_transposed_matrix_of_the_matrix_containing_the/,2024-11-21 00:27:15,0,0.33,0,0,4,0,0,False,False,True,False,False,Research,self,t3_1gw2zpw
MachineLearning,[P] Collection of SOTA TTS models,"As part of an ongoing project, I released what I think is the biggest collection of open-source voice-cloning TTS models here: [https://github.com/ttsds/datasets](https://github.com/ttsds/datasets)

I think it's very interesting how we haven't really reached a consensus on the rough ""best"" architecture for TTS yet, although I personally think audio token LLM-like approaches (with text prompts for style) will be the way forward.

https://preview.redd.it/2yru8a4oiu1e1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=73d48db7ce384e556e963385898c7f901d58c495

I'm currently evaluating the models across domains, will  be a more substantial post here when that's done :)

Edit: Also some trends (none of them surprising) that can be observed - we seem to be moving away from predicting prosodic correlates and training on only LibriVox data. Grapheme2Phoneme seems to be here to stay though (for now?)

Edit2: An older version of the benchmark with fewer models and only audiobook speech is available here: [https://huggingface.co/spaces/ttsds/benchmark](https://huggingface.co/spaces/ttsds/benchmark)",cdminix,1guv9jl,https://reddit.com/r/MachineLearning/comments/1guv9jl/p_collection_of_sota_tts_models/,https://www.reddit.com/r/MachineLearning/comments/1guv9jl/p_collection_of_sota_tts_models/,2024-11-19 11:45:27,36,0.89,36,0,5,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/PHihsfcsfAnEMoCYpzoamyk6xUb2_TYynph7c3JBQfc.jpg,t3_1guv9jl
MachineLearning,[D] What’s the most surprising or counterintuitive insight you’ve learned about machine learning recently?,ML often challenges assumptions. What’s something you learned that flipped your understanding or made you rethink a concept?,BrechtCorbeel_,1gujfj2,https://reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/,https://www.reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/,2024-11-18 23:51:36,262,0.95,262,0,85,0,0,False,False,True,False,False,Discussion,self,t3_1gujfj2
MachineLearning,[D] Optimal strategy for high volume image loading.,"Hi all,

Curious about what folks think but I'm trying to sample ~ 1-2 million images per epoch on a pretty modern home workstation (7950, 4090, NVMe drives). I started off as a baseline metric of randomly reading jpegs. This got me to about 5ms per image by optimizing decompression libraries, pre-processing, and so on so all I need to do is read off disk.

To push this further I loaded about 10k images into a numpy array and saved them in sequential block with the thought that each would represent chunks of a fold and I can shuffle and randomly sample from this batch that I pre-generate. In doing so I get an average of 1ms per image or so.

I tried to use threading since I thought this was mostly an IO bound task and if I run two threads I get ~20% speed up and loading at .7ms per image. 

The problem I see is that it's still ~18 minutes per epoch which is a bit slower than I was hoping. I think the current issue is that the sequential 10k image blocks are compressed numpy arrays (similar performance if I use hd5) and that multi threading gets me SOME benefit but the decompression is cpu limited.

The next thought I had was well I can use multi processing to help since it's mostly CPU bound, but have had very little luck getting this to work as the transfer times between processes (decompressed data being copied back from the separate process) is pretty costly.

So my general question, do folks have some ideas/approaches that would help? Memory mapped databases don't really help since I'm doing basically single hits on all these files and I need to intelligently cache and not just let the kernel do it. I've been thinking about offloading to a C++ so that can handle some more of this, but managing the GIL and debugging have been a nightmare so less included to go this way.",R2FuckYou,1gv3xif,https://reddit.com/r/MachineLearning/comments/1gv3xif/d_optimal_strategy_for_high_volume_image_loading/,https://www.reddit.com/r/MachineLearning/comments/1gv3xif/d_optimal_strategy_for_high_volume_image_loading/,2024-11-19 18:18:41,7,0.82,7,0,18,0,0,False,False,True,False,False,Discussion,self,t3_1gv3xif
MachineLearning,[D] What’s a machine learning paper or research breakthrough from the last year that everyone should know about?,Share a paper or idea that really stood out to you and why it matters to the field.,BrechtCorbeel_,1gujge8,https://reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/,https://www.reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/,2024-11-18 23:52:41,192,0.94,192,0,54,0,0,False,False,True,False,False,Discussion,self,t3_1gujge8
MachineLearning,[R] About dual submission in AI conferences.. help,"Hi, my advisor and I am new to this area, has no experience on submission via openreview.

I submitted a paper to AAAI and ICLR, and I should have cancelled ICLR one, but did not.

so its desk-rejected, and ICLR make it accessible publicly.

I'm concerning that when I try later, on other AI conferences (via openreview or CMT), would it be also desk-rejected because its now publicly accessible?

  
Thank you for any advice :) I'm suffering from it because I can't get clear answer from anyone I physically know...",catndante,1gvisp9,https://reddit.com/r/MachineLearning/comments/1gvisp9/r_about_dual_submission_in_ai_conferences_help/,https://www.reddit.com/r/MachineLearning/comments/1gvisp9/r_about_dual_submission_in_ai_conferences_help/,2024-11-20 05:53:54,0,0.46,0,0,8,0,0,False,False,True,False,False,Research,self,t3_1gvisp9
MachineLearning,[R] End to end learned planner for AVs,"I am trying to learn more on end to end learned systems. My background is on the perception &amp; mapping side, planning is kind of my blind spot. I am having trouble understanding how some of these planner approaches are implemented in practice. I've read the MP3 paper and the Lift,Splat,Shoot papers in detail. As much as I understand the premise of both approaches I am failing to understand how they have implemented the planning step. I was unable to find a repo which attempts implementing these to see how they had done it.

Can someone point me the way to replicate the implementations there at least at a high level? If there is a better place to get started on this topic please point that out too.

From the LSS and MP3 papers, what I don't understand is what kind of network architecture they might have used and anything else I should consider here that is not perhaps explained in the paper well.

What I understand from the papers is: They both do binning of expert trajectories so that they can score and select the best trajectory according to their loss functions. This step looks more like classification over template trajectories than anything else to me. They both rely on the intermediate representations as part of their cost functions. Both papers explain the loss functions (mp3 is a lot more involved than LSS) but they do not explain how they implemented the respective networks. MP3 rolls out a bicycle model according to the initial conditions of the ego vehicle over the selected trajectory, where LSS uses the trajectory directly.",HonestConcentrate947,1gv8b5i,https://reddit.com/r/MachineLearning/comments/1gv8b5i/r_end_to_end_learned_planner_for_avs/,https://www.reddit.com/r/MachineLearning/comments/1gv8b5i/r_end_to_end_learned_planner_for_avs/,2024-11-19 21:20:43,2,0.58,2,0,0,0,0,False,False,True,False,False,Research,self,t3_1gv8b5i
MachineLearning,[D] Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Link to the paper: [https://arxiv.org/pdf/2411.10436](https://arxiv.org/pdf/2411.10436)  
  
Link to the podcast summarizing the paper: [https://youtu.be/w993gQ4TjSU](https://youtu.be/w993gQ4TjSU)  
  
The paper is titled ""Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization"" authored by researchers from Renmin University of China and Tencent.  
  
Key Points  
  
This research presents a novel approach called Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in Multimodal Large Language Models (MLLMs). The paper addresses three key causes of hallucinations:  
  
Insufficient Visual Capabilities: When MLLMs' visual encoders lack strength and get distracted by unimportant visual information  
Long Context Generation: Hallucinations increase as generated content grows longer  
Multimodal Conflicts: Conflicts between text and image information lead to hallucinations  
  
Methodology  
  
The researchers developed three types of preference pair data targeting these hallucination causes:  
  
Visual Distracted Hallucination (VDH): Preserves only low-attention visual tokens to produce targeted negative responses  
Long Context Hallucination (LCH): Creates negative examples where later parts of responses deviate from image content  
Multimodal Conflict Hallucination (MCH): Adds conflicting information to prompts to generate negative examples  
  
The experimental results demonstrate that HDPO achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art methods while requiring significantly less training data.",Busy-Basket-5291,1guzluu,https://reddit.com/r/MachineLearning/comments/1guzluu/d_mitigating_hallucination_in_multimodal_large/,https://www.reddit.com/r/MachineLearning/comments/1guzluu/d_mitigating_hallucination_in_multimodal_large/,2024-11-19 15:22:04,5,0.67,5,0,2,0,0,False,False,True,False,False,Discussion,self,t3_1guzluu
MachineLearning,[D] Looking for a Realistic German Text-to-Speech Voice (Alternative to Edge-TTS),"Hi everyone,

I’m looking for a way to generate realistic German speech that doesn’t rely on Edge-TTS. While I’ve been using Edge-TTS, I’m not satisfied with the quality and need something that sounds more natural. Ideally, I’m looking for a solution that works locally and is not cloud-based. Has anyone here had experience with tools or services that offer high-quality German TTS voices?

Here are some key points I’m looking for:
- The voice must be German.

- The voice should sound as realistic and natural as possible.

- The tool should be flexible and easy to use.

- It needs to be a local solution, not cloud-based.

- Cost is not a primary concern – I’m looking for the best option available.

I’d really appreciate any suggestions or experiences you can share!
",yeah280,1guxo18,https://reddit.com/r/MachineLearning/comments/1guxo18/d_looking_for_a_realistic_german_texttospeech/,https://www.reddit.com/r/MachineLearning/comments/1guxo18/d_looking_for_a_realistic_german_texttospeech/,2024-11-19 13:55:09,6,0.88,6,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1guxo18
MachineLearning,[D] Why ML PhD is so competitive?,"In recent years, ML PhD admissions at top schools or relatively top schools getting out of the blue. Most programs require prior top-tier papers to get in. Which considered as a bare minimum.

On the other hand, post PhD Industry ML RS roles are also extremely competitive as well.

But if you see, EE jobs at Intel, NVIDIA, Qualcomm and others are relatively easy to get, publication requirements to get into PhD or get the PhD degree not tight at all compared to ML. And I don’t see these EE jobs require “highly-skilled” people who know everything like CS people (don’t get me wrong that I devalued an EE PhD). Only few skills that all you need and those are not that hard to grasp (speaking from my experience as a former EE graduate).

I graduated with an EE degree, later joined a CS PhD at a moderate school (QS &lt; 150). But once I see my friends, I just regret to do the CS PhD rather following the traditional path to join in EE PhD. ML is too competitive, despite having a better profile than my EE PhD friends, I can’t even think of a good job (RS is way too far considering my profile). 

They will get a job after PhD, and most will join at top companies as an Engineer. And I feel, interviews at EE roles as not as difficult as solving leetcode for years to crack CS roles. And also less number of rounds in most cases. ",AntelopeWilling2928,1gu9os9,https://reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/,https://www.reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/,2024-11-18 17:07:35,192,0.87,192,0,88,0,0,False,False,True,False,False,Discussion,self,t3_1gu9os9
MachineLearning,"[D] Eric Schmidt says that scaling laws are not yet stopping AI, what do you guys think?","This is the article in question: https://www.windowscentral.com/software-apps/theres-no-evidence-scaling-laws-have-begun-to-stop-former-google-ceo-claims-ai-systems-will-be-100-times-more-powerful

(I am sure there are far better articles on this topic, but I read this one first)",Born_Replacement_687,1guncvr,https://reddit.com/r/MachineLearning/comments/1guncvr/d_eric_schmidt_says_that_scaling_laws_are_not_yet/,https://www.reddit.com/r/MachineLearning/comments/1guncvr/d_eric_schmidt_says_that_scaling_laws_are_not_yet/,2024-11-19 03:01:50,31,0.72,31,0,56,0,0,False,False,True,False,False,Discussion,self,t3_1guncvr
MachineLearning,[R] Dialog2Flow: Pre-training Soft-Contrastive Sentence Embeddings for Automatic Dialog Flow Extraction,"[This paper](https://aclanthology.org/2024.emnlp-main.310/), presented at EMNLP 2024 main conference, introduces a novel sentence embedding model that captures both the semantics and communicative intention of utterances. This allows for the modeling of conversational ""steps"" and thus the extraction of dialog flows.

**Key Contributions:**

* **Intent-Aware Embeddings:** The model encodes utterances with a richer representation that includes their intended communicative purpose ([available in Hugging Face](https://huggingface.co/collections/sergioburdisso/dialog2flow-67162ca33155cb90a533b7fa)).
* **Dialog Flow Extraction:** By clustering utterance embeddings, the model can automatically identify the ""steps"" or transitions within a conversation, effectively generating a dialog flow graph ([Github code available](https://github.com/idiap/dialog2flow)).
* **Soft-Contrastive Loss:** The paper introduces a new supervised loss function that can be beneficial for representation learning tasks with numerous labels ([implementation available](https://github.com/idiap/dialog2flow?tab=readme-ov-file#chart_with_downwards_trend-proposed-soft-contrastive-loss)).
* **Dataset:** A collection of 3.4 million utterances annotated with ground truth intent ([available in Hugging Face](https://huggingface.co/datasets/sergioburdisso/dialog2flow-dataset)).

**Resources:**

* **Paper:** [here](https://aclanthology.org/2024.emnlp-main.310/)
* **Github repo:** [here](https://github.com/idiap/dialog2flow) (including code to replicate paper and generate also [the interactive 3D Voronoi plots for sentence embeddings](http://tworld.io/extra/dialog2flow_example/voronoi_user_dialog2flow-joint-bert-base.html))
* **Hugging Face models:** [here](https://huggingface.co/collections/sergioburdisso/dialog2flow-67162ca33155cb90a533b7fa)
* **Hugging Face dataset:** [here](https://huggingface.co/datasets/sergioburdisso/dialog2flow-dataset)
* **License:** MIT License

Hope some of you find it useful and let me know if you have any questions or thoughts! :)",sergbur,1guus4a,https://reddit.com/r/MachineLearning/comments/1guus4a/r_dialog2flow_pretraining_softcontrastive/,https://www.reddit.com/r/MachineLearning/comments/1guus4a/r_dialog2flow_pretraining_softcontrastive/,2024-11-19 11:12:58,6,0.88,6,0,0,0,0,False,False,True,False,False,Research,self,t3_1guus4a
MachineLearning,[D] [R] Utilizing an LLM as a de-noising step in an Autoencoder to better understand time-series to language modality matching? Please give feedback!,"Hi all, I was hoping some of you could give feedback to an idea that's been on y mind for a while! There's been a lot of recent advancement in the LLM-for-timeseries space (because of course there is...), which has been hard to ignore. In particular, [Time-LLM ](https://arxiv.org/abs/2310.01728)produced SOTA results for time-series forecasting by reprogramming time-series patch embeddings to produce representations over the text-embedding space an fed them to a frozen LLM's attention + MLP layers, turning the output into a forecast. This is a bit of a surprising result for me as it is not a natural idea, and it is confusing how such distinct modalities can align in such a way.

The proposed project aims to utilize a similar strategy for the multiple imputation of time-series task via using a basic LLM such as BERT as the de-noising step in an autoencoder, treating the word embedding space as a latent space to encode onto. We utilize the decoder to decode both LLM input and output, targeting observation reconstruction for the input and masked imputation for the output. We can then examine the latent space produced, and compare it to latent spaces produced by more traditional models.

I mainly want feedback on the following points:

1. Interpretability: Theres a pretty large gap in theoretical understanding of how these modalities align; in the text-to-visual case, it's easier to understand how modality matching is feasible as language is inherently descriptive and caries *visual* meaning (i.e. words for colors, shapes, animals, things can be visualized). As such, Time-LLM was heavily critiqued for gaps in theoretical grounding and interpretability, despite empirical performance. By using an autoencoder structure that attempts to preserve observations in this way do you think these concerns can be adequately addressed? 

2. Latent Space Cartography: I am not so well versed in studying latent spaces, and have not been able to understand literature relating to it (that I have found). Does anyone know if there's any useful tool / papers that I can utilize for better understanding the properties of the latent space my model would uncover? 

3. Usefulness: Is this idea even a good, useful idea? Or is my subconscious being hooked by the LLM craze T\_T

Thanks for taking the time to read this! Any and all feedback would be appreciated. ",TheOneYourSon,1gvcvkc,https://reddit.com/r/MachineLearning/comments/1gvcvkc/d_r_utilizing_an_llm_as_a_denoising_step_in_an/,https://www.reddit.com/r/MachineLearning/comments/1gvcvkc/d_r_utilizing_an_llm_as_a_denoising_step_in_an/,2024-11-20 00:41:21,0,0.33,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gvcvkc
MachineLearning,[Discussion] : Anomaly Detection Data Differences and suggestions,"Hello fellow Redditors, I am a student who is trying to do an unsupervised anomaly detection on a telecommunication stack data. I have different types of messages that include different features with different lengths. For example,

PDSCH, have n-many real-valued numerical values.  
PDCP DL, have m-many real-valued numerical values.  
… and so on.

What I'm trying to find is to pinpoint the lines (which is either of these message types), and the feature of that message that contributes most to the anomaly score. Currently, I have a LSTM-VAE model that I developed, but it doesn't quite well. I have a question for the representation of the data. Which one should I choose, assuming the message types are not heavily dependent? They have some implicit dependency, but not directly visible to humans.

1- Separate encoder-decoders for each message type that creates the same dimensional embedding latent space.  
2- Shared embedding space whose dimensionality is the maximum dimensionality of the message types. For example, if the most number of features are on PDSCH with n, then the embedding would be n-by-d with other message types vectors are padded with zeros.

Also, I'm open to all suggestions on detecting the anomalies, currently my plan is to use a 95-percentile as threshold and marking the others as anomaly. For each message type calculate the normalized loss and find the most contributing feature of each anomaly message type.

Also, since we don't have any labels for the anomalies, I am planning to create a online-learning mechanism to label the anomalies on the run if they are good or bad, so to decrease the manual labelling effort. Do you think is this a good idea?

Thank you so much if you read so far. I appreciate any help for this miserable student. :)",hezarfenserden,1gv21ox,https://reddit.com/r/MachineLearning/comments/1gv21ox/discussion_anomaly_detection_data_differences_and/,https://www.reddit.com/r/MachineLearning/comments/1gv21ox/discussion_anomaly_detection_data_differences_and/,2024-11-19 17:02:55,2,0.75,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1gv21ox
MachineLearning,[D] Tips on building efficient offline video data pipelines in production?,Anyone have experience processing/analyzing large amounts of video data offline in a production setting?What does your pipeline look like right? What kinds of tooling/infrastructure do you use?,answersareallyouneed,1gusx2j,https://reddit.com/r/MachineLearning/comments/1gusx2j/d_tips_on_building_efficient_offline_video_data/,https://www.reddit.com/r/MachineLearning/comments/1gusx2j/d_tips_on_building_efficient_offline_video_data/,2024-11-19 08:56:06,7,0.89,7,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1gusx2j
MachineLearning,[R] Automating Python Package Creation with Agentic Workflow,"**Research**

PyGen is an open-source tool designed to automate the generation of Python packages from user-provided prompts. By leveraging advanced language models, PyGen streamlines the development process, producing packages complete with testing and documentation. This approach has been applied to create tools such as AutoML (automated machine learning), AutoVision (computer vision), AutoSpeech, and Quantum Error Correction utilities.

**Key Contributions:**

* **Automated Package Generation:** PyGen simplifies the creation of Python packages by generating code, tests, and documentation based on user inputs.
* **Advanced Language Model Integration:** Utilizes sophisticated language models to interpret prompts and produce relevant code structures.
* **Versatile Applications:** Demonstrated effectiveness in developing diverse tools, including AutoML, AutoVision, AutoSpeech, and Quantum Error Correction utilities.

**Resources:**

* **Paper:** [here](https://www.arxiv.org/abs/2411.08932)
* **GitHub Repository:** [here](https://github.com/GitsSaikat/PyGen)
* **License:** MIT License

We hope this tool proves useful for your projects. Feel free to explore the resources and share your feedback or questions.",Any_Code_4027,1gv4hnv,https://reddit.com/r/MachineLearning/comments/1gv4hnv/r_automating_python_package_creation_with_agentic/,https://www.reddit.com/r/MachineLearning/comments/1gv4hnv/r_automating_python_package_creation_with_agentic/,2024-11-19 18:41:09,0,0.33,0,0,0,0,0,False,False,True,False,False,Research,self,t3_1gv4hnv
MachineLearning,[R] LLaVA-o1: Multi-Stage Visual Reasoning through Inference-Time Scaling,"I've been analyzing this new approach to visual reasoning that enhances vision-language models through step-by-step reasoning capabilities. The key innovation is integrating chain-of-thought prompting techniques with visual analysis, allowing models to break down complex visual tasks into discrete reasoning steps.

The main technical components and results:

- Integration of chain-of-thought prompting with vision-language models
- Two-stage process: general visual understanding followed by task-specific reasoning
- Achieves 15% improvement on complex visual reasoning benchmarks
- Architecture combines visual encoder with language model for staged processing
- Uses contrastive learning to align visual and textual representations

The methodology is particularly effective for:
- Counting tasks requiring sequential attention
- Comparative analysis between multiple objects
- Spatial reasoning problems
- Multi-step visual problem solving

Key implementation details:
- Built on LLaVA architecture with enhanced reasoning modules
- Uses CLIP-based visual encoder
- Implements temperature-controlled sampling for reasoning steps
- Employs beam search for answer generation

Mostly matters for applications requiring detailed visual analysis. The step-by-step approach makes the reasoning process more transparent and debuggable, though at the cost of increased computational overhead. This tradeoff between performance and efficiency will need consideration for real-world deployments.

TLDR: New method improves visual reasoning in vision-language models by implementing step-by-step analysis, achieving 15% better performance on complex tasks while providing more transparent reasoning processes.

[Full summary is here](https://aimodels.fyi/papers/arxiv/llava-o1-let-vision-language-models-reason). Paper [here](https://arxiv.org/abs/2411.10440).",Successful-Western27,1guyrr8,https://reddit.com/r/MachineLearning/comments/1guyrr8/r_llavao1_multistage_visual_reasoning_through/,https://www.reddit.com/r/MachineLearning/comments/1guyrr8/r_llavao1_multistage_visual_reasoning_through/,2024-11-19 14:45:40,1,0.57,1,0,1,0,0,False,False,True,False,False,Research,self,t3_1guyrr8
MachineLearning,[D] A mature agentic system for common white color work tasks,"Many ""white collar""  tasks - e.g., checking / responding to email / slack messages, etc., moving meetings around, writing ""memos"", etc., - can be emulated / effectively dealt with in principle.   I've seen plenty of ""demos"" around illustrating some of these components  / functions.

Are there any projects / apps are out there that flush this out more fully?  e.g., any [llama stack](https://github.com/meta-llama/llama-stack) projects?",neonwatty,1gv0us8,https://reddit.com/r/MachineLearning/comments/1gv0us8/d_a_mature_agentic_system_for_common_white_color/,https://www.reddit.com/r/MachineLearning/comments/1gv0us8/d_a_mature_agentic_system_for_common_white_color/,2024-11-19 16:14:20,1,0.6,1,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1gv0us8
MachineLearning,[R] Performance Analysis of GPU Interconnect Technologies Across Three Modern Supercomputer Architectures,"I found this study examining GPU-to-GPU communication in supercomputer systems to be quite informative. The key contribution is a systematic analysis of different interconnect technologies and their impact on multi-GPU performance.

The main technical findings include:

- NVLink provides highest bandwidth (up to 900 GB/s bidirectional) but comes with higher latency overhead
- InfiniBand shows lower latency (1-2μs) but reduced bandwidth compared to NVLink
- PCIe demonstrates consistent but lower performance metrics across all tests
- Topology and physical GPU arrangement significantly impact communication patterns

Some key methodology points:
- Tested multiple hardware configurations with 4-16 GPUs
- Measured bandwidth, latency, and completion time for standard communication patterns
- Analyzed impact of different data sizes and communication patterns
- Compared theoretical vs achieved bandwidth across interconnects

The practical implications I think are...

- Optimal interconnect choice depends heavily on workload characteristics
- Large model training benefits more from high bandwidth (NVLink)
- Distributed inference may prefer lower latency solutions (InfiniBand)
- Physical GPU topology should match common communication patterns

**TLDR**: Comprehensive analysis of GPU interconnect performance showing tradeoffs between bandwidth, latency, and topology. Results suggest workload-specific optimization of interconnect choice is crucial for multi-GPU systems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/exploring-gpu-to-gpu-communication-insights-into). Paper [here](https://arxiv.org/abs/2408.14090).",Successful-Western27,1gu7swl,https://reddit.com/r/MachineLearning/comments/1gu7swl/r_performance_analysis_of_gpu_interconnect/,https://www.reddit.com/r/MachineLearning/comments/1gu7swl/r_performance_analysis_of_gpu_interconnect/,2024-11-18 15:50:20,25,0.93,25,0,1,0,0,False,False,True,False,False,Research,self,t3_1gu7swl
MachineLearning,[P] Small object detection without SAHI ,"Hi everyone, I hope I have come to the right place.

I am currently working on a project which needs to detect the very small objects with a messy background in phone camera. These objects only has 10\~20 pixels out of 3024 x 4032 pictures.

I have trained a yolov8 model with SAHI and tiling. To me, the results are good enough with map of 80%, making some false positive in the background but basically detect all the small ones. But my supervisor wasn't very happy about it since there is still false positives and SAHI can't work in real time in a phone.

Would you have any suggestions, that could be implement in a phone setting?",Delay_no_more_1999,1gubjpn,https://reddit.com/r/MachineLearning/comments/1gubjpn/p_small_object_detection_without_sahi/,https://www.reddit.com/r/MachineLearning/comments/1gubjpn/p_small_object_detection_without_sahi/,2024-11-18 18:22:23,6,0.88,6,0,1,0,0,False,False,True,False,False,Project,self,t3_1gubjpn
MachineLearning,[P] Still Drowning in Research Papers? Ribbit Ribbit Hops to Web and Android!,"Hey friends! Last month, we shared Ribbit Ribbit, our little research paper discovery tool on iOS, and wow—thank you so much for the love! Over the past few weeks, we’ve been hopping around to bring it to more places, and now we’re excited to share:

* **The full website** [**https://ribbitribbit.co**](https://ribbitribbit.co) **is live!** It has all the features from the app. You can ribbit your way through papers on a big screen for extra clarity or keep it mobile on your phone to browse anywhere—research, your way! 
* **Android is (almost) here!** It’s available through Google Play Testing. Google needs enough testers before it can go live, so if you’re up for trying it early, join our tester squad here: [https://ribbitribbit.co/request?testandroid=true](https://ribbitribbit.co/request?testandroid=true). You’d totally be our hero!

Ribbit Ribbit helps you find personalized paper recommendations, shrinks them into tweet-sized summaries, and even reads them to you like a podcast. We’re just trying to make the whole research thing a little more fun. We’d love for you to check it out. Your support means the world to us!

https://preview.redd.it/hyf9e6rmxk1e1.png?width=1492&amp;format=png&amp;auto=webp&amp;s=9a4deb6f3b70c9cf79d3441846ee03d6d6b93d22

",haoyuan8,1gtvmpn,https://reddit.com/r/MachineLearning/comments/1gtvmpn/p_still_drowning_in_research_papers_ribbit_ribbit/,https://www.reddit.com/r/MachineLearning/comments/1gtvmpn/p_still_drowning_in_research_papers_ribbit_ribbit/,2024-11-18 03:31:32,63,0.82,63,0,15,0,0,False,False,True,False,False,Project,https://b.thumbs.redditmedia.com/fdYlst9z35K5UyqCk_r7ygy9p66P_iACbCxUsFDmoSY.jpg,t3_1gtvmpn
MachineLearning,Free GPU/TPU powered Notebook Service [Project],"I need to do a Machine Learning project. I want to try and test out various architectures and research about them in some datasets and hopefully write a good research paper. The size of dataset is around 60GB. Is there a free good GPU powered AI/Ml Notebook service except Colab or Sagemaker Studio Lab? I want better than these. I tried Azure for Students, but it frustratingly doesn't allow me to use powerful NVIDIA GPUs. Unfortunately, my university doesn't provide any GPUs to students. Can't imagine training models in my laptop. Any help/suggestions would be really helpful.",Due-Rest6652,1gu8ej2,https://reddit.com/r/MachineLearning/comments/1gu8ej2/free_gputpu_powered_notebook_service_project/,https://www.reddit.com/r/MachineLearning/comments/1gu8ej2/free_gputpu_powered_notebook_service_project/,2024-11-18 16:15:22,3,0.62,3,0,8,0,0,False,False,True,False,False,Project,self,t3_1gu8ej2
MachineLearning,[D] Expectation from Machine Learning Engineering jobs,"Hey everyone,

I’ve seen a lot of posts here about careers in ML and landing internships or jobs, and two things come up a lot

1. Building a strong research portfolio and publishing at conferences like NeurIPS, ICLR, and ICML, which seems to focus more on getting research scientist roles.

2. The growing demand for Machine Learning Engineer (MLE) roles, which are apparently more in demand than research scientist positions.

I’m curious about the difference between these two roles and what kind of portfolio would be ideal for landing an MLE position. I know having a master’s degree is often preferred, but is an impressive publication record necessary for MLE roles? Or is it not that big of a deal?

What are your thoughts?",ziggyboom30,1gtt099,https://reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/,https://www.reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/,2024-11-18 01:14:14,76,0.89,76,0,22,0,0,False,False,True,False,False,Discussion,self,t3_1gtt099
MachineLearning,[P] CNN Model Having High Test Accuracy but Failing in Custom Inputs,"
I am working on a project where I trained a model using SAT-6 Satellite Image Dataset (The Source for this dataset is NAIP Images from NASA) and my ultimate goal is to make a mapping tool that can detect and large map areas using satellite image inputs using sliding windows method. 

I implemented the DeepSat-V2 model and created promising results on my testing data with around %99 accuracy. 

However, when I try with my own input images I rarely get a significantly accurate return that shows this accuracy. It has a hard time making correct predictions especially its in a city environment. City blocks usually gets recognized as barren land and lakes as trees for some different colored water bodies and buildings as well. 

It seems like it’s a dataset issue but I don’t get how 6 classes with 405,000 28x28 images in total is not enough. Maybe need to preprocess data better? 

What would you suggest doing to solve this situation? 

The first picture is a google earth image input, while the second one is a picture from the NAIP dataset (the one SAT-6 got it’s data from). The NAIP one clearly performs beautifully where the google earth gets image gets consistently wrong predictions.

SAT-6: https://csc.lsu.edu/~saikat/deepsat/

DeepSat V2: https://arxiv.org/abs/1911.07747",yagellaaether,1gujazf,https://reddit.com/r/MachineLearning/comments/1gujazf/p_cnn_model_having_high_test_accuracy_but_failing/,https://www.reddit.com/gallery/1gujazf,2024-11-18 23:46:06,1,1.0,1,0,2,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/V1dyMH_CEVEhPokLqFUsGhGXa5-IKEQbh9TVxYanZoo.jpg,t3_1gujazf
