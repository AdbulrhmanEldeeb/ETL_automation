subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[D]Stuck in AI Hell: What to do in post LLM world,"
Hey Reddit,

I’ve been in an AI/ML role for a few years now, and I’m starting to feel disconnected from the work. When I started, deep learning models were getting good, and I quickly fell in love with designing architectures, training models, and fine-tuning them for specific use cases. Seeing a loss curve finally converge, experimenting with layers, and debugging training runs—it all felt like a craft, a blend of science and creativity. I enjoyed implementing research papers to see how things worked under the hood. Backprop, gradients, optimization—it was a mental workout I loved.

But these days, it feels like everything has shifted. LLMs dominate the scene, and instead of building and training models, the focus is on using pre-trained APIs, crafting prompt chains, and setting up integrations. Sure, there’s engineering involved, but it feels less like creating and more like assembling. I miss the hands-on nature of experimenting with architectures and solving math-heavy problems.

It’s not just the creativity I miss. The economics of this new era also feel strange to me. Back when I started, compute was a luxury. We had limited GPUs, and a lot of the work was about being resourceful—quantizing models, distilling them, removing layers, and squeezing every bit of performance out of constrained setups. Now, it feels like no one cares about cost. We’re paying by tokens. Tokens! Who would’ve thought we’d get to a point where we’re not designing efficient models but feeding pre-trained giants like they’re vending machines?

I get it—abstraction has always been part of the field. TensorFlow and PyTorch abstracted tensor operations, Python abstracts C. But deep learning still left room for creation. We weren’t just abstracting away math; we were solving it. We could experiment, fail, and tweak. Working with LLMs doesn’t feel the same. It’s like fitting pieces into a pre-defined puzzle instead of building the puzzle itself.

I understand that LLMs are here to stay. They’re incredible tools, and I respect their potential to revolutionize industries. Building real-world products with them is still challenging, requiring a deep understanding of engineering, prompt design, and integrating them effectively into workflows. By no means is it an “easy” task. But the work doesn’t give me the same thrill. It’s not about solving math or optimization problems—it’s about gluing together APIs, tweaking outputs, and wrestling with opaque systems. It’s like we’ve traded craftsmanship for convenience.

Which brings me to my questions:

1. Is there still room for those of us who enjoy the deep work of model design and training? Or is this the inevitable evolution of the field, where everything converges on pre-trained systems?


2. What use cases still need traditional ML expertise? Are there industries or problems that will always require specialized models instead of general-purpose LLMs?


3. Am I missing the bigger picture here? LLMs feel like the “kernel” of a new computing paradigm, and we don’t fully understand their second- and third-order effects. Could this shift lead to new, exciting opportunities I’m just not seeing yet?


4. How do you stay inspired when the focus shifts? I still love AI, but I miss the feeling of building something from scratch. Is this just a matter of adapting my mindset, or should I seek out niches where traditional ML still thrives?



I’m not asking this to rant (though clearly, I needed to get some of this off my chest). I want to figure out where to go next from here. If you’ve been in AI/ML long enough to see major shifts—like the move from feature engineering to deep learning—how did you navigate them? What advice would you give someone in my position?

And yeah, before anyone roasts me for using an LLM to structure this post (guilty!), I just wanted to get my thoughts out in a coherent way. Guess that’s a sign of where we’re headed, huh?

Thanks for reading, and I’d love to hear your thoughts!

TL;DR: I entered AI during the deep learning boom, fell in love with designing and training models, and thrived on creativity, math, and optimization. Now it feels like the field is all about tweaking prompts and orchestrating APIs for pre-trained LLMs. I miss the thrill of crafting something unique. Is there still room for people who enjoy traditional ML, or is this just the inevitable evolution of the field? How do you stay inspired amidst such shifts?

Update: Wow, this blew up. Thanks everyone for your comments and suggestions. I really like some of those. This thing was on my mind for a long time, glad that I put it here. Thanks again!",Educational_News_371,1h7jg87,https://reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,2024-12-05 20:49:57,495,0.97,495,0,161,0,0,False,False,True,False,False,Discussion,self,t3_1h7jg87
MachineLearning,[D] Any OCR recommendations for illegible handwriting?,"
Has anyone had experience using an ML model to recognize handwriting like this? The notebook contains important information that could help me decode a puzzle I’m solving. I have a total of five notebooks, all from the same person, with consistent handwriting patterns. My goal is to use ML to recognize and extract the notes, then convert them into a digital format.

I was considering Google API after knowing that Tesseract might not work well with illegible samples like this. However, I’m not sure if Google API will be able to read it either. I read somewhere that OCR+ CNN might work, so I’m here asking for suggestions. Thanks! Any advice/suggestions are welcomed! ",SpaceSheep23,1h7x5us,https://reddit.com/r/MachineLearning/comments/1h7x5us/d_any_ocr_recommendations_for_illegible/,https://www.reddit.com/gallery/1h7x5us,2024-12-06 08:53:03,70,0.87,70,0,82,0,0,False,False,False,False,False,Discussion,https://b.thumbs.redditmedia.com/LeDstkPiGT-yO7zkyvRRyx6sE33qB2W32UGe1t81Kkk.jpg,t3_1h7x5us
MachineLearning,[D] How does OpenAI’s O1 outperform others in math despite limitations noted in recent papers?,"Recent research has revealed that state-of-the-art LLMs often struggle with mathematical reasoning:

1. The GSM-Symbolic benchmark highlights that LLMs frequently fail when numerical values or question wording change, suggesting reliance on memorization rather than true mathematical understanding ([source](https://arxiv.org/pdf/2410.05229.pdf)).
2. Logical reasoning studies, like the AIW problem, show inconsistent performance even for basic reasoning tasks ([source](https://arxiv.org/pdf/2406.02061.pdf)).
3. Furthermore, research indicates LLMs lack effective self-correction capabilities, with performance degrading after multiple iterations ([source](https://arxiv.org/pdf/2310.01798.pdf)).

Despite these challenges, OpenAI’s new O1 model reportedly exceeds all other models in math benchmarks. How does it address these known issues in mathematical reasoning, such as:

* Reliance on memorization instead of understanding?
* Inconsistencies in reasoning across problem variations?
* Inability to self-correct errors effectively?

Would love to hear insights or hypotheses!",AImSamy,1h7vj5t,https://reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,2024-12-06 06:53:09,33,0.87,33,0,16,0,0,False,False,True,False,False,Discussion,self,t3_1h7vj5t
MachineLearning,U-Net Vs Attention U-Net [D],"Hello folks,

Young researcher here, working on a in-house dataset to build a foundational model for a interesting use-case. But I have thesis to finsh, which will be just the tail of my current research.

For my thesis, we have decided to have a subsection for comparing how my segmentation results differ when used attention blocks are used within a U-Net. I've referred few papers on how this works and how can this be implemented.

Results are promising (att unet outperformimg unets, nothing suprising) but I see a concerning opposing point i.e. attention Unet having more number of parameters that the unet. Is there a way I can conduct this study where I compare results with and without attention? And there are no other additional factors influencing the results (layers, params, etc).

Does conducting ablation study makes sense in this case? I've not seen any other paper comparing similar use-case using this study.

Any papers I can look through, suggestions and tips are welcome.",ade17_in,1h7cjnd,https://reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,2024-12-05 16:03:47,20,0.86,20,0,6,0,0,False,False,True,False,False,Discussion,self,t3_1h7cjnd
MachineLearning,[D] Encode over 100 million rows into embeddings ,"Hey everyone,

I'm working on a pipeline to encode over **100 million rows** into embeddings using **SentenceTransformers**, **PySpark**, and **Pandas UDF** on **Dataproc Serverless**.

Currently, it takes several hours to process everything. I only have one column containing sentences, each under 30 characters long. These are encoded into **64-dimensional vectors** using a custom model in a Docker image.

At the moment, the job has been running for over **12 hours** with **57 executors** (each with **24GB of memory and 4 cores**). I’ve partitioned the data into **2000 partitions**, hoping to speed up the process, but it's still slow.

Here’s the core part of my code:

    F.pandas_udf(returnType=ArrayType(FloatType()))
    def encode_pd(x: pd.Series) -&gt; pd.Series:
        try:
            model = load_model()
            return pd.Series(model.encode(x, batch_size=512).tolist())
        except Exception as e:
            logger.error(f""Error in encode_pd function: {str(e)}"")
            raise

The `load_model` function is as follows:

    def load_model() -&gt; SentenceTransformer:
        model = SentenceTransformer(
            ""custom_model"", 
            device=""cpu"", 
            cache_folder=os.environ['SENTENCE_TRANSFORMERS_HOME'], 
            truncate_dim=64
        )
        return model

I tried broadcasting the model, but I couldn't refer to it inside the Pandas UDF.

Does anyone have suggestions to optimize this? Perhaps ways to load the model more efficiently, reduce execution time, or better utilize resources?",nidalap24,1h7xnce,https://reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,2024-12-06 09:29:37,12,0.88,12,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1h7xnce
MachineLearning,[R] Towards Time Series Reasoning with LLMs,,HydrousIt,1h7sr3n,https://reddit.com/r/MachineLearning/comments/1h7sr3n/r_towards_time_series_reasoning_with_llms/,https://arxiv.org/abs/2409.11376,2024-12-06 04:06:32,13,1.0,13,0,1,0,0,False,False,False,False,False,Research,default,t3_1h7sr3n
MachineLearning,[R] Mastering Board Games by External and Internal Planning with Language Models - DeepMind,"Paper: https://storage.googleapis.com/deepmind-media/papers/SchultzAdamek24Mastering/SchultzAdamek24Mastering.pdf

Abstract:

While large language models perform well on a range of complex tasks (e.g., text generation, question
answering, summarization), robust multi-step planning and reasoning remains a considerable challenge
for them. In this paper we show that search-based planning can significantly improve LLMs’ playing
strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We
introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo
Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search,
the model directly generates in-context a linearized tree of potential futures and a resulting final choice.
Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and
value functions across these games. We find that our pre-training method minimizes hallucinations, as
our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and
external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level
performance in chess while operating on a similar move count search budget per decision as human
Grandmasters. The way we combine search with domain knowledge is not specific to board games,
suggesting direct extensions into more general language model inference and training techniques.",RobbinDeBank,1h7nshy,https://reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,https://www.reddit.com/r/MachineLearning/comments/1h7nshy/r_mastering_board_games_by_external_and_internal/,2024-12-05 23:58:37,8,0.85,8,0,1,0,0,False,False,True,False,False,Research,self,t3_1h7nshy
MachineLearning,[D] How to remove noise in this dataset,"I have a dataset that, when plotted, shows a noisy black line. I'd like to smooth out this noise to get a cleaner trend line (similar to the red line shown). What methods would you recommend for noise reduction?

https://preview.redd.it/p8q0i4f9h45e1.png?width=1574&amp;format=png&amp;auto=webp&amp;s=29cd8c82af7b54a1d3da22655502fd5cf406e807",mrtule,1h7ohd0,https://reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h7ohd0/d_how_to_remove_noise_in_this_dataset/,2024-12-06 00:30:30,5,1.0,5,0,12,0,0,False,False,True,False,False,Discussion,self,t3_1h7ohd0
MachineLearning,"Image Generation Model Evaluation Challenge (Würstchen, KOALA, PixArt-α) [P]","# Project Description:

We are inviting skilled professionals to participate in an evaluation challenge to produce a GenAI image based upon a prompt and a set of component images.  The candidate may choose whatever models they like to complete the task, so long as they are open source. 

The results will be reviewed and compared across participants, and the candidate with the most effective and high-quality outputs will be selected for a larger, production-focused engagement. 

# Entry Process:

Submit your Github handle and intention to participate to email acr0batproduce@gmail.com.  Provide a short bio and resume in your email.  If you’re selected, we will provide you access to a repository to contribute to.  

**Challenge Scope:**

1. **Model Setup and Testing**:
   * Create a unique set of prompts to test your image generation model.  A set of [sample prompts](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) have been provided for your convenience.  A minimum of 3 tests are required, but candidates may provide more if they wish.
   * For each image generation, provide at least 3 component images to be used in the final output.  A set of [component images](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.p13d4p443fpl) related to the [sample prompt](https://docs.google.com/document/d/1BU6gu9Y1LkhSqRNhRVl82jqHmX9xq7J1_RiHNdlV91g/edit?tab=t.0#heading=h.mws7uwdbs4m) has been provided for your convenience.
   * Develop the model, and test its performance with your sample input prompts and component images.  Source code must be pushed to the main branch of the provided repo.
   * Provide the output images in ./static in the repo.
   * Document your findings and results
2. **Evaluation Criteria**:
   * **Image Quality**: The final image produced should incorporate all the features from your sample prompt, and component images.  Above all, the items in the component images need to be naturally incorporated into the final image.  They should not be significantly distorted, or look like they were copy and pasted from the input images.    
   * **Brand Element Incorporation**: Expanding upon the above point, the final image must accurately reflect the input component images.  So, if the input image is a Rolex Oyster Perpetual Day-Date 40 in 18 kt yellow gold with a champagne colour, diamond-set dial, fluted bezel and a President bracelet, then the output image must incorporate that same product.
   * **Creative Flexibility**: Generate diverse variations of prompts, component images and output images.
   * **Customization**: Showcase how well your model responds to different parameters.
   * **Code Quality**: All aspects of code quality will be assessed. Examples include: repo structure, IaC, CI/CD, unit testing, performance, documentation, etc.
   * **Extensibility**: Your sample prompts, input images, and output images will be used to quickly screen for accuracy and limit submissions; however, importantly, your model will be tested against our internal prompts and input images to gauge how well it performs on different types of problems. 
3. **Deliverables**:
   * The repo structure is up to you, but the README should make it clear where your model, input images, output images and documentation reside. 

# Challenge Benefits:

* The candidate with the most effective and high-quality outputs will be selected for a larger, production-focused project with a significant budget.
* This is an opportunity to contribute to an innovative, high-growth startup that has already secured investor funding and is positioned for significant market impact.
* Gain the opportunity to showcase your expertise in image generation models and secure a long-term collaboration.

# Requirements:

* Proficiency in all areas of data science, with a focus on AI image generation
* Expert in Python, SQL, and at least 1 cloud provider such as AWS or GCP 
* A compute environment will not be provided; the candidate can develop the solution locally or on the cloud, but at their own expense.
* Candidate must comply with provided NDA

# Submission Guidelines:

* Entry Deadline: 1/1/25
* Project Deadline: 1/7/25
* All source code should be submitted to the repo by the deadline; the candidate will have their access revoked on that date.
* Provide a short bio in your README, and relevant contact information. 

# Selection Process:

* All submissions will be reviewed and compared based on the outlined criteria.
* The most effective and high-quality submission will result in the candidate being awarded a contract for a larger production project.

**I**f you're interested in participating, please reach out! ",AlpacaRampage,1h7hg7w,https://reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,https://www.reddit.com/r/MachineLearning/comments/1h7hg7w/image_generation_model_evaluation_challenge/,2024-12-05 19:26:31,5,0.78,5,0,0,0,0,False,False,True,False,False,Project,self,t3_1h7hg7w
MachineLearning,[D] Have we officially figured out yet how O1 models differ from previous models?,"Edit: I have misworded the title as if OpenAI would confirm how O1 was implemented. I have changed the text to reflect what I meant say.



I really want to deep dive into the technicals of how the O1 models perform better than previous models.

Have researchers come to any definitive agreement as to what OpenAI could have possible done to achieve O1?

From reading online I hear about MCTS, COT... etc, but are any of these methods in large agreement by researhers?
",Daveboi7,1h7zfjg,https://reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,2024-12-06 11:37:51,4,0.83,4,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1h7zfjg
MachineLearning,[D] What approaches can we use to train an open vocabulary or image referential detector for configurable specificity?,"Open vocabulary object detectors allow you to pass in a prompt and an image and attempt to output bounding boxes around objects matching your prompt. Image referential detectors allow you to pass in an image of an object as the prompt and a target image and attempts to output bounding boxes around objects matching the image prompt in the target image.   
  
For a reference, [YOLOWorld](https://github.com/AILab-CVC/YOLO-World) provides both image referential and open vocabulary modes.  
  
The idea I've been toying with is whether there is a good way to train for greater control over specificity. For example, If I pass in an image of a golden retriever, am I looking for golden retrievers specifically? All dogs? All animals? 

Language is a bit more specific, but the same principle can apply. If I search for red cars, do red trucks count? Do maroon cars? In my experience, trying to be too specific textually with OVD models causes erratic behavior. IE, ""A red car or van but not truck "" would give bad performance, as it doesn't really match what would be in grounding captions.

My initial idea for how to systematically define the distance between potential targets and the query is via embedding distance. If I take a phrase grounding dataset, I could compute embeddings separately for each crop of a region and for its corresponding text using a model like CLIP.

A sample training process would be like this

* Select a random image. Select a random image crop embedding.
* Select a random similarity threshold.
* Do an approx KNN using that random image crop embedding, stopping once we reach a sample with an embedding above the similarity threshold. This is our prompt embedding If we selected an image region embedding, we are doing image referential detection. If we selected a text embedding, we are doing open vocabulary detection.
* Calculate prompt embedding similarity to all image crop embeddings in the primary image. Mark all objects with similarity above the threshold as positive examples.
* Run the network, using the selected image, prompt embedding, and similarity threshold as input. Use the previously calculated positives examples as labels.

  
Does anyone know of any papers that work with similar ideas, or have thoughts on whether this process would be useful or could be improved? I'm pretty early into looking into this, so just references or even field terms that would point me in the right direction would be helpful.

Other ideas 

* Always detect all objects given a target phrase, but set the label confidence equal to the embedding similarity. 
* Make the model capable of handling multiple input queries corresponding to the same object, to better indicate the intended domain. Possibly with both negative and positive queries. Not sure how to train yet, possibly sample clusters of similar embeddings from the dataset to build prompt sets.
* Perform instructional tuning, similar to what is done for some LLMs and VLMs, to make the model better handle complex text prompts, and allow instructional text prompts to be paired with images for image referential mode.

  
Related questions

*    Is CLIP still standard for computing text and image embeddings that share a unified embedding space?",Revolutionary-Fig660,1h7knty,https://reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,https://www.reddit.com/r/MachineLearning/comments/1h7knty/d_what_approaches_can_we_use_to_train_an_open/,2024-12-05 21:40:09,2,0.67,2,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7knty
MachineLearning,[D] My fine-tuning loss looks weird,"I am finetuning Qwen2.5 instruct using qLoRA, for a instruction tuning like dataset with around 50k samples, and my training loss is looking weird. What might be the issue, and how can i possibly fix it? Finetuning details are as following, along with training loss graphs:

Code:

\`\`\`  
model, tokenizer = FastLanguageModel.from\_pretrained(

model\_name = ""Qwen/Qwen2.5-32B-Instruct"",

max\_seq\_length = max\_seq\_length,

dtype = None,

load\_in\_4bit = True,

)



\# Do model patching and add fast LoRA weights

model = FastLanguageModel.get\_peft\_model(

model,

r = 64,

target\_modules = \[""q\_proj"", ""k\_proj"", ""v\_proj"", ""o\_proj"",

""gate\_proj"", ""up\_proj"", ""down\_proj"",\],

lora\_alpha = 128,

lora\_dropout = 0, # Supports any, but = 0 is optimized

bias = ""none"",    # Supports any, but = ""none"" is optimized

use\_gradient\_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context

random\_state = 3407,

max\_seq\_length = max\_seq\_length,

use\_rslora = True,  # We support rank stabilized LoRA

loftq\_config = None, # And LoftQ

)



from trl import SFTTrainer

from transformers import TrainingArguments

from unsloth import is\_bfloat16\_supported



trainer = SFTTrainer(

model = model,

tokenizer = tokenizer,

train\_dataset = dataset\['train'\],

dataset\_text\_field = ""text"",

max\_seq\_length = max\_seq\_length,

dataset\_num\_proc = 2,

packing = False,

args = TrainingArguments(

per\_device\_train\_batch\_size = 4,

gradient\_accumulation\_steps = 2,

warmup\_steps = 5,

num\_train\_epochs = 3,

learning\_rate = 0.0002,

fp16 = not is\_bfloat16\_supported(),

bf16 = is\_bfloat16\_supported(),

logging\_steps = 10,

optim = ""adamw\_8bit"",

weight\_decay = 0.01,

lr\_scheduler\_type = ""linear"",

seed = 69,

output\_dir = ""outputs"",

report\_to = ""wandb"",

save\_strategy = ""steps"",

save\_steps = 50,

save\_total\_limit=10

),

)  
\`\`\`

Training Loss:

https://preview.redd.it/s2vn2z44y55e1.png?width=2888&amp;format=png&amp;auto=webp&amp;s=a4e1038e9c27dae96d7e25fcb5db852c794efd97

",Raise_Fickle,1h7u38s,https://reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,https://www.reddit.com/r/MachineLearning/comments/1h7u38s/d_my_finetuning_loss_looks_weird/,2024-12-06 05:22:31,2,0.75,2,0,5,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/I78OJiWF97hbVSTiF0mWFfXxPxZbdE3PavnYvIriLI8.jpg,t3_1h7u38s
MachineLearning,[D] Exploring a New Approach for Decision Trees in Feature Space Using Linear Projections and Boosting,"Hello everyone,

I've been working on a project for some time now and wanted to share a concept I'm exploring. As we know, decision tree-based models typically split the feature space using certain metrics like MSE, entropy, etc.

I started thinking about an alternative approach: instead of splitting individual features, what if we could split the entire space directly? However, this seemed quite difficult, as determining boundaries and regions in the space is challenging.

Then I had an idea—what if I project the data onto a line within the feature space, and then split that line, like how trees are typically built on individual features? In essence, I’m thinking of projecting points onto a line and then using tree-based methods to split them progressively.

Here's a high-level view of the algorithm:

1. Fit a linear regression model to the dataset (normalized values).
2. Project the data onto the line defined by the regression.
3. Apply a decision tree on this projection, effectively splitting one feature (the projection axis).
4. Calculate the residuals and fit another linear model on the residuals, applying boosting in the process.

Since the new linear regressions fitted on the residuals will define separate lines, I assume that through boosting, the model will gradually divide the data in the desired manner over time.

You can read a more detailed description of the algorithm here: [Algorithm PDF](https://github.com/sametcopur/spacetree/blob/main/algo/algo.pdf).

To visualize how the decision boundaries are formed in a 2D dataset:

[SpaceBoostingRegressor](https://i.redd.it/1i10qp6dr85e1.gif)



**Note:** If you want to see a visual example, uploading high-dimensional GIFs can sometimes be an issue. You can check out the example here: [Gif on GitHub](https://github.com/sametcopur/spacetree/blob/main/utils/tree_animation.gif).

Also you can check the code in the repository: [Repository](https://github.com/sametcopur/spacetree/tree/main)

This approach is simple because it assumes linearity, and it works in scenarios where there is a high linear correlation between the target and features while also allowing for some non-linear relationships. You can see an example in the repo,`example.ipynb` file. However, I’m not sure how well it would perform on real-world datasets, as the linear assumption may not always hold.

I want to take this algorithm further, but speed is important for scaling. Techniques like PCA don't seem to help because I need the line to reflect the variance in both the target and feature space, rather than just feature variance. I tried using MLPs and extracting the embeddings from a hidden layer before the output layer, which works better since we're evaluating the target in a larger space, but this approach becomes too slow and isn’t feasible in practice.

I think this project has great potential, and I’m looking for feedback, ideas, or anyone interested in collaborating. Any comments or suggestions are welcome!",zedeleyici3401,1h838y5,https://reddit.com/r/MachineLearning/comments/1h838y5/d_exploring_a_new_approach_for_decision_trees_in/,https://www.reddit.com/r/MachineLearning/comments/1h838y5/d_exploring_a_new_approach_for_decision_trees_in/,2024-12-06 15:00:41,0,0.5,0,0,0,0,0,False,False,True,False,False,Discussion,https://a.thumbs.redditmedia.com/e0DAeFrPHci5HsV6mlnNzXtgQVmldE3CJmgS7Ro-3b0.jpg,t3_1h838y5
MachineLearning,"Guys I need a help regarding my project on TMDB Dataset from Kaggle ""[P]"", ""[Project]""","So we are working on a clge project on TMDB Dataset from kaggle

[https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies](https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies)

We are trying to do regression. But the problem is this data contains mostly 0's I mean 75% of columns have zero values in it.

I need your help for performing this model either it is regression or classification. But we werent taught NLP yet so we will be working on ML (supervised and unsupervised - clustering).

[datatypes](https://preview.redd.it/52uy9rpoo85e1.png?width=569&amp;format=png&amp;auto=webp&amp;s=8ebf1a97ebba1c81c927af1d2d4e2e20139b7deb)

[columns](https://preview.redd.it/ebdm2mzqo85e1.png?width=818&amp;format=png&amp;auto=webp&amp;s=0b9fc7b1aa5baf60edd4f49feed48c28a72756af)

[there are more than 75&amp;#37; records with zeros in revenue, budget, vote columns](https://preview.redd.it/6p44ilxuo85e1.png?width=968&amp;format=png&amp;auto=webp&amp;s=60e913dd8673ae786e3ad161048e851b3df19d82)

So I need your help in performing this project. Or am I missing something I can fill them with Mean but it wont fit with the context of the data. I need your help. You can dm me as well

Thanks",Bloodshot12_,1h82s34,https://reddit.com/r/MachineLearning/comments/1h82s34/guys_i_need_a_help_regarding_my_project_on_tmdb/,https://www.reddit.com/r/MachineLearning/comments/1h82s34/guys_i_need_a_help_regarding_my_project_on_tmdb/,2024-12-06 14:39:19,0,0.5,0,0,0,0,0,False,False,True,False,False,Project,https://a.thumbs.redditmedia.com/fnTizOkAUlYn8zUb1oL5ISKQws5-Rt2iqGm_N6bMSJ4.jpg,t3_1h82s34
MachineLearning,[P] MRNet dataset,can any one help with build a model for this dataset ? i did the EDA but stuck as unbalanced labels.,LahmeriMohamed,1h823hx,https://reddit.com/r/MachineLearning/comments/1h823hx/p_mrnet_dataset/,https://www.reddit.com/r/MachineLearning/comments/1h823hx/p_mrnet_dataset/,2024-12-06 14:06:02,1,0.67,1,0,0,0,0,False,False,True,False,False,Project,self,t3_1h823hx
MachineLearning,[D] Any public LLM inference APIs with input token log-probs?,"Does anyone know of any services that offer input token log-probs from open source LLMs like LLama-8B?

I'm looking for a cost-effective way to host an LLM-based application with low traffic, so I'd like per-query or per-token pricing, rather than per-hour GPU rental. It's unfortunately dependent on direct-access to log-probs on user-provided tokens.

None of the ""chat completion"" APIs I've found seem to expose this. It makes sense for private models, but for open source models, I don't see any downside to exposing it.",severed-identity,1h7o30r,https://reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,https://www.reddit.com/r/MachineLearning/comments/1h7o30r/d_any_public_llm_inference_apis_with_input_token/,2024-12-06 00:11:38,1,0.67,1,0,5,0,0,False,False,True,False,False,Discussion,self,t3_1h7o30r
MachineLearning,[P] Look-a-like modeling,"Hi everybody. I have a list of user actions (around 1m objects) where only a small fraction (less than 1000) are labeled. I want to find most similar objects to them. What is a good way to approach it? 

I personally have 2 ideas in mind: one class classification or unsupervised clustering. My problem with the first is that I know only 1 suitable model (one class svm) and it can be too simple for my data. Problem with second one is obvious - it's unsupervised and labeling will be used only at the final step, so their efficiency is not guaranteed.",Jor_ez,1h7nv2n,https://reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,https://www.reddit.com/r/MachineLearning/comments/1h7nv2n/p_lookalike_modeling/,2024-12-06 00:01:36,0,0.5,0,0,2,0,0,False,False,True,False,False,Project,self,t3_1h7nv2n
MachineLearning,OpenAI CLIP model [D],"How long do you think OpenAI researchers were working on CLIP model before they published the results? 
The paper in my opinion is revolutionary.",NeatJealous8110,1h7wc59,https://reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,https://www.reddit.com/r/MachineLearning/comments/1h7wc59/openai_clip_model_d/,2024-12-06 07:50:28,0,0.33,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1h7wc59
MachineLearning,[D] Advice for a new Machine Learning Tutor: what projects will get my students hired?,"I've just started giving lessons as a machine learning tutor. I have a masters degree in computer science and two years professional experience. But I've never been a tutor (atleast not in this field). Today I was giving my first lesson on ML, just a powerpoint on the basics when my student stopped and told me the powerpoint was too basic for her.

She wanted to talk more about projects that she could do that would attract employers and get an internship. And she asked me point blank what kind of projects she should make. To be honest I wasn't entirely sure, what flashed in my head were things like training a model to recognize the MNIST digits or other simple projects suitable for a (relative) novice. But would those really help her get an internship? I doubted myself so I turned it around on her and asked her what kinds of things related to machine learning she is passionate about and would motivate her to work hard? She responded that she could do anything related to machine learning and she just wants to do what would make her money and get her recognized by a company.

So basically I felt like I failed as a tutor for not having a good answer, and I would like to have an answer prepared if this happens again. What do you all think? What are some projects that a novice, or not so novice students can take on that will make them more hireable for jobs and internships?

And while we're at it, what kinds of things do you think I should be preparing to be a better tutor in general. What kinds of things would you want your machine learning tutor to prepare for you? Would you want slideshow deck lessons on key concepts? Jupyter notebooks with exercises for practice? Something else? I'm not sure what I should be doing to get ready for these lessons honestly.",Seijiteki,1h7bftd,https://reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,https://www.reddit.com/r/MachineLearning/comments/1h7bftd/d_advice_for_a_new_machine_learning_tutor_what/,2024-12-05 15:16:00,0,0.31,0,0,8,0,0,False,False,True,False,False,Discussion,self,t3_1h7bftd
