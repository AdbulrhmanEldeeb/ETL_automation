subreddit,title,selftext,author,id,permalink,url,created_utc,score,upvote_ratio,ups,downs,num_comments,total_awards_received,gilded,is_video,is_original_content,is_self,over_18,spoiler,link_flair_text,thumbnail,name
MachineLearning,[D] What's your favorite paper you've read this year and why? ,"Haven't made this thread in many years, but holiday travel demands are great and would love to have a repository of papers to read during it.",bin_und_zeit,1hfljy3,https://reddit.com/r/MachineLearning/comments/1hfljy3/d_whats_your_favorite_paper_youve_read_this_year/,https://www.reddit.com/r/MachineLearning/comments/1hfljy3/d_whats_your_favorite_paper_youve_read_this_year/,2024-12-16 15:26:39,14,0.94,14,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hfljy3
MachineLearning,[P] I made wut ‚Äì a CLI that explains your last command using a LLM,,jsonathan,1hew6wy,https://reddit.com/r/MachineLearning/comments/1hew6wy/p_i_made_wut_a_cli_that_explains_your_last/,https://i.redd.it/nwo0a660h17e1.gif,2024-12-15 16:29:00,415,0.95,415,0,29,0,0,False,False,False,False,False,Project,https://b.thumbs.redditmedia.com/n6rBdpFgqI8fg41ShW7IZmHAlTJez92XiIXOoxXZklI.jpg,t3_1hew6wy
MachineLearning,[R] Optimizing LLM merging to reduce performance tradeoffs,"We just released our new [work](https://huggingface.co/papers/2412.04144) on large-scale merging across checkpoints trained with different hyperparameters, data mixture, objectives, etc. to optimize tradeoffs. For instance, some models that perform well on code generation do worse on instruction following and vice versa. An interesting question to ask whether model merging can lead to a Pareto-optimal model in such setup.

  
**TL;DR:**

AS you're developing an LLM, it is common to obtain different checkpoints, where each excels at a single or a group of tasks/capabilities. Typically, you can keep tuning your hyperparameters until you obtain a Pareto-Optimal model, but the process can be super expensive. We show that you can collect all these models into a pool, and optimize merging parameters to reduce the task tradeoffs. We show that simple linear merging can yield Pareto-Optimal models in different tradeoff scenarios---better tradeoffs than individual models and strong merging baselines. Interestingly, our optimized merges outperform the original models in some cases. Our study uses large-scale models (Command R+ 104B from Cohere) and explores merging up to 16 checkpoints from SFT/RLHF training. 

https://preview.redd.it/3b2her78d57e1.png?width=2269&amp;format=png&amp;auto=webp&amp;s=3974e5b7bbf2f1b5b602eb6aa3a4092978b286d5

  
Happy to hear your thoughts.

  
üìÑ Paper: [https://huggingface.co/papers/2412.04144](https://huggingface.co/papers/2412.04144)

üßµ Twitter/X: [thread](https://x.com/MKhalifaaaa/status/1866126245014200467?t=2MmryY1FWBz1Ddk_OQ392w&amp;s=19)",moyle,1hfc8s5,https://reddit.com/r/MachineLearning/comments/1hfc8s5/r_optimizing_llm_merging_to_reduce_performance/,https://www.reddit.com/r/MachineLearning/comments/1hfc8s5/r_optimizing_llm_merging_to_reduce_performance/,2024-12-16 05:33:52,20,0.95,20,0,3,0,0,False,False,True,False,False,Research,https://b.thumbs.redditmedia.com/2JsEIjnFlrbmDXzNhq_s1Yhp8kJ_lpHHMm5Jj7UKxDw.jpg,t3_1hfc8s5
MachineLearning,[N] Special session on Privacy-Preserving Machine and Deep Learning at IJCNN 2025 ,"The special session on Privacy-Preserving Machine and Deep Learning will be held at the 2025 International Joint Conference on Neural Networks (IJCNN), which will be hosted from 30 June to 5 July 2025 in the beautiful city of Rome! üçù ¬†üéâ

The special session invites papers presenting innovative uses of privacy-preserving techniques (such as Homomorphic Encryption) for AI applications, as well as algorithms, hardware, and ethic contributions. This includes, but is not limited to, the design of new families of ML and DL models for Privacy-Preserving Machine and Deep Learning (PP-MDL) as well as the introduction of innovative application scenarios for PP-MDL.

We are thrilled to receive your submissions! The deadline is 15 January 2025. üöÄ",DuckMySick12,1hfjav8,https://reddit.com/r/MachineLearning/comments/1hfjav8/n_special_session_on_privacypreserving_machine/,https://www.reddit.com/r/MachineLearning/comments/1hfjav8/n_special_session_on_privacypreserving_machine/,2024-12-16 13:40:13,3,0.8,3,0,0,0,0,False,False,True,False,False,News,self,t3_1hfjav8
MachineLearning,[D] Understanding the Technology Behind Chat GPT‚Äôs Agent Routing ,"Good morning. When talking with ChatGPT, you‚Äôll notice it routes to different agents. This means that if you want to create an image using DALL¬∑E, it automatically identifies your intent and directs you to that agent. I was wondering what technology is behind this. Is it a small transformer model that classifies prompts and sends them to the corresponding agents? Or perhaps a smaller LLM that routes to the agents using a structured output?

I‚Äôm building a router that needs to decide which agent to use, and I‚Äôd love to know how other companies are handling this. Thank you so much.",hardyy_19,1hffplg,https://reddit.com/r/MachineLearning/comments/1hffplg/d_understanding_the_technology_behind_chat_gpts/,https://www.reddit.com/r/MachineLearning/comments/1hffplg/d_understanding_the_technology_behind_chat_gpts/,2024-12-16 09:49:36,5,0.62,5,0,14,0,0,False,False,True,False,False,Discussion,self,t3_1hffplg
MachineLearning,[D] Synthetic tabular data augmentation/generation using GANs,"I was tasked with training a GAN to generate synthetic tabular data in the brain imaging field from a dataset with only ~80-100 entries. Being unfamiliar with this space, I'm wondering if using GANs is the right way to approach this problem given the size of my dataset and whether there are more effective solutions out there? TIA",InfinityZeroFive,1hfgxfe,https://reddit.com/r/MachineLearning/comments/1hfgxfe/d_synthetic_tabular_data_augmentationgeneration/,https://www.reddit.com/r/MachineLearning/comments/1hfgxfe/d_synthetic_tabular_data_augmentationgeneration/,2024-12-16 11:18:42,2,0.59,2,0,3,0,0,False,False,True,False,False,Discussion,self,t3_1hfgxfe
MachineLearning,"[R] Quantifying/ visualizing the activity of the forget, input and output gate in lstms for interpretability. ","

Has there been working quantifying how much inforyis forgotten and retained in lstms or gru models for interpretability reasons. For instance it would be interesting to see if the model uses the hidden vector more for some examples over the other showing more need for past information in some examples. ",Sandy_dude,1hfcbzl,https://reddit.com/r/MachineLearning/comments/1hfcbzl/r_quantifying_visualizing_the_activity_of_the/,https://www.reddit.com/r/MachineLearning/comments/1hfcbzl/r_quantifying_visualizing_the_activity_of_the/,2024-12-16 05:39:34,6,0.81,6,0,0,0,0,False,False,True,False,False,Research,self,t3_1hfcbzl
MachineLearning,[D] Issue with ArXiv,"Hi, I‚Äôm trying to publish a paper on arXiv, but the uploaded PDF contains typos that aren‚Äôt present in my Overleaf version. Has anyone experienced this issue before? Any advice would be appreciated. Thank you!",Shiptonm25,1hfl19n,https://reddit.com/r/MachineLearning/comments/1hfl19n/d_issue_with_arxiv/,https://www.reddit.com/r/MachineLearning/comments/1hfl19n/d_issue_with_arxiv/,2024-12-16 15:02:40,0,0.2,0,0,0,0,0,False,False,True,False,False,Discussion,self,t3_1hfl19n
MachineLearning,[D] Preparing for a Computer Vision Interview: Focus on Classical CV Knowledge,"Hello everyone!

I hope you're all doing well. I have an upcoming interview for a startup for a mid-senior Computer Vision Engineer role in Robotics. The position requires a strong focus on both classical computer vision and 3D point cloud algorithms, in addition to deep learning expertise.

For the classical computer vision and 3D point cloud aspects, I need to review topics like feature extraction and matching, 6D pose estimation, image and point cloud registration, and alignment. Do you have any tips on how to efficiently review these concepts, solve related problems, or practice for this part of the interview? Any specific resources, exercises, or advice would be highly appreciated. Thanks in advance!",DeepBlue-96,1hfgt3m,https://reddit.com/r/MachineLearning/comments/1hfgt3m/d_preparing_for_a_computer_vision_interview_focus/,https://www.reddit.com/r/MachineLearning/comments/1hfgt3m/d_preparing_for_a_computer_vision_interview_focus/,2024-12-16 11:10:12,0,0.5,0,0,1,0,0,False,False,True,False,False,Discussion,self,t3_1hfgt3m
MachineLearning,[D] What is the meaning behind the values for the Scaling Layer in LPIPS Implementation? ,"I have been reading the paper ""The unreasonable effectiveness of deep features as a perceptual metric"" and was looking into the implementation. They have used a Scaling Layer

https://preview.redd.it/ctwnc4gsk47e1.png?width=1560&amp;format=png&amp;auto=webp&amp;s=45ba5e41249d4af1cb05e53c6855531fbccd544f

However, I thought the mean and Standard Deviation for Imagenet normalization was `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]` . Curious as to how those values came by? ",Snoo_65491,1hf9ido,https://reddit.com/r/MachineLearning/comments/1hf9ido/d_what_is_the_meaning_behind_the_values_for_the/,https://www.reddit.com/r/MachineLearning/comments/1hf9ido/d_what_is_the_meaning_behind_the_values_for_the/,2024-12-16 02:55:39,2,0.63,2,0,3,0,0,False,False,True,False,False,Discussion,https://b.thumbs.redditmedia.com/mD6UXEW97TsuUHsdzE4kCkROfHzBMbejnOwEAU1KHrE.jpg,t3_1hf9ido
MachineLearning,[D] What do you do while your model is training?,"I am bascilly baby sitting my model while it is training, watch some *House M.D.* or play some minecraft. I have done all my literture review and paper writting, what should I do now while my model is training?",Striking-Warning9533,1hemhil,https://reddit.com/r/MachineLearning/comments/1hemhil/d_what_do_you_do_while_your_model_is_training/,https://www.reddit.com/r/MachineLearning/comments/1hemhil/d_what_do_you_do_while_your_model_is_training/,2024-12-15 06:11:22,120,0.9,120,0,67,0,0,False,False,True,False,False,Discussion,self,t3_1hemhil
